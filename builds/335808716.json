{
  "@type": "build",
  "@href": "/v3/build/335808716",
  "@representation": "standard",
  "@permissions": {
    "read": true,
    "cancel": false,
    "restart": false
  },
  "id": 335808716,
  "number": "29010",
  "state": "failed",
  "duration": 18154,
  "event_type": "pull_request",
  "previous_state": "failed",
  "pull_request_title": "Fix potential deadlock in QueryContext::setMemoryPool",
  "pull_request_number": 9869,
  "started_at": "2018-01-31T20:28:23Z",
  "finished_at": "2018-01-31T21:09:10Z",
  "private": false,
  "repository": {
    "@type": "repository",
    "@href": "/v3/repo/1504827",
    "@representation": "minimal",
    "id": 1504827,
    "name": "presto",
    "slug": "prestodb/presto"
  },
  "branch": {
    "@type": "branch",
    "@href": "/v3/repo/1504827/branch/master",
    "@representation": "minimal",
    "name": "master"
  },
  "tag": null,
  "commit": {
    "@type": "commit",
    "@representation": "minimal",
    "id": 99818130,
    "sha": "835ab4b8020d43fd44c27fb2feb3be055bf23b6a",
    "ref": "refs/pull/9869/merge",
    "message": "Fix potential deadlock in QueryContext::setMemoryPool\n\nPreviously when setMemoryPool is called while an operator is allocating memory\nit's possible that these two threads deadlock as they acquire the monitors in\ndifferent orders. The thread calling setMemoryPool was acquiring the monitor of\nthe QueryContext instance/this first and then the monitor of the user/revocable\nmemory contexts in the queryMemoryContext. However, the driver threads reserving memory\nwere acquiring the monitors of the user/revocable memory contexts first, and\nthen the monitor of the QueryContext instance.\n\nThis change solve the issue by preventing locking of the user/revocable memory contexts\nin the queryMemoryContext by getting the same information from the MemoryPool.",
    "compare_url": "https://github.com/prestodb/presto/pull/9869",
    "committed_at": "2018-01-31T20:25:14Z"
  },
  "jobs": [
    {
      "@type": "job",
      "@href": "/v3/job/335808717",
      "@representation": "minimal",
      "id": 335808717
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808718",
      "@representation": "minimal",
      "id": 335808718
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808719",
      "@representation": "minimal",
      "id": 335808719
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808720",
      "@representation": "minimal",
      "id": 335808720
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808721",
      "@representation": "minimal",
      "id": 335808721
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808722",
      "@representation": "minimal",
      "id": 335808722
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808723",
      "@representation": "minimal",
      "id": 335808723
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808724",
      "@representation": "minimal",
      "id": 335808724
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808725",
      "@representation": "minimal",
      "id": 335808725
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808726",
      "@representation": "minimal",
      "id": 335808726
    },
    {
      "@type": "job",
      "@href": "/v3/job/335808727",
      "@representation": "minimal",
      "id": 335808727
    }
  ],
  "stages": [

  ],
  "created_by": {
    "@type": "user",
    "@href": "/v3/user/507362",
    "@representation": "minimal",
    "id": 507362,
    "login": "nezihyigitbasi"
  },
  "updated_at": "2019-04-10T22:49:23.371Z"
}