{
  "totalNumberSkippingTests": 0,
  "bugType": "only_fail",
  "totalNumberErroringTests": 12,
  "repo": "apache/storm",
  "metrics": {
    "FailureNames": [
      "java.lang.AssertionError",
      "java.lang.NoClassDefFoundError",
      "org.apache.hadoop.ipc.RemoteException"
    ],
    "StepsDurationsInSeconds": {
      "CheckoutBuggyBuild": 3,
      "ComputeClasspath": 3,
      "ComputeTestDir": 0,
      "ResolveDependency": 26,
      "BuildProject": 1720,
      "ComputeSourceDir": 0,
      "InitRepoToPush": 35,
      "CloneRepository": 32,
      "NopolRepair": 19,
      "CheckoutPatchedBuild": 0,
      "TestProject": 386,
      "GatherTestInformation": 0,
      "PushIncriminatedBuild": 6,
      "CommitPatch": 31
    },
    "NbFailingTests": 14,
    "ReproductionDate": "May 16, 2017 5:13:25 PM",
    "BuggyBuildId": 232796019,
    "PatchedBuilId": 0,
    "BuggyBuildURL": "http://travis-ci.org/apache/storm/builds/232796019",
    "BuggyBuildDate": "May 16, 2017 2:57:40 PM",
    "BugCommit": "8bbc0509ff9909d47ad2f8f5e3d6352b4961167b",
    "PatchChangedFiles": 1,
    "PatchAddedLines": 0,
    "PatchDeletedLines": 0,
    "NbRunningTests": 358,
    "NbLibraries": 171,
    "NbFileApp": 2122,
    "NbFileTests": 260,
    "NbCPU": 4,
    "FreeMemory": 385657936,
    "TotalMemory": 798490624,
    "AngelicValuesByTest": {
      "org/apache/storm/hdfs/spout/TestHdfsSpout": 0,
      "org/apache/storm/hdfs/spout/TestFileLock": 0,
      "org/apache/storm/hdfs/blobstore/BlobStoreTest": 0
    },
    "FreeMemoryByStep": {
      "CheckoutBuggyBuild": 264503128,
      "ComputeClasspath": 132829240,
      "ComputeTestDir": 115783728,
      "ResolveDependency": 96660448,
      "BuildProject": 214917504,
      "ComputeSourceDir": 132829240,
      "InitRepoToPush": 103591912,
      "CloneRepository": 176931920,
      "NopolRepair": 605330224,
      "CheckoutPatchedBuild": 385657936,
      "TestProject": 205652152,
      "GatherTestInformation": 186565144,
      "PushIncriminatedBuild": 134887072,
      "CommitPatch": 385657936
    },
    "BugCommitUrl": "http://github.com/apache/storm/commit/8bbc0509ff9909d47ad2f8f5e3d6352b4961167b"
  },
  "failingModule": "/root/workspace/apache/storm/232796019/external/storm-hdfs",
  "hostname": "repairnator",
  "totalNumberFailingTests": 2,
  "error-types": [
    "java.lang.AssertionError",
    "java.lang.NoClassDefFoundError",
    "org.apache.hadoop.ipc.RemoteException"
  ],
  "totalNumberRunningTests": 358,
  "failing-test-cases": [
    {
      "className": "org.apache.storm.hdfs.blobstore.BlobStoreTest",
      "failingMethods": [],
      "erroringMethods": [
        "org.apache.storm.hdfs.blobstore.BlobStoreTest#testHdfsReplication",
        "org.apache.storm.hdfs.blobstore.BlobStoreTest#testHdfsWithAuth",
        "org.apache.storm.hdfs.blobstore.BlobStoreTest#testMultipleHdfs",
        "org.apache.storm.hdfs.blobstore.BlobStoreTest#testBasicHdfs"
      ],
      "failures": [
        {
          "failureName": "java.lang.NoClassDefFoundError",
          "failureDetail": "org/apache/storm/thrift/TBase",
          "isError": true
        },
        {
          "failureName": "java.lang.NoClassDefFoundError",
          "failureDetail": "org/apache/storm/thrift/TBase",
          "isError": true
        },
        {
          "failureName": "java.lang.NoClassDefFoundError",
          "failureDetail": "org/apache/storm/thrift/TBase",
          "isError": true
        },
        {
          "failureName": "java.lang.NoClassDefFoundError",
          "failureDetail": "org/apache/storm/thrift/TBase",
          "isError": true
        }
      ],
      "nbFailures": 0,
      "nbErrors": 4
    },
    {
      "className": "org.apache.storm.hdfs.spout.TestFileLock",
      "failingMethods": [],
      "erroringMethods": [
        "org.apache.storm.hdfs.spout.TestFileLock#testStaleLockDetection_MultipleLocks",
        "org.apache.storm.hdfs.spout.TestFileLock#testLockRecovery",
        "org.apache.storm.hdfs.spout.TestFileLock#testHeartbeat"
      ],
      "failures": [
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/locskdir/file3 could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/locskdir/file1 could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/locskdir/file1 could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        }
      ],
      "nbFailures": 0,
      "nbErrors": 3
    },
    {
      "className": "org.apache.storm.hdfs.spout.TestHdfsSpout",
      "failingMethods": [
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testSimpleSequenceFile",
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testResumeAbandoned_Seq_NoAck"
      ],
      "erroringMethods": [
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testReadFailures",
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testLocking",
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testResumeAbandoned_Text_NoAck",
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testMultipleFileConsumption_Ack",
        "org.apache.storm.hdfs.spout.TestHdfsSpout#testSimpleText_noACK"
      ],
      "failures": [
        {
          "failureName": "java.lang.AssertionError",
          "failureDetail": "expected:\u003c10\u003e but was:\u003c0\u003e",
          "isError": false
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/junit3571870345478481437/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/junit5653002873441364721/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        },
        {
          "failureName": "java.lang.AssertionError",
          "failureDetail": "expected:\u003c2\u003e but was:\u003c0\u003e",
          "isError": false
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/junit3774061222145494423/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/junit2168872029454153848/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        },
        {
          "failureName": "org.apache.hadoop.ipc.RemoteException",
          "failureDetail": "File /tmp/junit9179995449552715618/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (\u003d1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)\n at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)\n at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)\n",
          "isError": true
        }
      ],
      "nbFailures": 2,
      "nbErrors": 5
    }
  ]
}