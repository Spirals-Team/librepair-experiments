[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-common:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354203634/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-client:jar:0.4.2-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hbase:hbase-client:jar -> version (?) vs 1.2.3 @ com.uber.hoodie:hoodie-client:[unknown-version], /root/workspace/uber/hudi/354203634/hoodie-client/pom.xml, line 193, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354203634/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-cli:jar:0.4.2-SNAPSHOT
[WARNING] 'dependencies.dependency.systemPath' for dnl.utils:textutils:jar should not point at files within the project directory, ${basedir}/lib/dnl/utils/textutils/0.3.3/textutils-0.3.3.jar will be unresolvable by dependent projects @ com.uber.hoodie:hoodie-cli:[unknown-version], /root/workspace/uber/hudi/354203634/hoodie-cli/pom.xml, line 162, column 19
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354203634/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hadoop-mr:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354203634/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hive:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354203634/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-utilities:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie-utilities:[unknown-version], /root/workspace/uber/hudi/354203634/hoodie-utilities/pom.xml, line 35, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie:pom:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ line 154, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Hoodie
[INFO] hoodie-common
[INFO] hoodie-hadoop-mr
[INFO] hoodie-client
[INFO] hoodie-spark
[INFO] hoodie-hive
[INFO] hoodie-utilities
[INFO] hoodie-cli
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hoodie 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-common 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-common ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/354203634/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/354203634/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- avro-maven-plugin:1.7.6:schema (default) @ hoodie-common ---
[INFO] Importing File: /root/workspace/uber/hudi/354203634/hoodie-common/src/main/avro/HoodieCommitMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354203634/hoodie-common/src/main/avro/HoodieSavePointMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354203634/hoodie-common/src/main/avro/HoodieCompactionMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354203634/hoodie-common/src/main/avro/HoodieCleanMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354203634/hoodie-common/src/main/avro/HoodieRollbackMetadata.avsc
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/354203634/hoodie-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-common ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 85 source files to /root/workspace/uber/hudi/354203634/hoodie-common/target/classes
[WARNING] /root/workspace/uber/hudi/354203634/hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogFileReader.java: Some input files use or override a deprecated API.
[WARNING] /root/workspace/uber/hudi/354203634/hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogFileReader.java: Recompile with -Xlint:deprecation for details.
[WARNING] /root/workspace/uber/hudi/354203634/hoodie-common/src/main/java/com/uber/hoodie/common/util/SpillableMapUtils.java: Some input files use unchecked or unsafe operations.
[WARNING] /root/workspace/uber/hudi/354203634/hoodie-common/src/main/java/com/uber/hoodie/common/util/SpillableMapUtils.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-common ---
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom (4 KB at 7.6 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom (3 KB at 125.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar (74 KB at 1636.7 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.common.util.TestFSUtils
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.551 sec - in com.uber.hoodie.common.util.TestFSUtils
Running com.uber.hoodie.common.util.TestNumericUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.common.util.TestNumericUtils
Running com.uber.hoodie.common.util.TestParquetUtils
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.679 sec - in com.uber.hoodie.common.util.TestParquetUtils
Running com.uber.hoodie.common.util.collection.TestExternalSpillableMap
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.395 sec - in com.uber.hoodie.common.util.collection.TestExternalSpillableMap
Running com.uber.hoodie.common.util.collection.TestDiskBasedMap
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.209 sec - in com.uber.hoodie.common.util.collection.TestDiskBasedMap
Running com.uber.hoodie.common.model.TestHoodieWriteStat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in com.uber.hoodie.common.model.TestHoodieWriteStat
Running com.uber.hoodie.common.TestBloomFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec - in com.uber.hoodie.common.TestBloomFilter
Running com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
1700 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7189236013254288639 as hoodie dataset /tmp/junit7189236013254288639
1732 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7189236013254288639
1735 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7189236013254288639/.hoodie/hoodie.properties
1736 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7189236013254288639
1736 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7189236013254288639
1743 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
1745 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
1755 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7189236013254288639/.hoodie/1.inflight
1756 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
1756 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>3__commit]
1761 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7189236013254288639/.hoodie/3.inflight
1761 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>3__commit]
1761 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>5__commit]
1765 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7189236013254288639/.hoodie/5.inflight
1766 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>5__commit]
1766 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>8__commit]
1771 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7189236013254288639/.hoodie/8.inflight
1772 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>8__commit]
1772 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>9__commit]
1776 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7189236013254288639/.hoodie/9.inflight
1779 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [3__commit], [5__commit], [8__commit], [==>9__commit]]
1790 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit779021394987687011 as hoodie dataset /tmp/junit779021394987687011
1820 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit779021394987687011
1821 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit779021394987687011/.hoodie/hoodie.properties
1822 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit779021394987687011
1822 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit779021394987687011
1834 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1238931748803147633 as hoodie dataset /tmp/junit1238931748803147633
1863 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1238931748803147633
1863 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1238931748803147633/.hoodie/hoodie.properties
1864 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1238931748803147633
1864 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1238931748803147633
1864 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.169 sec - in com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
Running com.uber.hoodie.common.table.log.HoodieLogFormatTest
1890 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - Cleaning HDFS cluster data at: /tmp/1521188977706-0/dfs and starting fresh.
1897 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS force binding to ip: 127.0.0.1
Formatting using clusterid: testClusterID
3948 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
6507 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS Minicluster service started.
6547 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper force binding to: 127.0.0.1
6596 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper Minicluster service started on client port: 2828
6647 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
7374 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
7384 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
7451 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
7452 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
7455 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
7455 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5616285718947594121
7468 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5616285718947594121 as 1
7468 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5616285718947594121/.test-fileid1_100.log.1
7472 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5616285718947594121/.test-fileid1_100.log.1} does not exist. Create a new file
7545 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
7552 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
7558 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
7656 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1}
7657 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7660 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1}
7660 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7663 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1}
7664 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1}
7667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
7671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1}
7671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit5616285718947594121/.test-fileid1_100.log.1
7671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
7672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
7672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
7672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
7672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
7672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
7691 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
8117 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
8125 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
8135 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
8135 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
8136 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
8136 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2541400101808558524
8138 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2541400101808558524 as 1
8138 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2541400101808558524/.test-fileid1_100.log.1
8140 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2541400101808558524/.test-fileid1_100.log.1} does not exist. Create a new file
8595 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
8622 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
8628 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
8634 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
8634 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
8634 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
8635 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2538953489888854496
8636 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2538953489888854496 as 1
8637 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2538953489888854496/.test-fileid1_100.log.1
8638 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2538953489888854496/.test-fileid1_100.log.1} does not exist. Create a new file
8671 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
8692 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
8697 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
8700 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
8700 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
8700 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
8700 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8418686187053671416
8702 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8418686187053671416 as 1
8702 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8418686187053671416/.test-fileid1_100.log.1
8703 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8418686187053671416/.test-fileid1_100.log.1} does not exist. Create a new file
8717 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
8717 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8418686187053671416
8719 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8418686187053671416 as 1
8720 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8418686187053671416/.test-fileid1_100.log.1
8721 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8418686187053671416/.test-fileid1_100.log.1} exists. Appending to existing file
8724 [IPC Server handler 2 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit8418686187053671416/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
8725 [IPC Server handler 2 on 45595] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit8418686187053671416/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
8730 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit8418686187053671416/.test-fileid1_100.log.1
8735 [IPC Server handler 3 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8418686187053671416/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1009 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-bee621c0-91ec-4b58-9fbb-63548195eb58:NORMAL:127.0.0.1:50010|RBW]]}
9407 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741832_1008] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741832_1008
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
9407 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37416 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741832_1008]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37416 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:37416]. 59315 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
9740 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit8418686187053671416/.test-fileid1_100.log.1
9826 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
9852 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
9858 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
9862 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
9862 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
9862 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
9863 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2485692874234590418
9864 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2485692874234590418 as 1
9865 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2485692874234590418/.test-fileid1_100.log.1
9866 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2485692874234590418/.test-fileid1_100.log.1} does not exist. Create a new file
10729 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
10730 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2485692874234590418
10732 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2485692874234590418 as 1
10732 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2485692874234590418/.test-fileid1_100.log.1
10734 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2485692874234590418/.test-fileid1_100.log.1} exists. Appending to existing file
10787 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
10792 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
10796 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
10802 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1}
10803 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1
10804 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1} has a corrupted block at 9289
12092 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1} starts at 9853
12093 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1}
12093 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1
12095 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1}
12095 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1
12095 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1
12095 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1
12095 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
12095 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1}
12095 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2485692874234590418/.test-fileid1_100.log.1
12096 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
12122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
12122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
12122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
12122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
12122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71816
12147 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
12571 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
12577 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
12582 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
12582 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
12617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
12617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit5509113244431895218/append_test
12617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit5509113244431895218/append_test as 1
12617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit5509113244431895218/append_test/.commits.archive_.archive.1
12618 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit5509113244431895218/append_test/.commits.archive_.archive.1} does not exist. Create a new file
12629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
12629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit5509113244431895218/append_test
12630 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit5509113244431895218/append_test as 1
12630 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit5509113244431895218/append_test/.commits.archive_.archive.1
12630 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit5509113244431895218/append_test/.commits.archive_.archive.1} exists. Appending to existing file
12630 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
12648 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
13067 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
13072 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
13076 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
13076 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
13076 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
13076 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1658193296840607977
13078 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1658193296840607977 as 1
13078 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1658193296840607977/.test-fileid1_100.log.1
13079 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1658193296840607977/.test-fileid1_100.log.1} does not exist. Create a new file
13520 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
13526 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
13531 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
13537 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1}
13537 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1
13538 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1}
13538 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1
13539 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1}
13539 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1
13539 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1
13539 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1
13539 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
13540 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1}
13540 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit1658193296840607977/.test-fileid1_100.log.1
13540 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
13557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
13557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
13557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
13557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
13557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71803
13580 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
13598 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
13602 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
13605 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
13605 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
13605 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
13605 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3217468754985517570
13607 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3217468754985517570 as 1
13607 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3217468754985517570/.test-fileid1_100.log.1
13608 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3217468754985517570/.test-fileid1_100.log.1} does not exist. Create a new file
14026 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14026 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3217468754985517570
14028 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3217468754985517570 as 1
14028 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3217468754985517570/.test-fileid1_100.log.1
14031 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3217468754985517570/.test-fileid1_100.log.1} exists. Appending to existing file
14070 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14070 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3217468754985517570
14071 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3217468754985517570 as 1
14072 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3217468754985517570/.test-fileid1_100.log.1
14073 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3217468754985517570/.test-fileid1_100.log.1} exists. Appending to existing file
14118 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
14540 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14546 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14551 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14551 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
14551 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14551 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2368545787760848767
14553 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2368545787760848767 as 1
14553 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2368545787760848767/.test-fileid1_100.log.1
14554 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2368545787760848767/.test-fileid1_100.log.1} does not exist. Create a new file
14978 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14978 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2368545787760848767
14981 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2368545787760848767 as 1
14981 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2368545787760848767/.test-fileid1_100.log.1
14983 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2368545787760848767/.test-fileid1_100.log.1} exists. Appending to existing file
15422 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15422 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2368545787760848767
15424 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2368545787760848767 as 1
15424 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2368545787760848767/.test-fileid1_100.log.1
15426 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2368545787760848767/.test-fileid1_100.log.1} exists. Appending to existing file
15877 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16298 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16303 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16307 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16307 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16307 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16307 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2221261650406419230
16308 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2221261650406419230 as 1
16308 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2221261650406419230/.test-fileid1_100.log.1
16309 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} does not exist. Create a new file
17163 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} has a corrupted block at 2163
17191 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} starts at 2195
17619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
17620 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2221261650406419230
17621 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2221261650406419230 as 1
17622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2221261650406419230/.test-fileid1_100.log.1
17623 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} exists. Appending to existing file
17659 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} has a corrupted block at 2163
17692 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} starts at 2195
17693 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} has a corrupted block at 2209
17732 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit2221261650406419230/.test-fileid1_100.log.1} starts at 2246
17740 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18158 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18164 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18169 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18169 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18170 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18170 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7332262115005415279
18172 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7332262115005415279 as 1
18172 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7332262115005415279/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7332262115005415279/.test-fileid1_100.log.1} does not exist. Create a new file
18206 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18211 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18217 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18229 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1}
18229 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1
18230 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1}
18230 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1
18231 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1}
18231 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1
18232 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1
18232 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1
18232 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1}
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit7332262115005415279/.test-fileid1_100.log.1
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
18233 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
18234 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
18240 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18665 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18671 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18676 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18676 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18677 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18677 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8704927868977189589
18678 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8704927868977189589 as 1
18679 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8704927868977189589/.test-fileid1_100.log.1
18680 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8704927868977189589/.test-fileid1_100.log.1} does not exist. Create a new file
18704 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18710 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18715 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18728 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8704927868977189589/.test-fileid1_100.log.1}
18728 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit8704927868977189589/.test-fileid1_100.log.1
18729 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8704927868977189589/.test-fileid1_100.log.1}
18729 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit8704927868977189589/.test-fileid1_100.log.1
18729 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:45595/tmp/junit8704927868977189589/.test-fileid1_100.log.1
18729 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
18730 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
18742 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18742 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
18742 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
18742 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 89
18743 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 33822
18758 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19181 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19186 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19191 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19191 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19191 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19192 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit414639287543317472
19193 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit414639287543317472 as 1
19193 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit414639287543317472/.test-fileid1_100.log.1
19194 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit414639287543317472/.test-fileid1_100.log.1} does not exist. Create a new file
19619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit414639287543317472
19621 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit414639287543317472 as 1
19621 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit414639287543317472/.test-fileid1_100.log.1
19622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit414639287543317472/.test-fileid1_100.log.1} exists. Appending to existing file
20060 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20061 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit414639287543317472
20063 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit414639287543317472 as 1
20063 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit414639287543317472/.test-fileid1_100.log.1
20064 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit414639287543317472/.test-fileid1_100.log.1} exists. Appending to existing file
20119 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
20539 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
20545 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
20549 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
20549 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
20550 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20550 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5271627517222173111
20551 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5271627517222173111 as 1
20552 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5271627517222173111/.test-fileid1_100.log.1
20553 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5271627517222173111/.test-fileid1_100.log.1} does not exist. Create a new file
21843 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21843 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5271627517222173111
21845 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5271627517222173111 as 1
21845 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5271627517222173111/.test-fileid1_100.log.1
21847 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5271627517222173111/.test-fileid1_100.log.1} exists. Appending to existing file
22712 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22713 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5271627517222173111
22714 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5271627517222173111 as 1
22714 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5271627517222173111/.test-fileid1_100.log.1
22716 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5271627517222173111/.test-fileid1_100.log.1} exists. Appending to existing file
22742 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22746 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22749 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22760 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22760 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22761 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22761 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22762 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22762 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22763 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1} has a corrupted block at 10486
22784 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1} starts at 10502
22784 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22785 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22786 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1} has a corrupted block at 10516
22805 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1} starts at 10532
22805 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22805 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22806 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22806 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22806 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1} has a corrupted block at 11139
22823 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1} starts at 11155
22823 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22823 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1}
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit5271627517222173111/.test-fileid1_100.log.1
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
22825 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
22832 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23251 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23256 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23260 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23260 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23261 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23261 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7915564004768753353
23262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7915564004768753353 as 1
23262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7915564004768753353/.test-fileid1_100.log.1
23263 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7915564004768753353/.test-fileid1_100.log.1} does not exist. Create a new file
23682 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7915564004768753353
23684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7915564004768753353 as 1
23685 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7915564004768753353/.test-fileid1_100.log.1
23686 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7915564004768753353/.test-fileid1_100.log.1} exists. Appending to existing file
23714 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4298 has reached threshold 2148. Rolling over to the next version
24122 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7915564004768753353/.test-fileid1_100.log.2} does not exist. Create a new file
24133 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
24551 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24556 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24561 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24561 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
24561 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
24561 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4952743107424805052
24563 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4952743107424805052 as 1
24563 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4952743107424805052/.test-fileid1_100.log.1
24564 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.1} does not exist. Create a new file
24582 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
24989 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.2} does not exist. Create a new file
25005 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
25412 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.3} does not exist. Create a new file
25428 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
25435 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.4} does not exist. Create a new file
25440 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25444 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25447 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25452 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.2}
25452 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4952743107424805052/.test-fileid1_100.log.2
25457 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.1}
25457 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4952743107424805052/.test-fileid1_100.log.1
25461 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4952743107424805052/.test-fileid1_100.log.3}
25461 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4952743107424805052/.test-fileid1_100.log.3
25461 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
25484 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
25484 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
25484 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8360
25484 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 289
25484 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 109797
25514 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
25928 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25933 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25936 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25936 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
25936 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25936 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8553685032427098829
25937 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8553685032427098829 as 1
25937 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8553685032427098829/.test-fileid1_100.log.1
25938 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8553685032427098829/.test-fileid1_100.log.1} does not exist. Create a new file
26362 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26362 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8553685032427098829
26364 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8553685032427098829 as 1
26364 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8553685032427098829/.test-fileid1_100.log.1
26365 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8553685032427098829/.test-fileid1_100.log.1} exists. Appending to existing file
26397 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26397 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8553685032427098829
26399 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8553685032427098829 as 1
26399 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8553685032427098829/.test-fileid1_100.log.1
26401 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8553685032427098829/.test-fileid1_100.log.1} exists. Appending to existing file
26402 [IPC Server handler 3 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit8553685032427098829/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
26403 [IPC Server handler 3 on 45595] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit8553685032427098829/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
26406 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit8553685032427098829/.test-fileid1_100.log.1
26407 [IPC Server handler 1 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8553685032427098829/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1056 for block blk_1073741859_1055{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec:NORMAL:127.0.0.1:50010|RBW]]}
27387 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37540 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741859_1054]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37540 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:37540]. 59013 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27387 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741859_1055] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741859_1055
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27409 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit8553685032427098829/.test-fileid1_100.log.1
27860 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
28278 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
28282 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
28286 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
28286 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
28287 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
28287 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2274331675952984591
28289 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2274331675952984591 as 1
28289 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2274331675952984591/.test-fileid1_100.log.1
28290 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2274331675952984591/.test-fileid1_100.log.1} does not exist. Create a new file
28303 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
28709 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2274331675952984591/.test-fileid1_100.log.2} does not exist. Create a new file
28724 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
29131 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2274331675952984591/.test-fileid1_100.log.3} does not exist. Create a new file
29138 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29143 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29146 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29151 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2274331675952984591/.test-fileid1_100.log.1}
29151 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2274331675952984591/.test-fileid1_100.log.1
29155 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2274331675952984591/.test-fileid1_100.log.2}
29155 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2274331675952984591/.test-fileid1_100.log.2
29156 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
29177 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29177 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
29177 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
29177 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
29177 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71815
29200 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
29617 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29621 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29627 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29627 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
29628 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
29628 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2438149585068410494
29629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2438149585068410494 as 1
29629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2438149585068410494/.test-fileid1_100.log.1
29630 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2438149585068410494/.test-fileid1_100.log.1} does not exist. Create a new file
29654 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29658 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29661 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1}
29671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1}
29672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29673 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1}
29673 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29674 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1}
29674 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29675 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1}
29675 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29675 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29675 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit2438149585068410494/.test-fileid1_100.log.1
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
29676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
29681 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
30097 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
30101 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
30105 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
30105 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
30105 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
30106 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3614578790968478400
30107 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3614578790968478400 as 1
30107 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3614578790968478400/.test-fileid1_100.log.1
30108 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3614578790968478400/.test-fileid1_100.log.1} does not exist. Create a new file
30136 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
30140 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
30144 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
30154 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1}
30154 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1
30155 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1}
30155 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1
30156 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1}
30156 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1
30156 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
30185 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
30185 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 6
30185 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 4512
30185 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 144
30185 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71818
30200 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
30205 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
30208 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
30218 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1}
30218 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1
30219 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1}
30219 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1
30220 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit3614578790968478400/.test-fileid1_100.log.1}
30220 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
30235 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
30235 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
30235 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
30235 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
30235 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71818
30263 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
30682 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
30688 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
30693 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
30693 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
30693 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
30693 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4519224362808720910
30694 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4519224362808720910 as 1
30695 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4519224362808720910/.test-fileid1_100.log.1
30696 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4519224362808720910/.test-fileid1_100.log.1} does not exist. Create a new file
31540 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
31540 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4519224362808720910
31541 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4519224362808720910 as 1
31541 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4519224362808720910/.test-fileid1_100.log.1
31542 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4519224362808720910/.test-fileid1_100.log.1} exists. Appending to existing file
31983 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
32398 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
32402 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
32405 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
32405 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
32405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
32405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2703702408706507587
32406 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2703702408706507587 as 1
32407 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2703702408706507587/.test-fileid1_100.log.1
32407 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2703702408706507587/.test-fileid1_100.log.1} does not exist. Create a new file
32823 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
32823 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2703702408706507587
32825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2703702408706507587 as 1
32825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2703702408706507587/.test-fileid1_100.log.1
32826 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2703702408706507587/.test-fileid1_100.log.1} exists. Appending to existing file
33256 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
33256 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2703702408706507587
33258 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2703702408706507587 as 1
33258 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2703702408706507587/.test-fileid1_100.log.1
33259 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2703702408706507587/.test-fileid1_100.log.1} exists. Appending to existing file
33719 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
34139 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
34143 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
34148 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
34148 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
34148 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
34148 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6841352101050912292
34149 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6841352101050912292 as 1
34150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6841352101050912292/.test-fileid1_100.log.1
34150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6841352101050912292/.test-fileid1_100.log.1} does not exist. Create a new file
34158 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
34573 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
34578 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
34581 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
34581 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
34581 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
34581 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9105193647464290622
34582 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9105193647464290622 as 1
34583 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9105193647464290622/.test-fileid1_100.log.1
34583 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9105193647464290622/.test-fileid1_100.log.1} does not exist. Create a new file
34605 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
34609 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
34611 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
34622 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1}
34622 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34623 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1}
34623 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34623 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1}
34623 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34624 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1}
34624 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34624 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34624 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34624 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34624 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1}
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit9105193647464290622/.test-fileid1_100.log.1
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
34625 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
34630 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
35044 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
35048 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
35052 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
35052 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
35052 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
35052 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5049324480338686567
35053 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5049324480338686567 as 1
35053 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5049324480338686567/.test-fileid1_100.log.1
35054 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5049324480338686567/.test-fileid1_100.log.1} does not exist. Create a new file
35482 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
35897 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
35901 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
35905 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
35905 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
35905 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
35905 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4535155317461227779
35906 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4535155317461227779 as 1
35906 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4535155317461227779/.test-fileid1_100.log.1
35907 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4535155317461227779/.test-fileid1_100.log.1} does not exist. Create a new file
36333 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
36751 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
36755 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
36759 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
36759 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
36759 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
36759 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2935193599389976241
36760 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2935193599389976241 as 1
36760 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2935193599389976241/.test-fileid1_100.log.1
36761 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2935193599389976241/.test-fileid1_100.log.1} does not exist. Create a new file
36777 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
36777 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2935193599389976241
36778 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2935193599389976241 as 1
36779 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2935193599389976241/.test-fileid1_100.log.1
36779 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2935193599389976241/.test-fileid1_100.log.1} exists. Appending to existing file
36780 [IPC Server handler 7 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit2935193599389976241/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
36781 [IPC Server handler 7 on 45595] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit2935193599389976241/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
36783 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit2935193599389976241/.test-fileid1_100.log.1
36784 [IPC Server handler 3 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit2935193599389976241/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1082 for block blk_1073741879_1081{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec:NORMAL:127.0.0.1:50010|RBW]]}
37786 [IPC Server handler 9 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit2935193599389976241/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1083 for block blk_1073741879_1081{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec:NORMAL:127.0.0.1:50010|RBW]]}
38789 [IPC Server handler 0 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit2935193599389976241/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1084 for block blk_1073741879_1081{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec:NORMAL:127.0.0.1:50010|RBW]]}
39385 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37618 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741879_1081]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37618 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:37618]. 57391 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
39385 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741879_1081] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741879_1081
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
39791 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit2935193599389976241/.test-fileid1_100.log.1
40233 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
40649 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
40653 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
40657 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
40657 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
40658 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
40658 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit651671376979983324
40659 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit651671376979983324 as 1
40659 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit651671376979983324/.test-fileid1_100.log.1
40660 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit651671376979983324/.test-fileid1_100.log.1} does not exist. Create a new file
41509 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
41509 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit651671376979983324
41511 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit651671376979983324 as 1
41512 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit651671376979983324/.test-fileid1_100.log.1
41513 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit651671376979983324/.test-fileid1_100.log.1} exists. Appending to existing file
41953 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
41957 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
41960 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
41966 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1}
41966 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1
41967 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1} has a corrupted block at 9289
42535 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1} starts at 9853
42535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1}
42535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1
42536 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1}
42536 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1
42536 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1
42536 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1
42536 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
42537 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1}
42537 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit651671376979983324/.test-fileid1_100.log.1
42537 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
42552 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
42552 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
42552 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
42552 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
42553 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71791
42571 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
42983 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
42988 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
42991 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
42991 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
43018 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
43018 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit2729182452038745878/append_test
43018 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit2729182452038745878/append_test as 1
43018 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit2729182452038745878/append_test/.commits.archive_.archive.1
43018 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit2729182452038745878/append_test/.commits.archive_.archive.1} does not exist. Create a new file
43033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
43033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit2729182452038745878/append_test
43033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit2729182452038745878/append_test as 1
43033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit2729182452038745878/append_test/.commits.archive_.archive.1
43033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit2729182452038745878/append_test/.commits.archive_.archive.1} exists. Appending to existing file
43034 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
43051 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
43467 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
43470 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
43474 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
43474 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
43474 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
43474 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6121318941275989864
43475 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6121318941275989864 as 1
43475 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6121318941275989864/.test-fileid1_100.log.1
43476 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6121318941275989864/.test-fileid1_100.log.1} does not exist. Create a new file
43503 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
43506 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
43509 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
43513 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1}
43513 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1
43514 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1}
43514 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1
43525 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1}
43526 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1
43526 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1
43526 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
43526 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1}
43526 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit6121318941275989864/.test-fileid1_100.log.1
43527 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
43535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
43535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
43535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
43535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
43535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71806
43558 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
43576 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
43579 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
43582 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
43582 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
43582 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
43582 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8587884159983889951
43583 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8587884159983889951 as 1
43583 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8587884159983889951/.test-fileid1_100.log.1
43584 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8587884159983889951/.test-fileid1_100.log.1} does not exist. Create a new file
43999 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
43999 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8587884159983889951
44000 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8587884159983889951 as 1
44001 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8587884159983889951/.test-fileid1_100.log.1
44002 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8587884159983889951/.test-fileid1_100.log.1} exists. Appending to existing file
44434 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
44434 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8587884159983889951
44436 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8587884159983889951 as 1
44436 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8587884159983889951/.test-fileid1_100.log.1
44437 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8587884159983889951/.test-fileid1_100.log.1} exists. Appending to existing file
44875 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
45290 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
45295 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
45297 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
45297 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
45298 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
45298 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit911155549210907407
45299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit911155549210907407 as 1
45299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit911155549210907407/.test-fileid1_100.log.1
45300 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit911155549210907407/.test-fileid1_100.log.1} does not exist. Create a new file
45720 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
45721 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit911155549210907407
45722 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit911155549210907407 as 1
45722 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit911155549210907407/.test-fileid1_100.log.1
45723 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit911155549210907407/.test-fileid1_100.log.1} exists. Appending to existing file
46159 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
46159 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit911155549210907407
46161 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit911155549210907407 as 1
46161 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit911155549210907407/.test-fileid1_100.log.1
46162 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit911155549210907407/.test-fileid1_100.log.1} exists. Appending to existing file
46606 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
47024 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
47028 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
47032 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
47032 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
47032 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
47032 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8545707635865546335
47033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8545707635865546335 as 1
47033 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8545707635865546335/.test-fileid1_100.log.1
47034 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} does not exist. Create a new file
47880 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} has a corrupted block at 2163
47914 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} starts at 2195
48341 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
48341 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8545707635865546335
48343 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8545707635865546335 as 1
48343 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8545707635865546335/.test-fileid1_100.log.1
48344 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} exists. Appending to existing file
48784 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} has a corrupted block at 2163
48815 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} starts at 2195
48816 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} has a corrupted block at 2209
48847 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit8545707635865546335/.test-fileid1_100.log.1} starts at 2246
48853 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
49270 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
49275 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
49279 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
49279 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
49280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
49280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8751810028144954973
49281 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8751810028144954973 as 1
49281 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8751810028144954973/.test-fileid1_100.log.1
49282 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8751810028144954973/.test-fileid1_100.log.1} does not exist. Create a new file
49302 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
49306 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
49314 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
49325 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1}
49325 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1
49326 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1}
49326 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1
49327 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1}
49328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1
49328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1
49328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1
49328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1}
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit8751810028144954973/.test-fileid1_100.log.1
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
49329 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
49334 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
49752 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
49756 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
49759 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
49759 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
49760 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
49760 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8778466027995216414
49761 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8778466027995216414 as 1
49762 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8778466027995216414/.test-fileid1_100.log.1
49763 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8778466027995216414/.test-fileid1_100.log.1} does not exist. Create a new file
49779 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
49783 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
49787 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
49798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8778466027995216414/.test-fileid1_100.log.1}
49798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit8778466027995216414/.test-fileid1_100.log.1
49799 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit8778466027995216414/.test-fileid1_100.log.1}
49799 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit8778466027995216414/.test-fileid1_100.log.1
49799 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:45595/tmp/junit8778466027995216414/.test-fileid1_100.log.1
49799 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
49799 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
49811 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
49811 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
49811 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
49811 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 89
49811 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 33806
49824 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
50242 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
50246 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
50250 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
50250 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
50250 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
50250 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3537162869224567359
50251 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3537162869224567359 as 1
50252 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3537162869224567359/.test-fileid1_100.log.1
50253 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3537162869224567359/.test-fileid1_100.log.1} does not exist. Create a new file
50672 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
50673 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3537162869224567359
50674 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3537162869224567359 as 1
50674 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3537162869224567359/.test-fileid1_100.log.1
50675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3537162869224567359/.test-fileid1_100.log.1} exists. Appending to existing file
51107 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
51107 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3537162869224567359
51109 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3537162869224567359 as 1
51109 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3537162869224567359/.test-fileid1_100.log.1
51110 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3537162869224567359/.test-fileid1_100.log.1} exists. Appending to existing file
51556 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
51972 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
51975 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
51979 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
51980 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
51980 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
51980 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9138657338360468329
51981 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9138657338360468329 as 1
51981 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9138657338360468329/.test-fileid1_100.log.1
51982 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9138657338360468329/.test-fileid1_100.log.1} does not exist. Create a new file
53389 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
53389 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9138657338360468329
53391 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9138657338360468329 as 1
53391 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9138657338360468329/.test-fileid1_100.log.1
53392 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9138657338360468329/.test-fileid1_100.log.1} exists. Appending to existing file
54242 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
54242 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9138657338360468329
54244 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9138657338360468329 as 1
54244 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9138657338360468329/.test-fileid1_100.log.1
54245 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9138657338360468329/.test-fileid1_100.log.1} exists. Appending to existing file
54268 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
54271 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
54274 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
54283 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54283 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54283 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54284 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54284 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54284 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54285 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1} has a corrupted block at 10486
54301 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1} starts at 10502
54302 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54302 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54303 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1} has a corrupted block at 10516
54318 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1} starts at 10532
54318 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54318 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54319 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54319 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54319 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1} has a corrupted block at 11139
54333 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1} starts at 11155
54333 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1}
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit9138657338360468329/.test-fileid1_100.log.1
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
54335 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
54339 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
54752 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
54756 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
54759 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
54759 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
54759 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
54759 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2625718749616576900
54760 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2625718749616576900 as 1
54760 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2625718749616576900/.test-fileid1_100.log.1
54761 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2625718749616576900/.test-fileid1_100.log.1} does not exist. Create a new file
55175 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
55176 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2625718749616576900
55178 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2625718749616576900 as 1
55178 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2625718749616576900/.test-fileid1_100.log.1
55179 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2625718749616576900/.test-fileid1_100.log.1} exists. Appending to existing file
55205 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4298 has reached threshold 2148. Rolling over to the next version
55611 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2625718749616576900/.test-fileid1_100.log.2} does not exist. Create a new file
55622 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
56037 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
56041 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
56044 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
56044 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
56044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
56044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8606175950321519036
56045 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8606175950321519036 as 1
56045 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8606175950321519036/.test-fileid1_100.log.1
56046 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.1} does not exist. Create a new file
56062 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
56468 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.2} does not exist. Create a new file
56481 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
56887 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.3} does not exist. Create a new file
56904 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
57310 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.4} does not exist. Create a new file
57314 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
57317 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
57320 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
57324 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.1}
57325 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit8606175950321519036/.test-fileid1_100.log.1
57328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.3}
57328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit8606175950321519036/.test-fileid1_100.log.3
57332 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit8606175950321519036/.test-fileid1_100.log.2}
57332 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit8606175950321519036/.test-fileid1_100.log.2
57332 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
57358 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
57358 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
57358 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
57358 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 289
57358 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 109796
57388 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
57801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
57805 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
57808 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
57808 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
57808 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
57808 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1191607477425622581
57809 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1191607477425622581 as 1
57810 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1191607477425622581/.test-fileid1_100.log.1
57810 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1191607477425622581/.test-fileid1_100.log.1} does not exist. Create a new file
58241 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
58241 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1191607477425622581
58243 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1191607477425622581 as 1
58244 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1191607477425622581/.test-fileid1_100.log.1
58245 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1191607477425622581/.test-fileid1_100.log.1} exists. Appending to existing file
58277 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
58277 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1191607477425622581
58278 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1191607477425622581 as 1
58278 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1191607477425622581/.test-fileid1_100.log.1
58279 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1191607477425622581/.test-fileid1_100.log.1} exists. Appending to existing file
58279 [IPC Server handler 9 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit1191607477425622581/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
58280 [IPC Server handler 9 on 45595] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit1191607477425622581/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1598071303_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
58282 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit1191607477425622581/.test-fileid1_100.log.1
58282 [IPC Server handler 5 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1191607477425622581/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1131 for block blk_1073741906_1130{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-bee621c0-91ec-4b58-9fbb-63548195eb58:NORMAL:127.0.0.1:50010|RBW]]}
59285 [IPC Server handler 7 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1191607477425622581/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1132 for block blk_1073741906_1130{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-bee621c0-91ec-4b58-9fbb-63548195eb58:NORMAL:127.0.0.1:50010|RBW]]}
60286 [IPC Server handler 1 on 45595] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1191607477425622581/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1133 for block blk_1073741906_1130{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-bee621c0-91ec-4b58-9fbb-63548195eb58:NORMAL:127.0.0.1:50010|RBW]]}
60385 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741906_1130] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741906_1130
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
60385 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37744 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741906_1129]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37744 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:37744]. 57891 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
61288 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit1191607477425622581/.test-fileid1_100.log.1
61737 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
61751 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
61753 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
61756 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
61756 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
61756 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
61756 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4762875096429072962
61757 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4762875096429072962 as 1
61757 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4762875096429072962/.test-fileid1_100.log.1
61758 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4762875096429072962/.test-fileid1_100.log.1} does not exist. Create a new file
61771 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
62176 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4762875096429072962/.test-fileid1_100.log.2} does not exist. Create a new file
62190 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
62595 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4762875096429072962/.test-fileid1_100.log.3} does not exist. Create a new file
62600 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
62609 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
62611 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
62616 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit4762875096429072962/.test-fileid1_100.log.1}
62616 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit4762875096429072962/.test-fileid1_100.log.1
62619 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit4762875096429072962/.test-fileid1_100.log.2}
62619 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit4762875096429072962/.test-fileid1_100.log.2
62620 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
62638 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
62638 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
62638 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
62638 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
62638 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71814
62660 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
63073 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
63076 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
63079 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
63080 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
63080 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
63080 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7158412095776077352
63081 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7158412095776077352 as 1
63081 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7158412095776077352/.test-fileid1_100.log.1
63082 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7158412095776077352/.test-fileid1_100.log.1} does not exist. Create a new file
63101 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
63104 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
63106 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
63115 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1}
63115 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63115 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1}
63115 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63116 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1}
63116 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63117 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1}
63117 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1}
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:45595/tmp/junit7158412095776077352/.test-fileid1_100.log.1
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
63118 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
63123 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
63539 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
63543 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
63547 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
63547 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
63548 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
63548 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9221601121776231921
63549 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9221601121776231921 as 1
63549 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9221601121776231921/.test-fileid1_100.log.1
63550 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9221601121776231921/.test-fileid1_100.log.1} does not exist. Create a new file
63570 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
63574 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
63578 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
63586 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1}
63586 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1
63587 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1}
63587 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1
63597 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1}
63597 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1
63604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
63608 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
63608 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 6
63608 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 4512
63608 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 144
63608 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71786
63621 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
63624 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
63628 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
63638 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1}
63639 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1
63639 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1}
63639 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1
63650 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:45595/tmp/junit9221601121776231921/.test-fileid1_100.log.1}
63650 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
63655 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
63655 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
63655 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
63655 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
63655 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71786
63669 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
64081 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
64084 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
64088 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
64088 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
64088 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
64088 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit252819179741570804
64089 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit252819179741570804 as 1
64089 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit252819179741570804/.test-fileid1_100.log.1
64089 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit252819179741570804/.test-fileid1_100.log.1} does not exist. Create a new file
64527 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
64527 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit252819179741570804
64529 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit252819179741570804 as 1
64529 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit252819179741570804/.test-fileid1_100.log.1
64530 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit252819179741570804/.test-fileid1_100.log.1} exists. Appending to existing file
64981 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
65395 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
65399 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
65403 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
65403 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
65403 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
65403 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5353768783821529861
65404 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5353768783821529861 as 1
65405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5353768783821529861/.test-fileid1_100.log.1
65405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5353768783821529861/.test-fileid1_100.log.1} does not exist. Create a new file
65825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
65825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5353768783821529861
65827 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5353768783821529861 as 1
65827 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5353768783821529861/.test-fileid1_100.log.1
65828 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5353768783821529861/.test-fileid1_100.log.1} exists. Appending to existing file
66259 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
66259 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5353768783821529861
66260 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5353768783821529861 as 1
66261 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5353768783821529861/.test-fileid1_100.log.1
66262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5353768783821529861/.test-fileid1_100.log.1} exists. Appending to existing file
66710 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
67123 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
67127 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
67130 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
67130 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
67130 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
67130 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6299889654082253504
67132 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6299889654082253504 as 1
67132 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6299889654082253504/.test-fileid1_100.log.1
67133 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6299889654082253504/.test-fileid1_100.log.1} does not exist. Create a new file
67137 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
67137 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741864_1062] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741864_1062
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67139 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741892_1107] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741892_1107
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67140 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741894_1109] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741894_1109
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67138 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741898_1120] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741898_1120
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67137 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741911_1139] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741911_1139
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67137 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741847_1034] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741847_1034
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67137 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741913_1141] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741913_1141
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67137 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741873_1075] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741873_1075
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67140 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741866_1064] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741866_1064
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67140 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741826_1002] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741826_1002
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67139 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741851_1045] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741851_1045
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67139 [ResponseProcessor for block BP-648920633-172.17.0.3-1521188978360:blk_1073741845_1032] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-648920633-172.17.0.3-1521188978360:blk_1073741845_1032
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
67280 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37766 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741911_1139]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37766 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55825 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67282 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37714 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741898_1119]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37714 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 46992 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67284 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37566 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741866_1064]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37566 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 52928 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67284 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37558 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741864_1062]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37558 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 52378 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67283 [DataNode: [[[DISK]file:/tmp/1521188977706-0/dfs/data/data1/, [DISK]file:/tmp/1521188977706-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45595] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-648920633-172.17.0.3-1521188978360 (Datanode Uuid 38e3909e-7051-4b52-990c-c21c6e8ebf44) service to localhost/127.0.0.1:45595 interrupted
67283 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37508 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741851_1044]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37508 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 45465 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67283 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37600 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741873_1075]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37600 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 57330 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67283 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37480 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741847_1034]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37480 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 41428 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67283 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37686 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741894_1109]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37686 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 42504 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67283 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37398 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741826_1002]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37398 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 30266 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67283 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37472 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741845_1032]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37472 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 40931 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67288 [DataNode: [[[DISK]file:/tmp/1521188977706-0/dfs/data/data1/, [DISK]file:/tmp/1521188977706-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45595] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-648920633-172.17.0.3-1521188978360 (Datanode Uuid 38e3909e-7051-4b52-990c-c21c6e8ebf44) service to localhost/127.0.0.1:45595
67285 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37774 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741913_1141]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37774 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 56346 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67284 [DataXceiver for client DFSClient_NONMAPREDUCE_-1598071303_1 at /127.0.0.1:37678 [Receiving block BP-648920633-172.17.0.3-1521188978360:blk_1073741892_1107]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:37678 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 42027 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
67314 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@3e32f0b2] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 46, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 65.719 sec - in com.uber.hoodie.common.table.log.HoodieLogFormatTest
Running com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
67590 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit4544229480605682086 as hoodie dataset /tmp/junit4544229480605682086
67627 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4544229480605682086
67627 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4544229480605682086/.hoodie/hoodie.properties
67627 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4544229480605682086
67628 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4544229480605682086
67632 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67639 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4544229480605682086
67639 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4544229480605682086/.hoodie/hoodie.properties
67640 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4544229480605682086
67640 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit4544229480605682086
67640 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
67661 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8877894787135827408 as hoodie dataset /tmp/junit8877894787135827408
67688 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8877894787135827408
67689 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8877894787135827408/.hoodie/hoodie.properties
67690 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8877894787135827408
67690 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8877894787135827408
67690 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67695 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8877894787135827408
67696 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8877894787135827408/.hoodie/hoodie.properties
67696 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8877894787135827408
67697 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit8877894787135827408
67697 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67698 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
67710 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit8877894787135827408/.hoodie/1.inflight
67711 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
67711 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8877894787135827408
67711 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8877894787135827408/.hoodie/hoodie.properties
67711 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8877894787135827408
67711 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit8877894787135827408
67712 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
67714 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8877894787135827408
67714 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8877894787135827408/.hoodie/hoodie.properties
67715 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8877894787135827408
67715 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit8877894787135827408
67715 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
67717 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>2__commit]
67728 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit8877894787135827408/.hoodie/2.inflight
67728 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>2__commit]
67728 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8877894787135827408
67729 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8877894787135827408/.hoodie/hoodie.properties
67729 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8877894787135827408
67729 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit8877894787135827408
67730 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit]]
67732 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit926578820128592482 as hoodie dataset /tmp/junit926578820128592482
67765 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit926578820128592482
67765 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit926578820128592482/.hoodie/hoodie.properties
67765 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit926578820128592482
67765 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit926578820128592482
67766 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67770 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit926578820128592482
67770 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit926578820128592482/.hoodie/hoodie.properties
67770 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit926578820128592482
67770 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit926578820128592482
67771 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
67784 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit9063481574101938057 as hoodie dataset /tmp/junit9063481574101938057
67814 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit9063481574101938057
67815 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit9063481574101938057/.hoodie/hoodie.properties
67815 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit9063481574101938057
67815 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit9063481574101938057
67816 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67819 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit9063481574101938057
67819 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit9063481574101938057/.hoodie/hoodie.properties
67820 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit9063481574101938057
67820 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit9063481574101938057
67820 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
67826 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7366581461463142362 as hoodie dataset /tmp/junit7366581461463142362
67852 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7366581461463142362
67852 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7366581461463142362/.hoodie/hoodie.properties
67853 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7366581461463142362
67853 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7366581461463142362
67853 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67856 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7366581461463142362
67856 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7366581461463142362/.hoodie/hoodie.properties
67857 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7366581461463142362
67857 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit7366581461463142362
67857 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
67861 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit5916644128857986234 as hoodie dataset /tmp/junit5916644128857986234
67888 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit5916644128857986234
67888 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit5916644128857986234/.hoodie/hoodie.properties
67888 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5916644128857986234
67888 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit5916644128857986234
67889 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
67892 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit5916644128857986234
67892 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit5916644128857986234/.hoodie/hoodie.properties
67892 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5916644128857986234
67892 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit5916644128857986234
67893 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.299 sec - in com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
Running com.uber.hoodie.common.table.HoodieTableMetaClientTest
67898 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit317693533504054903 as hoodie dataset /tmp/junit317693533504054903
67925 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit317693533504054903
67926 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit317693533504054903/.hoodie/hoodie.properties
67926 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit317693533504054903
67926 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit317693533504054903
68080 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
68080 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>1__commit]
68089 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit317693533504054903/.hoodie/1.inflight
68089 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
68098 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
68098 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
68102 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit4774192837124677817 as hoodie dataset /tmp/junit4774192837124677817
68128 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4774192837124677817
68128 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4774192837124677817/.hoodie/hoodie.properties
68129 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4774192837124677817
68129 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4774192837124677817
68129 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
68129 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>1__commit]
68138 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit4774192837124677817/.hoodie/1.inflight
68138 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
68147 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
68147 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
68149 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit4301587865824347615 as hoodie dataset /tmp/junit4301587865824347615
68177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4301587865824347615
68177 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4301587865824347615/.hoodie/hoodie.properties
68177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4301587865824347615
68177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4301587865824347615
68415 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6911168153698409676 as hoodie dataset /tmp/junit6911168153698409676
68440 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6911168153698409676
68440 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6911168153698409676/.hoodie/hoodie.properties
68440 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6911168153698409676
68440 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6911168153698409676
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.541 sec - in com.uber.hoodie.common.table.HoodieTableMetaClientTest
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 44,000
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 22,449B for [_hoodie_record_key] BINARY: 1,000 values, 40,007B raw, 22,347B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:29:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 20 ms. row count = 1000
68455 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16450
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-bee621c0-91ec-4b58-9fbb-63548195eb58,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68459 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16420
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68459 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16453
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68460 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16391
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68461 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16423
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68461 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16524
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-bee621c0-91ec-4b58-9fbb-63548195eb58,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68462 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16429
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68465 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16462
java.io.EOFException: End of File Exception between local host is: "spirals-vortex.lille.inria.fr/172.17.0.3"; destination host is: "localhost":45595; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy30.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1080)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:975)
68466 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16494
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-bee621c0-91ec-4b58-9fbb-63548195eb58,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68466 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16527
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-0db4dcdd-1606-44e2-8f6b-aa1943c139ec,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68467 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16465
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-bee621c0-91ec-4b58-9fbb-63548195eb58,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68467 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16497
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-bee621c0-91ec-4b58-9fbb-63548195eb58,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68468 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16503
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-bee621c0-91ec-4b58-9fbb-63548195eb58,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
68470 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16536
java.net.ConnectException: Call From spirals-vortex.lille.inria.fr/172.17.0.3 to localhost:45595 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy30.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 19 more

Results :

Tests run: 82, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:report (post-unit-test) @ hoodie-common ---
[INFO] Loading execution data file /root/workspace/uber/hudi/354203634/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] Analyzed bundle 'hoodie-common' with 116 classes
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-hadoop-mr 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/354203634/hoodie-hadoop-mr/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-hadoop-mr ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
175  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
343  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2477980870391752847 as hoodie dataset /tmp/junit2477980870391752847
344  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
367  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2477980870391752847
368  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
369  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2477980870391752847/.hoodie/hoodie.properties
374  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit2477980870391752847
374  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit2477980870391752847
376  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1058599308712570204 as hoodie dataset /tmp/junit1058599308712570204
400  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
407  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1058599308712570204
407  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
408  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1058599308712570204/.hoodie/hoodie.properties
408  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1058599308712570204
409  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1058599308712570204
1271 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
1271 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit1058599308712570204/2016/05/01
1283 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit1058599308712570204/2016/05/01 as 1
1284 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1058599308712570204/2016/05/01/.fileid0_100.log.1
1285 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1058599308712570204/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
1945 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favoriteIntNumber,favoriteNumber,favoriteFloatNumber,favoriteDoubleNumber,tags,testNestedRecord,stringArray
1955 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit1058599308712570204/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit1058599308712570204/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favoriteIntNumber, favoriteNumber, favoriteFloatNumber, favoriteDoubleNumber, tags, testNestedRecord, stringArray]
1957 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
1957 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1058599308712570204
1958 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
1958 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1058599308712570204/.hoodie/hoodie.properties
1959 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1058599308712570204
1963 [main] INFO  com.uber.hoodie.common.util.collection.DiskBasedMap  - Spilling to file location /tmp/d8ac290d-2002-405b-8792-b7ea089e6a32
1968 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit1058599308712570204/2016/05/01/.fileid0_100.log.1}
1971 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit1058599308712570204/2016/05/01/.fileid0_100.log.1
1971 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
2032 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 1640
2059 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
2059 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 1
2059 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 1640
2059 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 49
2059 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 21060
2165 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2166 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit904599568896051317 as hoodie dataset /tmp/junit904599568896051317
2167 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2174 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit904599568896051317
2175 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2175 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit904599568896051317/.hoodie/hoodie.properties
2176 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit904599568896051317
2176 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit904599568896051317
2177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7307107423682298067 as hoodie dataset /tmp/junit7307107423682298067
2200 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2207 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7307107423682298067
2208 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2208 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7307107423682298067/.hoodie/hoodie.properties
2208 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7307107423682298067
2209 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7307107423682298067
2358 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
2358 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit7307107423682298067/2016/05/01
2359 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit7307107423682298067/2016/05/01 as 1
2359 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7307107423682298067/2016/05/01/.fileid0_100.log.1
2363 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7307107423682298067/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
2519 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favorite_number,favorite_color,favorite_movie
2522 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit7307107423682298067/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit7307107423682298067/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favorite_number, favorite_color, favorite_movie]
2522 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2522 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7307107423682298067
2523 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2523 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7307107423682298067/.hoodie/hoodie.properties
2523 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7307107423682298067
2524 [main] INFO  com.uber.hoodie.common.util.collection.DiskBasedMap  - Spilling to file location /tmp/3f3ca4b3-d427-4bf6-96c2-6643683c1fe9
2525 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit7307107423682298067/2016/05/01/.fileid0_100.log.1}
2525 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit7307107423682298067/2016/05/01/.fileid0_100.log.1
2525 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
2533 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 728
2548 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
2548 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 1
2548 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 728
2548 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 99
2548 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 29036
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.11 sec - in com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
Running com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
2576 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2538289295222092089 as hoodie dataset /tmp/junit2538289295222092089
2591 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2597 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2538289295222092089
2598 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2598 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2538289295222092089/.hoodie/hoodie.properties
2598 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2538289295222092089
2598 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2538289295222092089
2632 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2538289295222092089
2632 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2632 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2538289295222092089/.hoodie/hoodie.properties
2633 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2538289295222092089
2650 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[001__commit], [002__commit], [==>003__commit]]
2671 [main] INFO  com.uber.hoodie.hadoop.HoodieROTablePathFilter  - Based on hoodie metadata from base path: file:/tmp/junit2538289295222092089, caching 3 files under file:/tmp/junit2538289295222092089/2017/01/01
2794 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/
2794 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@6ffa56fa]
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.218 sec - in com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
Running com.uber.hoodie.hadoop.AnnotationTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in com.uber.hoodie.hadoop.AnnotationTest
Running com.uber.hoodie.hadoop.HoodieInputFormatTest
2824 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6383966122497031883 as hoodie dataset /tmp/junit6383966122497031883
2841 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2847 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6383966122497031883
2847 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2848 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6383966122497031883/.hoodie/hoodie.properties
2848 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6383966122497031883
2848 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6383966122497031883
2944 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit6383966122497031883
2944 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6383966122497031883
2944 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2945 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6383966122497031883/.hoodie/hoodie.properties
2945 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6383966122497031883
2945 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
2951 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
2952 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
2956 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
2957 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
2957 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
2961 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid0_1_200.parquet
2961 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid0_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid2_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid2_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid1_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid1_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid4_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid4_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid3_1_200.parquet
2962 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid3_1_200.parquet
2963 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
3048 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit6383966122497031883
3048 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6383966122497031883
3048 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3048 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6383966122497031883/.hoodie/hoodie.properties
3049 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6383966122497031883
3049 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3050 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
3051 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
3053 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
3054 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 3
3054 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
3055 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid0_1_400.parquet
3055 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid0_1_400.parquet
3055 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid2_1_400.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid2_1_400.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid1_1_400.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid1_1_400.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid4_1_200.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid4_1_200.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid3_1_300.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid3_1_300.parquet
3056 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
3131 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit6383966122497031883
3131 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6383966122497031883
3131 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3132 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6383966122497031883/.hoodie/hoodie.properties
3132 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6383966122497031883
3132 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3133 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
3134 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
3137 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
3137 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 2147483647
3137 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
3138 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid0_1_600.parquet
3138 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid0_1_600.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid2_1_400.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid2_1_400.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid1_1_500.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid1_1_500.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid4_1_200.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid4_1_200.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit6383966122497031883/2016/05/01/fileid3_1_300.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6383966122497031883/2016/05/01/fileid3_1_300.parquet
3139 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
3159 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3280423549881591942 as hoodie dataset /tmp/junit3280423549881591942
3174 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3180 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3280423549881591942
3181 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3181 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3280423549881591942/.hoodie/hoodie.properties
3181 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3280423549881591942
3181 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3280423549881591942
3213 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3280423549881591942
3214 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3280423549881591942
3214 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3214 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3280423549881591942/.hoodie/hoodie.properties
3215 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3280423549881591942
3215 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3216 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3216 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3218 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3218 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid0_1_100.parquet
3218 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid2_1_100.parquet
3218 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid1_1_100.parquet
3218 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid4_1_100.parquet
3219 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid3_1_100.parquet
3219 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid6_1_100.parquet
3219 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid5_1_100.parquet
3219 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid8_1_100.parquet
3219 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid7_1_100.parquet
3220 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid9_1_100.parquet
3253 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3280423549881591942
3253 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3280423549881591942
3253 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3254 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3280423549881591942/.hoodie/hoodie.properties
3254 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3280423549881591942
3254 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3255 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3256 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3258 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3258 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid0_1_100.parquet
3258 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid2_1_100.parquet
3258 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid1_1_100.parquet
3259 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid4_1_100.parquet
3259 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid3_1_100.parquet
3259 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid6_1_100.parquet
3259 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid5_1_100.parquet
3259 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid8_1_100.parquet
3260 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid7_1_100.parquet
3260 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3280423549881591942/2016/05/01/fileid9_1_100.parquet
3281 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7889437665452328064 as hoodie dataset /tmp/junit7889437665452328064
3297 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3305 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7889437665452328064
3305 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3305 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7889437665452328064/.hoodie/hoodie.properties
3305 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7889437665452328064
3306 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7889437665452328064
3361 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit7889437665452328064
3361 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit7889437665452328064
3362 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3362 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit7889437665452328064/.hoodie/hoodie.properties
3362 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit7889437665452328064
3362 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3363 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
3363 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3365 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
3365 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
3365 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
3365 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 0
3385 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit652678531754781328 as hoodie dataset /tmp/junit652678531754781328
3399 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3407 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit652678531754781328
3407 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3407 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit652678531754781328/.hoodie/hoodie.properties
3407 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit652678531754781328
3407 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit652678531754781328
3448 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit652678531754781328
3448 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit652678531754781328
3448 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3449 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit652678531754781328/.hoodie/hoodie.properties
3449 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit652678531754781328
3449 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3450 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3450 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3453 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3453 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid0_1_100.parquet
3453 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid2_1_100.parquet
3453 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid1_1_100.parquet
3453 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid4_1_100.parquet
3454 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid3_1_100.parquet
3454 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid6_1_100.parquet
3454 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid5_1_100.parquet
3454 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid8_1_100.parquet
3454 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid7_1_100.parquet
3455 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid9_1_100.parquet
3504 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit652678531754781328
3504 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit652678531754781328
3504 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3504 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit652678531754781328/.hoodie/hoodie.properties
3505 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit652678531754781328
3505 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3505 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3506 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3507 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3507 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid0_1_100.parquet
3507 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid2_1_100.parquet
3507 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid1_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid4_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid3_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid6_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid5_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid8_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid7_1_100.parquet
3508 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid9_1_100.parquet
3561 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit652678531754781328
3561 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit652678531754781328
3561 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3562 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit652678531754781328/.hoodie/hoodie.properties
3562 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit652678531754781328
3562 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3563 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3564 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit]]
3565 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3565 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid0_1_100.parquet
3565 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid2_1_200.parquet
3565 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid1_1_200.parquet
3565 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid4_1_200.parquet
3566 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid3_1_100.parquet
3566 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid6_1_200.parquet
3566 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid5_1_100.parquet
3566 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid8_1_100.parquet
3566 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid7_1_200.parquet
3566 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit652678531754781328/2016/05/01/fileid9_1_100.parquet
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.767 sec - in com.uber.hoodie.hadoop.HoodieInputFormatTest
Mar 16, 2018 9:30:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 88,387
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteIntNumber] INT32: 100 values, 407B raw, 407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteNumber] INT64: 100 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteFloatNumber] FLOAT: 100 values, 407B raw, 407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteDoubleNumber] DOUBLE: 100 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 108B for [tags, map, key] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 2 entries, 24B raw, 2B comp}
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,156B for [tags, map, value, item1] BINARY: 200 values, 2,117B raw, 2,117B comp, 1 pages, encodings: [RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,963B for [tags, map, value, item2] BINARY: 200 values, 2,917B raw, 2,917B comp, 1 pages, encodings: [RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 48B for [testNestedRecord, isAdmin] BOOLEAN: 100 values, 20B raw, 20B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,647B for [testNestedRecord, userId] BINARY: 100 values, 1,597B raw, 1,597B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [stringArray, array] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 2 entries, 40B raw, 2B comp}
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8,589
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_number] INT64: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_color] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_movie] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 9:30:48 AM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 16, 2018 9:30:48 AM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 16, 2018 9:30:48 AM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:30:48 AM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 20 ms. row count = 100
Mar 16, 2018 9:30:49 AM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 16, 2018 9:30:49 AM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 16, 2018 9:30:49 AM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:30:49 AM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 100

Results :

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-client 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-client ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/354203634/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/354203634/hoodie-client/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-client ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.302 sec - in com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Running com.uber.hoodie.io.TestHoodieCompactor
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2706 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 100
4431 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=29, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=38, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=33, numUpdates=0}}}
4521 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
4522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
4522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 29, totalInsertBuckets => 1, recordsPerBucket => 500000
4523 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
4523 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
4524 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 38, totalInsertBuckets => 1, recordsPerBucket => 500000
4524 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
4524 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
4524 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 33, totalInsertBuckets => 1, recordsPerBucket => 500000
4524 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
4524 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
4599 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 100
4600 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 100
4997 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
4998 [pool-31-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
5086 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
5086 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
5548 [pool-31-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
5606 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
5606 [pool-32-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
5671 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
5671 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
5719 [pool-32-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
5768 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
5768 [pool-33-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
5816 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
5816 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,553
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 137B for [_hoodie_commit_seqno] BINARY: 29 values, 345B raw, 96B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 763B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 664B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,254
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_hoodie_commit_seqno] BINARY: 38 values, 462B raw, 112B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 965B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 866B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 956B for [_row_key] BINARY: 38 values, 1,520B raw, 857B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,315
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 150B for [_hoodie_commit_seqno] BINARY: 33 values, 403B raw, 106B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 850B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 751B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 843B for [_row_key] BINARY: 33 values, 1,320B raw, 744B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, e5840 [pool-33-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
5911 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
5911 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
5995 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
5995 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
6444 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
6486 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 100 as complete
6486 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 100
6665 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 101
7259 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
7259 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
7748 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit4798441439467164817/2015/03/17/7484eb33-cc82-4b9c-984d-adac395c9844_2_100.parquet
7842 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 33 row keys from /tmp/junit4798441439467164817/2015/03/17/7484eb33-cc82-4b9c-984d-adac395c9844_2_100.parquet
7842 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit4798441439467164817/2015/03/17/7484eb33-cc82-4b9c-984d-adac395c9844_2_100.parquet => [0aa50a22-5625-4e68-ad8d-c58f6ad68eff, 11af8171-43e4-4b3a-9646-7bcbe3eaec69, 1f1fedc0-59a1-4f5e-8e57-bfab877d402c, 1f961874-33be-452c-bd8f-44d4e3299d02, 2bba52e4-332a-4413-a67b-3b73fa77e17c, 2c9e0ad8-810f-4f10-ace3-b016029173bf, 37256b3a-1094-42b0-83d9-4c1dc404db22, 4b3f4306-7e0f-4d79-9111-f6ded3a79a0d, 564da150-9e86-4d76-9811-0faebb2a86ea, 5c22b324-a47f-4a08-9797-815a073a0818, 6466f5ac-e42f-4181-bc2b-1a7ac3c4bb2c, 6b3a8de5-f9ec-42a4-ba76-650bb7a48cf4, 7667d941-743c-4cec-88f1-52d83688db2a, 85a2ebdd-4ee8-4ee0-89e8-05f1c4d84835, 85fa524a-e898-4077-a51a-a75d24e8048c, 87739b1d-9635-4e48-a257-8cae2f86a24e, 8ad1b054-4f8f-45cc-b508-62d68a5fe8b9, 8f8f60af-44fe-45dc-9ef9-6bc6dc83782d, a0dc68f6-99e9-4e28-971e-abe71925d946, a5cb25b5-4779-4486-a7d4-57d4b830a94d, a610013b-cd58-41c0-b30a-40394ab8b57a, b5798c17-cfd9-49bd-be70-788bbbbebc57, b69f5931-55d3-4d8b-94b2-e22a76d07abd, bd171508-5c11-48db-a03f-06c10630632a, c0c133d4-2ac8-43c1-8d75-392f64215178, c486c7bb-0828-4b97-8321-31a01c30f7ca, c7b85772-bf3e-44e6-9ddf-687904502bd2, cd304e5e-a267-4e69-910c-c2e90b96ef07, d57babb4-bbbb-4f6f-80cc-5d14a66ee3e2, dc53607b-853e-4b4b-9e8e-37f7b0e2a722, e277ae3d-9c0e-4cbc-9fe4-d614eed55c91, fb945763-f172-4a45-a2bf-8f57696de6ad, fc423622-7294-465d-a831-e536e3d6d555]
7872 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit4798441439467164817/2016/03/15/bd463541-2c32-4ebe-bcf5-a1ceff884246_0_100.parquet
7889 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 29 row keys from /tmp/junit4798441439467164817/2016/03/15/bd463541-2c32-4ebe-bcf5-a1ceff884246_0_100.parquet
7889 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit4798441439467164817/2016/03/15/bd463541-2c32-4ebe-bcf5-a1ceff884246_0_100.parquet => [22ad1a5c-bd21-483f-95bd-f70cdeee4407, 24bff1f1-ee9a-4add-8c3f-f3ed5b48c43c, 270117f3-4384-4737-9ae1-b36b8f19225f, 28e1c3be-674a-4123-8c42-8da5ba147c46, 2de581cf-6a46-4bc2-888c-c230e36fb49b, 38f4facd-df35-4543-8e8e-27a6ceb7c95b, 3ce325c7-cfc0-4506-96ac-a3b15a5b8f59, 44790e75-0bcd-42b9-92af-d313cee6c9e7, 4b50a5d5-697c-4d11-8e20-dc62cff326d0, 528b8387-ab12-487b-9938-067b7512b73f, 53dcb3ff-9ccd-48fe-8aae-c72a3b176274, 5a22700b-1a43-4064-af5e-b7e0b0bae01d, 63190b78-39e5-4795-a28a-1a79ad6e3281, 665556b4-4a2c-41fc-8084-dd86f46ddcad, 7ab1c7c6-1ede-4694-8fec-0143c85310d1, 8418b5de-ed78-44e5-baee-67b109fc5240, 89af65b7-7528-4c8e-ad32-ad43c91f4844, 8f44274a-ef9c-4c5b-b8b6-d1add47a296e, 90ca2068-592c-442f-9d49-4f65c7427842, 94b12ba2-e3ad-4c92-912b-de5f4a42b5b9, aceac109-fce2-4afd-8b52-906bbb9a53b5, b6aa81b7-5176-4350-8652-4991d9da4011, bc4efa90-e7bc-431e-ace3-f0678b1f2123, c71fe5d8-0f13-42c2-a483-bca19ce96a9b, dc636f35-b312-4ae7-83fb-7875aaf70a8b, ec61e029-c45d-4dda-aea3-86d090c1ef0d, eefa6abc-4a10-484b-b503-429b49127313, f994db78-02e9-4841-a49a-5fe7d7091dfb, fef7ddb5-c045-4d70-9ac7-8ba6045196b5]
7906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit4798441439467164817/2015/03/16/e00768dc-cc9a-4d33-b94f-7fb896c60ba7_1_100.parquet
7926 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 38 row keys from /tmp/junit4798441439467164817/2015/03/16/e00768dc-cc9a-4d33-b94f-7fb896c60ba7_1_100.parquet
7926 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit4798441439467164817/2015/03/16/e00768dc-cc9a-4d33-b94f-7fb896c60ba7_1_100.parquet => [05fc1836-53e0-4d2e-af37-0b5b842cd318, 0ec07f0a-0755-44eb-9730-bdd34e179040, 1005df17-f448-44a6-8702-a295ea473d00, 14def09e-6db1-4d28-9c8e-11ffa0e66c8b, 17e53452-ae48-4694-9d2a-116174793217, 1b184e1b-93d5-423d-a07a-3b63cf10233d, 1c40d132-fb28-4957-89c2-95d53fde5c01, 1fc50eb7-f18e-4e93-8b09-f364d304440d, 2a4d06d1-c901-454d-b26b-eef441065566, 2e8ea2d0-0c0a-4c98-8875-a47996872a58, 3f9e8f84-4071-49f5-834c-9c14abc31826, 47e87428-c840-4c15-b96c-03393981e02a, 495fb7ff-e91e-4400-b3bd-12717c10dac5, 49d9f366-e6ea-484f-a455-6cb5d69f7d3b, 4a80a9bd-42e1-4ca9-90d3-dbd69166c40d, 540c6919-62ee-4b49-8a15-3a83066f090b, 54c5ea36-ab23-4d42-8329-7d4276766fd2, 5f8f501b-0d60-4290-a423-f5a62774c51f, 637fdaca-0ffc-4aa4-80cd-9c3ddcba7e64, 6a78d0ab-acfc-4151-a5b7-4047398f6072, 6fc35f29-4d50-4795-8ec0-342438e8bbf2, 793ef3ab-b817-4ad0-8e02-269617173e3a, 839dd472-78d1-4831-a607-5a00f32b2d04, 8f563206-33c5-4202-a126-8601abe9e66e, 9f03b56b-f5a3-4de1-85f3-3f3101ac1386, abaf5049-e56c-43c5-ae2d-a0c6a9b086dd, b9e38406-4f95-40bd-9db4-2067e0da4c94, c0b79300-f1bb-4242-bd1b-80961a05fcc3, c3bf0feb-f97b-4b04-b8f4-431e5ccf626b, c3ed1d32-b896-4aa9-b74d-bbeb9b4c2a4f, ce7a4df9-a7da-4763-8f6d-5849d3a1c21c, dc256a68-449a-4f4b-a95b-69b91d1973f6, eab228cb-dabc-4bd2-99b1-66016bc20632, ebaf4c32-3ea6-4356-ac43-5808864f0da1, f081d684-2893-4b42-807a-25870164a944, f68621a4-2a51-4ff0-b50a-45a78c2bf750, f9d67435-fdf8-44dd-9695-dbb12e70b5f5, fa7f7b48-afe9-4ca6-9830-ac5d9074ead6]
ncodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:30:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 33
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 38
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 38
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 127B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 965B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 866B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 956B for [_row_key] BINARY: 38 values, 1,520B raw, 857B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 188B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 121B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 850B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 751B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 843B for [_row_key] BINARY: 33 values, 1,320B raw, 744B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 179B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 112B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 763B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 664B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 38
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 127B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 965B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 866B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 956B for [_row_key] BINARY: 38 values, 1,520B raw, 857B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 187B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 120B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 850B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 751B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 843B for [_row_key] BINARY: 33 values, 1,320B raw, 744B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 113B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 763B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 664B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 38
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 127B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 965B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 866B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 956B for [_row_key] BINARY: 38 values, 1,520B raw, 857B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 187B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 120B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 850B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 751B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.par9410 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093103
9510 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=33, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=30, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=37, numUpdates=0}}}
9521 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
9521 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
9521 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 33, totalInsertBuckets => 1, recordsPerBucket => 500000
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 30, totalInsertBuckets => 1, recordsPerBucket => 500000
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
9522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
9537 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093103
9538 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093103
9751 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
9751 [pool-73-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
9783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
9783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
quet.hadoop.ColumnChunkPageWriteStore: written 843B for [_row_key] BINARY: 33 values, 1,320B raw, 744B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 113B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 763B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 664B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:02 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,754
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 185B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 118B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 752B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 844B for [_row_key] BINARY: 33 values, 1,320B raw, 745B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, 9816 [pool-73-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
9842 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
9842 [pool-74-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
9867 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
9867 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
9887 [pool-74-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
9909 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
9909 [pool-75-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
9934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
9934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
9955 [pool-75-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
9984 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
9984 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
10061 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
10061 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
10366 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
10375 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093103 as complete
10375 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093103
10736 [main] WARN  com.uber.hoodie.io.compact.HoodieRealtimeTableCompactor  - After filtering, Nothing to compact for /tmp/junit5570332008001635730
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.864 sec - in com.uber.hoodie.io.TestHoodieCompactor
Running com.uber.hoodie.io.TestHoodieCommitArchiveLog
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.24 sec - in com.uber.hoodie.io.TestHoodieCommitArchiveLog
Running com.uber.hoodie.index.TestHoodieIndex
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 sec - in com.uber.hoodie.index.TestHoodieIndex
Running com.uber.hoodie.index.TestHbaseIndex
Formatting using clusterid: testClusterID
14416 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
14601 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
15998 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
18862 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
19057 [spirals-vortex:40727.activeMasterManager] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
19123 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
21045 [spirals-vortex:40727.activeMasterManager] WARN  org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore  - Log directory not found: File hdfs://localhost:37631/user/root/test-data/f86af1b4-358c-47e1-9ff0-256fcd788a1d/MasterProcWALs does not exist.
21238 [RS:0;spirals-vortex:40244] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
27074 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093120
27806 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=56, numUpdates=0}}}
27813 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
28232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
28233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
28302 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
28533 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
28533 [pool-137-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
28587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
28587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,154
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 182B for [_hoodie_commit_seqno] BINARY: 30 values, 726B raw, 115B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 787B for [_hoodie_record_key] BINARY: 30 values, 1,206B raw, 688B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 30 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 779B for [_row_key] BINARY: 30 values, 1,200B raw, 680B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 30 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 30 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [begin_lat] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [begin_lon] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [end_lat] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [end_lon] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [fare] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,554
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 195B for [_hoodie_commit_seqno] BINARY: 37 values, 894B raw, 128B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 937B for [_hoodie_record_key] BINARY: 37 values, 1,486B raw, 838B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 929B for [_row_key] BINARY: 37 values, 1,480B raw, 830B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [fare] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,718
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 219B for [_hoodie_commit_seqno] BINARY: 72 values, 943B raw, 173B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,712B for [_hoodie_record_key] BINARY: 72 values, 2,887B raw, 1,612B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIO29048 [pool-137-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
29099 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
29099 [pool-138-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
29139 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
29139 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
30001 [pool-138-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
30056 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
30056 [pool-139-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
30099 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
30099 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
30940 [pool-139-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
31348 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
31560 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
31560 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
31584 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
31584 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
31873 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
32294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
32294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
32986 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093126
33237 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=70, numUpdates=0}}}
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
33653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
33654 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 70, totalInsertBuckets => 1, recordsPerBucket => 500000
33654 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
33654 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
33720 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180316093126
33934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
33934 [pool-140-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
33993 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
33994 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
NARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,704B for [_row_key] BINARY: 72 values, 2,880B raw, 1,604B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [fare] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,718
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 217B for [_hoodie_commit_seqno] BINARY: 72 values, 943B raw, 171B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,725B for [_hoodie_record_key] BINARY: 72 values, 2,887B raw, 1,625B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,717B for [_row_key] BINARY: 72 values, 2,880B raw, 1,617B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [fare] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,694
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 191B for [_hoodie_commit_seqno] BINARY: 56 values, 734B raw, 146B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,356B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,257B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,349B for [_row_key] BINARY: 56 values, 2,240B raw, 1,250B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,154
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.Colum34443 [pool-140-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
34478 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
34478 [pool-141-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
34536 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
34536 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
34975 [pool-141-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
35004 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
35004 [pool-142-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
35055 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
35055 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
nChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 247B for [_hoodie_commit_seqno] BINARY: 65 values, 1,567B raw, 179B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,566B for [_hoodie_record_key] BINARY: 65 values, 2,607B raw, 1,466B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,556B for [_row_key] BINARY: 65 values, 2,600B raw, 1,456B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:27 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [fare] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,154
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 256B for [_hoodie_commit_seqno] BINARY: 65 values, 1,567B raw, 188B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,563B for [_hoodie_record_key] BINARY: 65 values, 2,607B raw, 1,463B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,557B for [_row_key] BINARY: 65 values, 2,600B raw, 1,457B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:28 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [fare] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 14,154
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 70 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 259B for [_hoodie_commit_seqno] BINARY: 70 values, 1,687B raw, 191B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,672B for [_hoodie_record_key] BINARY: 70 values, 2,807B raw, 1,572B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 70 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 70 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 70 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,663B for [_row_key] BINARY: 70 values, 2,800B raw, 1,563B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 70 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore:35489 [pool-142-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
35522 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093126
36120 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
36120 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
36148 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
36148 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
36447 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
36466 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093126 as complete
36466 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093126
37350 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20180316093126]
37350 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20180316093126]
37565 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
37571 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:37631/tmp/junit6273670189829327350/2015/03/16/cbfc1d5e-991e-4906-bf4e-c434a6c16b67_1_20180316093126.parquet	true
37575 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
37578 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:37631/tmp/junit6273670189829327350/2015/03/17/7685418c-2577-4a01-bba0-02d969f7fdc0_2_20180316093126.parquet	true
37578 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
37581 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:37631/tmp/junit6273670189829327350/2016/03/15/4430a9e7-5a72-40fe-a56a-f8cdab72d421_0_20180316093126.parquet	true
37586 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20180316093126]
37587 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20180316093126]
37606 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20180316093126] rollback is complete
37606 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
38370 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093132
38684 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
39103 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
39103 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
39162 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180316093132
39330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093132
39623 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
39642 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
39642 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
39696 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180316093132
39705 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093132
39825 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
39825 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
39835 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
39835 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
39835 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093132
42403 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
42532 [DataNode: [[[DISK]file:/root/workspace/uber/hudi/354203634/hoodie-client/target/test-data/6d10f716-a135-4f05-a918-4565d216c337/dfscluster_29254585-c515-40b2-80f6-18955b4a44bd/dfs/data/data1/, [DISK]file:/root/workspace/uber/hudi/354203634/hoodie-client/target/test-data/6d10f716-a135-4f05-a918-4565d216c337/dfscluster_29254585-c515-40b2-80f6-18955b4a44bd/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:37631] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-301310339-172.17.0.3-1521189066537 (Datanode Uuid 60780fc4-1942-4cf9-8462-2633ab76f8b7) service to localhost/127.0.0.1:37631 interrupted
42532 [DataNode: [[[DISK]file:/root/workspace/uber/hudi/354203634/hoodie-client/target/test-data/6d10f716-a135-4f05-a918-4565d216c337/dfscluster_29254585-c515-40b2-80f6-18955b4a44bd/dfs/data/data1/, [DISK]file:/root/workspace/uber/hudi/354203634/hoodie-client/target/test-data/6d10f716-a135-4f05-a918-4565d216c337/dfscluster_29254585-c515-40b2-80f6-18955b4a44bd/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:37631] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-301310339-172.17.0.3-1521189066537 (Datanode Uuid 60780fc4-1942-4cf9-8462-2633ab76f8b7) service to localhost/127.0.0.1:37631
42555 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@75a2dbb3] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.777 sec - in com.uber.hoodie.index.TestHbaseIndex
Running com.uber.hoodie.index.bloom.TestHoodieBloomIndex
43080 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
43081 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${0}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
43600 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
43616 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
44520 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 1
44520 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
44626 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2488840594121986634/2016/01/31/ad6876f1-492c-4801-8a5f-8c24283615d3_1_20180316093137.parquet
44635 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2488840594121986634/2016/01/31/ad6876f1-492c-4801-8a5f-8c24283615d3_1_20180316093137.parquet
44635 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2488840594121986634/2016/01/31/ad6876f1-492c-4801-8a5f-8c24283615d3_1_20180316093137.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
45048 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
45048 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
45278 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
45328 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
46248 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
46881 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
47312 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
 written 91B for [driver] BINARY: 70 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [begin_lat] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [begin_lon] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [end_lat] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [end_lon] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:29 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [fare] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:38 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:38 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:38 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:41 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.Colu48705 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
48795 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
48803 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
48803 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
48985 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6162299595035316920/2015/01/31/31c23557-923d-4c83-93aa-c62abefd2b83_1_20180316093142.parquet
48996 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6162299595035316920/2015/01/31/31c23557-923d-4c83-93aa-c62abefd2b83_1_20180316093142.parquet
48996 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6162299595035316920/2015/01/31/31c23557-923d-4c83-93aa-c62abefd2b83_1_20180316093142.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
49006 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6162299595035316920/2016/01/31/ce061ab3-2380-40f6-9022-80d9b43a5faf_1_20180316093141.parquet
49017 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6162299595035316920/2016/01/31/ce061ab3-2380-40f6-9022-80d9b43a5faf_1_20180316093141.parquet
49017 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6162299595035316920/2016/01/31/ce061ab3-2380-40f6-9022-80d9b43a5faf_1_20180316093141.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
49021 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6162299595035316920/2016/01/31/ddb87cc8-99d4-4967-8c10-fdf37ca17d58_1_20180316093139.parquet
49030 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6162299595035316920/2016/01/31/ddb87cc8-99d4-4967-8c10-fdf37ca17d58_1_20180316093139.parquet
49030 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6162299595035316920/2016/01/31/ddb87cc8-99d4-4967-8c10-fdf37ca17d58_1_20180316093139.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
49541 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
49541 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
49950 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
50007 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
51994 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
52072 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
53301 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
53301 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
53336 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
53429 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5364082703677030986/2016/01/31/0bd0aa6a-3959-41c4-8256-ee5525edb997_1_20180316093145.parquet
mnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:42 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:44 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:45 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:46 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParq53443 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5364082703677030986/2016/01/31/0bd0aa6a-3959-41c4-8256-ee5525edb997_1_20180316093145.parquet
53443 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5364082703677030986/2016/01/31/0bd0aa6a-3959-41c4-8256-ee5525edb997_1_20180316093145.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
53451 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5364082703677030986/2015/01/31/883eca03-c8f6-41fe-9f9d-f0687f22ae2f_1_20180316093146.parquet
53462 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5364082703677030986/2015/01/31/883eca03-c8f6-41fe-9f9d-f0687f22ae2f_1_20180316093146.parquet
53462 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5364082703677030986/2015/01/31/883eca03-c8f6-41fe-9f9d-f0687f22ae2f_1_20180316093146.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
53466 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5364082703677030986/2016/01/31/9f6c4f9d-e4d5-46bb-a945-d91d986b9d34_1_20180316093144.parquet
53475 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5364082703677030986/2016/01/31/9f6c4f9d-e4d5-46bb-a945-d91d986b9d34_1_20180316093144.parquet
53475 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5364082703677030986/2016/01/31/9f6c4f9d-e4d5-46bb-a945-d91d986b9d34_1_20180316093144.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
53749 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
54862 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
54922 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
55037 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 2 row keys from /tmp/junit4520646415405604176/2016/01/31/0ea803ac-1501-4d6a-8f4f-5fc33c3e541c_1_20180316093148.parquet
55038 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit4520646415405604176/2016/01/31/0ea803ac-1501-4d6a-8f4f-5fc33c3e541c_1_20180316093148.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0, 2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
uetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 395
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 2 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_seqno] BINARY: 2 values, 16B raw, 32B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_record_key] BINARY: 2 values, 86B raw, 76B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 2 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 2 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 171B for [_row_key] BINARY: 2 values, 80B raw, 72B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 128B for [time] BINARY: 2 values, 56B raw, 55B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [number] INT32: 2 values, 14B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2 records.
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 2
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 145
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 93B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_record_key] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [_hoodie_file_name] BINARY: 1 values, 36B raw, 52B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 58B for [_row_key] BINARY: 1 values, 7B raw, 27B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 247
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_seqno] BINARY: 3 values, 21B raw, 35B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [_hoodie_record_key] BINARY: 3 values, 27B raw, 37B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 105B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 30B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [_row_key] BINARY: 3 values, 21B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 3 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 9:31:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICT55809 [Executor task launch worker-0] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit3344055182695793779/2016/04/01/2_0_20160401010101.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1521189108000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
55830 [Executor task launch worker-0] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit3344055182695793779/2015/03/12/1_0_20150312101010.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1521189108000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.096 sec - in com.uber.hoodie.index.bloom.TestHoodieBloomIndex
Running com.uber.hoodie.metrics.TestHoodieMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.026 sec - in com.uber.hoodie.metrics.TestHoodieMetrics
Running com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
56031 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
56128 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
56136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
56137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
56137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
56137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
56137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
56155 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
56155 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
56155 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
56342 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
56342 [pool-294-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
56354 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
56406 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
56406 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
56440 [pool-294-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
56466 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
56466 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
56508 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
56508 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
56732 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
56738 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
56738 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
56906 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
56983 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=40, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=40, numUpdates=0}}}
56997 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 4439
56999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=df501644-d476-47be-9c0a-ca1de5e4a350}, sizeBytes=443801}]
56999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 40 inserts to new update bucket 0
56999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
56999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=df501644-d476-47be-9c0a-ca1de5e4a350}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{df501644-d476-47be-9c0a-ca1de5e4a350=0}
57020 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
57020 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
57404 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
57404 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
57467 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
57467 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
57470 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
57692 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
57698 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
57699 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
57746 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
IONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 19,011
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 276B for [_hoodie_commit_seqno] BINARY: 100 values, 1,308B raw, 229B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,344B for [_hoodie_record_key] BINARY: 100 values, 4,007B raw, 2,244B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 100 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,337B for [_row_key] BINARY: 100 values, 4,000B raw, 2,237B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 100 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 100 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 867B for [begin_lat] DOUBLE: 100 values, 800B raw, 823B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 867B for [begin_lon] DOUBLE: 100 values, 800B raw, 823B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 864B for [end_lat] DOUBLE: 100 values, 800B raw, 820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 867B for [end_lon] DOUBLE: 100 values, 800B raw, 823B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 867B for [fare] DOUBLE: 100 values, 800B raw, 823B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 26,699
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [_hoodie_commit_time] BINARY: 140 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 14B raw, 2B comp}
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 359B for [_hoodie_commit_seqno] BINARY: 140 values, 1,868B raw, 311B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,241B for [_hoodie_record_key] BINARY: 140 values, 5,607B raw, 3,141B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 140 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_file_name] BINARY: 140 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 108B raw, 2B comp}
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 140 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,233B for [_row_key] BINARY: 140 values, 5,600B raw, 3,133B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [rider] BINARY: 140 values, 6B raw, 26B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 26B raw, 2B comp}
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 72B for [driver] BINARY: 140 values, 6B raw, 26B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 28B raw, 2B comp}
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,172B for [begin_lat] DOUBLE: 140 values, 1,120B raw, 1,128B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,170B for [begin_lon] DOUBLE: 140 values, 1,120B raw, 1,126B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,167B for [end_lat] DOUBLE: 140 values, 1,120B raw, 1,123B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,169B for [end_lon] DOUBLE: 140 values, 1,120B raw, 1,125B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,169B for [fare] DOUBLE: 140 values, 1,120B raw, 1,125B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action wit57875 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
57984 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=200, numUpdates=0}}}
57998 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 3195
57999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=002, fileId=df501644-d476-47be-9c0a-ca1de5e4a350}, sizeBytes=447233}]
57999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 180 inserts to new update bucket 0
57999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 20, totalInsertBuckets => 1, recordsPerBucket => 100
57999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]
57999 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :2, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=df501644-d476-47be-9c0a-ca1de5e4a350}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]}, 
UpdateLocations mapped to buckets =>{df501644-d476-47be-9c0a-ca1de5e4a350=0}
58021 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
58021 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
58466 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
58466 [pool-305-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
58474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
58474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
h parallelism: 5
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 140
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 140
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 140
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 61,747
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [_hoodie_commit_time] BINARY: 324 values, 16B raw, 36B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 21B raw, 3B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 729B for [_hoodie_commit_seqno] BINARY: 324 values, 4,444B raw, 681B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 7,332B for [_hoodie_record_key] BINARY: 324 values, 12,967B raw, 7,231B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 324 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_file_name] BINARY: 324 values, 16B raw, 36B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 162B raw, 3B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 324 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 7,322B for [_row_key] BINARY: 324 values, 12,960B raw, 7,221B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [rider] BINARY: 324 values, 9B raw, 29B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 39B raw, 3B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 75B for [driver] BINARY: 324 values, 9B raw, 29B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 42B raw, 3B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,544B for [begin_lat] DOUBLE: 324 values, 2,592B raw, 2,500B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,545B for [begin_lon] DOUBLE: 324 values, 2,592B raw, 2,501B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,546B for [end_lat] DOUBLE: 324 values, 2,592B raw, 2,502B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,548B for [end_lon] DOUBLE: 324 values, 2,592B raw, 2,504B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,544B for [fare] DOUBLE: 324 values, 2,592B raw, 2,500B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3,150
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 16 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 127B for [_hoodie_commit_seqno] BINARY: 16 values, 230B raw, 80B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 487B for [_hoodie_record_key] BINARY: 16 values, 646B raw, 388B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 16 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 16 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 16 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 479B for [_row_key] BINARY: 16 values, 640B raw, 380B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 16 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 16 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [begin_lat] DOUBLE: 16 values, 128B raw, 151B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [begin_lon] DOUBLE: 16 values, 128B raw, 151B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [end_lat] DOUBLE: 16 values, 128B raw, 151B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [end_lon] DOUBLE: 16 values, 128B raw, 151B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [fare] DOUBLE: 16 values, 128B raw, 151B comp, 1 pages, encodings: [PLAIN, BIT_PAC58510 [pool-305-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
58536 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
58536 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
58616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
58616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
58742 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
58748 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
58748 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
59019 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093152
59127 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
59233 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=172, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=172, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=156, numUpdates=0}}}
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 172, totalInsertBuckets => 1, recordsPerBucket => 500000
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 172, totalInsertBuckets => 1, recordsPerBucket => 500000
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 156, totalInsertBuckets => 1, recordsPerBucket => 500000
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
59240 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
59255 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093152
59255 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093152
59535 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
59535 [pool-325-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
59619 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
59619 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
59620 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
59786 [pool-325-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
59815 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
59815 [pool-326-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
59892 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
59892 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
59925 [pool-326-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
59944 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
59944 [pool-327-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
60028 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
60028 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
KED]
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 16 records.
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 16
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 324 records.
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 324
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34,726
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 488B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 418B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,409B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,411B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,367B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,402B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,358B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,405B for [fare] DOUBLE: 172 values, 1,376B raw, 1,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34,726
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 483B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 413B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,402B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,358B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [fare] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,510
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIO60057 [pool-327-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
60085 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
60085 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
60163 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
60164 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
60384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
60390 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093152 as complete
60390 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093152
60533 [dispatcher-event-loop-26] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 9 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
60881 [dispatcher-event-loop-21] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 13 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
60903 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 500
60903 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
60958 [dispatcher-event-loop-29] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 14 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
60969 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
60992 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 16 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
61083 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 156 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet
61097 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet
61097 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 156 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet => [020326c3-bfd8-431b-8f17-4204ea43ea6c, 05da4131-078e-4ed1-9900-05d9e5c946c4, 09446258-a554-4093-bba9-b0bf8225bf4c, 0c728648-e9b8-45c7-994e-3760417c8071, 0e683517-ada0-4393-bcda-73cde3465b5d, 0eb4429f-ffec-4091-96ff-2c6fdbc38997, 0ec348b9-ff9a-4679-9cd1-cd9b91c80a28, 10572eee-6df7-41d9-9c5e-2aec84fedb16, 13ef16e4-cf48-443e-87a1-fb1e811a29fd, 162babd4-0ba1-4a2d-9f1c-d0e950dcff40, 179a8e58-54ca-4bb4-95b4-50d7360ded67, 19e8f13d-a17c-42e0-af78-fe1289b44725, 1a7445a7-10e1-4dcd-81b7-b5ec63b5c202, 1b077b60-49a2-461f-a6d7-8d38cf1b170d, 1c73211e-eb42-4ccb-8733-d9eafb24109e, 1c78df5f-f5f6-4b48-bb06-cfd5191c0708, 1cdda021-82c0-4139-92f6-c44b6fbbd0b5, 1cf8e5ac-a2be-47d6-bb45-b5195e3dd8f7, 1d1c5fe8-613e-4314-87d3-df17f8f8e2b0, 1daaae57-0f03-4f44-b728-49075d613659, 1edbf63b-ad6e-46da-ba26-71df475253fb, 20cd2599-d3b3-4a3d-b82c-264873039c7d, 23727252-bade-45ad-b2fa-6c1d37f06bed, 2399dbb9-4d7a-4939-951d-28ac673de1fc, 24d2f1c6-fbac-4b20-a630-ca3a56156a6f, 2865220c-dd20-45ca-a1fb-ff9ae207f313, 2ab3e39b-6caf-4f17-b542-28916267638b, 2b4abe5b-a912-4aec-85b8-8fda90d82568, 2b512b69-bffd-4c48-b2fe-ea565a04ef72, 2b8e5194-470d-4255-9fee-e30ba0fe5acc, 2c71be86-88d2-4697-b382-5b1026c9d606, 2fe09b22-c460-4e61-9fcb-38641628a335, 3ba7cc54-ff7b-48b4-9b68-b939881444ea, 3bf77a62-ca50-4cfb-a666-d53d31c0f39a, 3d5bded0-177c-474a-80ae-5748de4e2f38, 3ee8a1e8-3ff7-4965-88ec-0c59e010ed65, 3fad5d1e-366e-4c82-ae71-e4459d071513, 4229dfa5-7cfd-4aa1-b1b4-bde7f8598203, 45c2c9b5-f4f1-4616-a665-9b3495734641, 46759a43-1590-4265-bbff-9a62485ad279, 46792466-4bc3-4904-9362-c2cf1d0565f5, 46be29a9-cdf1-43ec-a089-c99024fe5b38, 47b1cfb7-d335-4bd5-87ac-e383cda09bc7, 4a32a914-5642-4d21-bdf3-e66bdef9158f, 4adf4fc9-5dfd-42ce-a7bc-968f01efc210, 4b824948-496f-4d28-959b-75430d232e21, 4c200b38-c805-4156-b226-c1d44e81c344, 4e1bf410-7e86-4795-bab4-9fee6f45df4f, 4e846f56-f21d-42f2-9279-39c4e0cf1041, 4fd26ee6-45c9-4470-8b72-444f5d9c9130, 503fb85f-9858-48a3-a8e9-e00469d50a72, 52064e42-a201-42f0-8347-b94d893f1f43, 5349784d-c7fd-4dd9-9c30-708ca2a7f0ee, 55706c87-36ef-418e-98be-ca2b22ca43c2, 5714a2b8-f489-4cf4-ad86-351d3a7c9914, 5719f1ab-4e91-4b95-b818-0991493fadc9, 5aceca14-f17f-46f7-8da4-b45eef04a588, 5e7bc237-a3d0-4b4a-b570-5e381b90a4f4, 5f1d577a-7227-4cd6-9904-a58059dae30f, 5fb026cd-0087-4a40-bee8-991b6b305994, 6326287c-9cf5-4ef1-b071-10b7b2b472eb, 649010b7-2165-4acf-8910-1fb3b2cd5296, 67ee95b5-6b4a-43e8-8541-45f90980fb0e, 6aefa54b-dfc2-4e30-9ad5-911dd751e423, 6b66106e-bdb6-48e1-80da-daa189716275, 6ce8094b-df2a-4cfa-987c-6a1d63e856fd, 6d53eedc-b6fc-437c-8d48-fa6754fa4acd, 6ee573f5-4fac-4b38-811e-00c492157288, 6face862-8c1f-4b24-a2ce-80e7807c4323, 6fe593c4-9af0-4e44-9e93-4d38335a56b5, 703dbd45-b56b-49c9-b806-1efd4b31b2c1, 74b43c6c-c1bf-4bcc-923f-85ede69b2e55, 74b9900d-ecfc-4d98-bbb3-dd29fd73ded1, 785ae42d-2a8e-40bb-a7b5-5ac6648f68de, 79367b7c-ab15-451f-a026-898e82d8a51e, 83bb8b00-0d60-431d-9b36-10d4a0c14013, 83ebe0a3-4ed1-435a-93d9-5d78a42945b6, 85b9c5ff-0f77-4565-b189-f969e02304d9, 87889d1b-f7ee-4544-a68b-ada57c77bc08, 879616c6-c25b-46cb-942c-88b2e597d0b3, 8ba86831-b522-4dde-90be-8453272be507, 8cc13bf8-5b8b-4087-84de-cc610f4ce4c0, 8cd16c2f-35f5-468f-99c6-59de2d404978, 8ed60896-c512-4158-b0cb-69c68a601c1d, 92638277-04fc-4742-8cbf-5b59f5c8060d, 93646f57-ecd4-440f-888b-efc925f5c1d4, 94e87868-bd6b-4e67-b54f-34663bc9e7eb, 959f36c8-29be-4081-9926-6f3dacbfad8b, 9838287b-3cb0-47e7-b574-0ce09dc737af, 9c3a1ee1-af81-4a8c-a36f-0220bcda32bf, 9efa0089-e3eb-4209-b89b-3139c794cea4, a02c64f8-c0e5-4cc2-8511-65be6d484853, a05e96e6-fb99-4215-b265-004d5851af7a, a17b8641-93a9-47da-b260-bc589463bf6c, a7036f88-ca64-44dd-acf3-7646d4343b8b, ad0f3b1e-40ad-4166-95db-335a54596764, ad959a39-ccf2-47c5-b084-fafb48ad871f, aecbaafa-cd73-4908-ae71-cfe809c7b97c, b04eda4c-efa2-4ad4-9b9d-0f58e03071c9, b0ce28bc-4e6e-4ae0-a02b-534d467101ef, b1b75b77-7874-4610-b332-fe7c5c6d077f, b1cb992a-ca5d-4362-b929-fd912e8dfe5b, b2b2ecec-015b-49a3-870c-5fc12d738db9, b4b5ff5b-15d7-43f0-a791-7f38d366a382, b56b20c3-3f50-429c-b816-dd702aa24f37, b98de3bc-015c-43c9-b8d7-a077f0f24dbb, ba05ee7c-4ccd-4893-958b-21fcc490335b, bae7aabb-8442-4df9-97d1-cb7bfc13fcc8, bb238546-c067-4896-a56e-2c1078a4c166, bbab88fd-fbbb-4581-9b33-937b91d0aae1, bc847ff5-8999-40a1-8c6b-e14b27ba81b6, bddf772b-3d70-4272-b8b1-294cb9f0c284, be98eeba-c88f-4b76-942a-05aaec27e60d, bf5a220d-f3e0-40c9-b3e0-a3c1bf963e20, c163f1ac-0ce7-42e3-a517-e773c76e4dc5, c2b9c692-8750-4c8b-973d-c5f87b3da169, c30b86fa-3400-4210-8f1f-97c7043a0adb, c3a652c2-8991-45cf-91c8-a2277a59b614, c467204c-8580-46c0-920c-90fc6d2ef0fa, c6c51d03-cb96-4eb7-bebe-f23ddb6bbc63, c70f2e39-24f8-425f-913b-8ff58451344a, c9469eb0-3efe-4fcc-8fc8-8963247d0c46, ce55981d-01f5-467f-8092-42353fe926c3, d2a68448-f9a5-453a-b4e8-708acc548a0e, d493e88c-8f61-4ed0-8f66-6b971b99492b, d7988ef8-3dea-483d-88e1-cf8b042e43c7, d91b6dab-8490-4fea-9363-d73e1e5af639, d966136f-d565-4db6-a42c-772e01b0a568, d97f084a-e764-445b-92fd-5ca1e786aac3, db1b44a1-6fe9-498a-a20b-70de55e379b7, dc14c210-00ee-4a76-a732-51d2906c3854, dd16ae1f-e735-4601-a70b-1f7d2c5ba081, df3e79eb-85a8-4238-bb33-425bf95b99fa, e33377dd-4f30-42d7-a867-b4a7fc2b5234, e39c2703-dc76-4bf1-bf3a-ec955d502d53, e7871974-03de-4750-9c12-5c25a2d54b6e, e83f7084-bc73-46d5-a792-77afa977f8fd, e879ac53-043b-4f22-a793-ee9841204508, ebb9b0bf-2cd6-41af-8969-8461c324639c, ed11c3aa-cb7c-44bf-aafe-866139da6891, edc5d028-26c2-4e30-ad79-2e6672cd6209, eeaab3a3-a020-44d7-bda8-d378ff7ceba5, ef28216d-b9c6-4df9-b79e-b056e459420b, f0e395a2-a329-4e83-9f39-2c8597493375, f29f7851-7f1d-4c06-8446-f4002fc811ee, f5e65581-131b-459c-b7d3-8a234df1d062, f61a3494-d022-4ab5-9622-49351ac42e27, f71f08f1-4573-4e88-893d-a11a21f9fdf4, f79fab88-93e0-40a0-856d-c74fc2e90be0, f98ae4d3-2e9f-4ba3-8eac-a4102e79aef8, fa9523a6-c4cc-4942-8bee-1883870443da, fba343d4-2561-4b20-b81c-6c2ad8885023, fcaeeb95-e555-4825-b9c3-c6f3cd21bcf5, fe6b0f2c-de4b-420e-8057-0611cf506669, fff2f929-1905-408e-b4fd-68231bdd22c2, ffff1c80-3282-45ac-b0f2-1ccd0b4b51eb]
61114 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 172 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet
61125 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet
61125 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 172 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet => [01dd5788-5da3-4b17-94bb-4188d1eacdbb, 029f12d0-8622-4f3e-b8c9-aa9b508ff441, 04ace1c4-368e-4eb5-bf61-160ff8c0a7c3, 0843a3a3-ca98-45ed-914a-465fa4923d89, 0ac63248-313a-4330-a5fc-7aa97ddaab1d, 0af75466-88ee-486c-8925-9be98f84b926, 0b516f2f-ac76-483d-8573-0a42b06d9dea, 0cc98183-efd0-4df6-a391-9e7feb762e52, 0d7fb83e-1315-4a4d-b747-7df20f946118, 103f20dc-eeac-46ba-876d-f965d2ae5a93, 116d2a1a-372f-4909-b769-08e73146ed2a, 12886a39-b0b9-44e2-a45f-a076768c8f83, 140621f2-587c-4f5a-b072-0af1db3a5366, 1525ff2c-e29d-420e-9819-436f98fc2c5d, 16ddab03-49b8-498e-9064-64a334711670, 1b06541f-2e40-4133-a59a-d1dab564cd7f, 205012bb-90b3-4e61-a247-113479133c44, 206f7363-42d9-4618-82f5-1ff0a6297ff3, 20ad2136-db1a-4bd1-9a9e-743b4e85a907, 21cf7b1d-9d53-46f5-9442-335d29329295, 23ef3f71-9873-48fb-a635-e0a6dd9bba43, 25880518-161a-4de5-8362-d15b226af343, 2840f20a-1a6f-4ada-9172-b700af932462, 292ba420-13a7-481a-9ebd-d729c989da1d, 29564ab3-b15f-435a-9f98-b895f76582d5, 29d33bf7-c96f-4455-ae9f-4d6d38f66094, 2a4b47f0-3011-4f4f-a4b1-6987fb0bab72, 2a92f19d-3b41-48a6-bf2e-d912705bd68b, 2f01eeab-82cf-4374-b791-4c378b953b30, 2f0cab5f-21f4-4373-aa4f-b159d80febfc, 2f14d6e1-86a2-4c30-805e-9bd85b7ef0ef, 2f7c09dc-c314-494b-8f88-2104b6cfd576, 3027605d-4305-4fc2-bd76-fa08b19acc52, 32dba623-636c-4f5d-a793-1027a250ba75, 32f7a2c7-905e-47c0-a69e-b1237811124b, 33f29468-a75f-4930-b04b-fd752caa680f, 343e5449-a9b8-4280-a540-0dc073125314, 35c9bd62-35dd-4369-a69a-243d31faa6da, 375d0d30-bcdf-4b8c-a6e3-850de354a36c, 382aa504-a7fa-4f7c-b958-37ee74227902, 3be04791-d819-43d7-8a4c-d0d4209b001c, 3c15081f-83c1-48a7-92d8-a2972f8c629e, 3cc6cd2f-20c2-4eba-b221-9cd723dd28da, 3ea33cba-a6aa-4111-94b5-6fb9dcd467aa, 3f211fe0-9c8f-4041-8eea-e2ead83b97d4, 3f528845-d9f5-4cc7-b63d-acb1801bdb85, 3f9896cc-07b7-4920-88ca-aeccb762c25f, 4069f82e-c422-4293-98b7-3fe3a7dd6e3a, 4460d2cc-0f5d-4816-81a0-c1abda0953e2, 477a1363-c289-46bd-bf80-2fa1c0d65a43, 47be2f06-c3b1-458f-82f1-090fdcc0e72b, 4b72b75f-5f74-445b-96a0-15c2c7e5d0ee, 4bba50e3-5664-49f9-9105-86cd330eee25, 4c14e90d-0875-4726-bca3-4a6b046c36b2, 4f6a6a31-0860-4915-9432-c0960b7caa68, 4faa5f83-f716-463a-aeb1-c63f441d2bfa, 5178f3c9-55c7-4e3b-8c63-ed0d93b3a99e, 52268886-01a0-4227-aa9c-6f0c99c27cbe, 53e3eef2-beed-4a7b-a023-8e82bc8fc3a3, 55f23a93-e657-411b-a5fe-ef200507f613, 566af623-4635-4439-b02e-cbcef48fc124, 56bdad96-01c4-4f6c-b7d8-3be00c53d465, 571d910e-1b55-454d-a916-fde8efcf0d1e, 57cfb488-b347-47ed-8f77-1aa0e6e5bf03, 59478a18-618c-4428-95cd-26e3320bbff2, 5a17b170-229e-4d9c-92b7-6458d9bff92e, 5bc35e33-5e95-4aca-9547-de1a3f7f98fd, 5ec34a56-6db3-4988-88e1-4053b59805d1, 60296de4-2b23-4268-8c93-b9878bd72a51, 60399dce-aab9-470a-9955-646ebeaa5fff, 6072876e-1206-498c-a5f4-bf6df96a1790, 62204fa4-2cbe-4fe9-a84c-d1348459aae7, 626972d5-b16f-4d21-a07d-75eece5daf17, 68449141-6247-4b05-9e5f-27899a75722e, 6b179b4a-948c-4379-9eb8-0feb28443eed, 6be0c1c6-110e-4ab6-9b2c-d7bd491eb9f8, 6d584ec2-b1a0-4bdb-b8a9-9193adc32427, 6dd6579d-f158-44cd-a37b-8067f058160a, 6e3fba9b-a8f8-4970-b066-2b4c3441e00a, 6e4c1dad-e875-47f1-b501-b8d4faa8df71, 7031f675-9ee5-436e-8a99-ed8611541058, 72b29520-07fc-408d-adaf-734a94434b7a, 72caf82b-ef92-4eb4-a65c-0add3f2d7ba8, 757c7e55-cc13-4aa5-bb0a-7fa4253a2f59, 7786f06c-92d7-4ca2-b31c-d10cb1f56692, 7b68bc83-2ddc-450c-bb66-43f2e2245a19, 7c77e18d-f259-463e-9356-74473ba4b5f8, 7d4a4173-f05a-419a-b035-72ef7d4de6e7, 7d71a23a-41c9-4ad0-aae9-4115494a8491, 7dffbca9-789c-44d3-b2f4-48e9f1904593, 83d754d0-18c6-44eb-af93-28e84ce30754, 83ec1164-c3ca-41e0-986f-3329bcf6475b, 852c36e2-2163-4969-b539-7b5ae2ce630b, 895c2ef0-5cc9-4983-852d-7bf2ce65d3b3, 8a29f96c-63af-4558-8cdb-bdaf8a51b918, 8c47a657-3a97-447a-ad51-7c660a332bc8, 8e69fa44-2376-4564-892d-5758583af22e, 90f55f8a-ec82-4000-8b68-b20eec26f483, 9135629f-d8ab-444f-b648-5ed1871236d4, 92b97284-d9e3-4042-b3bd-d4366b9e5236, 95613ce6-8741-4119-b35d-771aee682291, 97e3c59c-c310-44ae-9cc9-e64deba1ac03, 98057ca4-1bab-4308-93d5-b1b1e9c0304f, 98a252f7-f19d-464d-a277-ff3f5c3a6dfd, 98aef5e0-668c-4416-9e5b-e6ed2a9a9f13, 9935edf9-62e7-458c-bcea-89240a6e6826, 9af5c3ac-47b1-44b3-b438-a0f297340da9, 9b13d886-6f8f-4165-8370-f81e2025c196, 9b24f412-b5b2-400c-9668-698c9c0d39ce, 9bb60400-d0f3-460c-bf58-9d767816b030, 9c4f40b5-4cba-48d2-9e38-44cba7b3ca2a, 9d667a96-d0ed-4586-acda-1bb77e68edf3, 9dee9e3e-a175-4b13-9c46-646c8f11861f, a23a4399-6cef-4681-aa0b-d9b658217c0a, a8762382-ee27-480f-9cc4-397031ac81d9, a8ca9235-25c0-4bd9-8f65-264af60debe1, aa9d03d9-7a90-4f8c-8635-575c05b2ea04, ab1d4f46-fc9c-4da5-98e5-26b9ccb3e364, abd7cd56-065b-44e3-b00c-31ba7bf4f730, ac4137b3-fe6c-45fb-81de-2538fdca8ac8, acca0792-6dc3-4609-9a3c-0f8588f1c298, ad70a196-ddef-4053-aebf-2fab09e02e17, adc4c912-c7c7-4b47-86fa-06425f40b32a, add04199-4744-481e-aaee-0bdcbdc01f63, b240b265-43a6-48d1-92d2-1eefe0db0193, b2a08b00-76d9-466a-9959-3d7b058fd4a5, b481b18f-9ec3-4677-b31c-8086c47544a5, b4c5f447-61e8-4da1-b0d7-d671ba9e457f, b65048a0-a49f-47bd-b480-307e181a006c, b6a4d80e-70a5-468b-8c99-6fa33fac221a, b81b8f39-6a9c-4933-b81e-e39876872283, bc64b692-4832-4663-88ac-716e7594f971, be772702-7e1a-440b-ac9a-3a55fd77dc3a, c0682be8-3fb4-47be-97ee-2b107b97b761, c820561d-dfdb-4f43-8f38-981ad6700ea4, cb3bf892-a0a3-4901-9959-ce58268d0256, ccc86ddd-7f16-4c7c-986b-9fe94e18f191, cd93ab4a-9efe-4dbc-bd6e-0e4193fbdcf5, cdc472d2-6855-4eef-8116-029f25a15344, ce238c6c-ebcc-4769-81d1-28c7081c09ae, d4c14ee0-a7d2-4bc5-b235-68587370969c, d69e8e1e-3e9f-4796-9838-719bb4743ebb, dda85283-8d93-4ff5-8019-89f525b06595, ddec5848-d08c-4c3c-a218-6a0ca0636381, ded6eb21-c1c2-4dff-ac43-b6dae77a4d8a, df669622-2183-4e92-bf58-b031972bdfc1, e3292f02-0167-48a3-8a57-d9aef3f1e12e, e446a394-c001-4dc8-b527-2f9123a704f5, e46d7f1d-0fb3-4ca6-a6bf-bc8f27b46fd0, e4a9c79f-a7a9-415c-8012-57f254c12e9d, e759754f-378e-4ac2-924f-85d574f42d29, e80f2d23-8caa-4139-9b1d-cf03fff56e74, e8c61a1a-6022-4f4e-8c74-bb60b777317c, e976807a-bd96-4ceb-b1fb-18d708c77ba3, e9dbf770-73d0-4df2-8895-4cb52e1596b0, ea470da9-3b41-490a-8c1f-43c0c7eec69a, ebbae658-f3b2-4e0a-a10d-8ee5a59f7314, ec4e4ad5-6494-4b22-b84c-ca3db13c96f0, ef8c0d37-c726-48eb-822f-e28317d24c6b, f1a5337d-f626-4251-a6b6-ed3ab84712c8, f2414ab4-b092-46d6-a24e-d573ab73b9eb, f2940043-29dd-4400-9cf6-27be5b68c8a1, f52dfd44-0e6d-43c0-9230-ad2af5ed1ef7, f56fd01a-4c94-46c4-8c5a-fb6cdb3eaeee, f69cdefc-d5b9-4e48-809a-06813754bc5f, fa413d44-210b-450f-8fc4-2ea8f79872a6, faee66b6-1620-4db2-8b03-5d6793c437de, fb364221-1d9f-4dd1-810f-c2949ddb9f59, fbbd2867-bcd0-4128-a662-9829b9af60af, fd298de1-ac18-4bd2-877c-7ee54e44d858, fe28a20a-67dc-4f86-92eb-72ff992241f9, ff5bf9cc-291f-4895-b85c-99f1a937cec7]
61143 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 172 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet
61160 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet
61160 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 172 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet => [08abded7-2c32-4acd-a77b-0b02563e10e5, 0ab01ce6-e422-4b4d-8b1d-853f01d291a1, 0ab0676d-dd9e-4a29-af4c-0ff73956d8e4, 0ae7f50c-dfb0-4372-ac6f-fb738c99b1ff, 0ccc7753-df13-413f-9e06-cd75946e41df, 104746da-d08b-48d1-b6d4-e8f01200f7d9, 15a6c846-e460-4b95-89b0-555ace3ce6eb, 16af9aec-48aa-4b44-b470-14d72f569e52, 16c3fc89-58d1-4d44-977f-071c57925612, 180c49f8-18c6-40d8-9fc9-6924ec3cfd97, 19222e3a-07f0-49bb-8ba0-e27c1aed82cd, 19a0c400-3c5e-42d3-b3c6-84c15ba9f2f3, 1acbf49c-d8e8-4ea6-b023-7871ab6630c4, 1c0e3f42-3a34-4eec-ada3-7e4851a34d83, 1d374fd4-c686-4332-9b79-d63e72f7c62f, 1e0c61aa-b758-407d-9635-a64428737bee, 1e77fa9b-99be-4a2f-832d-52266441cd8a, 1f15e25e-2aba-4bc2-b4f6-2010eba92a7e, 1faea1ba-386a-49d7-99de-0f3037bba1e2, 1fccdc23-1734-4b51-8f5d-b65d4cbb7fb8, 21f8a5db-31b2-4022-b259-6262c1ed0ea3, 220ece3a-8085-43f8-8db6-ca46e9d44079, 272c82ad-d49a-4f5d-9f92-87b077662532, 27f49bd0-2908-43e1-b98c-fb500f7e181b, 282eff2a-93a2-4046-86d6-3d0adcf39e27, 2834c95e-de0b-4773-8eca-abc9434264e0, 288dbe7b-df2e-4a0f-bb4e-3696defc99c1, 2aa3a8dd-e945-4626-b824-1593afa3419f, 2af3249b-d25c-4e74-9e58-4f0f1cd71e9a, 2c5a6d0c-6b08-4aa6-8e79-78c5e6bddf21, 2c5f64f8-0176-4571-8a1c-980a47c8e33c, 2d7fdb8d-4ae9-4914-a7f8-661315d1472a, 2daed088-3534-4c1e-a09b-9fad3d721f8f, 2e6b1f06-fa8f-4ca8-8ff0-dcc15b2bdd60, 2f0b15a2-8456-483a-8820-6fede5fedd7a, 2f2ce57c-d2f7-4373-9cda-6daaac98ab09, 3057bba1-9e2e-464b-86c0-248a998c8e2f, 3136c042-d9f7-4d29-b1d6-5dbc05772352, 31399832-8125-4ddf-9351-3407200a3ee6, 323b43b6-5fcf-4079-aba9-5be8dbe7655c, 33436bc7-94ed-4aca-badc-2f75741f4214, 38e01b13-f504-4b0e-adac-60cf3bb3dec2, 3978da0d-bebe-4ab6-b1f9-4a21cd3c5f88, 3bb8d9fc-4395-485a-9359-231f0d41c861, 3c4b6419-99e9-484c-8cef-b5c251cbc5f5, 4077b110-796f-4302-b5f9-f586cd246a7c, 44196e37-1155-4bc2-b1e2-602715792f8f, 451aae7a-8a38-420b-aeee-4030557042fb, 46aae5a2-15bb-41be-9aff-11b203dcb5a8, 487e70c9-5ff8-4a65-ac8c-97fac53991c3, 4ad3e31b-5f41-45f3-89e6-2ea009e6a3b0, 4b06d92e-3379-42e6-8432-56bd9930b053, 4b6a3c83-4048-401f-a55b-3b90ca6311af, 4baca86b-c5e2-4e5c-8df3-64a49797179c, 4d000be1-3f24-494b-8a1f-81cdb74fd491, 4f6f3a29-26bb-4ac5-af18-ff85a55e8394, 50ad6914-88c4-44f1-a09d-43cbb568f950, 59ea70e6-ef11-4a1a-88a8-261a9c8ec2e4, 5af7f1af-35fe-470c-ab2c-d5cc731b9b82, 5e3dcd6c-0082-4372-bc92-6b0d7cdd5951, 5ea90d63-16dd-458f-8b2a-3be928f11950, 5f43e5c7-8397-4628-88c3-c715c55360ef, 5fed515a-328b-495d-af84-cb7fa1fc37e9, 6467ed19-9f10-408f-a66d-eaae4c520a21, 647b7480-20d4-4f64-b68b-55745a46b625, 655c6b50-8e44-4b2f-9b34-f972c0f5eb34, 68d77d36-93ea-43d0-9b88-334ea342cd45, 69255adf-8e12-46c8-86bf-acf5d5116ce1, 6b000e72-a6d1-452e-9d25-8ce6c4805295, 6b8ab587-4b34-4f14-b852-7933a7253037, 6f1e083a-fe73-4340-a8ff-a40531de08bd, 71672f51-3521-4a76-a793-1b0f319012a9, 719364d0-6463-4980-962c-63a6dafc397d, 729912a3-de52-4d55-be46-c82a6ac4f21e, 7348e58b-49a2-4170-b0bb-18fd0af498ae, 749da547-5b91-4a32-bbdc-626d1aaa0de8, 7538089c-5374-4924-8b9a-ebfe25bb7933, 75749a1b-ba95-4405-9993-b50b4658ee20, 79c9e598-41d6-4c78-99e3-e255e344f269, 7b226b08-2159-4a06-87a7-206de1a3a898, 7fa610e7-77c0-4a53-9391-804a32772688, 80190003-2597-4291-8507-0ff5ec7ada4e, 813b65aa-82c4-4722-a658-26c0000ffc8e, 88036808-f88f-4257-8a33-a2d51943b682, 8ccf2fb8-e934-4b4f-a8fc-35bc89ed145c, 8f87c002-cea0-49c0-9fa4-ccf46e9ed03b, 901e1338-1ece-4da7-af3f-5abffe75d365, 905d054c-d517-4ef6-9284-3a2d8476eadf, 912b14a0-947e-45f7-990f-23cec23cf2bb, 9175aeb2-83a1-4393-97e8-ab0ee6c83aa6, 95010668-beba-4ad5-8069-f7499fbd6606, 97a1a173-549f-44a8-a800-c5b5bdd66095, 9a960f80-acd0-4b07-9795-6ac47918c318, 9b76ab42-a853-4e02-a6fc-a751e6c74203, 9c71aa99-03b5-4134-a4ca-c86263c20f11, 9e86cf0b-539b-40a0-9335-f7a28b1ec25a, 9f254d61-223a-46b0-8753-c1289f3ba1db, a0140a94-c2a8-487c-a5a4-3c71b7cd625b, a106baa0-53f0-42e0-9dac-4b870a54dfb3, a10beff2-d3f7-4ac0-acf5-c2694c2a7dab, a381fcb5-139e-41eb-9d35-27d730a9c5f7, a38ea72a-8d4d-44dd-b158-778bdf285617, a3947f72-c20d-487a-888a-1ea00306e380, a690ad1e-ddbc-49ad-8b11-f94f95b90bb5, a6ee8d33-c1bf-4746-b55b-edfb4fba51a2, aa3d4d7b-b48c-4592-8eda-40c3ae9eee4a, aad6ae8a-5345-484e-8009-803351c73f41, ab8dc729-1c52-487e-ad4c-075fa6425d88, ac889602-c285-45ed-ad5b-1acf7e505c7a, ad5445d3-75ff-4ecd-9d91-a04220d6360b, ad8e3b9a-48f5-4682-8132-38585ed413f3, aef7e22d-8396-44b7-b68e-932dde3aa1a3, b43a8885-4fc4-42ab-bc52-a98687f91056, b4463655-49e8-4b7f-ac08-00a91b88dbf6, b447fccf-8011-4f34-bb8a-0c6617bc753b, b4e2d0bc-1c53-4995-a8e0-42a9930612f9, b624f5f7-735a-4fef-8293-d74e6ddbf4a9, b71d791f-bf01-43bc-9f54-184c6ab6ceeb, b73c8512-1cce-4de8-9bab-20728806caea, b8cc5d56-d7bc-4541-b054-dbb0324f4bfd, b911c3fd-796e-4809-98da-474016cab53c, bb6e2b8f-c5a3-4601-8b79-437e4a91da9f, bd9a711a-664c-4bf9-b066-9bfa8c50f041, bef0df03-699f-4c12-89d0-e53c291df9bc, bfa2681d-8081-4a58-9af6-31b6ca7d68ec, c0af065d-9469-4678-8832-178ae9579b36, c2386888-c3f9-4179-8edf-b4bd025c7cc5, c39172cb-4307-4d77-93f8-02d8a6814d22, c3a0c82a-c229-4d9d-b43e-bb269b2071d3, c42098cb-3409-4f01-a538-d5e93987dd66, c48e62f7-bf01-4837-a51a-6c987a9a4802, c74b492a-f195-4dc5-86e9-39855e2ceb90, c7f389ae-6ceb-42e6-ad0d-6541f8dfd5dd, c87312ee-2b8c-4bdb-bc5e-d76cc138d582, c89740b0-be3a-435d-955f-d19f62318442, cb7f7bf2-b096-4c78-b486-94a34b00a1d1, cbb0c72d-bdda-4ca1-9f6d-0a9a9983e8ac, cbcc6e24-636d-45e8-ac76-5ae3b388ee4d, cdde1888-be95-4355-aa6c-c28f49b38278, cf4de964-3150-4b91-8618-4838b2505866, cfff4dee-c2d5-4389-a7b1-24d78c98e38f, d17a5da8-bf0d-47af-8210-f0284127c5c3, d37265f9-d2f5-4598-b0ac-1cd123023822, d3d059f8-a1eb-4dcb-89ef-9924c01dd3b6, d477a944-0f25-4da5-ba71-7785ba4a9d07, d74df7a2-7e4a-430f-88e9-d02ececb201e, d7b4c332-6eb0-4177-a191-903322f4d98d, d80baf6d-4380-416c-9985-71129ef04459, da313535-fcb8-4205-98ae-13ba2ac0c99d, dba0988a-285f-4611-bd56-dc46c55b1a1a, de9a548a-2615-4bee-8e0d-1eb5a8fdf3c8, e0ddb2a2-baa6-4a3b-a2ef-91f3a1ccbc9b, e2b3fe8b-f8b7-4685-a79c-1d993539699f, e6978cf2-69f5-4627-8db9-f8746db06318, e71eac95-3917-4621-a6ec-a79f1871e2fb, e7da1256-fefd-4f0a-8057-b7e038c006de, eaafb0ba-f4b6-450d-84c8-3300e2f6a628, ebdf2582-2935-4dda-9f66-28c0c4e76e21, ec520e14-61c4-4bd6-aaf8-d12fc852f3e9, ee496ae9-758b-4183-9f7c-1b2934048f3e, ee5f556c-ebb4-4a93-8b48-f5de551e9377, ef07449f-b924-4c86-86f9-35a9a24ce373, f2c1c128-c172-4b7c-b5b1-9b646afa6ece, f3f3fa0c-d1ba-45ab-b6e5-67e810128218, f4330bcf-daea-4d39-88af-095fde078d45, f8350c2a-41b2-4a1c-a8f4-28effbb20cc8, f8ef5ac9-8d4d-471a-8502-0d0d9232e6ec, f94fee59-5818-43e3-b3f1-9e9b2f323eb6, fa3fabab-4926-46ab-a6ae-de86a0bdc9c1, faa010ea-a276-454f-a05c-964275c02c92, fe015d97-c86b-43a5-a814-e07d01b8903d, fe133ead-aabe-48e8-9086-ee789ebd7c2a]
61377 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
62080 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
62369 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093156
62782 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 88
62782 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
62908 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
63033 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet
63049 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet
63049 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet => [1cf8e5ac-a2be-47d6-bb45-b5195e3dd8f7, 2865220c-dd20-45ca-a1fb-ff9ae207f313, 2c71be86-88d2-4697-b382-5b1026c9d606, 3ba7cc54-ff7b-48b4-9b68-b939881444ea, 3bf77a62-ca50-4cfb-a666-d53d31c0f39a, 3fad5d1e-366e-4c82-ae71-e4459d071513, 45c2c9b5-f4f1-4616-a665-9b3495734641, 503fb85f-9858-48a3-a8e9-e00469d50a72, 5714a2b8-f489-4cf4-ad86-351d3a7c9914, 5719f1ab-4e91-4b95-b818-0991493fadc9, 5aceca14-f17f-46f7-8da4-b45eef04a588, 6d53eedc-b6fc-437c-8d48-fa6754fa4acd, 79367b7c-ab15-451f-a026-898e82d8a51e, 87889d1b-f7ee-4544-a68b-ada57c77bc08, 8cd16c2f-35f5-468f-99c6-59de2d404978, 94e87868-bd6b-4e67-b54f-34663bc9e7eb, ad959a39-ccf2-47c5-b084-fafb48ad871f, b4b5ff5b-15d7-43f0-a791-7f38d366a382, bbab88fd-fbbb-4581-9b33-937b91d0aae1, d493e88c-8f61-4ed0-8f66-6b971b99492b, db1b44a1-6fe9-498a-a20b-70de55e379b7, dd16ae1f-e735-4601-a70b-1f7d2c5ba081, f98ae4d3-2e9f-4ba3-8eac-a4102e79aef8, fa9523a6-c4cc-4942-8bee-1883870443da, fcaeeb95-e555-4825-b9c3-c6f3cd21bcf5]
63075 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet
63090 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet
63090 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet => [1b06541f-2e40-4133-a59a-d1dab564cd7f, 292ba420-13a7-481a-9ebd-d729c989da1d, 2a92f19d-3b41-48a6-bf2e-d912705bd68b, 2f01eeab-82cf-4374-b791-4c378b953b30, 2f14d6e1-86a2-4c30-805e-9bd85b7ef0ef, 3027605d-4305-4fc2-bd76-fa08b19acc52, 32f7a2c7-905e-47c0-a69e-b1237811124b, 33f29468-a75f-4930-b04b-fd752caa680f, 3be04791-d819-43d7-8a4c-d0d4209b001c, 4460d2cc-0f5d-4816-81a0-c1abda0953e2, 4b72b75f-5f74-445b-96a0-15c2c7e5d0ee, 5178f3c9-55c7-4e3b-8c63-ed0d93b3a99e, 571d910e-1b55-454d-a916-fde8efcf0d1e, 5ec34a56-6db3-4988-88e1-4053b59805d1, 6be0c1c6-110e-4ab6-9b2c-d7bd491eb9f8, 7b68bc83-2ddc-450c-bb66-43f2e2245a19, 7c77e18d-f259-463e-9356-74473ba4b5f8, 9135629f-d8ab-444f-b648-5ed1871236d4, 97e3c59c-c310-44ae-9cc9-e64deba1ac03]
63122 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet
63137 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet
63137 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet => [9935edf9-62e7-458c-bcea-89240a6e6826, b240b265-43a6-48d1-92d2-1eefe0db0193, b481b18f-9ec3-4677-b31c-8086c47544a5, c0682be8-3fb4-47be-97ee-2b107b97b761, c820561d-dfdb-4f43-8f38-981ad6700ea4, ccc86ddd-7f16-4c7c-986b-9fe94e18f191, ded6eb21-c1c2-4dff-ac43-b6dae77a4d8a, ea470da9-3b41-490a-8c1f-43c0c7eec69a, f1a5337d-f626-4251-a6b6-ed3ab84712c8, fbbd2867-bcd0-4128-a662-9829b9af60af, fe28a20a-67dc-4f86-92eb-72ff992241f9]
63155 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet
63169 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet
63169 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet => [180c49f8-18c6-40d8-9fc9-6924ec3cfd97, 19a0c400-3c5e-42d3-b3c6-84c15ba9f2f3, 1c0e3f42-3a34-4eec-ada3-7e4851a34d83, 27f49bd0-2908-43e1-b98c-fb500f7e181b, 2834c95e-de0b-4773-8eca-abc9434264e0, 2e6b1f06-fa8f-4ca8-8ff0-dcc15b2bdd60, 2f2ce57c-d2f7-4373-9cda-6daaac98ab09, 3136c042-d9f7-4d29-b1d6-5dbc05772352, 3978da0d-bebe-4ab6-b1f9-4a21cd3c5f88, 46aae5a2-15bb-41be-9aff-11b203dcb5a8, 5af7f1af-35fe-470c-ab2c-d5cc731b9b82, 5fed515a-328b-495d-af84-cb7fa1fc37e9, 6f1e083a-fe73-4340-a8ff-a40531de08bd, 719364d0-6463-4980-962c-63a6dafc397d, 7348e58b-49a2-4170-b0bb-18fd0af498ae, 7fa610e7-77c0-4a53-9391-804a32772688, 912b14a0-947e-45f7-990f-23cec23cf2bb, a0140a94-c2a8-487c-a5a4-3c71b7cd625b, b4463655-49e8-4b7f-ac08-00a91b88dbf6, b71d791f-bf01-43bc-9f54-184c6ab6ceeb, b73c8512-1cce-4de8-9bab-20728806caea, c39172cb-4307-4d77-93f8-02d8a6814d22, c48e62f7-bf01-4837-a51a-6c987a9a4802, cf4de964-3150-4b91-8618-4838b2505866, d17a5da8-bf0d-47af-8210-f0284127c5c3, d37265f9-d2f5-4598-b0ac-1cd123023822, de9a548a-2615-4bee-8e0d-1eb5a8fdf3c8, eaafb0ba-f4b6-450d-84c8-3300e2f6a628, ebdf2582-2935-4dda-9f66-28c0c4e76e21, ee5f556c-ebb4-4a93-8b48-f5de551e9377, f3f3fa0c-d1ba-45ab-b6e5-67e810128218, f4330bcf-daea-4d39-88af-095fde078d45, f8350c2a-41b2-4a1c-a8f4-28effbb20cc8]
63259 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=88}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=33}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=25}}}
63260 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
63274 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2699
63274 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
63290 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093156
63290 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093156
NARY, RLE, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 454B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 384B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,285B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,291B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,247B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,282B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,238B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,284B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,240B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:53 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,291B for [fare] DOUBLE: 156 values, 1,248B raw, 1,247B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 156
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 156
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,642
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 107B for [_hoodie_commit_time] BINARY: 156 values, 33B raw, 53B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 36B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 509B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 439B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 201B for [_hoodie_file_name] BINARY: 156 values, 33B raw, 53B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 130B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [rider] BINARY: 156 values, 26B raw, 46B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 48B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 114B for [driver] BINARY: 156 values, 26B raw, 46B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 50B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,285B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,282B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,238B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [fare] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34,858
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 114B for [_hoodie_commit_time] BINARY: 172 values, 37B raw, 60B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 36B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 543B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 473B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 172 values, 37B raw, 60B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 130B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [rider] BINARY: 172 values, 30B raw, 53B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 48B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [driver] BINARY: 172 values, 30B raw, 53B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 50B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,400B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,356B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,401B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [fare] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.h63741 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
63741 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
63827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
63827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
64066 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
64109 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
64114 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093156 as complete
64114 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093156
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093156.parquet; isDirectory=false; length=450343; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet; isDirectory=false; length=450138; replication=1; blocksize=33554432; modification_time=1521189113000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet; isDirectory=false; length=450329; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet; isDirectory=false; length=450138; replication=1; blocksize=33554432; modification_time=1521189113000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093156.parquet; isDirectory=false; length=448977; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet; isDirectory=false; length=448807; replication=1; blocksize=33554432; modification_time=1521189113000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
65049 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
65330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093158
65436 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
65795 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 91
65795 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
65996 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093156.parquet
66014 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093156.parquet
66014 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093156.parquet => [09446258-a554-4093-bba9-b0bf8225bf4c, 0eb4429f-ffec-4091-96ff-2c6fdbc38997, 162babd4-0ba1-4a2d-9f1c-d0e950dcff40, 1c73211e-eb42-4ccb-8733-d9eafb24109e, 1daaae57-0f03-4f44-b728-49075d613659, 2399dbb9-4d7a-4939-951d-28ac673de1fc, 4229dfa5-7cfd-4aa1-b1b4-bde7f8598203, 45c2c9b5-f4f1-4616-a665-9b3495734641, 4e846f56-f21d-42f2-9279-39c4e0cf1041, 785ae42d-2a8e-40bb-a7b5-5ac6648f68de, 83bb8b00-0d60-431d-9b36-10d4a0c14013, a17b8641-93a9-47da-b260-bc589463bf6c, b04eda4c-efa2-4ad4-9b9d-0f58e03071c9, b1b75b77-7874-4610-b332-fe7c5c6d077f, b2b2ecec-015b-49a3-870c-5fc12d738db9, bc847ff5-8999-40a1-8c6b-e14b27ba81b6, bf5a220d-f3e0-40c9-b3e0-a3c1bf963e20, c6c51d03-cb96-4eb7-bebe-f23ddb6bbc63, c70f2e39-24f8-425f-913b-8ff58451344a, d7988ef8-3dea-483d-88e1-cf8b042e43c7, dd16ae1f-e735-4601-a70b-1f7d2c5ba081, e83f7084-bc73-46d5-a792-77afa977f8fd, eeaab3a3-a020-44d7-bda8-d378ff7ceba5, ef28216d-b9c6-4df9-b79e-b056e459420b, fcaeeb95-e555-4825-b9c3-c6f3cd21bcf5, ffff1c80-3282-45ac-b0f2-1ccd0b4b51eb]
66033 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet
66049 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet
66049 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet => [0d7fb83e-1315-4a4d-b747-7df20f946118, 103f20dc-eeac-46ba-876d-f965d2ae5a93, 116d2a1a-372f-4909-b769-08e73146ed2a, 1525ff2c-e29d-420e-9819-436f98fc2c5d, 292ba420-13a7-481a-9ebd-d729c989da1d, 2a92f19d-3b41-48a6-bf2e-d912705bd68b, 3c15081f-83c1-48a7-92d8-a2972f8c629e, 3f211fe0-9c8f-4041-8eea-e2ead83b97d4, 47be2f06-c3b1-458f-82f1-090fdcc0e72b, 4bba50e3-5664-49f9-9105-86cd330eee25, 5178f3c9-55c7-4e3b-8c63-ed0d93b3a99e, 55f23a93-e657-411b-a5fe-ef200507f613, 57cfb488-b347-47ed-8f77-1aa0e6e5bf03, 6b179b4a-948c-4379-9eb8-0feb28443eed, 6e4c1dad-e875-47f1-b501-b8d4faa8df71, 7d71a23a-41c9-4ad0-aae9-4115494a8491, 895c2ef0-5cc9-4983-852d-7bf2ce65d3b3, 8a29f96c-63af-4558-8cdb-bdaf8a51b918, 8c47a657-3a97-447a-ad51-7c660a332bc8, 98a252f7-f19d-464d-a277-ff3f5c3a6dfd]
66085 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet
66099 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet
66100 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet => [9d667a96-d0ed-4586-acda-1bb77e68edf3, ad70a196-ddef-4053-aebf-2fab09e02e17, b4c5f447-61e8-4da1-b0d7-d671ba9e457f, cd93ab4a-9efe-4dbc-bd6e-0e4193fbdcf5, e9dbf770-73d0-4df2-8895-4cb52e1596b0]
66118 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 40 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093156.parquet
66130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093156.parquet
66130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 40 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093156.parquet => [15a6c846-e460-4b95-89b0-555ace3ce6eb, 19a0c400-3c5e-42d3-b3c6-84c15ba9f2f3, 1c0e3f42-3a34-4eec-ada3-7e4851a34d83, 1f15e25e-2aba-4bc2-b4f6-2010eba92a7e, 1fccdc23-1734-4b51-8f5d-b65d4cbb7fb8, 220ece3a-8085-43f8-8db6-ca46e9d44079, 27f49bd0-2908-43e1-b98c-fb500f7e181b, 2e6b1f06-fa8f-4ca8-8ff0-dcc15b2bdd60, 2f0b15a2-8456-483a-8820-6fede5fedd7a, 31399832-8125-4ddf-9351-3407200a3ee6, 3c4b6419-99e9-484c-8cef-b5c251cbc5f5, 44196e37-1155-4bc2-b1e2-602715792f8f, 46aae5a2-15bb-41be-9aff-11b203dcb5a8, 4d000be1-3f24-494b-8a1f-81cdb74fd491, 4f6f3a29-26bb-4ac5-af18-ff85a55e8394, 5ea90d63-16dd-458f-8b2a-3be928f11950, 729912a3-de52-4d55-be46-c82a6ac4f21e, 749da547-5b91-4a32-bbdc-626d1aaa0de8, 7fa610e7-77c0-4a53-9391-804a32772688, 80190003-2597-4291-8507-0ff5ec7ada4e, 912b14a0-947e-45f7-990f-23cec23cf2bb, 97a1a173-549f-44a8-a800-c5b5bdd66095, 9b76ab42-a853-4e02-a6fc-a751e6c74203, a381fcb5-139e-41eb-9d35-27d730a9c5f7, a38ea72a-8d4d-44dd-b158-778bdf285617, ab8dc729-1c52-487e-ad4c-075fa6425d88, ad8e3b9a-48f5-4682-8132-38585ed413f3, aef7e22d-8396-44b7-b68e-932dde3aa1a3, b624f5f7-735a-4fef-8293-d74e6ddbf4a9, b73c8512-1cce-4de8-9bab-20728806caea, cb7f7bf2-b096-4c78-b486-94a34b00a1d1, cbb0c72d-bdda-4ca1-9f6d-0a9a9983e8ac, cdde1888-be95-4355-aa6c-c28f49b38278, d3d059f8-a1eb-4dcb-89ef-9924c01dd3b6, e0ddb2a2-baa6-4a3b-a2ef-91f3a1ccbc9b, e2b3fe8b-f8b7-4685-a79c-1d993539699f, ee496ae9-758b-4183-9f7c-1b2934048f3e, ef07449f-b924-4c86-86f9-35a9a24ce373, f2c1c128-c172-4b7c-b5b1-9b646afa6ece, f8350c2a-41b2-4a1c-a8f4-28effbb20cc8]
66228 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=91}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=40}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=26}}}
66245 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2700
66245 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
66266 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093158
66266 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093158
adoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34,858
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 111B for [_hoodie_commit_time] BINARY: 172 values, 37B raw, 57B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 36B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 555B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 485B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 172 values, 37B raw, 57B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 2 entries, 130B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [rider] BINARY: 172 values, 30B raw, 50B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 48B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 118B for [driver] BINARY: 172 values, 30B raw, 50B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 50B raw, 2B comp}
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,411B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,367B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,409B for [fare] DOUBLE: 172 values, 1,376B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:31:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,774
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 123B for [_hoodie_commit_time] BINARY: 156 values, 49B raw, 68B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 54B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 536B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 466B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 217B for [_hoodie_file_name] BINARY: 156 values, 49B raw, 68B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 195B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 127B for [rider] BINARY: 156 values, 42B raw, 61B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 72B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [driver] BINARY: 156 values, 42B raw, 61B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 75B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,285B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,290B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,246B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,285B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,291B for [fare] DOUBLE: 156 values, 1,248B raw, 1,247B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34,990
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 128B for [_hoodie_commit_time] BINARY: 172 values, 53B raw, 73B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 54B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 572B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 502B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 222B for [_hoodie_file_name] BINARY: 172 values, 53B raw, 73B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 195B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 133B for [rider] BINARY: 172 values, 46B raw, 66B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 72B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 135B for [driver] BINARY: 172 values, 46B raw, 66B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 75B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,401B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,405B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,403B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,359B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,411B for [fare] DOUBLE: 172 values, 1,376B raw, 1,367B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 34,990
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 128B for [_hoodie_commit_time] BINARY: 172 values, 53B raw, 73B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 54B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 591B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 521B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 222B for [_hoodie_file_name] BINARY: 172 values, 53B raw, 73B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 195B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStor66741 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
66741 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
66854 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
66854 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
66935 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
67090 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
67097 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093158 as complete
67098 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093158
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093158.parquet; isDirectory=false; length=450450; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093156.parquet; isDirectory=false; length=450343; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_0_20180316093152.parquet; isDirectory=false; length=450138; replication=1; blocksize=33554432; modification_time=1521189113000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet; isDirectory=false; length=450436; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet; isDirectory=false; length=450329; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093152.parquet; isDirectory=false; length=450138; replication=1; blocksize=33554432; modification_time=1521189113000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093158.parquet; isDirectory=false; length=449090; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093156.parquet; isDirectory=false; length=448977; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_2_20180316093152.parquet; isDirectory=false; length=448807; replication=1; blocksize=33554432; modification_time=1521189113000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
67457 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
68342 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093202
68573 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
68832 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 88
68832 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
68990 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
69099 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093158.parquet
69122 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093158.parquet
69122 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093158.parquet => [020326c3-bfd8-431b-8f17-4204ea43ea6c, 10572eee-6df7-41d9-9c5e-2aec84fedb16, 19e8f13d-a17c-42e0-af78-fe1289b44725, 24d2f1c6-fbac-4b20-a630-ca3a56156a6f, 2b512b69-bffd-4c48-b2fe-ea565a04ef72, 2c71be86-88d2-4697-b382-5b1026c9d606, 503fb85f-9858-48a3-a8e9-e00469d50a72, 5aceca14-f17f-46f7-8da4-b45eef04a588, 5fb026cd-0087-4a40-bee8-991b6b305994, 6aefa54b-dfc2-4e30-9ad5-911dd751e423, 83ebe0a3-4ed1-435a-93d9-5d78a42945b6, b04eda4c-efa2-4ad4-9b9d-0f58e03071c9, c163f1ac-0ce7-42e3-a517-e773c76e4dc5, c30b86fa-3400-4210-8f1f-97c7043a0adb, c70f2e39-24f8-425f-913b-8ff58451344a, c9469eb0-3efe-4fcc-8fc8-8963247d0c46, ce55981d-01f5-467f-8092-42353fe926c3, d493e88c-8f61-4ed0-8f66-6b971b99492b, ebb9b0bf-2cd6-41af-8969-8461c324639c, f29f7851-7f1d-4c06-8446-f4002fc811ee, f71f08f1-4573-4e88-893d-a11a21f9fdf4, f98ae4d3-2e9f-4ba3-8eac-a4102e79aef8, fcaeeb95-e555-4825-b9c3-c6f3cd21bcf5, fff2f929-1905-408e-b4fd-68231bdd22c2]
69140 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet
69154 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet
69154 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet => [029f12d0-8622-4f3e-b8c9-aa9b508ff441, 04ace1c4-368e-4eb5-bf61-160ff8c0a7c3, 205012bb-90b3-4e61-a247-113479133c44, 2f14d6e1-86a2-4c30-805e-9bd85b7ef0ef, 33f29468-a75f-4930-b04b-fd752caa680f, 3f211fe0-9c8f-4041-8eea-e2ead83b97d4, 47be2f06-c3b1-458f-82f1-090fdcc0e72b, 4c14e90d-0875-4726-bca3-4a6b046c36b2, 52268886-01a0-4227-aa9c-6f0c99c27cbe, 56bdad96-01c4-4f6c-b7d8-3be00c53d465, 5a17b170-229e-4d9c-92b7-6458d9bff92e, 6072876e-1206-498c-a5f4-bf6df96a1790, 8a29f96c-63af-4558-8cdb-bdaf8a51b918, 98aef5e0-668c-4416-9e5b-e6ed2a9a9f13, 9af5c3ac-47b1-44b3-b438-a0f297340da9, 9c4f40b5-4cba-48d2-9e38-44cba7b3ca2a, a8ca9235-25c0-4bd9-8f65-264af60debe1, acca0792-6dc3-4609-9a3c-0f8588f1c298, b481b18f-9ec3-4677-b31c-8086c47544a5, b65048a0-a49f-47bd-b480-307e181a006c]
69182 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet
69197 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet
69197 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet => [b6a4d80e-70a5-468b-8c99-6fa33fac221a, c0682be8-3fb4-47be-97ee-2b107b97b761, cd93ab4a-9efe-4dbc-bd6e-0e4193fbdcf5, d69e8e1e-3e9f-4796-9838-719bb4743ebb, ded6eb21-c1c2-4dff-ac43-b6dae77a4d8a, e3292f02-0167-48a3-8a57-d9aef3f1e12e, e4a9c79f-a7a9-415c-8012-57f254c12e9d, e80f2d23-8caa-4139-9b1d-cf03fff56e74, fbbd2867-bcd0-4128-a662-9829b9af60af]
69214 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093158.parquet
69226 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093158.parquet
69226 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093158.parquet => [0ccc7753-df13-413f-9e06-cd75946e41df, 104746da-d08b-48d1-b6d4-e8f01200f7d9, 16c3fc89-58d1-4d44-977f-071c57925612, 1acbf49c-d8e8-4ea6-b023-7871ab6630c4, 1d374fd4-c686-4332-9b79-d63e72f7c62f, 1faea1ba-386a-49d7-99de-0f3037bba1e2, 1fccdc23-1734-4b51-8f5d-b65d4cbb7fb8, 21f8a5db-31b2-4022-b259-6262c1ed0ea3, 2c5a6d0c-6b08-4aa6-8e79-78c5e6bddf21, 2d7fdb8d-4ae9-4914-a7f8-661315d1472a, 2e6b1f06-fa8f-4ca8-8ff0-dcc15b2bdd60, 2f2ce57c-d2f7-4373-9cda-6daaac98ab09, 323b43b6-5fcf-4079-aba9-5be8dbe7655c, 3c4b6419-99e9-484c-8cef-b5c251cbc5f5, 4ad3e31b-5f41-45f3-89e6-2ea009e6a3b0, 50ad6914-88c4-44f1-a09d-43cbb568f950, 5af7f1af-35fe-470c-ab2c-d5cc731b9b82, 5ea90d63-16dd-458f-8b2a-3be928f11950, 647b7480-20d4-4f64-b68b-55745a46b625, 655c6b50-8e44-4b2f-9b34-f972c0f5eb34, 6f1e083a-fe73-4340-a8ff-a40531de08bd, 7538089c-5374-4924-8b9a-ebfe25bb7933, 80190003-2597-4291-8507-0ff5ec7ada4e, 8f87c002-cea0-49c0-9fa4-ccf46e9ed03b, 912b14a0-947e-45f7-990f-23cec23cf2bb, 9f254d61-223a-46b0-8753-c1289f3ba1db, aad6ae8a-5345-484e-8009-803351c73f41, b71d791f-bf01-43bc-9f54-184c6ab6ceeb, bd9a711a-664c-4bf9-b066-9bfa8c50f041, c39172cb-4307-4d77-93f8-02d8a6814d22, c48e62f7-bf01-4837-a51a-6c987a9a4802, c89740b0-be3a-435d-955f-d19f62318442, cbb0c72d-bdda-4ca1-9f6d-0a9a9983e8ac, d74df7a2-7e4a-430f-88e9-d02ececb201e, ec520e14-61c4-4bd6-aaf8-d12fc852f3e9]
69335 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=88}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=24}}}
69352 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2700
69352 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
69367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093202
69367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093202
e: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 133B for [rider] BINARY: 172 values, 46B raw, 66B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 72B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 135B for [driver] BINARY: 172 values, 46B raw, 66B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 75B raw, 3B comp}
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,405B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:00 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,403B for [fare] DOUBLE: 172 values, 1,376B raw, 1,359B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 156
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:02 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,906
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 125B for [_hoodie_commit_time] BINARY: 156 values, 49B raw, 70B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 4 entries, 72B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 571B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 501B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 219B for [_hoodie_file_name] BINARY: 156 values, 49B raw, 70B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 4 entries, 260B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [rider] BINARY: 156 values, 42B raw, 63B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 96B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 131B for [driver] BINARY: 156 values, 42B raw, 63B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 100B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,286B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,291B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,247B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,285B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,241B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:369836 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
69836 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
69864 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
69995 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
69995 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
70278 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
70285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093202 as complete
70285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093202
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093202.parquet; isDirectory=false; length=450505; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093158.parquet; isDirectory=false; length=450450; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093156.parquet; isDirectory=false; length=450343; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet; isDirectory=false; length=450479; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet; isDirectory=false; length=450436; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093156.parquet; isDirectory=false; length=450329; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093202.parquet; isDirectory=false; length=449154; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093158.parquet; isDirectory=false; length=449090; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093156.parquet; isDirectory=false; length=448977; replication=1; blocksize=33554432; modification_time=1521189117000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
70799 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
71496 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093205
71881 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
72072 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 91
72072 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
72278 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 22 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093202.parquet
2:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [fare] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,122
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [_hoodie_commit_time] BINARY: 172 values, 53B raw, 74B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 4 entries, 72B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 590B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 520B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 223B for [_hoodie_file_name] BINARY: 172 values, 53B raw, 74B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 4 entries, 260B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 134B for [rider] BINARY: 172 values, 46B raw, 67B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 96B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 136B for [driver] BINARY: 172 values, 46B raw, 67B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 100B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,400B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,356B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,403B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,359B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,400B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,356B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,412B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,368B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,409B for [fare] DOUBLE: 172 values, 1,376B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,122
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [_hoodie_commit_time] BINARY: 172 values, 53B raw, 74B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 4 entries, 72B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 617B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 547B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 223B for [_hoodie_file_name] BINARY: 172 values, 53B raw, 74B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 4 entries, 260B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 134B for [rider] BINARY: 172 values, 46B raw, 67B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 96B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 136B for [driver] BINARY: 172 values, 46B raw, 67B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 100B raw, 4B comp}
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [fare] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:05 AM INFO: org.a72293 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093202.parquet
72293 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 22 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093202.parquet => [020326c3-bfd8-431b-8f17-4204ea43ea6c, 0c728648-e9b8-45c7-994e-3760417c8071, 1a7445a7-10e1-4dcd-81b7-b5ec63b5c202, 1d1c5fe8-613e-4314-87d3-df17f8f8e2b0, 2865220c-dd20-45ca-a1fb-ff9ae207f313, 2b4abe5b-a912-4aec-85b8-8fda90d82568, 4229dfa5-7cfd-4aa1-b1b4-bde7f8598203, 4fd26ee6-45c9-4470-8b72-444f5d9c9130, 5719f1ab-4e91-4b95-b818-0991493fadc9, 6d53eedc-b6fc-437c-8d48-fa6754fa4acd, 6fe593c4-9af0-4e44-9e93-4d38335a56b5, 85b9c5ff-0f77-4565-b189-f969e02304d9, 92638277-04fc-4742-8cbf-5b59f5c8060d, a05e96e6-fb99-4215-b265-004d5851af7a, b56b20c3-3f50-429c-b816-dd702aa24f37, c163f1ac-0ce7-42e3-a517-e773c76e4dc5, c9469eb0-3efe-4fcc-8fc8-8963247d0c46, dd16ae1f-e735-4601-a70b-1f7d2c5ba081, e879ac53-043b-4f22-a793-ee9841204508, eeaab3a3-a020-44d7-bda8-d378ff7ceba5, f0e395a2-a329-4e83-9f39-2c8597493375, fff2f929-1905-408e-b4fd-68231bdd22c2]
72311 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet
72323 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet
72323 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet => [04ace1c4-368e-4eb5-bf61-160ff8c0a7c3, 0af75466-88ee-486c-8925-9be98f84b926, 103f20dc-eeac-46ba-876d-f965d2ae5a93, 205012bb-90b3-4e61-a247-113479133c44, 2840f20a-1a6f-4ada-9172-b700af932462, 29564ab3-b15f-435a-9f98-b895f76582d5, 3ea33cba-a6aa-4111-94b5-6fb9dcd467aa, 3f9896cc-07b7-4920-88ca-aeccb762c25f, 4b72b75f-5f74-445b-96a0-15c2c7e5d0ee, 4bba50e3-5664-49f9-9105-86cd330eee25, 52268886-01a0-4227-aa9c-6f0c99c27cbe, 571d910e-1b55-454d-a916-fde8efcf0d1e, 57cfb488-b347-47ed-8f77-1aa0e6e5bf03, 6072876e-1206-498c-a5f4-bf6df96a1790, 62204fa4-2cbe-4fe9-a84c-d1348459aae7, 626972d5-b16f-4d21-a07d-75eece5daf17, 6b179b4a-948c-4379-9eb8-0feb28443eed, 6d584ec2-b1a0-4bdb-b8a9-9193adc32427, 6e3fba9b-a8f8-4970-b066-2b4c3441e00a, 6e4c1dad-e875-47f1-b501-b8d4faa8df71, 7786f06c-92d7-4ca2-b31c-d10cb1f56692, 90f55f8a-ec82-4000-8b68-b20eec26f483, 9bb60400-d0f3-460c-bf58-9d767816b030, 9d667a96-d0ed-4586-acda-1bb77e68edf3]
72355 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet
72370 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet
72371 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet => [ab1d4f46-fc9c-4da5-98e5-26b9ccb3e364, ac4137b3-fe6c-45fb-81de-2538fdca8ac8, ad70a196-ddef-4053-aebf-2fab09e02e17, add04199-4744-481e-aaee-0bdcbdc01f63, b240b265-43a6-48d1-92d2-1eefe0db0193, b65048a0-a49f-47bd-b480-307e181a006c, ccc86ddd-7f16-4c7c-986b-9fe94e18f191, cd93ab4a-9efe-4dbc-bd6e-0e4193fbdcf5, e759754f-378e-4ac2-924f-85d574f42d29, fa413d44-210b-450f-8fc4-2ea8f79872a6]
72389 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093202.parquet
72404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093202.parquet
72405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093202.parquet => [0ab0676d-dd9e-4a29-af4c-0ff73956d8e4, 1faea1ba-386a-49d7-99de-0f3037bba1e2, 21f8a5db-31b2-4022-b259-6262c1ed0ea3, 38e01b13-f504-4b0e-adac-60cf3bb3dec2, 4077b110-796f-4302-b5f9-f586cd246a7c, 451aae7a-8a38-420b-aeee-4030557042fb, 46aae5a2-15bb-41be-9aff-11b203dcb5a8, 4ad3e31b-5f41-45f3-89e6-2ea009e6a3b0, 4b06d92e-3379-42e6-8432-56bd9930b053, 4f6f3a29-26bb-4ac5-af18-ff85a55e8394, 5fed515a-328b-495d-af84-cb7fa1fc37e9, 719364d0-6463-4980-962c-63a6dafc397d, 912b14a0-947e-45f7-990f-23cec23cf2bb, 9b76ab42-a853-4e02-a6fc-a751e6c74203, 9f254d61-223a-46b0-8753-c1289f3ba1db, aad6ae8a-5345-484e-8009-803351c73f41, ad8e3b9a-48f5-4682-8132-38585ed413f3, b43a8885-4fc4-42ab-bc52-a98687f91056, b4463655-49e8-4b7f-ac08-00a91b88dbf6, b4e2d0bc-1c53-4995-a8e0-42a9930612f9, bd9a711a-664c-4bf9-b066-9bfa8c50f041, bfa2681d-8081-4a58-9af6-31b6ca7d68ec, c39172cb-4307-4d77-93f8-02d8a6814d22, c3a0c82a-c229-4d9d-b43e-bb269b2071d3, c87312ee-2b8c-4bdb-bc5e-d76cc138d582, cbcc6e24-636d-45e8-ac76-5ae3b388ee4d, d17a5da8-bf0d-47af-8210-f0284127c5c3, d37265f9-d2f5-4598-b0ac-1cd123023822, d3d059f8-a1eb-4dcb-89ef-9924c01dd3b6, dba0988a-285f-4611-bd56-dc46c55b1a1a, de9a548a-2615-4bee-8e0d-1eb5a8fdf3c8, e2b3fe8b-f8b7-4685-a79c-1d993539699f, f2c1c128-c172-4b7c-b5b1-9b646afa6ece, f3f3fa0c-d1ba-45ab-b6e5-67e810128218, f94fee59-5818-43e3-b3f1-9e9b2f323eb6]
72487 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=91}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=34}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=22}}}
72500 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
72500 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
72513 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093205
72513 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093205
72776 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
pache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 156
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 32,038
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 146B for [_hoodie_commit_time] BINARY: 156 values, 69B raw, 90B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 90B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 591B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 521B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 240B for [_hoodie_file_name] BINARY: 156 values, 69B raw, 90B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 325B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 150B for [rider] BINARY: 156 values, 62B raw, 83B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 120B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [driver] BINARY: 156 values, 62B raw, 83B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 125B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,289B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,245B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,286B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,284B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,240B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [fare] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,254
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 96B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 90B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 603B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 533B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 172950 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
72950 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
73112 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
73112 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
73356 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
73366 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
73372 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093205 as complete
73372 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093205
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093205.parquet; isDirectory=false; length=450630; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093202.parquet; isDirectory=false; length=450505; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093158.parquet; isDirectory=false; length=450450; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet; isDirectory=false; length=450598; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet; isDirectory=false; length=450479; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093158.parquet; isDirectory=false; length=450436; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093205.parquet; isDirectory=false; length=449267; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093202.parquet; isDirectory=false; length=449154; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093158.parquet; isDirectory=false; length=449090; replication=1; blocksize=33554432; modification_time=1521189120000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
74106 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
74591 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093208
75308 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 91
75308 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
75372 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
75379 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
75515 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093205.parquet
75531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093205.parquet
75531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093205.parquet => [020326c3-bfd8-431b-8f17-4204ea43ea6c, 09446258-a554-4093-bba9-b0bf8225bf4c, 0eb4429f-ffec-4091-96ff-2c6fdbc38997, 10572eee-6df7-41d9-9c5e-2aec84fedb16, 1d1c5fe8-613e-4314-87d3-df17f8f8e2b0, 2399dbb9-4d7a-4939-951d-28ac673de1fc, 2865220c-dd20-45ca-a1fb-ff9ae207f313, 2b512b69-bffd-4c48-b2fe-ea565a04ef72, 3fad5d1e-366e-4c82-ae71-e4459d071513, 4229dfa5-7cfd-4aa1-b1b4-bde7f8598203, 52064e42-a201-42f0-8347-b94d893f1f43, 5719f1ab-4e91-4b95-b818-0991493fadc9, 5f1d577a-7227-4cd6-9904-a58059dae30f, 6d53eedc-b6fc-437c-8d48-fa6754fa4acd, 703dbd45-b56b-49c9-b806-1efd4b31b2c1, 79367b7c-ab15-451f-a026-898e82d8a51e, 83ebe0a3-4ed1-435a-93d9-5d78a42945b6, a02c64f8-c0e5-4cc2-8511-65be6d484853, a7036f88-ca64-44dd-acf3-7646d4343b8b, ad0f3b1e-40ad-4166-95db-335a54596764, b2b2ecec-015b-49a3-870c-5fc12d738db9, be98eeba-c88f-4b76-942a-05aaec27e60d, c9469eb0-3efe-4fcc-8fc8-8963247d0c46, dd16ae1f-e735-4601-a70b-1f7d2c5ba081, e39c2703-dc76-4bf1-bf3a-ec955d502d53, eeaab3a3-a020-44d7-bda8-d378ff7ceba5, ef28216d-b9c6-4df9-b79e-b056e459420b, f79fab88-93e0-40a0-856d-c74fc2e90be0, ffff1c80-3282-45ac-b0f2-1ccd0b4b51eb]
75548 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet
75566 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet
75566 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet => [0ac63248-313a-4330-a5fc-7aa97ddaab1d, 16ddab03-49b8-498e-9064-64a334711670, 29564ab3-b15f-435a-9f98-b895f76582d5, 2a4b47f0-3011-4f4f-a4b1-6987fb0bab72, 2a92f19d-3b41-48a6-bf2e-d912705bd68b, 3027605d-4305-4fc2-bd76-fa08b19acc52, 382aa504-a7fa-4f7c-b958-37ee74227902, 3c15081f-83c1-48a7-92d8-a2972f8c629e, 3ea33cba-a6aa-4111-94b5-6fb9dcd467aa, 3f9896cc-07b7-4920-88ca-aeccb762c25f, 477a1363-c289-46bd-bf80-2fa1c0d65a43, 47be2f06-c3b1-458f-82f1-090fdcc0e72b, 4faa5f83-f716-463a-aeb1-c63f441d2bfa, 5178f3c9-55c7-4e3b-8c63-ed0d93b3a99e, 56bdad96-01c4-4f6c-b7d8-3be00c53d465, 62204fa4-2cbe-4fe9-a84c-d1348459aae7, 895c2ef0-5cc9-4983-852d-7bf2ce65d3b3]
75608 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet
6, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 246B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 96B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 325B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [rider] BINARY: 172 values, 68B raw, 89B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 120B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [driver] BINARY: 172 values, 68B raw, 89B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 125B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,399B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,355B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,403B for [fare] DOUBLE: 172 values, 1,376B raw, 1,359B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,254
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 96B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 90B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 636B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 566B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 246B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 96B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 325B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [rider] BINARY: 172 values, 68B raw, 89B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 120B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [driver] BINARY: 172 values, 68B raw, 89B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 125B raw, 5B comp}
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,401B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,411B for [fare] DOUBLE: 172 values, 1,376B raw, 1,367B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 156
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initializ75620 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet
75620 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet => [8a29f96c-63af-4558-8cdb-bdaf8a51b918, 98a252f7-f19d-464d-a277-ff3f5c3a6dfd, 98aef5e0-668c-4416-9e5b-e6ed2a9a9f13, 9b24f412-b5b2-400c-9668-698c9c0d39ce, 9d667a96-d0ed-4586-acda-1bb77e68edf3, ac4137b3-fe6c-45fb-81de-2538fdca8ac8, add04199-4744-481e-aaee-0bdcbdc01f63, b240b265-43a6-48d1-92d2-1eefe0db0193, be772702-7e1a-440b-ac9a-3a55fd77dc3a, e3292f02-0167-48a3-8a57-d9aef3f1e12e, ec4e4ad5-6494-4b22-b84c-ca3db13c96f0, f1a5337d-f626-4251-a6b6-ed3ab84712c8, fe28a20a-67dc-4f86-92eb-72ff992241f9, ff5bf9cc-291f-4895-b85c-99f1a937cec7]
75640 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093205.parquet
75651 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093205.parquet
75651 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093205.parquet => [0ab01ce6-e422-4b4d-8b1d-853f01d291a1, 0ab0676d-dd9e-4a29-af4c-0ff73956d8e4, 15a6c846-e460-4b95-89b0-555ace3ce6eb, 16af9aec-48aa-4b44-b470-14d72f569e52, 19222e3a-07f0-49bb-8ba0-e27c1aed82cd, 1acbf49c-d8e8-4ea6-b023-7871ab6630c4, 220ece3a-8085-43f8-8db6-ca46e9d44079, 46aae5a2-15bb-41be-9aff-11b203dcb5a8, 4b06d92e-3379-42e6-8432-56bd9930b053, 68d77d36-93ea-43d0-9b88-334ea342cd45, 6b8ab587-4b34-4f14-b852-7933a7253037, 6f1e083a-fe73-4340-a8ff-a40531de08bd, 75749a1b-ba95-4405-9993-b50b4658ee20, 8ccf2fb8-e934-4b4f-a8fc-35bc89ed145c, 8f87c002-cea0-49c0-9fa4-ccf46e9ed03b, a106baa0-53f0-42e0-9dac-4b870a54dfb3, a10beff2-d3f7-4ac0-acf5-c2694c2a7dab, ac889602-c285-45ed-ad5b-1acf7e505c7a, b73c8512-1cce-4de8-9bab-20728806caea, b911c3fd-796e-4809-98da-474016cab53c, bfa2681d-8081-4a58-9af6-31b6ca7d68ec, c0af065d-9469-4678-8832-178ae9579b36, c2386888-c3f9-4179-8edf-b4bd025c7cc5, c48e62f7-bf01-4837-a51a-6c987a9a4802, cf4de964-3150-4b91-8618-4838b2505866, d37265f9-d2f5-4598-b0ac-1cd123023822, d74df7a2-7e4a-430f-88e9-d02ececb201e, d7b4c332-6eb0-4177-a191-903322f4d98d, e6978cf2-69f5-4627-8db9-f8746db06318, f4330bcf-daea-4d39-88af-095fde078d45, faa010ea-a276-454f-a05c-964275c02c92]
75725 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=91}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=29}}}
75744 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
75744 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
75759 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093208
75759 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093208
ed will read a total of 172 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 32,170
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 146B for [_hoodie_commit_time] BINARY: 156 values, 69B raw, 90B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 6 entries, 108B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 604B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 534B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 240B for [_hoodie_file_name] BINARY: 156 values, 69B raw, 90B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 6 entries, 390B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 150B for [rider] BINARY: 156 values, 62B raw, 83B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 6 entries, 144B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [driver] BINARY: 156 values, 62B raw, 83B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 6 entries, 150B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,286B for [fare] DOUBLE: 156 values, 1,248B raw, 1,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,386
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 6 entries, 108B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 636B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 566B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 6 entries, 390B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [rider] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 6 entries, 144B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 161B for [driver] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 6 entries, 150B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,399B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,355B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [begin_lo76136 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
76136 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
76323 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
76323 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
76507 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
76512 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093208 as complete
76513 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093208
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093208.parquet; isDirectory=false; length=450676; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093205.parquet; isDirectory=false; length=450630; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093202.parquet; isDirectory=false; length=450505; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet; isDirectory=false; length=450646; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet; isDirectory=false; length=450598; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093202.parquet; isDirectory=false; length=450479; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093208.parquet; isDirectory=false; length=449298; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093205.parquet; isDirectory=false; length=449267; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093202.parquet; isDirectory=false; length=449154; replication=1; blocksize=33554432; modification_time=1521189123000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
76655 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
77268 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
77680 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093211
77813 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
78083 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 92
78084 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
78285 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093208.parquet
78300 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093208.parquet
78300 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093208.parquet => [020326c3-bfd8-431b-8f17-4204ea43ea6c, 09446258-a554-4093-bba9-b0bf8225bf4c, 0ec348b9-ff9a-4679-9cd1-cd9b91c80a28, 179a8e58-54ca-4bb4-95b4-50d7360ded67, 1a7445a7-10e1-4dcd-81b7-b5ec63b5c202, 1cf8e5ac-a2be-47d6-bb45-b5195e3dd8f7, 2c71be86-88d2-4697-b382-5b1026c9d606, 3ee8a1e8-3ff7-4965-88ec-0c59e010ed65, 3fad5d1e-366e-4c82-ae71-e4459d071513, 46759a43-1590-4265-bbff-9a62485ad279, 4b824948-496f-4d28-959b-75430d232e21, 52064e42-a201-42f0-8347-b94d893f1f43, 6aefa54b-dfc2-4e30-9ad5-911dd751e423, 6d53eedc-b6fc-437c-8d48-fa6754fa4acd, 79367b7c-ab15-451f-a026-898e82d8a51e, 8ba86831-b522-4dde-90be-8453272be507, 92638277-04fc-4742-8cbf-5b59f5c8060d, c163f1ac-0ce7-42e3-a517-e773c76e4dc5, c3a652c2-8991-45cf-91c8-a2277a59b614, c9469eb0-3efe-4fcc-8fc8-8963247d0c46, df3e79eb-85a8-4238-bb33-425bf95b99fa, e39c2703-dc76-4bf1-bf3a-ec955d502d53, ebb9b0bf-2cd6-41af-8969-8461c324639c, f71f08f1-4573-4e88-893d-a11a21f9fdf4, fba343d4-2561-4b20-b81c-6c2ad8885023, fcaeeb95-e555-4825-b9c3-c6f3cd21bcf5]
78317 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet
78332 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet
78332 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet => [0843a3a3-ca98-45ed-914a-465fa4923d89, 0b516f2f-ac76-483d-8573-0a42b06d9dea, 16ddab03-49b8-498e-9064-64a334711670, 23ef3f71-9873-48fb-a635-e0a6dd9bba43, 4069f82e-c422-4293-98b7-3fe3a7dd6e3a, 571d910e-1b55-454d-a916-fde8efcf0d1e, 6072876e-1206-498c-a5f4-bf6df96a1790, 62204fa4-2cbe-4fe9-a84c-d1348459aae7, 626972d5-b16f-4d21-a07d-75eece5daf17, 6b179b4a-948c-4379-9eb8-0feb28443eed, 7786f06c-92d7-4ca2-b31c-d10cb1f56692, 7d4a4173-f05a-419a-b035-72ef7d4de6e7, 83ec1164-c3ca-41e0-986f-3329bcf6475b, 98a252f7-f19d-464d-a277-ff3f5c3a6dfd, 98aef5e0-668c-4416-9e5b-e6ed2a9a9f13, adc4c912-c7c7-4b47-86fa-06425f40b32a, b2a08b00-76d9-466a-9959-3d7b058fd4a5, be772702-7e1a-440b-ac9a-3a55fd77dc3a, cb3bf892-a0a3-4901-9959-ce58268d0256, d4c14ee0-a7d2-4bc5-b235-68587370969c]
78360 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 8 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet
78373 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet
78373 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 8 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet => [ddec5848-d08c-4c3c-a218-6a0ca0636381, ded6eb21-c1c2-4dff-ac43-b6dae77a4d8a, e8c61a1a-6022-4f4e-8c74-bb60b777317c, ec4e4ad5-6494-4b22-b84c-ca3db13c96f0, f2414ab4-b092-46d6-a24e-d573ab73b9eb, f2940043-29dd-4400-9cf6-27be5b68c8a1, f56fd01a-4c94-46c4-8c5a-fb6cdb3eaeee, faee66b6-1620-4db2-8b03-5d6793c437de]
78389 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093208.parquet
78404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093208.parquet
78404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093208.parquet => [0ab01ce6-e422-4b4d-8b1d-853f01d291a1, 15a6c846-e460-4b95-89b0-555ace3ce6eb, 19a0c400-3c5e-42d3-b3c6-84c15ba9f2f3, 1d374fd4-c686-4332-9b79-d63e72f7c62f, 282eff2a-93a2-4046-86d6-3d0adcf39e27, 2d7fdb8d-4ae9-4914-a7f8-661315d1472a, 2daed088-3534-4c1e-a09b-9fad3d721f8f, 2e6b1f06-fa8f-4ca8-8ff0-dcc15b2bdd60, 33436bc7-94ed-4aca-badc-2f75741f4214, 46aae5a2-15bb-41be-9aff-11b203dcb5a8, 4b6a3c83-4048-401f-a55b-3b90ca6311af, 5fed515a-328b-495d-af84-cb7fa1fc37e9, 6467ed19-9f10-408f-a66d-eaae4c520a21, 68d77d36-93ea-43d0-9b88-334ea342cd45, 719364d0-6463-4980-962c-63a6dafc397d, 729912a3-de52-4d55-be46-c82a6ac4f21e, 7348e58b-49a2-4170-b0bb-18fd0af498ae, 749da547-5b91-4a32-bbdc-626d1aaa0de8, 813b65aa-82c4-4722-a658-26c0000ffc8e, 88036808-f88f-4257-8a33-a2d51943b682, 9e86cf0b-539b-40a0-9335-f7a28b1ec25a, a381fcb5-139e-41eb-9d35-27d730a9c5f7, b43a8885-4fc4-42ab-bc52-a98687f91056, b911c3fd-796e-4809-98da-474016cab53c, bef0df03-699f-4c12-89d0-e53c291df9bc, c3a0c82a-c229-4d9d-b43e-bb269b2071d3, c42098cb-3409-4f01-a538-d5e93987dd66, c7f389ae-6ceb-42e6-ad0d-6541f8dfd5dd, c87312ee-2b8c-4bdb-bc5e-d76cc138d582, c89740b0-be3a-435d-955f-d19f62318442, cf4de964-3150-4b91-8618-4838b2505866, dba0988a-285f-4611-bd56-dc46c55b1a1a, de9a548a-2615-4bee-8e0d-1eb5a8fdf3c8, e71eac95-3917-4621-a6ec-a79f1871e2fb, eaafb0ba-f4b6-450d-84c8-3300e2f6a628, ef07449f-b924-4c86-86f9-35a9a24ce373, f3f3fa0c-d1ba-45ab-b6e5-67e810128218, f94fee59-5818-43e3-b3f1-9e9b2f323eb6]
78496 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=92}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=38}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=26}}}
78509 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
78509 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
78526 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093211
78526 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093211
78613 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
n] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,402B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,358B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [fare] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,386
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 6 entries, 108B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 647B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 577B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 6 entries, 390B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [rider] BINARY: 172 values, 68B raw, 89B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 6 entries, 144B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [driver] BINARY: 172 values, 68B raw, 89B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 6 entries, 150B raw, 6B comp}
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,415B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,371B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,409B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:09 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [fare] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 32,302
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 148B for [_hoodie_commit_time] BINARY: 156 values, 69B raw, 92B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 126B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 615B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 545B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 242B for [_hoodie_file_name] BINARY: 156 values, 69B raw, 92B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 455B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [rider] BINARY: 156 values, 62B raw, 85B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 168B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [driver] BINARY: 156 values, 62B raw, 85B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 175B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,286B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,289B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,245B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,286B for [fare] DOUBLE: 156 values, 1,248B raw, 1,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,518
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 126B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 581B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 455B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [rider] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 168B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 161B for [driver] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 175B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,401B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,400B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,356B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [fare] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,518
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings78948 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
78949 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
79115 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
79115 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
79352 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
79357 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093211 as complete
79358 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093211
79439 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093211.parquet; isDirectory=false; length=450685; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093208.parquet; isDirectory=false; length=450676; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093205.parquet; isDirectory=false; length=450630; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet; isDirectory=false; length=450687; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet; isDirectory=false; length=450646; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093205.parquet; isDirectory=false; length=450598; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093211.parquet; isDirectory=false; length=449332; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093208.parquet; isDirectory=false; length=449298; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093205.parquet; isDirectory=false; length=449267; replication=1; blocksize=33554432; modification_time=1521189126000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
79888 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
80541 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093214
80863 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
80931 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 92
80931 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
81151 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 32 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093211.parquet
81167 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093211.parquet
81167 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 32 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093211.parquet => [162babd4-0ba1-4a2d-9f1c-d0e950dcff40, 179a8e58-54ca-4bb4-95b4-50d7360ded67, 1d1c5fe8-613e-4314-87d3-df17f8f8e2b0, 1edbf63b-ad6e-46da-ba26-71df475253fb, 20cd2599-d3b3-4a3d-b82c-264873039c7d, 2865220c-dd20-45ca-a1fb-ff9ae207f313, 2fe09b22-c460-4e61-9fcb-38641628a335, 3bf77a62-ca50-4cfb-a666-d53d31c0f39a, 4adf4fc9-5dfd-42ce-a7bc-968f01efc210, 4c200b38-c805-4156-b226-c1d44e81c344, 4e1bf410-7e86-4795-bab4-9fee6f45df4f, 6326287c-9cf5-4ef1-b071-10b7b2b472eb, 649010b7-2165-4acf-8910-1fb3b2cd5296, 6face862-8c1f-4b24-a2ce-80e7807c4323, 6fe593c4-9af0-4e44-9e93-4d38335a56b5, 83bb8b00-0d60-431d-9b36-10d4a0c14013, 8cd16c2f-35f5-468f-99c6-59de2d404978, 92638277-04fc-4742-8cbf-5b59f5c8060d, 94e87868-bd6b-4e67-b54f-34663bc9e7eb, 959f36c8-29be-4081-9926-6f3dacbfad8b, 9c3a1ee1-af81-4a8c-a36f-0220bcda32bf, 9efa0089-e3eb-4209-b89b-3139c794cea4, a05e96e6-fb99-4215-b265-004d5851af7a, ad0f3b1e-40ad-4166-95db-335a54596764, c163f1ac-0ce7-42e3-a517-e773c76e4dc5, d7988ef8-3dea-483d-88e1-cf8b042e43c7, dc14c210-00ee-4a76-a732-51d2906c3854, dd16ae1f-e735-4601-a70b-1f7d2c5ba081, e39c2703-dc76-4bf1-bf3a-ec955d502d53, e7871974-03de-4750-9c12-5c25a2d54b6e, ed11c3aa-cb7c-44bf-aafe-866139da6891, f61a3494-d022-4ab5-9622-49351ac42e27]
81186 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet
81200 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet
81200 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet => [0843a3a3-ca98-45ed-914a-465fa4923d89, 1525ff2c-e29d-420e-9819-436f98fc2c5d, 1b06541f-2e40-4133-a59a-d1dab564cd7f, 292ba420-13a7-481a-9ebd-d729c989da1d, 2a4b47f0-3011-4f4f-a4b1-6987fb0bab72, 33f29468-a75f-4930-b04b-fd752caa680f, 3cc6cd2f-20c2-4eba-b221-9cd723dd28da, 3f9896cc-07b7-4920-88ca-aeccb762c25f, 4c14e90d-0875-4726-bca3-4a6b046c36b2, 4faa5f83-f716-463a-aeb1-c63f441d2bfa, 6b179b4a-948c-4379-9eb8-0feb28443eed, 6dd6579d-f158-44cd-a37b-8067f058160a, 6e4c1dad-e875-47f1-b501-b8d4faa8df71, 7d4a4173-f05a-419a-b035-72ef7d4de6e7]
81229 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 16 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet
81240 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet
81240 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 16 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet => [852c36e2-2163-4969-b539-7b5ae2ce630b, 98a252f7-f19d-464d-a277-ff3f5c3a6dfd, 98aef5e0-668c-4416-9e5b-e6ed2a9a9f13, 9b24f412-b5b2-400c-9668-698c9c0d39ce, 9c4f40b5-4cba-48d2-9e38-44cba7b3ca2a, a8ca9235-25c0-4bd9-8f65-264af60debe1, aa9d03d9-7a90-4f8c-8635-575c05b2ea04, abd7cd56-065b-44e3-b00c-31ba7bf4f730, adc4c912-c7c7-4b47-86fa-06425f40b32a, b6a4d80e-70a5-468b-8c99-6fa33fac221a, ce238c6c-ebcc-4769-81d1-28c7081c09ae, d4c14ee0-a7d2-4bc5-b235-68587370969c, ddec5848-d08c-4c3c-a218-6a0ca0636381, e976807a-bd96-4ceb-b1fb-18d708c77ba3, f1a5337d-f626-4251-a6b6-ed3ab84712c8, fd298de1-ac18-4bd2-877c-7ee54e44d858]
81257 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 30 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093211.parquet
81267 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093211.parquet
81267 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 30 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093211.parquet => [0ae7f50c-dfb0-4372-ac6f-fb738c99b1ff, 16c3fc89-58d1-4d44-977f-071c57925612, 19222e3a-07f0-49bb-8ba0-e27c1aed82cd, 1e77fa9b-99be-4a2f-832d-52266441cd8a, 21f8a5db-31b2-4022-b259-6262c1ed0ea3, 2834c95e-de0b-4773-8eca-abc9434264e0, 4ad3e31b-5f41-45f3-89e6-2ea009e6a3b0, 4b06d92e-3379-42e6-8432-56bd9930b053, 4baca86b-c5e2-4e5c-8df3-64a49797179c, 50ad6914-88c4-44f1-a09d-43cbb568f950, 79c9e598-41d6-4c78-99e3-e255e344f269, 7b226b08-2159-4a06-87a7-206de1a3a898, 901e1338-1ece-4da7-af3f-5abffe75d365, 912b14a0-947e-45f7-990f-23cec23cf2bb, 9175aeb2-83a1-4393-97e8-ab0ee6c83aa6, 9b76ab42-a853-4e02-a6fc-a751e6c74203, a6ee8d33-c1bf-4746-b55b-edfb4fba51a2, ac889602-c285-45ed-ad5b-1acf7e505c7a, ad5445d3-75ff-4ecd-9d91-a04220d6360b, aef7e22d-8396-44b7-b68e-932dde3aa1a3, b43a8885-4fc4-42ab-bc52-a98687f91056, b4463655-49e8-4b7f-ac08-00a91b88dbf6, b8cc5d56-d7bc-4541-b054-dbb0324f4bfd, bef0df03-699f-4c12-89d0-e53c291df9bc, c74b492a-f195-4dc5-86e9-39855e2ceb90, cdde1888-be95-4355-aa6c-c28f49b38278, d17a5da8-bf0d-47af-8210-f0284127c5c3, dba0988a-285f-4611-bd56-dc46c55b1a1a, ebdf2582-2935-4dda-9f66-28c0c4e76e21, f8ef5ac9-8d4d-471a-8502-0d0d9232e6ec]
81343 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=92}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=32}}}
81355 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
81355 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
81367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093214
81367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093214
: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 126B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 656B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 586B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 455B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [rider] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 168B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 161B for [driver] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 175B raw, 7B comp}
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,405B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,404B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,360B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:12 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,407B for [fare] DOUBLE: 172 values, 1,376B raw, 1,363B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 156
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:14 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 156
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 32,434
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 148B for [_hoodie_commit_time] BINARY: 156 values, 69B raw, 92B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 144B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 631B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 561B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 242B for [_hoodie_file_name] BINARY: 156 values, 69B raw, 92B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 520B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B 81696 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [rider] BINARY: 156 values, 62B raw, 85B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 8 entries, 192B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [driver] BINARY: 156 values, 62B raw, 85B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 8 entries, 200B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,283B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,239B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,288B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,244B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,289B for [fare] DOUBLE: 156 values, 1,248B raw, 1,245B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,650
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 144B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 663B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 593B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 520B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [rider] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 8 entries, 192B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 161B for [driver] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 8 entries, 200B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,401B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,409B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,412B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,368B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,399B for [fare] DOUBLE: 172 values, 1,376B raw, 1,355B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,650
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_commit_time] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 144B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 677B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 607B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_hoodie_file_name] BINARY: 172 values, 75B raw, 98B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 520B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [rider] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 8 entries, 192B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.81787 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
81787 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
81953 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
81953 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
82145 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
82153 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093214 as complete
82154 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093214
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093214.parquet; isDirectory=false; length=450726; replication=1; blocksize=33554432; modification_time=1521189135000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093211.parquet; isDirectory=false; length=450685; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093208.parquet; isDirectory=false; length=450676; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet; isDirectory=false; length=450717; replication=1; blocksize=33554432; modification_time=1521189135000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet; isDirectory=false; length=450687; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093208.parquet; isDirectory=false; length=450646; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093214.parquet; isDirectory=false; length=449364; replication=1; blocksize=33554432; modification_time=1521189135000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093211.parquet; isDirectory=false; length=449332; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093208.parquet; isDirectory=false; length=449298; replication=1; blocksize=33554432; modification_time=1521189129000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
82435 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
83074 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
83345 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093217
83761 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 94
83761 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
83944 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
83961 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093214.parquet
83980 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 156 row keys from /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093214.parquet
83980 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093214.parquet => [0e683517-ada0-4393-bcda-73cde3465b5d, 10572eee-6df7-41d9-9c5e-2aec84fedb16, 13ef16e4-cf48-443e-87a1-fb1e811a29fd, 23727252-bade-45ad-b2fa-6c1d37f06bed, 2b4abe5b-a912-4aec-85b8-8fda90d82568, 46759a43-1590-4265-bbff-9a62485ad279, 46792466-4bc3-4904-9362-c2cf1d0565f5, 4adf4fc9-5dfd-42ce-a7bc-968f01efc210, 4c200b38-c805-4156-b226-c1d44e81c344, 6ce8094b-df2a-4cfa-987c-6a1d63e856fd, 83bb8b00-0d60-431d-9b36-10d4a0c14013, 8ba86831-b522-4dde-90be-8453272be507, 9838287b-3cb0-47e7-b574-0ce09dc737af, b04eda4c-efa2-4ad4-9b9d-0f58e03071c9, bf5a220d-f3e0-40c9-b3e0-a3c1bf963e20, c30b86fa-3400-4210-8f1f-97c7043a0adb, c467204c-8580-46c0-920c-90fc6d2ef0fa, d2a68448-f9a5-453a-b4e8-708acc548a0e, d493e88c-8f61-4ed0-8f66-6b971b99492b, d7988ef8-3dea-483d-88e1-cf8b042e43c7, d91b6dab-8490-4fea-9363-d73e1e5af639, e39c2703-dc76-4bf1-bf3a-ec955d502d53, e7871974-03de-4750-9c12-5c25a2d54b6e, e83f7084-bc73-46d5-a792-77afa977f8fd, ef28216d-b9c6-4df9-b79e-b056e459420b, f29f7851-7f1d-4c06-8446-f4002fc811ee, f61a3494-d022-4ab5-9622-49351ac42e27, f71f08f1-4573-4e88-893d-a11a21f9fdf4, f98ae4d3-2e9f-4ba3-8eac-a4102e79aef8, fcaeeb95-e555-4825-b9c3-c6f3cd21bcf5, fff2f929-1905-408e-b4fd-68231bdd22c2]
83999 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 16 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet
84012 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet
84012 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 16 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet => [029f12d0-8622-4f3e-b8c9-aa9b508ff441, 04ace1c4-368e-4eb5-bf61-160ff8c0a7c3, 0843a3a3-ca98-45ed-914a-465fa4923d89, 12886a39-b0b9-44e2-a45f-a076768c8f83, 292ba420-13a7-481a-9ebd-d729c989da1d, 29564ab3-b15f-435a-9f98-b895f76582d5, 2a4b47f0-3011-4f4f-a4b1-6987fb0bab72, 2f0cab5f-21f4-4373-aa4f-b159d80febfc, 382aa504-a7fa-4f7c-b958-37ee74227902, 3be04791-d819-43d7-8a4c-d0d4209b001c, 3cc6cd2f-20c2-4eba-b221-9cd723dd28da, 477a1363-c289-46bd-bf80-2fa1c0d65a43, 52268886-01a0-4227-aa9c-6f0c99c27cbe, 53e3eef2-beed-4a7b-a023-8e82bc8fc3a3, 72b29520-07fc-408d-adaf-734a94434b7a, 895c2ef0-5cc9-4983-852d-7bf2ce65d3b3]
84043 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet
84059 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet
84059 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet => [97e3c59c-c310-44ae-9cc9-e64deba1ac03, 98aef5e0-668c-4416-9e5b-e6ed2a9a9f13, aa9d03d9-7a90-4f8c-8635-575c05b2ea04, cb3bf892-a0a3-4901-9959-ce58268d0256, cdc472d2-6855-4eef-8116-029f25a15344, d4c14ee0-a7d2-4bc5-b235-68587370969c, d69e8e1e-3e9f-4796-9838-719bb4743ebb, e8c61a1a-6022-4f4e-8c74-bb60b777317c, ea470da9-3b41-490a-8c1f-43c0c7eec69a, ec4e4ad5-6494-4b22-b84c-ca3db13c96f0, f1a5337d-f626-4251-a6b6-ed3ab84712c8, fe28a20a-67dc-4f86-92eb-72ff992241f9]
84085 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093214.parquet
84101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 172 row keys from /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093214.parquet
84101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093214.parquet => [0ab0676d-dd9e-4a29-af4c-0ff73956d8e4, 0ae7f50c-dfb0-4372-ac6f-fb738c99b1ff, 16af9aec-48aa-4b44-b470-14d72f569e52, 19a0c400-3c5e-42d3-b3c6-84c15ba9f2f3, 2834c95e-de0b-4773-8eca-abc9434264e0, 2c5a6d0c-6b08-4aa6-8e79-78c5e6bddf21, 2daed088-3534-4c1e-a09b-9fad3d721f8f, 2f0b15a2-8456-483a-8820-6fede5fedd7a, 33436bc7-94ed-4aca-badc-2f75741f4214, 3978da0d-bebe-4ab6-b1f9-4a21cd3c5f88, 3c4b6419-99e9-484c-8cef-b5c251cbc5f5, 4b06d92e-3379-42e6-8432-56bd9930b053, 5ea90d63-16dd-458f-8b2a-3be928f11950, 8f87c002-cea0-49c0-9fa4-ccf46e9ed03b, 912b14a0-947e-45f7-990f-23cec23cf2bb, a10beff2-d3f7-4ac0-acf5-c2694c2a7dab, ab8dc729-1c52-487e-ad4c-075fa6425d88, ad8e3b9a-48f5-4682-8132-38585ed413f3, aef7e22d-8396-44b7-b68e-932dde3aa1a3, b43a8885-4fc4-42ab-bc52-a98687f91056, b4e2d0bc-1c53-4995-a8e0-42a9930612f9, b73c8512-1cce-4de8-9bab-20728806caea, c7f389ae-6ceb-42e6-ad0d-6541f8dfd5dd, c87312ee-2b8c-4bdb-bc5e-d76cc138d582, c89740b0-be3a-435d-955f-d19f62318442, cb7f7bf2-b096-4c78-b486-94a34b00a1d1, cbcc6e24-636d-45e8-ac76-5ae3b388ee4d, cfff4dee-c2d5-4389-a7b1-24d78c98e38f, d37265f9-d2f5-4598-b0ac-1cd123023822, da313535-fcb8-4205-98ae-13ba2ac0c99d, dba0988a-285f-4611-bd56-dc46c55b1a1a, e0ddb2a2-baa6-4a3b-a2ef-91f3a1ccbc9b, e2b3fe8b-f8b7-4685-a79c-1d993539699f, f2c1c128-c172-4b7c-b5b1-9b646afa6ece, fe015d97-c86b-43a5-a814-e07d01b8903d]
84187 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=94}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=31}}}
84201 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
84202 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7158fc7b-b623-4c72-b076-2ff1a57cf749}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7e1d932d-9e2b-4152-ab4a-70da9006e279}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c3d5a350-f75f-4346-a703-5c619f753dfd}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7158fc7b-b623-4c72-b076-2ff1a57cf749=0, 7e1d932d-9e2b-4152-ab4a-70da9006e279=1, c3d5a350-f75f-4346-a703-5c619f753dfd=2}
84221 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093217
84221 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093217
parquet.hadoop.ColumnChunkPageWriteStore: written 161B for [driver] BINARY: 172 values, 68B raw, 91B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 8 entries, 200B raw, 8B comp}
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,402B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,358B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,410B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,366B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:15 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [fare] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:17 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 32,566
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 165B for [_hoodie_commit_time] BINARY: 156 values, 89B raw, 109B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 162B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 641B for [_hoodie_commit_seqno] BINARY: 156 values, 3,907B raw, 571B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,582B for [_hoodie_record_key] BINARY: 156 values, 6,247B raw, 3,482B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 156 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 259B for [_hoodie_file_name] BINARY: 156 values, 89B raw, 109B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 585B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 156 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,573B for [_row_key] BINARY: 156 values, 6,240B raw, 3,473B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 170B for [rider] BINARY: 156 values, 82B raw, 102B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 9 entries, 216B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 172B for [driver] BINARY: 156 values, 82B raw, 102B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 9 entries, 225B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,287B for [begin_lat] DOUBLE: 156 values, 1,248B raw, 1,243B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,286B for [begin_lon] DOUBLE: 156 values, 1,248B raw, 1,242B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,295B for [end_lat] DOUBLE: 156 values, 1,248B raw, 1,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,290B for [end_lon] DOUBLE: 156 values, 1,248B raw, 1,246B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,289B for [fare] DOUBLE: 156 values, 1,248B raw, 1,245B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 fo84647 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
84647 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
84842 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
84842 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
85005 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
85026 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
85030 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093217 as complete
85030 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093217
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093217.parquet; isDirectory=false; length=450809; replication=1; blocksize=33554432; modification_time=1521189138000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093214.parquet; isDirectory=false; length=450726; replication=1; blocksize=33554432; modification_time=1521189135000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2016/03/15/c3d5a350-f75f-4346-a703-5c619f753dfd_2_20180316093211.parquet; isDirectory=false; length=450685; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093217.parquet; isDirectory=false; length=450816; replication=1; blocksize=33554432; modification_time=1521189138000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093214.parquet; isDirectory=false; length=450717; replication=1; blocksize=33554432; modification_time=1521189135000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/16/7e1d932d-9e2b-4152-ab4a-70da9006e279_1_20180316093211.parquet; isDirectory=false; length=450687; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093217.parquet; isDirectory=false; length=449472; replication=1; blocksize=33554432; modification_time=1521189138000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093214.parquet; isDirectory=false; length=449364; replication=1; blocksize=33554432; modification_time=1521189135000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5608466732130393238/2015/03/17/7158fc7b-b623-4c72-b076-2ff1a57cf749_0_20180316093211.parquet; isDirectory=false; length=449332; replication=1; blocksize=33554432; modification_time=1521189132000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
85238 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093218
85257 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
85512 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
85512 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
85775 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093218
85775 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093218
86009 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86009 [pool-461-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86010 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86010 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
oters
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 172
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,782
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 173B for [_hoodie_commit_time] BINARY: 172 values, 97B raw, 117B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 162B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 678B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 608B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,934B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,834B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 267B for [_hoodie_file_name] BINARY: 172 values, 97B raw, 117B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 585B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,926B for [_row_key] BINARY: 172 values, 6,880B raw, 3,826B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [rider] BINARY: 172 values, 90B raw, 110B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 9 entries, 216B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [driver] BINARY: 172 values, 90B raw, 110B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 9 entries, 225B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,401B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,357B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,410B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,366B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,409B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,365B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,399B for [fare] DOUBLE: 172 values, 1,376B raw, 1,355B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 35,782
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 173B for [_hoodie_commit_time] BINARY: 172 values, 97B raw, 117B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 162B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 676B for [_hoodie_commit_seqno] BINARY: 172 values, 4,307B raw, 606B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,930B for [_hoodie_record_key] BINARY: 172 values, 6,887B raw, 3,830B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 172 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 267B for [_hoodie_file_name] BINARY: 172 values, 97B raw, 117B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 585B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 172 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 3,920B for [_row_key] BINARY: 172 values, 6,880B raw, 3,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [rider] BINARY: 172 values, 90B raw, 110B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 9 entries, 216B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [driver] BINARY: 172 values, 90B raw, 110B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 9 entries, 225B raw, 9B comp}
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,405B for [begin_lat] DOUBLE: 172 values, 1,376B raw, 1,361B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,403B for [begin_lon] DOUBLE: 172 values, 1,376B raw, 1,359B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [end_lat] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,408B for [end_lon] DOUBLE: 172 values, 1,376B raw, 1,364B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:18 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,406B for [fare] DOUBLE: 172 values, 1,376B raw, 1,362B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings86042 [pool-461-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86074 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86074 [pool-462-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86075 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86075 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86116 [pool-462-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86148 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86149 [pool-463-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86150 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86150 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86182 [pool-463-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86209 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86209 [pool-464-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 233B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 84B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteS86240 [pool-464-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86265 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86265 [pool-465-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86290 [pool-465-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86315 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86315 [pool-466-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
tore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B f86352 [pool-466-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86377 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86377 [pool-467-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86414 [pool-467-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86447 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86447 [pool-468-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86448 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86448 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86485 [pool-468-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86517 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86517 [pool-469-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86518 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86518 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
or [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [end_lat] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encoding86565 [pool-469-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86595 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86595 [pool-470-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86596 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86596 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86626 [pool-470-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86637 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
86653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86653 [pool-471-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86654 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86654 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86689 [pool-471-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86713 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86713 [pool-472-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86714 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86714 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
s: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [end_lon] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 355
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parq86750 [pool-472-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86777 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86778 [pool-473-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86778 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86778 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86812 [pool-473-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86837 [pool-474-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86869 [pool-474-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86893 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86893 [pool-475-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86894 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86894 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
uet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINAR86921 [pool-475-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86943 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86943 [pool-476-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86944 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86944 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86970 [pool-476-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86992 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86992 [pool-477-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86993 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86993 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87011 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
87022 [pool-477-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87044 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87045 [pool-478-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87045 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87045 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
Y: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 84B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PAC87069 [pool-478-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87092 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87092 [pool-479-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87093 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87093 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87117 [pool-479-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87139 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87139 [pool-480-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87140 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87140 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87165 [pool-480-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87185 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87185 [pool-481-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87186 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87186 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
KED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 118B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChun87210 [pool-481-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87238 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87239 [pool-482-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87239 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87240 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87266 [pool-482-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87286 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87286 [pool-483-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87287 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87287 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87325 [pool-483-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87353 [pool-484-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87354 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87354 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
kPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [begin_lon] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 84B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, 87393 [pool-484-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87417 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87418 [pool-485-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87418 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87418 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87460 [pool-485-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87491 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87491 [pool-486-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87492 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87492 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87530 [pool-486-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87566 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87566 [pool-487-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87567 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87567 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 118B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPa87596 [pool-487-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87624 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87624 [pool-488-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87660 [pool-488-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87687 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87687 [pool-489-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87688 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87688 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87722 [pool-489-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87743 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87743 [pool-490-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87744 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87744 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
geWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 118B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 49B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodi87769 [pool-490-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87803 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87803 [pool-491-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87804 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87804 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87839 [pool-491-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87871 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87871 [pool-492-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87872 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87872 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
87904 [pool-492-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87933 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
87933 [pool-493-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
87934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
87934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ngs: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2087973 [pool-493-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88002 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88002 [pool-494-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88003 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88003 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88036 [pool-494-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88070 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88070 [pool-495-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
18 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWr88104 [pool-495-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88133 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88134 [pool-496-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88135 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88135 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88170 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
88173 [pool-496-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88206 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88206 [pool-497-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88207 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88207 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88246 [pool-497-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88274 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88274 [pool-498-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88275 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88275 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
iteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 62B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_row_key] BINARY: 1 values, 40B raw, 57B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B 88301 [pool-498-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88326 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88327 [pool-499-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88328 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88328 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88362 [pool-499-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88386 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88386 [pool-500-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88387 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88387 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88413 [pool-500-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88443 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88443 [pool-501-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM 88482 [pool-501-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88514 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88514 [pool-502-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88516 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88516 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88547 [pool-502-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88581 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88581 [pool-503-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88582 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88582 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88590 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
88622 [pool-503-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88653 [pool-504-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88654 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88654 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 88693 [pool-504-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88726 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88726 [pool-505-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88727 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88727 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88766 [pool-505-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88801 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88801 [pool-506-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88802 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88802 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88826 [pool-506-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88856 [pool-507-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88857 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88857 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodi88895 [pool-507-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88928 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88928 [pool-508-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
88961 [pool-508-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
88991 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
88992 [pool-509-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
88993 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
88993 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89023 [pool-509-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89053 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89053 [pool-510-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89054 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89054 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ngs: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [end_lon] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.Colu89287 [pool-510-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89308 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89308 [pool-511-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89309 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89309 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89343 [pool-511-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89362 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89363 [pool-512-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89363 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89363 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89391 [pool-512-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89411 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89411 [pool-513-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89411 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89412 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
mnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:22 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages89443 [pool-513-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89473 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89473 [pool-514-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89502 [pool-514-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89529 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89529 [pool-515-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89530 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89530 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89560 [pool-515-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89584 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89584 [pool-516-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89585 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89585 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChun89616 [pool-516-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89639 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89639 [pool-517-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89640 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89640 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89676 [pool-517-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89700 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89700 [pool-518-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89701 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89701 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89724 [pool-518-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89744 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89744 [pool-519-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89744 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89744 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
kPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnSt89775 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
89776 [pool-519-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89794 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89794 [pool-520-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89795 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89795 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89819 [pool-520-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89838 [pool-521-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLA89871 [pool-521-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89898 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89898 [pool-522-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89934 [pool-522-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
89962 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
89962 [pool-523-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
89963 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
89963 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
89999 [pool-523-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90029 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90029 [pool-524-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90030 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90030 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
IN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.h90061 [pool-524-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90087 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90087 [pool-525-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90088 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90088 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90125 [pool-525-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90156 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90156 [pool-526-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90157 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90157 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90197 [pool-526-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90224 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90224 [pool-527-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
adoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 190255 [pool-527-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90279 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90280 [pool-528-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90280 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90280 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90323 [pool-528-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90356 [pool-529-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90357 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90357 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90391 [pool-529-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90422 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90422 [pool-530-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90423 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90423 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:23 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 87B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Ma90453 [pool-530-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90478 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90478 [pool-531-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90479 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90479 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90495 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
90513 [pool-531-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90535 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90535 [pool-532-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90536 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90536 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90578 [pool-532-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90608 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90608 [pool-533-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90609 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90609 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
r 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 236B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPage90642 [pool-533-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90675 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90675 [pool-534-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90676 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90676 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90704 [pool-534-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90728 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
90728 [pool-535-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
90729 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
90729 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
90752 [pool-535-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
90772 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
90772 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
91197 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
91197 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
91345 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
91352 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093218 as complete
91352 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093218
91633 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
91843 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
93297 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
93671 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
93774 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 75
93774 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
93925 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/04e25b72-8755-4b69-8c6e-173d0d7fc623_7_20180316093218.parquet
93941 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/04e25b72-8755-4b69-8c6e-173d0d7fc623_7_20180316093218.parquet
93941 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/04e25b72-8755-4b69-8c6e-173d0d7fc623_7_20180316093218.parquet => [72c15dc7-f153-4f47-9cdb-57459b8c1f04]
93961 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/05156678-87ab-4e23-bd71-b939f7f463f8_0_20180316093218.parquet
WriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 357
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [_hoodie_commit_seqno] BINARY: 1 values, 32B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 237B for [_hoodie_file_name] BINARY: 1 values, 72B raw, 86B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [rider] BINARY: 1 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [driver] BINARY: 1 values, 25B raw, 45B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetReco93977 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/05156678-87ab-4e23-bd71-b939f7f463f8_0_20180316093218.parquet
93977 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/05156678-87ab-4e23-bd71-b939f7f463f8_0_20180316093218.parquet => [014a65a8-f06f-4720-bb7e-a450f007ac40]
93993 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/0551a831-2908-434c-9d7a-639ee9fa6578_18_20180316093218.parquet
94007 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/0551a831-2908-434c-9d7a-639ee9fa6578_18_20180316093218.parquet
94007 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/0551a831-2908-434c-9d7a-639ee9fa6578_18_20180316093218.parquet => [c1a2418d-8269-4f98-912c-e76df3e569a8]
94022 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/07dbcd65-3c73-44bb-845e-ffeafb7966df_19_20180316093218.parquet
94035 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/07dbcd65-3c73-44bb-845e-ffeafb7966df_19_20180316093218.parquet
94035 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/07dbcd65-3c73-44bb-845e-ffeafb7966df_19_20180316093218.parquet => [d720c7e1-eba6-4878-a873-ef006216f962]
94049 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/084d253f-81d7-4ee7-814c-8f006d6bd14f_21_20180316093218.parquet
94063 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/084d253f-81d7-4ee7-814c-8f006d6bd14f_21_20180316093218.parquet
94063 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/084d253f-81d7-4ee7-814c-8f006d6bd14f_21_20180316093218.parquet => [ea529716-81c1-4508-806b-b783bc5ecfa9]
94077 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/08ca5854-cd75-41a7-9478-99d98855dd1d_67_20180316093218.parquet
94092 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/08ca5854-cd75-41a7-9478-99d98855dd1d_67_20180316093218.parquet
94092 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/08ca5854-cd75-41a7-9478-99d98855dd1d_67_20180316093218.parquet => [ba6d2864-b297-4d2b-9eca-674c82e451b9]
94106 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/0e178210-0649-477a-8cbe-f8abcedbce48_55_20180316093218.parquet
94120 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/0e178210-0649-477a-8cbe-f8abcedbce48_55_20180316093218.parquet
94120 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/0e178210-0649-477a-8cbe-f8abcedbce48_55_20180316093218.parquet => [4b34a4e1-6073-4bc6-9aaf-f575d46c0797]
94134 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/137a1d2e-813b-48d4-a06e-31dbdad028a9_71_20180316093218.parquet
94147 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/137a1d2e-813b-48d4-a06e-31dbdad028a9_71_20180316093218.parquet
94147 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/137a1d2e-813b-48d4-a06e-31dbdad028a9_71_20180316093218.parquet => [df15dbe8-be32-4b80-8f39-aa42947b4d7b]
94164 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/1886e11b-8440-4a31-9580-4fbb5e4a9d4d_62_20180316093218.parquet
94177 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/1886e11b-8440-4a31-9580-4fbb5e4a9d4d_62_20180316093218.parquet
94177 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/1886e11b-8440-4a31-9580-4fbb5e4a9d4d_62_20180316093218.parquet => [9154a374-096b-413a-bba0-2f218cb8d19b]
94196 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/22a9351c-8923-41b8-b32c-d5a9bc03a2cb_22_20180316093218.parquet
94210 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/22a9351c-8923-41b8-b32c-d5a9bc03a2cb_22_20180316093218.parquet
94211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/22a9351c-8923-41b8-b32c-d5a9bc03a2cb_22_20180316093218.parquet => [ef7a0fe3-f864-474e-b380-7856f2729db9]
94228 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/24f8e237-121a-4b38-9fbf-310cef700597_54_20180316093218.parquet
94241 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/24f8e237-121a-4b38-9fbf-310cef700597_54_20180316093218.parquet
94241 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/24f8e237-121a-4b38-9fbf-310cef700597_54_20180316093218.parquet => [45eac52b-68b0-4c6d-8196-b113dfae8331]
94257 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/26108ac4-81bb-4423-9679-687efdec516f_6_20180316093218.parquet
94269 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/26108ac4-81bb-4423-9679-687efdec516f_6_20180316093218.parquet
94269 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/26108ac4-81bb-4423-9679-687efdec516f_6_20180316093218.parquet => [625c84f4-b0a9-4ee1-b534-5c8defef090b]
94284 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/270ab5eb-ac48-4869-a299-70d9eec195d6_11_20180316093218.parquet
94294 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/270ab5eb-ac48-4869-a299-70d9eec195d6_11_20180316093218.parquet
94294 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/270ab5eb-ac48-4869-a299-70d9eec195d6_11_20180316093218.parquet => [80744ce0-5af1-445e-bff3-75ff10836011]
94309 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/2a97f1c2-f7d4-4da8-b66d-c4c6690a91ea_3_20180316093218.parquet
rdReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.h94322 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/2a97f1c2-f7d4-4da8-b66d-c4c6690a91ea_3_20180316093218.parquet
94322 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/2a97f1c2-f7d4-4da8-b66d-c4c6690a91ea_3_20180316093218.parquet => [41eb1b77-c565-45af-a3c7-306ff5e6eebf]
94336 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/3420af4c-8194-4813-a35e-e09e3f396991_4_20180316093218.parquet
94345 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/3420af4c-8194-4813-a35e-e09e3f396991_4_20180316093218.parquet
94345 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/3420af4c-8194-4813-a35e-e09e3f396991_4_20180316093218.parquet => [43a2455a-7a2c-4b47-af67-41f7e26493d6]
94359 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/3923d6f4-8455-4226-a93d-dfca4fec85dd_59_20180316093218.parquet
94369 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/3923d6f4-8455-4226-a93d-dfca4fec85dd_59_20180316093218.parquet
94369 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/3923d6f4-8455-4226-a93d-dfca4fec85dd_59_20180316093218.parquet => [8bda311a-9961-4025-97f7-6607ad20c5d5]
94382 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/41369afe-685c-41f4-9e47-431976d82696_65_20180316093218.parquet
94391 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/41369afe-685c-41f4-9e47-431976d82696_65_20180316093218.parquet
94391 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/41369afe-685c-41f4-9e47-431976d82696_65_20180316093218.parquet => [aa3fac99-b26d-4dc8-a21f-b79e154c81a2]
94405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/52f38c6e-9297-4ddf-b30c-32f5625fbdb2_33_20180316093218.parquet
94414 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/52f38c6e-9297-4ddf-b30c-32f5625fbdb2_33_20180316093218.parquet
94414 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/52f38c6e-9297-4ddf-b30c-32f5625fbdb2_33_20180316093218.parquet => [a1ed4bcf-f229-4899-bc8f-13b9f7916ed8]
94428 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/59c36fec-6df9-4e62-8c1e-95ba4ef33e4a_45_20180316093218.parquet
94437 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/59c36fec-6df9-4e62-8c1e-95ba4ef33e4a_45_20180316093218.parquet
94437 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/59c36fec-6df9-4e62-8c1e-95ba4ef33e4a_45_20180316093218.parquet => [07f98f8a-8524-4df1-98dc-ec9acb1a3b56]
94445 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
94451 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/5c3cfadb-71d6-47ad-a16d-dc2fc921505a_60_20180316093218.parquet
94463 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/5c3cfadb-71d6-47ad-a16d-dc2fc921505a_60_20180316093218.parquet
94463 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/5c3cfadb-71d6-47ad-a16d-dc2fc921505a_60_20180316093218.parquet => [8c5f311c-99be-47d4-8bb1-5324b4ca16ae]
94477 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/5cff3e9d-56b4-4be8-b992-064303e01991_73_20180316093218.parquet
94485 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/5cff3e9d-56b4-4be8-b992-064303e01991_73_20180316093218.parquet
94485 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/5cff3e9d-56b4-4be8-b992-064303e01991_73_20180316093218.parquet => [ea0fe0ef-dcf6-4f09-ba60-07af6de0a43a]
94499 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/5dfc8964-ea03-4e6a-9524-c17ae5762315_46_20180316093218.parquet
94508 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/5dfc8964-ea03-4e6a-9524-c17ae5762315_46_20180316093218.parquet
94508 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/5dfc8964-ea03-4e6a-9524-c17ae5762315_46_20180316093218.parquet => [0891b4b2-072b-4791-8e09-0812f8115650]
94521 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/5e15294c-59ec-4132-8208-65d1449a26c6_17_20180316093218.parquet
94531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/5e15294c-59ec-4132-8208-65d1449a26c6_17_20180316093218.parquet
94531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/5e15294c-59ec-4132-8208-65d1449a26c6_17_20180316093218.parquet => [bf928b2c-7566-4515-808d-1d13ac3a1630]
94545 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/612e727e-15a2-4dfe-8251-1fabb2c2fb97_36_20180316093218.parquet
94553 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/612e727e-15a2-4dfe-8251-1fabb2c2fb97_36_20180316093218.parquet
94553 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/612e727e-15a2-4dfe-8251-1fabb2c2fb97_36_20180316093218.parquet => [cb97bc8a-80ce-46c6-939b-8f614a58ccaa]
94567 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/620d2f03-3180-49f0-beeb-a245fafbd520_20_20180316093218.parquet
adoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:27 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader94576 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/620d2f03-3180-49f0-beeb-a245fafbd520_20_20180316093218.parquet
94576 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/620d2f03-3180-49f0-beeb-a245fafbd520_20_20180316093218.parquet => [d7577194-6cb5-42b7-9809-ce13f5883ce6]
94591 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/64d73dfe-698d-4171-9fc6-63a15db26137_14_20180316093218.parquet
94599 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/64d73dfe-698d-4171-9fc6-63a15db26137_14_20180316093218.parquet
94600 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/64d73dfe-698d-4171-9fc6-63a15db26137_14_20180316093218.parquet => [a5183055-bfae-4e7e-89c8-399b5fe1980d]
94613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/6a2968e5-adfc-4432-b40b-c25bbb673659_61_20180316093218.parquet
94622 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/6a2968e5-adfc-4432-b40b-c25bbb673659_61_20180316093218.parquet
94622 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/6a2968e5-adfc-4432-b40b-c25bbb673659_61_20180316093218.parquet => [907dfcdb-024b-4043-ba6e-086a862d8f17]
94636 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/6c996118-9591-4371-bfe0-a313a5a69eeb_34_20180316093218.parquet
94648 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/6c996118-9591-4371-bfe0-a313a5a69eeb_34_20180316093218.parquet
94648 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/6c996118-9591-4371-bfe0-a313a5a69eeb_34_20180316093218.parquet => [b76ec9d5-970e-4363-9ceb-ea7284e2129d]
94662 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/78c16220-da5f-4520-8d2a-d4935ba9ef86_25_20180316093218.parquet
94673 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/78c16220-da5f-4520-8d2a-d4935ba9ef86_25_20180316093218.parquet
94673 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/78c16220-da5f-4520-8d2a-d4935ba9ef86_25_20180316093218.parquet => [231ecea1-dca0-4f3f-af56-3ea6b3e88347]
94687 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/7979a836-a335-4010-9207-5c9e20200aa5_1_20180316093218.parquet
94698 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/7979a836-a335-4010-9207-5c9e20200aa5_1_20180316093218.parquet
94698 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/7979a836-a335-4010-9207-5c9e20200aa5_1_20180316093218.parquet => [190da5a6-e478-44a3-8dc2-2430e54231f6]
94711 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/7b4a817b-b590-40bd-b455-4ce9527f7a22_57_20180316093218.parquet
94726 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/7b4a817b-b590-40bd-b455-4ce9527f7a22_57_20180316093218.parquet
94726 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/7b4a817b-b590-40bd-b455-4ce9527f7a22_57_20180316093218.parquet => [69063e55-65d2-4fb4-99cd-541ad8a4155a]
94740 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/7ee0b2cd-0563-487b-a38c-98dc8620a0ca_13_20180316093218.parquet
94751 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/7ee0b2cd-0563-487b-a38c-98dc8620a0ca_13_20180316093218.parquet
94751 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/7ee0b2cd-0563-487b-a38c-98dc8620a0ca_13_20180316093218.parquet => [9db03ee8-01e2-4348-a8e9-93a0cc745671]
94765 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/83b63fce-e2a0-48f4-bdbc-ecff2bc048fe_10_20180316093218.parquet
94779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/83b63fce-e2a0-48f4-bdbc-ecff2bc048fe_10_20180316093218.parquet
94779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/83b63fce-e2a0-48f4-bdbc-ecff2bc048fe_10_20180316093218.parquet => [7b9402e8-0536-4e32-a05b-839275a5c511]
94793 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/83c35268-777c-4999-85ba-d922183272b3_52_20180316093218.parquet
94803 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/83c35268-777c-4999-85ba-d922183272b3_52_20180316093218.parquet
94803 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/83c35268-777c-4999-85ba-d922183272b3_52_20180316093218.parquet => [3d3f1e11-c6ae-48c3-9a63-8941cc6f9ea4]
94819 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/857a5823-f5e4-43ae-b4dc-706a499c1189_66_20180316093218.parquet
94829 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/857a5823-f5e4-43ae-b4dc-706a499c1189_66_20180316093218.parquet
94829 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/857a5823-f5e4-43ae-b4dc-706a499c1189_66_20180316093218.parquet => [adfb19e4-826e-45ca-b7bd-c2e3b99db7a4]
94843 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/88b14963-072d-4dc0-8bcb-3bcbd8763fbd_49_20180316093218.parquet
94854 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/88b14963-072d-4dc0-8bcb-3bcbd8763fbd_49_20180316093218.parquet
94854 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/88b14963-072d-4dc0-8bcb-3bcbd8763fbd_49_20180316093218.parquet => [1da9b9ea-8b27-4961-8416-a42997506388]
94867 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/8bb789e8-88b8-4332-8fa7-5e1769292d0e_42_20180316093218.parquet
: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.Pa94880 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/8bb789e8-88b8-4332-8fa7-5e1769292d0e_42_20180316093218.parquet
94880 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/8bb789e8-88b8-4332-8fa7-5e1769292d0e_42_20180316093218.parquet => [ec7e9f8e-8830-4196-93cf-ed32533fb2ab]
94894 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/8c6d461a-63e0-4402-b7aa-ffeff808ca27_72_20180316093218.parquet
94905 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/8c6d461a-63e0-4402-b7aa-ffeff808ca27_72_20180316093218.parquet
94905 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/8c6d461a-63e0-4402-b7aa-ffeff808ca27_72_20180316093218.parquet => [ea00f8d2-f609-472c-ad3d-62f7762a9ad5]
94927 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/8e09ebc8-bfe7-41ea-9a43-ebfff1f0be42_39_20180316093218.parquet
94942 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/8e09ebc8-bfe7-41ea-9a43-ebfff1f0be42_39_20180316093218.parquet
94942 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/8e09ebc8-bfe7-41ea-9a43-ebfff1f0be42_39_20180316093218.parquet => [dfadfd11-8070-41fe-af95-d3d8906e9d34]
94957 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/94a18863-fb44-4a65-8dac-d14dee4ace85_8_20180316093218.parquet
94969 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/94a18863-fb44-4a65-8dac-d14dee4ace85_8_20180316093218.parquet
94969 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/94a18863-fb44-4a65-8dac-d14dee4ace85_8_20180316093218.parquet => [778da3f2-d6aa-4691-b358-924a268bd4db]
94983 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/97fea2dc-d40d-4a4c-a6d6-4f072f8921cd_2_20180316093218.parquet
94996 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/97fea2dc-d40d-4a4c-a6d6-4f072f8921cd_2_20180316093218.parquet
94996 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/97fea2dc-d40d-4a4c-a6d6-4f072f8921cd_2_20180316093218.parquet => [1f1a9582-d9b3-45c2-b9b2-e63bca0df29c]
95009 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/9bd0c2a4-5889-446d-bf08-69a063b66757_26_20180316093218.parquet
95020 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/9bd0c2a4-5889-446d-bf08-69a063b66757_26_20180316093218.parquet
95020 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/9bd0c2a4-5889-446d-bf08-69a063b66757_26_20180316093218.parquet => [2c446f04-37ea-48ba-a719-e1fe1bd1b1fd]
95035 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/9e8359c4-1d22-4c55-9dad-54a0f621356b_28_20180316093218.parquet
95048 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/9e8359c4-1d22-4c55-9dad-54a0f621356b_28_20180316093218.parquet
95048 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/9e8359c4-1d22-4c55-9dad-54a0f621356b_28_20180316093218.parquet => [86cc922b-dd05-4409-94b0-ddbb7deb09c8]
95063 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/ab1313f9-b8c8-4b1d-a43c-3a4080db2fd2_47_20180316093218.parquet
95074 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/ab1313f9-b8c8-4b1d-a43c-3a4080db2fd2_47_20180316093218.parquet
95074 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/ab1313f9-b8c8-4b1d-a43c-3a4080db2fd2_47_20180316093218.parquet => [0fc3a546-3685-4e5a-b77f-a79f0fdf2861]
95091 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/ad70c7fe-701b-4484-a002-997dc37ad9c7_35_20180316093218.parquet
95104 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/ad70c7fe-701b-4484-a002-997dc37ad9c7_35_20180316093218.parquet
95104 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/ad70c7fe-701b-4484-a002-997dc37ad9c7_35_20180316093218.parquet => [bf5ad03d-a5e6-492f-b75c-492b012e73e6]
95120 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/aeed6fcc-d8a1-4684-ba5e-9daf242a24dd_58_20180316093218.parquet
95131 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/aeed6fcc-d8a1-4684-ba5e-9daf242a24dd_58_20180316093218.parquet
95131 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/aeed6fcc-d8a1-4684-ba5e-9daf242a24dd_58_20180316093218.parquet => [7449e017-72ab-454d-a5d3-531c79bcdbaf]
95147 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/b16cdeac-846f-4eaf-9348-e09460410a52_5_20180316093218.parquet
95159 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/b16cdeac-846f-4eaf-9348-e09460410a52_5_20180316093218.parquet
95159 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/b16cdeac-846f-4eaf-9348-e09460410a52_5_20180316093218.parquet => [5571dd65-484b-492f-96db-fd3624938462]
95174 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/b64178a2-67fa-44b4-9d0f-0171fd981bd2_74_20180316093218.parquet
rquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: Record95183 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/b64178a2-67fa-44b4-9d0f-0171fd981bd2_74_20180316093218.parquet
95184 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/b64178a2-67fa-44b4-9d0f-0171fd981bd2_74_20180316093218.parquet => [f2be253b-8163-4bfe-bd3b-a6da0d36a4f6]
95197 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/b7a155d6-1f46-4192-bf72-4f52c71f0eda_56_20180316093218.parquet
95206 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/b7a155d6-1f46-4192-bf72-4f52c71f0eda_56_20180316093218.parquet
95206 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/b7a155d6-1f46-4192-bf72-4f52c71f0eda_56_20180316093218.parquet => [542b4ec9-564c-47bb-b2ba-2b41874bee3e]
95220 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/bda15825-ddd0-4501-961c-6f8541863f5e_37_20180316093218.parquet
95228 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/bda15825-ddd0-4501-961c-6f8541863f5e_37_20180316093218.parquet
95228 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/bda15825-ddd0-4501-961c-6f8541863f5e_37_20180316093218.parquet => [d80d1736-a7bc-46a8-b6a9-2bb64c3bd9cc]
95242 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/be7252f6-f1c2-43f0-ab14-882e4ed4a9e7_53_20180316093218.parquet
95254 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/be7252f6-f1c2-43f0-ab14-882e4ed4a9e7_53_20180316093218.parquet
95254 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/be7252f6-f1c2-43f0-ab14-882e4ed4a9e7_53_20180316093218.parquet => [4152a297-bce7-4421-9727-c0c75fda635d]
95268 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/c0a04a4d-7211-4493-86da-0a1a2e897a6f_30_20180316093218.parquet
95280 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/c0a04a4d-7211-4493-86da-0a1a2e897a6f_30_20180316093218.parquet
95280 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/c0a04a4d-7211-4493-86da-0a1a2e897a6f_30_20180316093218.parquet => [904ad348-c190-4608-a159-e2e7803e228f]
95294 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/c92f3e0a-1b67-4ef0-b291-78035c5383af_68_20180316093218.parquet
95305 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/c92f3e0a-1b67-4ef0-b291-78035c5383af_68_20180316093218.parquet
95305 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/c92f3e0a-1b67-4ef0-b291-78035c5383af_68_20180316093218.parquet => [d84e4434-c716-40cf-8d38-d582a0a16d0e]
95318 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/ca1db679-f86b-463d-957b-e2fa693415da_32_20180316093218.parquet
95329 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/ca1db679-f86b-463d-957b-e2fa693415da_32_20180316093218.parquet
95329 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/ca1db679-f86b-463d-957b-e2fa693415da_32_20180316093218.parquet => [9afc91de-faf6-45b4-9064-d4b7c8b6798b]
95343 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/ce8ac46e-6f88-44b8-9085-b83faf457abb_63_20180316093218.parquet
95355 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/ce8ac46e-6f88-44b8-9085-b83faf457abb_63_20180316093218.parquet
95355 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/ce8ac46e-6f88-44b8-9085-b83faf457abb_63_20180316093218.parquet => [9c8cad55-375d-48af-8b96-c5fc34887aaa]
95369 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/d245b9fd-0b94-4dc4-8e65-1c3275d2c8b5_41_20180316093218.parquet
95378 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/d245b9fd-0b94-4dc4-8e65-1c3275d2c8b5_41_20180316093218.parquet
95378 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/d245b9fd-0b94-4dc4-8e65-1c3275d2c8b5_41_20180316093218.parquet => [eb731bda-ff40-42e5-a247-1e175d23ce53]
95392 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/d40a0161-91fd-44a7-aafb-706eca180843_43_20180316093218.parquet
95404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/d40a0161-91fd-44a7-aafb-706eca180843_43_20180316093218.parquet
95404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/d40a0161-91fd-44a7-aafb-706eca180843_43_20180316093218.parquet => [f95c8b27-f237-4745-b6e0-73e11a6800c6]
95406 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
95417 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/d4a0214c-cd21-4188-8126-0c2be2816a74_70_20180316093218.parquet
95430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/d4a0214c-cd21-4188-8126-0c2be2816a74_70_20180316093218.parquet
95430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/d4a0214c-cd21-4188-8126-0c2be2816a74_70_20180316093218.parquet => [db4b3a93-da38-43fc-bf9c-cf103003a81b]
95443 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/d65dc432-9c61-458a-8413-faf8369dcced_38_20180316093218.parquet
95452 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/d65dc432-9c61-458a-8413-faf8369dcced_38_20180316093218.parquet
95452 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/d65dc432-9c61-458a-8413-faf8369dcced_38_20180316093218.parquet => [de5ea073-612a-4938-9e9f-d32b8a06ec58]
95466 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/dd8311e0-93f1-457c-bcaf-5ffad520c13d_50_20180316093218.parquet
Reader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:28 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFil95475 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/dd8311e0-93f1-457c-bcaf-5ffad520c13d_50_20180316093218.parquet
95475 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/dd8311e0-93f1-457c-bcaf-5ffad520c13d_50_20180316093218.parquet => [22d3c034-f84e-422f-aa7c-dec7dc40a6f4]
95488 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/ddab328d-e428-4f24-a674-66c838df6853_24_20180316093218.parquet
95500 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/ddab328d-e428-4f24-a674-66c838df6853_24_20180316093218.parquet
95500 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/ddab328d-e428-4f24-a674-66c838df6853_24_20180316093218.parquet => [f4709d0b-ff98-4bce-8ad6-c3e0e54607fa]
95515 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/e14f466f-15ea-43c9-9bd3-f9f81cea3b24_23_20180316093218.parquet
95528 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/e14f466f-15ea-43c9-9bd3-f9f81cea3b24_23_20180316093218.parquet
95528 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/e14f466f-15ea-43c9-9bd3-f9f81cea3b24_23_20180316093218.parquet => [f11a7380-4033-4218-adad-5295a122af36]
95542 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/e17ab771-e413-4479-abe5-9118e36d9a1b_29_20180316093218.parquet
95553 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/e17ab771-e413-4479-abe5-9118e36d9a1b_29_20180316093218.parquet
95553 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/e17ab771-e413-4479-abe5-9118e36d9a1b_29_20180316093218.parquet => [8b90b9b3-48d8-43d5-808d-ac77cdb69995]
95566 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/e7fda4cb-4ae4-482a-bf33-cf15ef9fb300_16_20180316093218.parquet
95578 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/e7fda4cb-4ae4-482a-bf33-cf15ef9fb300_16_20180316093218.parquet
95578 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/e7fda4cb-4ae4-482a-bf33-cf15ef9fb300_16_20180316093218.parquet => [abc941de-6ae2-4fec-8ee2-e0d84a8d3004]
95592 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/e9bcf210-4d4d-407e-a8a3-0656d81427ce_40_20180316093218.parquet
95603 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/e9bcf210-4d4d-407e-a8a3-0656d81427ce_40_20180316093218.parquet
95603 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/e9bcf210-4d4d-407e-a8a3-0656d81427ce_40_20180316093218.parquet => [dfd4c377-e446-4f9e-bf61-45393c306342]
95616 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/eb695e3a-7db4-4ede-bcfe-cd24514f4278_9_20180316093218.parquet
95627 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/eb695e3a-7db4-4ede-bcfe-cd24514f4278_9_20180316093218.parquet
95627 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/eb695e3a-7db4-4ede-bcfe-cd24514f4278_9_20180316093218.parquet => [799fed31-f197-4dcb-85bf-8bebc79575d6]
95641 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/ebbac5fa-0e09-417e-90c4-e9a832e43f0c_64_20180316093218.parquet
95651 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/ebbac5fa-0e09-417e-90c4-e9a832e43f0c_64_20180316093218.parquet
95651 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/ebbac5fa-0e09-417e-90c4-e9a832e43f0c_64_20180316093218.parquet => [a2d01d51-3f29-46cc-bc3c-df366c82185f]
95665 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/ecd35903-0750-4276-b5d7-a5251df72f11_51_20180316093218.parquet
95675 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/ecd35903-0750-4276-b5d7-a5251df72f11_51_20180316093218.parquet
95675 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/ecd35903-0750-4276-b5d7-a5251df72f11_51_20180316093218.parquet => [36fb7e3a-af20-46ca-aafd-24fc3774651c]
95689 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/ed67b57d-ad6e-4d0b-a2df-99be521e3296_44_20180316093218.parquet
95699 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/ed67b57d-ad6e-4d0b-a2df-99be521e3296_44_20180316093218.parquet
95699 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/ed67b57d-ad6e-4d0b-a2df-99be521e3296_44_20180316093218.parquet => [0515d337-e9db-4f6c-ba45-4fbab2a6f6e7]
95713 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/f1bf949c-0dde-4368-9a16-a98b598982bf_15_20180316093218.parquet
95723 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/f1bf949c-0dde-4368-9a16-a98b598982bf_15_20180316093218.parquet
95723 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/f1bf949c-0dde-4368-9a16-a98b598982bf_15_20180316093218.parquet => [a6837e8b-bfc1-40fa-8ef8-23c06baa4ed6]
95737 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/f3eb77e9-eac8-4e75-a98e-fd8487b6dabf_27_20180316093218.parquet
eReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader i95747 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/f3eb77e9-eac8-4e75-a98e-fd8487b6dabf_27_20180316093218.parquet
95747 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/f3eb77e9-eac8-4e75-a98e-fd8487b6dabf_27_20180316093218.parquet => [65fc6ae8-9a99-4973-84e8-a11c052f644c]
95761 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/16/f67af5e2-5ef8-4a4f-ae4b-54109c3427ff_12_20180316093218.parquet
95774 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/16/f67af5e2-5ef8-4a4f-ae4b-54109c3427ff_12_20180316093218.parquet
95774 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/16/f67af5e2-5ef8-4a4f-ae4b-54109c3427ff_12_20180316093218.parquet => [98fb0085-3627-406c-9054-eeb9328339f5]
95788 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/fa2103fd-d974-487c-9f4b-3f2eec0dd485_69_20180316093218.parquet
95801 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/fa2103fd-d974-487c-9f4b-3f2eec0dd485_69_20180316093218.parquet
95801 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/fa2103fd-d974-487c-9f4b-3f2eec0dd485_69_20180316093218.parquet => [d9eb25de-5190-4ec6-9f22-a725735366a6]
95815 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2015/03/17/fb23223e-6afc-4bc6-87af-fe589c6f60e6_31_20180316093218.parquet
95826 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2015/03/17/fb23223e-6afc-4bc6-87af-fe589c6f60e6_31_20180316093218.parquet
95826 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2015/03/17/fb23223e-6afc-4bc6-87af-fe589c6f60e6_31_20180316093218.parquet => [914a4ff1-b0e7-4789-99c0-f936aa4a9cfe]
95841 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit2073866909383877392/2016/03/15/fe03bbdf-3d28-41db-bbcb-2bd6eb9e5a35_48_20180316093218.parquet
95850 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit2073866909383877392/2016/03/15/fe03bbdf-3d28-41db-bbcb-2bd6eb9e5a35_48_20180316093218.parquet
95850 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit2073866909383877392/2016/03/15/fe03bbdf-3d28-41db-bbcb-2bd6eb9e5a35_48_20180316093218.parquet => [1039f1ea-c22c-4b19-bfc8-71c3aab640d3]
95973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
96058 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
96059 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
96444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96444 [pool-701-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96445 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96445 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96450 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
96481 [pool-701-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96509 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96510 [pool-702-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96510 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96510 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
nitialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:29 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 2896546 [pool-702-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96570 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96570 [pool-703-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96571 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96571 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96608 [pool-703-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96635 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96635 [pool-704-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96635 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96635 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96665 [pool-704-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96682 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96682 [pool-705-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96683 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96683 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: o96714 [pool-705-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96731 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96731 [pool-706-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96756 [pool-706-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96777 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96777 [pool-707-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96777 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96777 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96819 [pool-707-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96837 [pool-708-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96838 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
rg.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider96874 [pool-708-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96902 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96902 [pool-709-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96903 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96903 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96925 [pool-709-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96950 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96950 [pool-710-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96950 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96951 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
96973 [pool-710-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
96996 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
96996 [pool-711-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
96997 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
96997 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED97045 [pool-711-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97063 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97063 [pool-712-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97088 [pool-712-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97106 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97106 [pool-713-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97107 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97107 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97131 [pool-713-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97149 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97149 [pool-714-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97150 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97150 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 297180 [pool-714-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97204 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97204 [pool-715-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97236 [pool-715-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97260 [pool-716-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97261 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97261 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97299 [pool-716-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97327 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97327 [pool-717-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97328 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97328 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
06B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar97357 [pool-717-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97376 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97376 [pool-718-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97377 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97377 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97398 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
97405 [pool-718-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97426 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97426 [pool-719-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97426 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97426 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97470 [pool-719-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97493 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97493 [pool-720-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97494 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97494 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 16, 2018 9:32:30 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_97535 [pool-720-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97557 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97557 [pool-721-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97558 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97558 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97590 [pool-721-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97610 [pool-722-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97611 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97611 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97641 [pool-722-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97661 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97661 [pool-723-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97661 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97661 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 85B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM97701 [pool-723-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97731 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97731 [pool-724-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97768 [pool-724-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97798 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97798 [pool-725-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.ha97841 [pool-725-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97871 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97871 [pool-726-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97872 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97872 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97905 [pool-726-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
97931 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
97931 [pool-727-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
97932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
97932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
97974 [pool-727-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98005 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98006 [pool-728-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98007 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98007 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
doop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 85B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values,98047 [pool-728-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98070 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98070 [pool-729-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98101 [pool-729-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98123 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98123 [pool-730-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98124 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98124 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98137 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
98148 [pool-730-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98172 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98172 [pool-731-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:98204 [pool-731-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98227 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98227 [pool-732-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98228 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98228 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98260 [pool-732-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98281 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98281 [pool-733-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98314 [pool-733-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98334 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98335 [pool-734-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98335 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98335 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:31 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B 98370 [pool-734-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98393 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98394 [pool-735-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98394 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98394 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98424 [pool-735-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98447 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98447 [pool-736-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98447 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98448 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98474 [pool-736-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98499 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98499 [pool-737-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98500 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98500 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLA98534 [pool-737-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98558 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98558 [pool-738-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98559 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98559 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98591 [pool-738-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98615 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98615 [pool-739-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98616 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98616 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98641 [pool-739-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98663 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98664 [pool-740-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
IN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnCh98695 [pool-740-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98715 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98716 [pool-741-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98716 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98716 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98742 [pool-741-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98762 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98763 [pool-742-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98763 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98763 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98794 [pool-742-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98812 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98812 [pool-743-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98812 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98812 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
unkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodin98845 [pool-743-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98860 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98860 [pool-744-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98882 [pool-744-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98899 [pool-745-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
98920 [pool-745-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98937 [pool-746-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
gs: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: wr98966 [pool-746-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
98982 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
98982 [pool-747-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
98983 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
98983 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99012 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
99017 [pool-747-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99033 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99033 [pool-748-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99033 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99034 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99061 [pool-748-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99077 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99077 [pool-749-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99077 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99077 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
itten 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PA99098 [pool-749-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99114 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99114 [pool-750-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99115 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99115 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99143 [pool-750-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99167 [pool-751-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99168 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99168 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99200 [pool-751-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99224 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99224 [pool-752-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
CKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:32 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.pa99382 [pool-752-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99400 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99400 [pool-753-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99401 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99401 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99421 [pool-753-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99438 [pool-754-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
rquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE99460 [pool-754-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99477 [pool-755-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99502 [pool-755-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99517 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99517 [pool-756-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99518 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99518 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99543 [pool-756-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99560 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99560 [pool-757-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99561 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99561 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 199583 [pool-757-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99601 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99601 [pool-758-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99602 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99602 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99634 [pool-758-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99653 [pool-759-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99675 [pool-759-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99694 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99694 [pool-760-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99695 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99695 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
6, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: w99721 [pool-760-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99745 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99745 [pool-761-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99745 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99745 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99766 [pool-761-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99783 [pool-762-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99805 [pool-762-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99827 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99827 [pool-763-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ritten 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encod99857 [pool-763-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99879 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99879 [pool-764-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99880 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99880 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99912 [pool-764-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99937 [pool-765-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
99964 [pool-765-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
99980 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
99980 [pool-766-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
99980 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
99980 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.ha100008 [pool-766-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100023 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100023 [pool-767-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100023 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100023 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100048 [pool-767-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100063 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100063 [pool-768-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100096 [pool-768-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100113 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100113 [pool-769-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100114 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100114 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
doop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw100147 [pool-769-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100162 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100162 [pool-770-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100163 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100163 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100194 [pool-770-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100209 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100210 [pool-771-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100216 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
100238 [pool-771-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100253 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100254 [pool-772-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100254 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100254 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.100288 [pool-772-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100319 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100319 [pool-773-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100320 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100320 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100349 [pool-773-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100351 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
100371 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100371 [pool-774-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100372 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100372 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100401 [pool-774-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100420 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100420 [pool-775-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100421 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100421 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:33 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, 100447 [pool-775-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100464 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100464 [pool-776-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100465 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100465 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100496 [pool-776-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100512 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100512 [pool-777-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100512 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100512 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100550 [pool-777-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100569 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100569 [pool-778-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100570 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100570 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWrite100602 [pool-778-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100624 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100624 [pool-779-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100659 [pool-779-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100676 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100676 [pool-780-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100677 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100677 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
Store: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare100706 [pool-780-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100724 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100724 [pool-781-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100724 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100724 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100752 [pool-781-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100768 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100768 [pool-782-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100768 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100768 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100800 [pool-782-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100823 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100823 [pool-783-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100824 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100824 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PA100853 [pool-783-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100874 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100874 [pool-784-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100875 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100875 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100908 [pool-784-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100928 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100928 [pool-785-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
100950 [pool-785-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
100969 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
100969 [pool-786-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
100970 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
100970 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
CKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPage100989 [pool-786-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101006 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101006 [pool-787-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101007 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101007 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101037 [pool-787-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101062 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101062 [pool-788-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101063 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101063 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101094 [pool-788-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101121 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101121 [pool-789-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101122 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101122 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
WriteStore: written 70B for [begin_lon] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 p101148 [pool-789-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101172 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101172 [pool-790-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101193 [pool-790-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101217 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101217 [pool-791-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101218 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101218 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101240 [pool-791-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101265 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101265 [pool-792-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apa101288 [pool-792-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101315 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101315 [pool-793-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101343 [pool-793-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101367 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101367 [pool-794-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101368 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101368 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101401 [pool-794-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101428 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101428 [pool-795-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101429 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101429 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
che.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:34 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE101461 [pool-795-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101486 [pool-796-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101486 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101486 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101512 [pool-796-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101533 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101533 [pool-797-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101533 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101533 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101565 [pool-797-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101586 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101586 [pool-798-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 62B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_row_key] BINARY: 1 values, 40B raw, 57B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [end_lon] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM 101618 [pool-798-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101636 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101636 [pool-799-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101636 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101636 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101637 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
101664 [pool-799-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101685 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101686 [pool-800-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101686 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101686 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101717 [pool-800-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101754 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101754 [pool-801-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101755 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101755 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values,101783 [pool-801-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101803 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101804 [pool-802-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101804 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101804 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101837 [pool-802-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101856 [pool-803-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101885 [pool-803-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101906 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
101909 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101909 [pool-804-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101909 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101909 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 88B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 210B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 80B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 210B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 80B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parque101937 [pool-804-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
101963 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
101963 [pool-805-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
101964 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
101964 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
101995 [pool-805-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102016 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102016 [pool-806-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102017 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102017 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102045 [pool-806-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102065 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102066 [pool-807-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102066 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102066 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
t.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing me102098 [pool-807-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102125 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102125 [pool-808-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102126 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102126 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102149 [pool-808-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102172 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102172 [pool-809-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
m columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PL102203 [pool-809-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102224 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102224 [pool-810-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102224 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102224 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102259 [pool-810-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102280 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102281 [pool-811-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102281 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102281 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102313 [pool-811-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102341 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102341 [pool-812-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102342 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102342 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
AIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 210B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 80B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:35 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.Co102378 [pool-812-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102403 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102403 [pool-813-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102404 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102404 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102448 [pool-813-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102465 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102465 [pool-814-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102466 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102466 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102511 [pool-814-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102534 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102534 [pool-815-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102535 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102535 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
lumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 62B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_row_key] BINARY: 1 values, 40B raw, 57B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 102566 [pool-815-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102594 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102594 [pool-816-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102595 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102595 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102640 [pool-816-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102664 [pool-817-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102692 [pool-817-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102711 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102711 [pool-818-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102711 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102711 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INF102734 [pool-818-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102750 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102750 [pool-819-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102751 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102751 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102775 [pool-819-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102790 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102790 [pool-820-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102790 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102790 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102814 [pool-820-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102829 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102829 [pool-821-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102830 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102830 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
O: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [begin_lat] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_ro102862 [pool-821-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102880 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102880 [pool-822-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102904 [pool-822-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102918 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102919 [pool-823-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102919 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102919 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
102950 [pool-823-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
102965 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
102966 [pool-824-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
102966 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
102966 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
w_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACK102991 [pool-824-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103008 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103008 [pool-825-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103008 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103008 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103040 [pool-825-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103065 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103065 [pool-826-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103066 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103066 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103100 [pool-826-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103124 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103124 [pool-827-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103125 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103125 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_p103158 [pool-827-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103180 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103180 [pool-828-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103180 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103180 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103212 [pool-828-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103235 [pool-829-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103264 [pool-829-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103284 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
103289 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103289 [pool-830-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103290 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103290 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
artition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32103327 [pool-830-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103352 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103352 [pool-831-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103390 [pool-831-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103413 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103413 [pool-832-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103414 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103414 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103448 [pool-832-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103470 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103470 [pool-833-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103471 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103471 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:36 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [end_lat] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 103500 [pool-833-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103527 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103527 [pool-834-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103556 [pool-834-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103578 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103578 [pool-835-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103579 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103579 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103579 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodi103616 [pool-835-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103631 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103631 [pool-836-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103632 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103632 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103657 [pool-836-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103675 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103675 [pool-837-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103676 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103676 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103700 [pool-837-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103716 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103716 [pool-838-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103716 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103716 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ngs: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parque103738 [pool-838-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103754 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103754 [pool-839-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103755 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103755 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103789 [pool-839-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103805 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103805 [pool-840-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103805 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103805 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103832 [pool-840-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103848 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103848 [pool-841-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103848 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103849 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
t.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 210B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 80B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 val103870 [pool-841-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103885 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103885 [pool-842-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103906 [pool-842-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103921 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103921 [pool-843-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103921 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103921 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
103941 [pool-843-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
103961 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
103961 [pool-844-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
103962 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
103962 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ues, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:3103982 [pool-844-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104003 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104003 [pool-845-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104004 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104004 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104024 [pool-845-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104040 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104040 [pool-846-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104041 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104041 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104062 [pool-846-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104078 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104078 [pool-847-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104078 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104078 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
2:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 62B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_row_key] BINARY: 1 values, 40B raw, 57B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written104103 [pool-847-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104119 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104119 [pool-848-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104120 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104120 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104141 [pool-848-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104157 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104158 [pool-849-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104158 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104158 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104188 [pool-849-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104207 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104207 [pool-850-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104208 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104208 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PL104239 [pool-850-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104255 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104255 [pool-851-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104256 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104256 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104281 [pool-851-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104299 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104299 [pool-852-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104299 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104299 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104320 [pool-852-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104335 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104335 [pool-853-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104336 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104336 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
AIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B for [begin_lat] DOUBLE: 1 values, 8B raw, 29B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:37 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteS104357 [pool-853-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104373 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104373 [pool-854-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104374 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104374 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104399 [pool-854-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104424 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104424 [pool-855-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104425 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104425 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104462 [pool-855-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104488 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104488 [pool-856-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104488 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104488 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
tore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, 104524 [pool-856-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104548 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104548 [pool-857-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104549 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104549 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104585 [pool-857-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104609 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104609 [pool-858-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104647 [pool-858-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104669 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104669 [pool-859-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104670 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104670 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [104698 [pool-859-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104724 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104724 [pool-860-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104725 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104725 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104750 [pool-860-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104767 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104767 [pool-861-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104767 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104767 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104799 [pool-861-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104814 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104814 [pool-862-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104814 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104814 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 104828 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
104841 [pool-862-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104856 [pool-863-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104857 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104857 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104885 [pool-863-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104904 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104904 [pool-864-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104905 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104905 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.ap104931 [pool-864-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104947 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104947 [pool-865-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104948 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104948 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104980 [pool-865-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104995 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104995 [pool-866-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104996 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104996 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105021 [pool-866-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105036 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105036 [pool-867-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DO105065 [pool-867-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105082 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105082 [pool-868-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105082 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105083 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105103 [pool-868-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105118 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105118 [pool-869-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105118 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105118 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105138 [pool-869-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105153 [pool-870-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105154 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105154 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
UBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar105178 [pool-870-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105181 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
105194 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105194 [pool-871-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105195 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105195 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105229 [pool-871-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105244 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105244 [pool-872-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105245 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105245 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105277 [pool-872-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105292 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105292 [pool-873-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStor105314 [pool-873-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105329 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105329 [pool-874-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105330 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105330 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105358 [pool-874-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105377 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105377 [pool-875-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105410 [pool-875-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105431 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105431 [pool-876-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105432 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105432 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
e: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:38 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, 105459 [pool-876-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105476 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105476 [pool-877-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105500 [pool-877-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105517 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105517 [pool-878-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105518 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105518 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105539 [pool-878-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105554 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105554 [pool-879-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105554 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105555 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.105579 [pool-879-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105594 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105594 [pool-880-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105594 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105594 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105620 [pool-880-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105636 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105636 [pool-881-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105637 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105637 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105673 [pool-881-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105696 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105696 [pool-882-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105696 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105696 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1105722 [pool-882-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105742 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105742 [pool-883-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105743 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105743 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105763 [pool-883-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105780 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105780 [pool-884-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105781 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105781 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105807 [pool-884-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105821 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105821 [pool-885-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105822 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105822 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPag105846 [pool-885-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105860 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105860 [pool-886-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105883 [pool-886-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105899 [pool-887-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105900 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105900 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105921 [pool-887-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105937 [pool-888-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
eWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE105961 [pool-888-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105976 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105976 [pool-889-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105999 [pool-889-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106015 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106015 [pool-890-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106016 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106016 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106039 [pool-890-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106057 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106058 [pool-891-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106058 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106058 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO106104 [pool-891-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106113 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
106133 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106133 [pool-892-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106134 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106134 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106177 [pool-892-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106207 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106207 [pool-893-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106208 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106208 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [106240 [pool-893-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106268 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106268 [pool-894-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106269 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106269 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106299 [pool-894-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106323 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106324 [pool-895-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106324 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106324 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106350 [pool-895-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106382 [pool-896-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:39 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BI106414 [pool-896-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106434 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106434 [pool-897-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106434 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106434 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106463 [pool-897-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106485 [pool-898-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106518 [pool-898-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106537 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106537 [pool-899-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106538 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106538 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
T_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 208B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPa106570 [pool-899-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106588 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106588 [pool-900-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106589 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106589 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106615 [pool-900-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106649 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
106649 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
106750 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
107625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
107625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
107702 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
107788 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
107794 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
107794 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
107794 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
107951 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
109511 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
109783 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
111009 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
111958 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 77
111958 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
112010 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
112179 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/0174b052-1ba1-4033-892a-acdcd8af2b77_178_001.parquet
112199 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/0174b052-1ba1-4033-892a-acdcd8af2b77_178_001.parquet
112199 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/0174b052-1ba1-4033-892a-acdcd8af2b77_178_001.parquet => [b824c05e-765c-4309-9a9b-ebf218c18e1e]
112220 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/023de473-8831-4940-a6ce-31bc89f68767_3_001.parquet
112234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/023de473-8831-4940-a6ce-31bc89f68767_3_001.parquet
112234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/023de473-8831-4940-a6ce-31bc89f68767_3_001.parquet => [07ede2b7-7fcb-45f1-a019-20736e3f4e2f]
112251 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/059c925b-6036-4857-85ca-03abfcd1e5de_101_001.parquet
112267 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/059c925b-6036-4857-85ca-03abfcd1e5de_101_001.parquet
112267 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/059c925b-6036-4857-85ca-03abfcd1e5de_101_001.parquet => [74200da5-3928-45e8-be2c-6d6f84f2ba4b]
112283 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/0830fe67-965b-4049-9d30-2b19cabbc5ee_120_001.parquet
112296 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/0830fe67-965b-4049-9d30-2b19cabbc5ee_120_001.parquet
112296 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/0830fe67-965b-4049-9d30-2b19cabbc5ee_120_001.parquet => [b1a485a2-515c-465b-b28a-2fb4015e9d3d]
112312 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/0b72f43a-c7d2-4232-8489-88c08a2bf376_59_001.parquet
112325 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/0b72f43a-c7d2-4232-8489-88c08a2bf376_59_001.parquet
112325 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/0b72f43a-c7d2-4232-8489-88c08a2bf376_59_001.parquet => [d6728d27-2e85-4ccb-9029-4380881f5825]
112338 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/0ba00e2b-b05d-4336-800b-4c20ea6ff52e_37_001.parquet
112350 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/0ba00e2b-b05d-4336-800b-4c20ea6ff52e_37_001.parquet
112350 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/0ba00e2b-b05d-4336-800b-4c20ea6ff52e_37_001.parquet => [953fea16-e4f5-4cdc-945c-063f1d74c42f]
112363 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/0ca9e234-b030-4668-ad3a-54c1c8d293cf_118_001.parquet
geWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 304
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [_hoodie_commit_seqno] BINARY: 1 values, 22B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 210B for [_hoodie_file_name] BINARY: 1 values, 62B raw, 80B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:40 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:45 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46112374 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/0ca9e234-b030-4668-ad3a-54c1c8d293cf_118_001.parquet
112374 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/0ca9e234-b030-4668-ad3a-54c1c8d293cf_118_001.parquet => [ac8b5f87-1144-4cfb-b526-4f9d5a9a1195]
112388 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/0d7e9fd5-4417-4b20-9a03-a7667369eb92_110_001.parquet
112403 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/0d7e9fd5-4417-4b20-9a03-a7667369eb92_110_001.parquet
112403 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/0d7e9fd5-4417-4b20-9a03-a7667369eb92_110_001.parquet => [94dac219-0874-4bec-b863-6c2f04bf3c9d]
112416 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/106c74ea-6ab0-4409-b3b5-2be0f21c2c9c_21_001.parquet
112430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/106c74ea-6ab0-4409-b3b5-2be0f21c2c9c_21_001.parquet
112430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/106c74ea-6ab0-4409-b3b5-2be0f21c2c9c_21_001.parquet => [407c30dc-4560-481f-8eea-3fd88ecec75c]
112444 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/13a34b92-e232-4bde-860d-fa54ff4209ca_64_001.parquet
112457 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/13a34b92-e232-4bde-860d-fa54ff4209ca_64_001.parquet
112457 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/13a34b92-e232-4bde-860d-fa54ff4209ca_64_001.parquet => [def744fc-cf21-44bd-9963-4b65f75f986a]
112471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/146d6445-977b-4b44-b029-64bd9b96c3ca_144_001.parquet
112480 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/146d6445-977b-4b44-b029-64bd9b96c3ca_144_001.parquet
112480 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/146d6445-977b-4b44-b029-64bd9b96c3ca_144_001.parquet => [1f8c6c24-0eb7-404d-8c62-0e17fcaaeb50]
112494 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/193436b9-0355-4341-9bef-d3c4a8351a52_71_001.parquet
112505 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/193436b9-0355-4341-9bef-d3c4a8351a52_71_001.parquet
112505 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/193436b9-0355-4341-9bef-d3c4a8351a52_71_001.parquet => [04c16a41-a3f3-46a8-b010-1072dd214df6]
112518 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/2065376f-202e-45d6-815d-06178eefffa1_80_001.parquet
112527 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/2065376f-202e-45d6-815d-06178eefffa1_80_001.parquet
112527 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/2065376f-202e-45d6-815d-06178eefffa1_80_001.parquet => [3242dcd7-de50-4248-aef3-ae4c24958373]
112540 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/21c3b9c9-4940-4573-ab8a-e2584334ef95_154_001.parquet
112549 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/21c3b9c9-4940-4573-ab8a-e2584334ef95_154_001.parquet
112549 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/21c3b9c9-4940-4573-ab8a-e2584334ef95_154_001.parquet => [6309149a-e024-47ec-ad8e-9af885b771d9]
112563 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/25e7debf-e628-44b8-a80a-74f00da7a565_127_001.parquet
112572 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/25e7debf-e628-44b8-a80a-74f00da7a565_127_001.parquet
112572 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/25e7debf-e628-44b8-a80a-74f00da7a565_127_001.parquet => [ce6e6852-f6d2-489b-8572-a62ab3befc64]
112586 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/29788d4f-51d1-4b95-9873-b41df4fd35c9_137_001.parquet
112598 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/29788d4f-51d1-4b95-9873-b41df4fd35c9_137_001.parquet
112598 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/29788d4f-51d1-4b95-9873-b41df4fd35c9_137_001.parquet => [fe1f20b5-136c-41d2-ab9f-670efa4bb988]
112611 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/2c569c59-de2d-4f14-a0bd-3284ec72f554_122_001.parquet
112625 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/2c569c59-de2d-4f14-a0bd-3284ec72f554_122_001.parquet
112625 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/2c569c59-de2d-4f14-a0bd-3284ec72f554_122_001.parquet => [b5d6f603-556a-4353-9c52-9193ea71fa31]
112639 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/2c7546c1-60a3-4899-ac0a-a262b0aab0e4_146_001.parquet
 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 201112650 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/2c7546c1-60a3-4899-ac0a-a262b0aab0e4_146_001.parquet
112650 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/2c7546c1-60a3-4899-ac0a-a262b0aab0e4_146_001.parquet => [2c79c001-d489-4f91-b5ec-75b1bcf3bd79]
112663 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/2ed8a782-d7da-4a53-bfda-7b30575069ef_159_001.parquet
112675 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/2ed8a782-d7da-4a53-bfda-7b30575069ef_159_001.parquet
112675 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/2ed8a782-d7da-4a53-bfda-7b30575069ef_159_001.parquet => [72363405-ec63-47e7-aaf6-cee18ff36e60]
112689 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/2ff50cc5-b11a-4068-ad69-3e88d1ebf691_179_001.parquet
112700 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/2ff50cc5-b11a-4068-ad69-3e88d1ebf691_179_001.parquet
112700 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/2ff50cc5-b11a-4068-ad69-3e88d1ebf691_179_001.parquet => [ba18bce5-a9f6-4c06-a073-205c229cab1c]
112713 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/337d3e5c-5453-4748-bb2c-3981ac0340f8_168_001.parquet
112723 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/337d3e5c-5453-4748-bb2c-3981ac0340f8_168_001.parquet
112723 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/337d3e5c-5453-4748-bb2c-3981ac0340f8_168_001.parquet => [93280741-e484-4a8a-97ff-e295aa16067e]
112737 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/36c4252a-5aea-46b8-a7ca-a096a2648b09_164_001.parquet
112748 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/36c4252a-5aea-46b8-a7ca-a096a2648b09_164_001.parquet
112748 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/36c4252a-5aea-46b8-a7ca-a096a2648b09_164_001.parquet => [7b6f3f22-9b5e-4f10-ac78-5dfbd83b7f9f]
112761 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/36eae9f0-2baa-45b0-ab23-9852ff4fa337_167_001.parquet
112771 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
112772 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/36eae9f0-2baa-45b0-ab23-9852ff4fa337_167_001.parquet
112772 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/36eae9f0-2baa-45b0-ab23-9852ff4fa337_167_001.parquet => [89f91a24-e214-4c12-ac5b-831097e8c83d]
112785 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/388c0f0c-3f76-4bd8-abff-ae5606b3968f_111_001.parquet
112798 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/388c0f0c-3f76-4bd8-abff-ae5606b3968f_111_001.parquet
112798 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/388c0f0c-3f76-4bd8-abff-ae5606b3968f_111_001.parquet => [99a25d47-bc8d-42c3-a282-d63e204e719d]
112822 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/38cd590f-cd34-44c2-b4ba-9aae8e7bba9c_54_001.parquet
112837 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/38cd590f-cd34-44c2-b4ba-9aae8e7bba9c_54_001.parquet
112837 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/38cd590f-cd34-44c2-b4ba-9aae8e7bba9c_54_001.parquet => [be0220fd-17fb-42f4-9b43-2e5b55525d6e]
112859 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/3995a85f-1f9e-4892-b65b-661b1cfbbc7b_106_001.parquet
112872 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/3995a85f-1f9e-4892-b65b-661b1cfbbc7b_106_001.parquet
112872 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/3995a85f-1f9e-4892-b65b-661b1cfbbc7b_106_001.parquet => [829f22f1-d858-42c2-a7ff-9514ee9b36d2]
112892 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/3f525143-d6ab-49ed-b271-f3aeebfe7d53_186_001.parquet
112904 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/3f525143-d6ab-49ed-b271-f3aeebfe7d53_186_001.parquet
112904 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/3f525143-d6ab-49ed-b271-f3aeebfe7d53_186_001.parquet => [d89a22dd-2520-4603-b1bd-d8b599520c75]
112923 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/4249d97b-556d-4021-a4a0-abb7f99214b0_123_001.parquet
112936 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/4249d97b-556d-4021-a4a0-abb7f99214b0_123_001.parquet
112936 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/4249d97b-556d-4021-a4a0-abb7f99214b0_123_001.parquet => [bdc4f8bd-6414-4320-a139-6181eeb4a54d]
112954 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/4b888a2a-49a5-469e-9f9e-35b13dc916c0_86_001.parquet
112968 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/4b888a2a-49a5-469e-9f9e-35b13dc916c0_86_001.parquet
112968 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/4b888a2a-49a5-469e-9f9e-35b13dc916c0_86_001.parquet => [3f37ef99-d1ac-462e-a5e5-a7295242dcab]
112988 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/4df7d6b1-9ad6-4e1b-8e00-fb547714715a_44_001.parquet
8 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO112999 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/4df7d6b1-9ad6-4e1b-8e00-fb547714715a_44_001.parquet
112999 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/4df7d6b1-9ad6-4e1b-8e00-fb547714715a_44_001.parquet => [a934b229-d185-483a-b469-b5ca05db8c8e]
113016 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/4edde39d-9eaa-4f32-847f-918cc4d291f8_98_001.parquet
113026 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/4edde39d-9eaa-4f32-847f-918cc4d291f8_98_001.parquet
113026 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/4edde39d-9eaa-4f32-847f-918cc4d291f8_98_001.parquet => [6db348b2-ce8e-4952-8c69-e1297e840dfb]
113041 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/50cf8005-cc57-499c-8096-6e2a4c6900b3_68_001.parquet
113054 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/50cf8005-cc57-499c-8096-6e2a4c6900b3_68_001.parquet
113054 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/50cf8005-cc57-499c-8096-6e2a4c6900b3_68_001.parquet => [fa28a652-202d-44da-94ac-a6722690f390]
113069 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/55fc3f58-8fc9-4977-b9a4-f05d26047eef_166_001.parquet
113080 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/55fc3f58-8fc9-4977-b9a4-f05d26047eef_166_001.parquet
113080 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/55fc3f58-8fc9-4977-b9a4-f05d26047eef_166_001.parquet => [861871fe-1bf8-4e3d-bbc7-6fad6cc42e1e]
113095 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/57a988bc-150a-435b-89c1-99a1aa396cb2_65_001.parquet
113105 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/57a988bc-150a-435b-89c1-99a1aa396cb2_65_001.parquet
113105 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/57a988bc-150a-435b-89c1-99a1aa396cb2_65_001.parquet => [e8a1c87f-b9c5-44ad-9297-adce78292047]
113120 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/5a6106bf-06b6-40c1-a584-f905cd3b6f78_129_001.parquet
113130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/5a6106bf-06b6-40c1-a584-f905cd3b6f78_129_001.parquet
113130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/5a6106bf-06b6-40c1-a584-f905cd3b6f78_129_001.parquet => [dc6d6e0f-7e16-4c77-91cb-73884dd94b0c]
113144 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/64c0d763-8189-4ecd-97ab-7a98a9776e14_197_001.parquet
113153 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/64c0d763-8189-4ecd-97ab-7a98a9776e14_197_001.parquet
113153 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/64c0d763-8189-4ecd-97ab-7a98a9776e14_197_001.parquet => [f5d0d5e2-f52d-4c42-8487-a2b0a1f51efe]
113163 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
113168 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/665d76bf-0202-4daa-b1be-a65bcddd08e6_73_001.parquet
113183 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/665d76bf-0202-4daa-b1be-a65bcddd08e6_73_001.parquet
113183 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/665d76bf-0202-4daa-b1be-a65bcddd08e6_73_001.parquet => [0ec02e26-dfdf-45ca-ab29-6946635bd508]
113204 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/692f5e60-eece-4833-8e3e-420122448568_172_001.parquet
113222 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/692f5e60-eece-4833-8e3e-420122448568_172_001.parquet
113222 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/692f5e60-eece-4833-8e3e-420122448568_172_001.parquet => [a4cd159d-091f-4353-a55a-0205ca43ee19]
113247 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/6ac791ee-079e-47cf-82ea-5e40372fb25a_156_001.parquet
113259 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/6ac791ee-079e-47cf-82ea-5e40372fb25a_156_001.parquet
113259 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/6ac791ee-079e-47cf-82ea-5e40372fb25a_156_001.parquet => [6ec2a0ba-ca1e-49ee-8fc9-e1ba3c453ad6]
113295 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/6c96339b-e7c8-40ce-990e-af29d53945d5_58_001.parquet
113306 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/6c96339b-e7c8-40ce-990e-af29d53945d5_58_001.parquet
113306 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/6c96339b-e7c8-40ce-990e-af29d53945d5_58_001.parquet => [d4480751-e105-4a44-b37b-ad3ca8459381]
113324 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/6d0a3bab-b781-4ed1-a8a7-1a9ecb1b1fae_7_001.parquet
: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:46 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:4113335 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/6d0a3bab-b781-4ed1-a8a7-1a9ecb1b1fae_7_001.parquet
113335 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/6d0a3bab-b781-4ed1-a8a7-1a9ecb1b1fae_7_001.parquet => [0fd73435-4716-49d9-b21f-9e8e64049491]
113350 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/7602314f-2bc4-488b-b0c9-bbe568de32a6_170_001.parquet
113361 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/7602314f-2bc4-488b-b0c9-bbe568de32a6_170_001.parquet
113361 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/7602314f-2bc4-488b-b0c9-bbe568de32a6_170_001.parquet => [9874b95f-166f-4afb-868e-f832e4247296]
113375 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/785f84ef-9bd2-41a0-b3af-13fdd158e5f1_4_001.parquet
113386 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/785f84ef-9bd2-41a0-b3af-13fdd158e5f1_4_001.parquet
113386 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/785f84ef-9bd2-41a0-b3af-13fdd158e5f1_4_001.parquet => [09cfcdc3-0fe0-4950-850c-80aaf692953d]
113402 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/7b0fa27d-c498-4b27-9cae-27e7f4134bd0_105_001.parquet
113413 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/7b0fa27d-c498-4b27-9cae-27e7f4134bd0_105_001.parquet
113414 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/7b0fa27d-c498-4b27-9cae-27e7f4134bd0_105_001.parquet => [8222895e-469d-4eb6-9918-2a88b263d35b]
113426 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/893f2cc0-c0fa-41b2-84d6-910dbe1324dc_49_001.parquet
113439 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/893f2cc0-c0fa-41b2-84d6-910dbe1324dc_49_001.parquet
113439 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/893f2cc0-c0fa-41b2-84d6-910dbe1324dc_49_001.parquet => [b08ce3a0-4159-426d-8db7-3cc57d77d595]
113452 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/8ce1efa4-2fae-4e4f-b468-4a9d8290baf6_145_001.parquet
113464 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/8ce1efa4-2fae-4e4f-b468-4a9d8290baf6_145_001.parquet
113464 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/8ce1efa4-2fae-4e4f-b468-4a9d8290baf6_145_001.parquet => [275e1123-383b-45df-94ac-395805a8e178]
113478 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/96681dbb-1828-468f-a9ce-4876c1f905fe_113_001.parquet
113488 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/96681dbb-1828-468f-a9ce-4876c1f905fe_113_001.parquet
113488 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/96681dbb-1828-468f-a9ce-4876c1f905fe_113_001.parquet => [9ad92a14-e787-4dc4-8d7a-bbce8a21ff8e]
113501 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/975ede62-cde8-4890-b01d-29f03f915e45_6_001.parquet
113510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/975ede62-cde8-4890-b01d-29f03f915e45_6_001.parquet
113510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/975ede62-cde8-4890-b01d-29f03f915e45_6_001.parquet => [0f056142-e0ea-4eb5-9df9-4f85a105d81a]
113523 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/9ceabc82-5813-47a8-a16b-e12f39e97375_28_001.parquet
113532 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/9ceabc82-5813-47a8-a16b-e12f39e97375_28_001.parquet
113532 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/9ceabc82-5813-47a8-a16b-e12f39e97375_28_001.parquet => [595995f0-fbb4-4103-8f9f-c2c844e322da]
113545 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/9e37f982-944d-4a0c-adc3-da388f8d4002_194_001.parquet
113554 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/9e37f982-944d-4a0c-adc3-da388f8d4002_194_001.parquet
113554 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/9e37f982-944d-4a0c-adc3-da388f8d4002_194_001.parquet => [f121eeb7-116c-49bf-97b1-d15411dfc183]
113567 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/a097451e-423a-4c08-83da-0198849a2e9d_45_001.parquet
113575 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/a097451e-423a-4c08-83da-0198849a2e9d_45_001.parquet
113575 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/a097451e-423a-4c08-83da-0198849a2e9d_45_001.parquet => [aa04f85b-f148-49fb-8fd4-c3dfe3f96297]
113588 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/a1f1b8cd-0029-4c83-8af6-018e43b5da8a_112_001.parquet
113596 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/a1f1b8cd-0029-4c83-8af6-018e43b5da8a_112_001.parquet
113596 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/a1f1b8cd-0029-4c83-8af6-018e43b5da8a_112_001.parquet => [9abdb179-4165-4e12-95dd-26cb69e9a3b5]
113609 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/a42cc404-8d17-4a59-9a9c-3a60646a3512_180_001.parquet
6 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.ap113617 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/a42cc404-8d17-4a59-9a9c-3a60646a3512_180_001.parquet
113617 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/a42cc404-8d17-4a59-9a9c-3a60646a3512_180_001.parquet => [bdb199b0-d2f6-4d11-9976-707fdf6ccfa4]
113631 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/a9e2ce5e-a002-495f-ae45-95b134d8e8bf_82_001.parquet
113639 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/a9e2ce5e-a002-495f-ae45-95b134d8e8bf_82_001.parquet
113639 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/a9e2ce5e-a002-495f-ae45-95b134d8e8bf_82_001.parquet => [3ae829a4-a7b5-45b4-9f75-02bdba30cab7]
113652 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/b1755135-d42b-4ab2-86a8-4ea7d5a5675e_143_001.parquet
113661 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/b1755135-d42b-4ab2-86a8-4ea7d5a5675e_143_001.parquet
113661 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/b1755135-d42b-4ab2-86a8-4ea7d5a5675e_143_001.parquet => [1c03b879-48cf-49f3-8e0d-4ce22d9c953b]
113675 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/b1a7c998-50e9-48a6-a686-930d7a4d6dc2_72_001.parquet
113683 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/b1a7c998-50e9-48a6-a686-930d7a4d6dc2_72_001.parquet
113683 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/b1a7c998-50e9-48a6-a686-930d7a4d6dc2_72_001.parquet => [0e0fd7bd-db38-4351-811f-6fefb0c95de3]
113697 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/b8f67be5-30b7-49d1-8c95-e8de7c7e5be5_175_001.parquet
113707 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/b8f67be5-30b7-49d1-8c95-e8de7c7e5be5_175_001.parquet
113707 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/b8f67be5-30b7-49d1-8c95-e8de7c7e5be5_175_001.parquet => [b385d9db-93d5-4b0c-b99b-f0cf7022d2df]
113720 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/bd475300-9751-49b3-8f16-7ab052006995_138_001.parquet
113729 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/bd475300-9751-49b3-8f16-7ab052006995_138_001.parquet
113729 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/bd475300-9751-49b3-8f16-7ab052006995_138_001.parquet => [03c13273-a4c8-4903-ae25-1f73e592f53a]
113742 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/c75f7ed4-cbeb-4f71-8328-db6e30de849b_16_001.parquet
113750 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/c75f7ed4-cbeb-4f71-8328-db6e30de849b_16_001.parquet
113750 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/c75f7ed4-cbeb-4f71-8328-db6e30de849b_16_001.parquet => [2c9dc1c2-4cb7-43e4-982d-5ac75bc983b3]
113764 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/c7cc0fa3-654d-474a-82ef-dc73a780bc7a_81_001.parquet
113772 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/c7cc0fa3-654d-474a-82ef-dc73a780bc7a_81_001.parquet
113772 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/c7cc0fa3-654d-474a-82ef-dc73a780bc7a_81_001.parquet => [351ccae4-515f-4590-aae7-d5c13c676c1e]
113786 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/c893fd69-4f10-4070-9e39-395885667e1e_133_001.parquet
113794 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/c893fd69-4f10-4070-9e39-395885667e1e_133_001.parquet
113794 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/c893fd69-4f10-4070-9e39-395885667e1e_133_001.parquet => [f2fe405e-6fe1-420c-ab2f-805281969f6d]
113808 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/c89f6ff8-007c-431a-afc0-74ed6b09d534_140_001.parquet
113816 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/c89f6ff8-007c-431a-afc0-74ed6b09d534_140_001.parquet
113816 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/c89f6ff8-007c-431a-afc0-74ed6b09d534_140_001.parquet => [087ac8fe-5acf-4be3-a21d-227cba7e8791]
113829 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/c8ec34e0-80c4-4ca8-aad7-a88d9ee3b842_55_001.parquet
113837 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/c8ec34e0-80c4-4ca8-aad7-a88d9ee3b842_55_001.parquet
113837 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/c8ec34e0-80c4-4ca8-aad7-a88d9ee3b842_55_001.parquet => [c2007225-fdb7-4411-ae41-8e60e09ce6d5]
113850 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/cb511c7d-05a6-4ce2-b70c-9b3f6ffee7d2_128_001.parquet
ache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INF113859 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/cb511c7d-05a6-4ce2-b70c-9b3f6ffee7d2_128_001.parquet
113859 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/cb511c7d-05a6-4ce2-b70c-9b3f6ffee7d2_128_001.parquet => [daf2661b-66ff-4bf0-b157-6d57e027c9ae]
113872 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/d9ae8b58-c820-4579-8f4a-f5df9020c420_142_001.parquet
113879 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/d9ae8b58-c820-4579-8f4a-f5df9020c420_142_001.parquet
113880 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/d9ae8b58-c820-4579-8f4a-f5df9020c420_142_001.parquet => [1b2b0e21-fe8b-408d-94bc-f937d5c1c0b4]
113893 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/da03ffce-2090-45a8-80de-98e67a630b2f_60_001.parquet
113902 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/da03ffce-2090-45a8-80de-98e67a630b2f_60_001.parquet
113902 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/da03ffce-2090-45a8-80de-98e67a630b2f_60_001.parquet => [d7ba7d84-80c6-49b8-b705-f39a24a91224]
113915 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/e0b2d703-0c0f-480e-a248-39249678d169_63_001.parquet
113925 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/e0b2d703-0c0f-480e-a248-39249678d169_63_001.parquet
113925 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/e0b2d703-0c0f-480e-a248-39249678d169_63_001.parquet => [de6d4387-8c31-4946-a737-c660dd96a111]
113938 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/e2931a58-6b1b-4697-be05-2d8bd7ad464e_15_001.parquet
113946 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/e2931a58-6b1b-4697-be05-2d8bd7ad464e_15_001.parquet
113946 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/e2931a58-6b1b-4697-be05-2d8bd7ad464e_15_001.parquet => [2c65dcb2-6ebb-401b-8d6b-f2cc911e98bd]
113960 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2016/03/15/e5891d29-5e3e-4621-989f-e6f416bc5612_193_001.parquet
113968 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2016/03/15/e5891d29-5e3e-4621-989f-e6f416bc5612_193_001.parquet
113968 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2016/03/15/e5891d29-5e3e-4621-989f-e6f416bc5612_193_001.parquet => [f1012797-c55b-4185-8454-95a52ab85f88]
113984 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/e7c5dac1-6af6-448e-8b86-3d9bd8ea82d7_30_001.parquet
113992 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/e7c5dac1-6af6-448e-8b86-3d9bd8ea82d7_30_001.parquet
113992 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/e7c5dac1-6af6-448e-8b86-3d9bd8ea82d7_30_001.parquet => [649da078-40d7-4000-96ce-5c8f3ef7079f]
114005 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/ee399a02-bde8-4cdc-9942-127bebf29ae9_36_001.parquet
114013 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/ee399a02-bde8-4cdc-9942-127bebf29ae9_36_001.parquet
114013 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/ee399a02-bde8-4cdc-9942-127bebf29ae9_36_001.parquet => [90b0eb7f-6244-4a62-b1f1-8bacdf74e712]
114027 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/f0023344-0111-493b-b948-c3ea25f9cb0b_97_001.parquet
114035 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/f0023344-0111-493b-b948-c3ea25f9cb0b_97_001.parquet
114035 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/f0023344-0111-493b-b948-c3ea25f9cb0b_97_001.parquet => [6af829df-0c4f-4c53-954a-7facf34fc018]
114048 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/f148dfb4-fbaf-4cc8-a1db-449ed71c8a92_107_001.parquet
114057 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/f148dfb4-fbaf-4cc8-a1db-449ed71c8a92_107_001.parquet
114057 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/f148dfb4-fbaf-4cc8-a1db-449ed71c8a92_107_001.parquet => [8657999e-f675-46eb-b025-1ca833179be3]
114071 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/f4cba25a-673c-470d-b827-3a51b41b81e2_51_001.parquet
114079 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/f4cba25a-673c-470d-b827-3a51b41b81e2_51_001.parquet
114079 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/f4cba25a-673c-470d-b827-3a51b41b81e2_51_001.parquet => [b94d37d9-0a9e-458e-9b6c-3c3801d1edb2]
114093 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/faed457c-c18c-4d05-ab1d-8e00e9e395d3_126_001.parquet
114101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/faed457c-c18c-4d05-ab1d-8e00e9e395d3_126_001.parquet
114102 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/faed457c-c18c-4d05-ab1d-8e00e9e395d3_126_001.parquet => [ce2eef2a-4db2-4b07-810a-60a25e69067b]
114115 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/16/fe0b5e65-4d74-4dbe-8707-e53d93496083_38_001.parquet
O: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.par114127 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/16/fe0b5e65-4d74-4dbe-8707-e53d93496083_38_001.parquet
114127 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/16/fe0b5e65-4d74-4dbe-8707-e53d93496083_38_001.parquet => [97146c6e-a179-4986-8169-3b670df013dd]
114141 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8242919542810974429/2015/03/17/ff8c5b91-4d96-4e85-9e61-2738a1bb450e_90_001.parquet
114153 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8242919542810974429/2015/03/17/ff8c5b91-4d96-4e85-9e61-2738a1bb450e_90_001.parquet
114153 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8242919542810974429/2015/03/17/ff8c5b91-4d96-4e85-9e61-2738a1bb450e_90_001.parquet => [4b5a092f-700e-4b9e-9a3f-c75f2d231cd6]
114222 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=77}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=24}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=28}}}
114252 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 435021
114252 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :77, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=893f2cc0-c0fa-41b2-84d6-910dbe1324dc}, 1=BucketInfo {bucketType=UPDATE, fileLoc=d9ae8b58-c820-4579-8f4a-f5df9020c420}, 2=BucketInfo {bucketType=UPDATE, fileLoc=b8f67be5-30b7-49d1-8c95-e8de7c7e5be5}, 3=BucketInfo {bucketType=UPDATE, fileLoc=ee399a02-bde8-4cdc-9942-127bebf29ae9}, 4=BucketInfo {bucketType=UPDATE, fileLoc=7b0fa27d-c498-4b27-9cae-27e7f4134bd0}, 5=BucketInfo {bucketType=UPDATE, fileLoc=fe0b5e65-4d74-4dbe-8707-e53d93496083}, 6=BucketInfo {bucketType=UPDATE, fileLoc=96681dbb-1828-468f-a9ce-4876c1f905fe}, 7=BucketInfo {bucketType=UPDATE, fileLoc=e7c5dac1-6af6-448e-8b86-3d9bd8ea82d7}, 8=BucketInfo {bucketType=UPDATE, fileLoc=f0023344-0111-493b-b948-c3ea25f9cb0b}, 9=BucketInfo {bucketType=UPDATE, fileLoc=f148dfb4-fbaf-4cc8-a1db-449ed71c8a92}, 10=BucketInfo {bucketType=UPDATE, fileLoc=4df7d6b1-9ad6-4e1b-8e00-fb547714715a}, 11=BucketInfo {bucketType=UPDATE, fileLoc=0174b052-1ba1-4033-892a-acdcd8af2b77}, 12=BucketInfo {bucketType=UPDATE, fileLoc=0b72f43a-c7d2-4232-8489-88c08a2bf376}, 13=BucketInfo {bucketType=UPDATE, fileLoc=ff8c5b91-4d96-4e85-9e61-2738a1bb450e}, 14=BucketInfo {bucketType=UPDATE, fileLoc=6c96339b-e7c8-40ce-990e-af29d53945d5}, 15=BucketInfo {bucketType=UPDATE, fileLoc=059c925b-6036-4857-85ca-03abfcd1e5de}, 16=BucketInfo {bucketType=UPDATE, fileLoc=193436b9-0355-4341-9bef-d3c4a8351a52}, 17=BucketInfo {bucketType=UPDATE, fileLoc=9ceabc82-5813-47a8-a16b-e12f39e97375}, 18=BucketInfo {bucketType=UPDATE, fileLoc=bd475300-9751-49b3-8f16-7ab052006995}, 19=BucketInfo {bucketType=UPDATE, fileLoc=2c7546c1-60a3-4899-ac0a-a262b0aab0e4}, 20=BucketInfo {bucketType=UPDATE, fileLoc=29788d4f-51d1-4b95-9873-b41df4fd35c9}, 21=BucketInfo {bucketType=UPDATE, fileLoc=692f5e60-eece-4833-8e3e-420122448568}, 22=BucketInfo {bucketType=UPDATE, fileLoc=f4cba25a-673c-470d-b827-3a51b41b81e2}, 23=BucketInfo {bucketType=UPDATE, fileLoc=023de473-8831-4940-a6ce-31bc89f68767}, 24=BucketInfo {bucketType=UPDATE, fileLoc=13a34b92-e232-4bde-860d-fa54ff4209ca}, 25=BucketInfo {bucketType=UPDATE, fileLoc=2ed8a782-d7da-4a53-bfda-7b30575069ef}, 26=BucketInfo {bucketType=UPDATE, fileLoc=50cf8005-cc57-499c-8096-6e2a4c6900b3}, 27=BucketInfo {bucketType=UPDATE, fileLoc=665d76bf-0202-4daa-b1be-a65bcddd08e6}, 28=BucketInfo {bucketType=UPDATE, fileLoc=975ede62-cde8-4890-b01d-29f03f915e45}, 29=BucketInfo {bucketType=UPDATE, fileLoc=cb511c7d-05a6-4ce2-b70c-9b3f6ffee7d2}, 30=BucketInfo {bucketType=UPDATE, fileLoc=106c74ea-6ab0-4409-b3b5-2be0f21c2c9c}, 31=BucketInfo {bucketType=UPDATE, fileLoc=55fc3f58-8fc9-4977-b9a4-f05d26047eef}, 32=BucketInfo {bucketType=UPDATE, fileLoc=388c0f0c-3f76-4bd8-abff-ae5606b3968f}, 33=BucketInfo {bucketType=UPDATE, fileLoc=36eae9f0-2baa-45b0-ab23-9852ff4fa337}, 34=BucketInfo {bucketType=UPDATE, fileLoc=4b888a2a-49a5-469e-9f9e-35b13dc916c0}, 35=BucketInfo {bucketType=UPDATE, fileLoc=a1f1b8cd-0029-4c83-8af6-018e43b5da8a}, 36=BucketInfo {bucketType=UPDATE, fileLoc=c893fd69-4f10-4070-9e39-395885667e1e}, 37=BucketInfo {bucketType=UPDATE, fileLoc=36c4252a-5aea-46b8-a7ca-a096a2648b09}, 38=BucketInfo {bucketType=UPDATE, fileLoc=9e37f982-944d-4a0c-adc3-da388f8d4002}, 39=BucketInfo {bucketType=UPDATE, fileLoc=a42cc404-8d17-4a59-9a9c-3a60646a3512}, 40=BucketInfo {bucketType=UPDATE, fileLoc=b1755135-d42b-4ab2-86a8-4ea7d5a5675e}, 41=BucketInfo {bucketType=UPDATE, fileLoc=4249d97b-556d-4021-a4a0-abb7f99214b0}, 42=BucketInfo {bucketType=UPDATE, fileLoc=2ff50cc5-b11a-4068-ad69-3e88d1ebf691}, 43=BucketInfo {bucketType=UPDATE, fileLoc=e5891d29-5e3e-4621-989f-e6f416bc5612}, 44=BucketInfo {bucketType=UPDATE, fileLoc=0d7e9fd5-4417-4b20-9a03-a7667369eb92}, 45=BucketInfo {bucketType=UPDATE, fileLoc=c7cc0fa3-654d-474a-82ef-dc73a780bc7a}, 46=BucketInfo {bucketType=UPDATE, fileLoc=6d0a3bab-b781-4ed1-a8a7-1a9ecb1b1fae}, 47=BucketInfo {bucketType=UPDATE, fileLoc=5a6106bf-06b6-40c1-a584-f905cd3b6f78}, 48=BucketInfo {bucketType=UPDATE, fileLoc=6ac791ee-079e-47cf-82ea-5e40372fb25a}, 49=BucketInfo {bucketType=UPDATE, fileLoc=3995a85f-1f9e-4892-b65b-661b1cfbbc7b}, 50=BucketInfo {bucketType=UPDATE, fileLoc=2065376f-202e-45d6-815d-06178eefffa1}, 51=BucketInfo {bucketType=UPDATE, fileLoc=b1a7c998-50e9-48a6-a686-930d7a4d6dc2}, 52=BucketInfo {bucketType=UPDATE, fileLoc=4edde39d-9eaa-4f32-847f-918cc4d291f8}, 53=BucketInfo {bucketType=UPDATE, fileLoc=da03ffce-2090-45a8-80de-98e67a630b2f}, 54=BucketInfo {bucketType=UPDATE, fileLoc=0ba00e2b-b05d-4336-800b-4c20ea6ff52e}, 55=BucketInfo {bucketType=UPDATE, fileLoc=c8ec34e0-80c4-4ca8-aad7-a88d9ee3b842}, 56=BucketInfo {bucketType=UPDATE, fileLoc=337d3e5c-5453-4748-bb2c-3981ac0340f8}, 57=BucketInfo {bucketType=UPDATE, fileLoc=0ca9e234-b030-4668-ad3a-54c1c8d293cf}, 58=BucketInfo {bucketType=UPDATE, fileLoc=faed457c-c18c-4d05-ab1d-8e00e9e395d3}, 59=BucketInfo {bucketType=UPDATE, fileLoc=785f84ef-9bd2-41a0-b3af-13fdd158e5f1}, 60=BucketInfo {bucketType=UPDATE, fileLoc=25e7debf-e628-44b8-a80a-74f00da7a565}, 61=BucketInfo {bucketType=UPDATE, fileLoc=0830fe67-965b-4049-9d30-2b19cabbc5ee}, 62=BucketInfo {bucketType=UPDATE, fileLoc=21c3b9c9-4940-4573-ab8a-e2584334ef95}, 63=BucketInfo {bucketType=UPDATE, fileLoc=a097451e-423a-4c08-83da-0198849a2e9d}, 64=BucketInfo {bucketType=UPDATE, fileLoc=a9e2ce5e-a002-495f-ae45-95b134d8e8bf}, 65=BucketInfo {bucketType=UPDATE, fileLoc=2c569c59-de2d-4f14-a0bd-3284ec72f554}, 66=BucketInfo {bucketType=UPDATE, fileLoc=57a988bc-150a-435b-89c1-99a1aa396cb2}, 67=BucketInfo {bucketType=UPDATE, fileLoc=e2931a58-6b1b-4697-be05-2d8bd7ad464e}, 68=BucketInfo {bucketType=UPDATE, fileLoc=e0b2d703-0c0f-480e-a248-39249678d169}, 69=BucketInfo {bucketType=UPDATE, fileLoc=38cd590f-cd34-44c2-b4ba-9aae8e7bba9c}, 70=BucketInfo {bucketType=UPDATE, fileLoc=c89f6ff8-007c-431a-afc0-74ed6b09d534}, 71=BucketInfo {bucketType=UPDATE, fileLoc=7602314f-2bc4-488b-b0c9-bbe568de32a6}, 72=BucketInfo {bucketType=UPDATE, fileLoc=146d6445-977b-4b44-b029-64bd9b96c3ca}, 73=BucketInfo {bucketType=UPDATE, fileLoc=8ce1efa4-2fae-4e4f-b468-4a9d8290baf6}, 74=BucketInfo {bucketType=UPDATE, fileLoc=64c0d763-8189-4ecd-97ab-7a98a9776e14}, 75=BucketInfo {bucketType=UPDATE, fileLoc=3f525143-d6ab-49ed-b271-f3aeebfe7d53}, 76=BucketInfo {bucketType=UPDATE, fileLoc=c75f7ed4-cbeb-4f71-8328-db6e30de849b}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{893f2cc0-c0fa-41b2-84d6-910dbe1324dc=0, d9ae8b58-c820-4579-8f4a-f5df9020c420=1, b8f67be5-30b7-49d1-8c95-e8de7c7e5be5=2, ee399a02-bde8-4cdc-9942-127bebf29ae9=3, 7b0fa27d-c498-4b27-9cae-27e7f4134bd0=4, fe0b5e65-4d74-4dbe-8707-e53d93496083=5, 96681dbb-1828-468f-a9ce-4876c1f905fe=6, e7c5dac1-6af6-448e-8b86-3d9bd8ea82d7=7, f0023344-0111-493b-b948-c3ea25f9cb0b=8, f148dfb4-fbaf-4cc8-a1db-449ed71c8a92=9, 4df7d6b1-9ad6-4e1b-8e00-fb547714715a=10, 0174b052-1ba1-4033-892a-acdcd8af2b77=11, 0b72f43a-c7d2-4232-8489-88c08a2bf376=12, ff8c5b91-4d96-4e85-9e61-2738a1bb450e=13, 6c96339b-e7c8-40ce-990e-af29d53945d5=14, 059c925b-6036-4857-85ca-03abfcd1e5de=15, 193436b9-0355-4341-9bef-d3c4a8351a52=16, 9ceabc82-5813-47a8-a16b-e12f39e97375=17, bd475300-9751-49b3-8f16-7ab052006995=18, 2c7546c1-60a3-4899-ac0a-a262b0aab0e4=19, 29788d4f-51d1-4b95-9873-b41df4fd35c9=20, 692f5e60-eece-4833-8e3e-420122448568=21, f4cba25a-673c-470d-b827-3a51b41b81e2=22, 023de473-8831-4940-a6ce-31bc89f68767=23, 13a34b92-e232-4bde-860d-fa54ff4209ca=24, 2ed8a782-d7da-4a53-bfda-7b30575069ef=25, 50cf8005-cc57-499c-8096-6e2a4c6900b3=26, 665d76bf-0202-4daa-b1be-a65bcddd08e6=27, 975ede62-cde8-4890-b01d-29f03f915e45=28, cb511c7d-05a6-4ce2-b70c-9b3f6ffee7d2=29, 106c74ea-6ab0-4409-b3b5-2be0f21c2c9c=30, 55fc3f58-8fc9-4977-b9a4-f05d26047eef=31, 388c0f0c-3f76-4bd8-abff-ae5606b3968f=32, 36eae9f0-2baa-45b0-ab23-9852ff4fa337=33, 4b888a2a-49a5-469e-9f9e-35b13dc916c0=34, a1f1b8cd-0029-4c83-8af6-018e43b5da8a=35, c893fd69-4f10-4070-9e39-395885667e1e=36, 36c4252a-5aea-46b8-a7ca-a096a2648b09=37, 9e37f982-944d-4a0c-adc3-da388f8d4002=38, a42cc404-8d17-4a59-9a9c-3a60646a3512=39, b1755135-d42b-4ab2-86a8-4ea7d5a5675e=40, 4249d97b-556d-4021-a4a0-abb7f99214b0=41, 2ff50cc5-b11a-4068-ad69-3e88d1ebf691=42, e5891d29-5e3e-4621-989f-e6f416bc5612=43, 0d7e9fd5-4417-4b20-9a03-a7667369eb92=44, c7cc0fa3-654d-474a-82ef-dc73a780bc7a=45, 6d0a3bab-b781-4ed1-a8a7-1a9ecb1b1fae=46, 5a6106bf-06b6-40c1-a584-f905cd3b6f78=47, 6ac791ee-079e-47cf-82ea-5e40372fb25a=48, 3995a85f-1f9e-4892-b65b-661b1cfbbc7b=49, 2065376f-202e-45d6-815d-06178eefffa1=50, b1a7c998-50e9-48a6-a686-930d7a4d6dc2=51, 4edde39d-9eaa-4f32-847f-918cc4d291f8=52, da03ffce-2090-45a8-80de-98e67a630b2f=53, 0ba00e2b-b05d-4336-800b-4c20ea6ff52e=54, c8ec34e0-80c4-4ca8-aad7-a88d9ee3b842=55, 337d3e5c-5453-4748-bb2c-3981ac0340f8=56, 0ca9e234-b030-4668-ad3a-54c1c8d293cf=57, faed457c-c18c-4d05-ab1d-8e00e9e395d3=58, 785f84ef-9bd2-41a0-b3af-13fdd158e5f1=59, 25e7debf-e628-44b8-a80a-74f00da7a565=60, 0830fe67-965b-4049-9d30-2b19cabbc5ee=61, 21c3b9c9-4940-4573-ab8a-e2584334ef95=62, a097451e-423a-4c08-83da-0198849a2e9d=63, a9e2ce5e-a002-495f-ae45-95b134d8e8bf=64, 2c569c59-de2d-4f14-a0bd-3284ec72f554=65, 57a988bc-150a-435b-89c1-99a1aa396cb2=66, e2931a58-6b1b-4697-be05-2d8bd7ad464e=67, e0b2d703-0c0f-480e-a248-39249678d169=68, 38cd590f-cd34-44c2-b4ba-9aae8e7bba9c=69, c89f6ff8-007c-431a-afc0-74ed6b09d534=70, 7602314f-2bc4-488b-b0c9-bbe568de32a6=71, 146d6445-977b-4b44-b029-64bd9b96c3ca=72, 8ce1efa4-2fae-4e4f-b468-4a9d8290baf6=73, 64c0d763-8189-4ecd-97ab-7a98a9776e14=74, 3f525143-d6ab-49ed-b271-f3aeebfe7d53=75, c75f7ed4-cbeb-4f71-8328-db6e30de849b=76}
114270 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 002
114272 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
114363 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
quet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:47 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 37B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [115040 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 300
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_seqno] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 203B for [_hoodie_file_name] BINARY: 1 values, 60B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 85B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 85B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.Colu116329 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
mnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, enco116903 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
dings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:50 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memor117592 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
y in 1 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path]118115 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
 BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 77B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:51 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 62B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_row_key] BINARY: 1 values, 40B raw, 57B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 206B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 156B for [_row_key] BINARY: 1 values, 40B raw, 59B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoo118779 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
118780 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
118841 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
119971 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
120097 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
120097 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
120345 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
120352 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
120352 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
120450 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
120894 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
120894 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
120901 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
121176 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=56, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=81, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=63, numUpdates=0}}}
121183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
121183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
121183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
121183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
121183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
121184 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 81, totalInsertBuckets => 1, recordsPerBucket => 500000
121184 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
121184 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
121184 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
121184 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
121184 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
121206 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
121206 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
121412 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121412 [pool-1224-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121450 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121450 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121482 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
121487 [pool-1224-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121513 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121513 [pool-1225-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121568 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121568 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
p.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 302
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_time] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 1 values, 21B raw, 39B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 207B for [_hoodie_file_name] BINARY: 1 values, 61B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [timestamp] DOUBLE: 1 values, 8B raw, 23B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 1 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 1 values, 14B raw, 34B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [begin_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lat] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [end_lon] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:52 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [fare] DOUBLE: 1 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,750
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 56 values, 790B raw, 147B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,357B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,258B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,350B for [_row_key] BINARY: 56 values, 2,240B raw, 1,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 15,500
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_commit_seqno] BINARY: 81 values, 1,141B raw, 190B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,913B for [_hoodie_record_key] BINARY: 81 values, 3,247B raw, 1,813B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 121604 [pool-1225-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121632 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121632 [pool-1226-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121673 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121673 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121712 [pool-1226-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121735 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
121735 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
121851 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
121851 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
122122 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
122130 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
122130 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
122255 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
122607 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
122794 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
122795 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
123041 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 63 for /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_001.parquet
123051 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_001.parquet
123051 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 63 results, for file /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_001.parquet => [00d88660-89da-4423-affb-90a28d357f33, 07b99bef-0761-4908-b1df-ddea116b8cb1, 0b92f262-f0e0-499b-8946-d00806b49169, 0db303be-c841-499f-9bdd-5c60fc8b208f, 0f0d109a-f166-4ed0-9294-bae10caaed0d, 1065220a-591a-499d-a57d-8475fcc8b49d, 11fa3aa5-ccd1-4697-81f4-4178c9a9a3d9, 123f49b1-bc47-4858-b503-7dba194ab059, 18c24250-0371-4253-9576-be19ea98c1f0, 1bcc4be5-d9b9-414b-a880-3faea176d80a, 2ac3a513-ce82-4a76-a9e6-ecdd1a79a907, 2ace23dc-4106-4dca-be17-b560d1e9f6b4, 2e641553-071c-4149-9c96-ce4d2c8ca2b1, 307782b8-52d6-4a30-9688-4c8b4696ba8d, 30fdcebf-f63a-4e1d-a09a-07991cab02be, 369bdaa1-2f46-4aea-82af-0aebbca07dff, 409be3ad-1c61-42b7-9f57-9e926bb66094, 4228058c-208a-4593-adad-f1f18cc47b43, 43754ffd-1afe-44a9-b5c5-0d8e65d843db, 46da2242-904c-4659-bd5f-32288de37ab6, 4d948c8a-6f2a-4647-95f7-eea2c8dde369, 5122bd2e-3000-47aa-ac55-c770247f96cc, 5ef567a9-1a7e-4df8-8c62-62017d3e181c, 60e2cefa-19c8-49af-bc9d-4cf2502bf6b8, 68061fcf-b75a-4fe1-a37f-8366727dda6f, 694e2883-82c7-474f-9996-020f4c0d402e, 6b3e5459-70bc-4ecd-a6fd-fc756e9b7ab9, 6c131bfa-5658-4298-91ac-755967d9ccd5, 6ed9eadc-44b8-404e-8027-0f0b639bb84e, 7698cce0-0927-461d-90b2-ae1fdcebbc60, 8647e913-6a10-43bd-a32f-8660c2cb0ac1, 8c73a627-4480-4424-9ce7-bd7edbc274f8, 8f0c76d7-ab49-47cb-8af8-574cecadb742, 9163d9fc-0d64-4c72-ac03-a8c400a8ebe1, 93945806-db23-48ca-ba70-cddcbe89d9a4, 9b04a5f8-5206-43f4-9735-edf721a2972f, 9d68ef6b-01aa-4124-a46c-258596e5f3ea, a1441588-6bb4-4bec-80b9-707fca89c7ac, a29b5870-d8d6-47b4-8fac-09d2dcb71d75, a2fbad0c-649e-403d-9a18-8a9faa10bfbe, a49fd3c5-4ab5-4f71-929c-577f790e54b3, ab066755-7ed0-411c-aef0-b329a5b90f9c, aceefc56-345f-4930-a37d-6ed3afbf0704, ad00aca6-049a-4201-8c69-379a4b42db46, b3dcbe47-c32e-4db2-a2e0-1bca81ee1c59, b88da126-cd7c-4820-a476-c31b568d41d5, ba6f3cac-2d7f-4c61-8b89-25cd71d4b371, bf80b2c2-9e7d-427c-a2c9-c30f53923c67, c40524b8-9cbe-4227-830c-730a816f04fb, ca1ff377-1173-464a-bada-630966cf2237, cc918b30-cc7d-4ade-8f5d-0a358db2b126, cd34c3d9-3d0e-437c-bb46-fbdb3a79f485, ce007441-a690-42ff-b21a-fd869dee8d29, cfa83edb-3566-410c-9319-4a25f999727f, d36b93a2-56c2-462c-8ac3-de720cc890d4, dbf1a668-ff00-4a3a-ab16-7ff131610cdf, dd3ffa54-8054-4821-a2c8-b623f9a8515f, dfe8c313-ff73-441f-8ac0-639a8fe42a97, e31a9404-1e42-43fa-b6c0-029fcc6b76ab, e52ba2aa-3cfb-4cb0-8516-dff715c0eeaa, e7e18583-c68d-4a3e-a030-cd4fc2e54852, ebffeef5-48ce-4ad7-b452-44d62d35dc8e, eee3f1c8-97d1-4167-ab37-583d9d9d8e21]
123069 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_001.parquet
123084 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_001.parquet
123084 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_001.parquet => [013579a7-835e-4fc1-8b57-cf8d0c2d40a0, 01ea82d2-dfe7-4fe5-aa7b-d1691845aaa2, 090480df-bbfb-43c3-8750-e2b822d9260d, 1543a8a2-0cd0-42ba-8312-39900672c71a, 1e72280f-eb4c-441d-ab8c-369350c5e26b, 20929e4a-80e8-4776-8ef9-aad022d51b06, 21184043-a37a-47e7-b02e-b6e7d82bf10c, 295fba54-1fb8-48b8-a758-f83bedc6f7f5, 308ed0a9-43ae-44ac-ae69-857a9a5de8fa, 3ba7d052-02b3-4cbc-a529-bbfa1b2c9b9f, 3c6b332b-d90a-47dd-8a30-8aa39144061c, 3cb9f120-20b7-4508-b7c3-b520f7169997, 4130176e-a660-45c5-98c9-e0e32a071817, 4b6717c5-3912-43ea-8f28-f1b36728e929, 54d68f76-e57f-4db2-a4bb-016bc440da65, 54eef9d5-95da-4745-903d-9156e558aabb, 5bf0fadd-9185-4434-8a45-90751d12a0ea, 5d8418a2-c1ef-4cc8-b56b-bf4bee709076, 5df596cd-b6cf-4c1e-a127-41cd3b6b7a0d, 5ec5c3cb-a81c-41f0-92a2-bc267ab00918, 6196a3e0-92ec-4a20-962f-d2f5d5d36a29, 67cdf1ce-0100-4692-bbb2-a6e90f350ba0, 687f2e10-f9fc-478a-be18-171df1f4d02d, 6ad687e4-4d54-4962-b351-3c5c49fbc65c, 6b5c239a-c962-4df0-b317-fc7e9134bd9c, 6fd88142-f0f7-4eb0-9896-d71ade9d1ef7, 72d5f19c-ac2f-45d7-9a0f-50d890c13e8e, 74f60029-1cc8-4d32-b440-26c6504173c1, 77f038d4-76e1-4935-b558-e5fca1e4e757, 80518ef9-c050-4ea3-9f36-230b92a33586, 84e6651a-5f95-4c7e-a0df-12e5b7269d4a]
123119 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_001.parquet
123130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_001.parquet
123130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_001.parquet => [892fc762-2177-4a29-b69b-799a3489678c, 8c3da508-6a1a-4cd3-a1ee-70d94676b8ac, 94f54c74-e3ac-4124-8d4e-c098f0ad6156, 972c75fe-c7a1-45b9-897b-efc6442ff1dd, 9a037592-5a47-432c-92aa-6d31e8d08af7, 9bc8fc39-c27b-473e-9fdd-b9f3beb9fd57, 9dc52f20-14c6-48cc-a384-21d5e9a17586, 9ef274cf-01f4-4973-938b-5007ffd16c7c, a5a63c86-1719-4c7b-bbb5-46d6fb470eb0, a9a42894-5899-4a81-ad53-9656c775746a, aea2e94d-905a-42ad-8ae1-114488fa157b, af24bcf9-6a8e-4fdc-985a-b4613c6c975a, b7cf6665-5ac7-40eb-83c3-3fa8be93b76c, b82450b4-6128-401a-a1f1-a08e6b7a6e36, b96e0644-0ea5-47fa-a2ff-b085ad780c67, bc45af31-b60c-4bb3-ba4d-6bc1ca992439, d1a5f169-bf69-48ae-bfdd-3a83f6e0f37e, d537ccbc-9fdd-4cbd-90b9-dcf5dfbbe87b, d83de30c-f91e-4540-9502-331064b2573a, d9e2d563-98c5-4a96-9408-bcdcd89078aa, e2034d3e-2c0f-406d-a1f6-0ed109d08988, ea215ff4-7e26-4b4f-a14c-252068b7fdc8, eff10e4c-f862-40b7-8d19-a6146ff79347, f3139028-0726-4153-9395-52d454f8ef13, fa292beb-bdb9-4dea-aa3c-44272f058cbe]
123146 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 81 for /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_001.parquet
123387 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 81 row keys from /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_001.parquet
123387 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 81 results, for file /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_001.parquet => [052455df-eb2b-4561-9853-95b2a3cee6e8, 07885b59-8d32-4df6-8d34-3ae315902959, 0a808a8f-42dd-46f0-a0ff-f71bd310cf8e, 0bef0af8-a6b4-49c0-aac8-10afcc5c8c07, 0c91ba1c-efb4-4535-8f53-c8afdbf25c24, 1159c25e-f18d-485a-883b-779138d59b80, 116c0f47-139b-4269-a307-9c006dce6cd7, 18020a36-3f81-41fb-a4b5-221f9fee62d8, 1bcb2d76-e949-410d-aae0-0c4caf7973d7, 1eadc47f-2cfe-4741-aeed-b95e706f357e, 214509c3-edc8-47d5-bf93-3f54d459a70c, 2b199bf6-d195-4916-95df-251156c75fd9, 3c101e6a-7985-42be-9834-14a8c35ab4b6, 3d26cf6f-df68-4318-a434-2a9778cb81ef, 44332f3f-c532-42ce-82eb-3cbc74f28a99, 45acbc30-52ca-49a2-b2ed-63533ee6ea92, 4e21372e-4e4a-4357-9d47-1b55ad50e9a2, 51797452-eb67-4b2f-b5ee-62c2d255f08d, 537cf838-563a-431a-8d86-46d48ac9bda7, 56ceda6b-7d2d-4fec-8dbf-0e1945f5a0dd, 570631ab-8024-4111-9339-4b84f469ab17, 571b8bb8-82f3-47cb-b7ca-81451ed9d23d, 575d9904-0733-43c9-924a-b619ac09cda1, 57c610f3-fc0f-4254-bcf6-a571cc8d6057, 5e95da04-fc93-47a1-a521-394183817e3e, 61602e44-2a48-4c93-8a9c-ae31c3a82cf8, 61614252-ce11-4855-95f2-71e93d6c6ee2, 682a9f3f-d865-4a3f-aa5c-d0ce2ff62b3a, 68908573-1779-43d8-9e31-0d4b316d19d3, 68a7eb92-3cba-4cc4-986e-6a6b098a137b, 6ff15af0-6e39-4ac3-a3dd-eb71a334ccec, 717123ee-f90d-4d09-a3d6-21b099a64628, 7621fd15-d89b-4df2-b73c-07818c8c7098, 772da660-5355-4da7-affd-eabba459d163, 79a1e73a-5350-4266-89e8-c7225873c8a4, 7ae9072c-7ef5-4af4-beaa-dc565ed74c28, 7e699b96-b196-4237-923b-fb022f02010b, 82526a74-c36f-4ec5-9d34-5e377895649c, 82826283-9569-4476-88b2-b7cb913cba22, 84c554e0-5270-4652-8fb3-2b1b56fa6b83, 88925ddb-f4a7-4a35-8744-dc5d0cd066fa, 8918ef7a-d98a-4797-847f-064ce8ef32b9, 89377385-025f-4ee1-81f8-4123e2678cf3, 89435e10-b2a2-4577-b1f9-2a31a5ddd18e, 8a064233-e531-4e53-b1ed-6e0cdc2f8cac, 8c296cc4-5a30-4447-bf08-4207cbeafe38, 8dd5ff9b-af06-4350-8972-121af3eefb9f, 90118a4f-6c24-4589-a066-e6cc7687add1, 902d96d8-458d-4d4e-80d3-b07df272d123, 99e9a052-6eae-4ddd-97fd-07555bbc409e, 9aa73fbf-283b-4da7-8575-48bbee1d382d, 9af0e637-488f-42c5-813a-f940fc21f82c, 9c1a4423-cfd5-4f1c-ada9-a5e8c31e4be4, 9f2d5a98-1654-4170-8241-5b343d1581b6, a3249b26-9c17-408f-a577-e62c3c5716b3, a4634a41-3f39-44c8-a2fe-1a92e96b8c0b, a8afb1aa-c794-4f89-afb2-9ef26d0e79a3, a9bf476a-9b48-4821-9847-e6c1b80b66e5, b4dc8202-1dc3-44dc-a9dc-b574169b271a, b9a38647-8d0a-4e1e-af86-6be8c61b0df4, bc4b1ae0-24aa-4f2e-a911-239105e7da11, c4250122-118c-4809-9e54-de9603cbc6c6, c5696213-107c-4651-beef-e79a6cc4da37, c75b63b0-4d4f-4a6c-82d8-804388441681, c9e4fa0c-c67f-4993-b753-0815dc501190, cb21d449-8e19-4e5e-9ec7-9867b9f503de, cb6ab4ae-8337-4b15-830e-b06b56c79ff1, ce28e247-affb-47d8-89d7-d73c0b90d390, d3c28961-76ca-4ed4-9609-26050e75b1e8, d4203916-01d7-43c9-a09f-1b8aa87fdf0b, d66e50f2-89ce-43d9-b273-50230c9d8add, d80cbfc7-db4d-4f43-992f-b05a92d0195c, dc5e52d7-4e36-4cf0-9474-2ae5fa497f5a, e18fe628-597f-470f-9784-0cbf15654c4f, e8241bde-2366-440d-940c-0dc229290ca4, eef6005f-855a-44c1-bd4b-3fe72061cf49, f01a1f20-ad0c-4e12-b220-88f275cf4761, f029ccb6-5990-43dc-bac4-6fadd6164f2a, f15b587d-1ce3-42f5-84c8-71b2e9e3806f, f86d4a4d-a17f-4f96-b82e-eb93eaa6d7d5, f8bc2632-e0ae-43f8-b5ff-dc68b03cb8fa]
123495 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=56}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=81}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=63}}}
123510 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
123510 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=c0927ade-4885-4f0b-867f-59099273f276}, 1=BucketInfo {bucketType=UPDATE, fileLoc=dfed181e-f8b2-4034-aeba-1cfad57c7437}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{c0927ade-4885-4f0b-867f-59099273f276=0, dfed181e-f8b2-4034-aeba-1cfad57c7437=1, 8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d=2}
123528 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
123528 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
123576 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,904B for [_row_key] BINARY: 81 values, 3,240B raw, 1,804B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [fare] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 158B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,510B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,411B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,505B for [_row_key] BINARY: 63 values, 2,520B raw, 1,406B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:55 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 63
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 81
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 56
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,750
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 193B for [_hoodie_commit_seqno] BINARY: 56 values, 790B raw, 146B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,357B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,258B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,350B for [_row_key] BINARY: 56 values, 2,240B raw, 1,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 81
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 15,500
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_commit_seqno] BINARY: 81 values, 1,141B raw, 190B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,913B for [_hoodie_record_key] BINARY: 81 values, 3,247B raw, 1,813B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,904B for [_row_key] BINARY: 81 values, 3,240B raw, 1,804B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [fare] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 63
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 value124099 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
124099 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
124189 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
124189 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
124322 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
124328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
124328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
124401 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepointing latest commit 002
124468 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
124582 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2016/03/15
124583 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/16
124583 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/17
124594 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepoint 002 created
124596 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
124999 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
124999 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
125034 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
125199 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 63 for /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_002.parquet
125211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_002.parquet
125211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 63 results, for file /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_002.parquet => [00d88660-89da-4423-affb-90a28d357f33, 07b99bef-0761-4908-b1df-ddea116b8cb1, 0b92f262-f0e0-499b-8946-d00806b49169, 0db303be-c841-499f-9bdd-5c60fc8b208f, 0f0d109a-f166-4ed0-9294-bae10caaed0d, 1065220a-591a-499d-a57d-8475fcc8b49d, 11fa3aa5-ccd1-4697-81f4-4178c9a9a3d9, 123f49b1-bc47-4858-b503-7dba194ab059, 18c24250-0371-4253-9576-be19ea98c1f0, 1bcc4be5-d9b9-414b-a880-3faea176d80a, 2ac3a513-ce82-4a76-a9e6-ecdd1a79a907, 2ace23dc-4106-4dca-be17-b560d1e9f6b4, 2e641553-071c-4149-9c96-ce4d2c8ca2b1, 307782b8-52d6-4a30-9688-4c8b4696ba8d, 30fdcebf-f63a-4e1d-a09a-07991cab02be, 369bdaa1-2f46-4aea-82af-0aebbca07dff, 409be3ad-1c61-42b7-9f57-9e926bb66094, 4228058c-208a-4593-adad-f1f18cc47b43, 43754ffd-1afe-44a9-b5c5-0d8e65d843db, 46da2242-904c-4659-bd5f-32288de37ab6, 4d948c8a-6f2a-4647-95f7-eea2c8dde369, 5122bd2e-3000-47aa-ac55-c770247f96cc, 5ef567a9-1a7e-4df8-8c62-62017d3e181c, 60e2cefa-19c8-49af-bc9d-4cf2502bf6b8, 68061fcf-b75a-4fe1-a37f-8366727dda6f, 694e2883-82c7-474f-9996-020f4c0d402e, 6b3e5459-70bc-4ecd-a6fd-fc756e9b7ab9, 6c131bfa-5658-4298-91ac-755967d9ccd5, 6ed9eadc-44b8-404e-8027-0f0b639bb84e, 7698cce0-0927-461d-90b2-ae1fdcebbc60, 8647e913-6a10-43bd-a32f-8660c2cb0ac1, 8c73a627-4480-4424-9ce7-bd7edbc274f8, 8f0c76d7-ab49-47cb-8af8-574cecadb742, 9163d9fc-0d64-4c72-ac03-a8c400a8ebe1, 93945806-db23-48ca-ba70-cddcbe89d9a4, 9b04a5f8-5206-43f4-9735-edf721a2972f, 9d68ef6b-01aa-4124-a46c-258596e5f3ea, a1441588-6bb4-4bec-80b9-707fca89c7ac, a29b5870-d8d6-47b4-8fac-09d2dcb71d75, a2fbad0c-649e-403d-9a18-8a9faa10bfbe, a49fd3c5-4ab5-4f71-929c-577f790e54b3, ab066755-7ed0-411c-aef0-b329a5b90f9c, aceefc56-345f-4930-a37d-6ed3afbf0704, ad00aca6-049a-4201-8c69-379a4b42db46, b3dcbe47-c32e-4db2-a2e0-1bca81ee1c59, b88da126-cd7c-4820-a476-c31b568d41d5, ba6f3cac-2d7f-4c61-8b89-25cd71d4b371, bf80b2c2-9e7d-427c-a2c9-c30f53923c67, c40524b8-9cbe-4227-830c-730a816f04fb, ca1ff377-1173-464a-bada-630966cf2237, cc918b30-cc7d-4ade-8f5d-0a358db2b126, cd34c3d9-3d0e-437c-bb46-fbdb3a79f485, ce007441-a690-42ff-b21a-fd869dee8d29, cfa83edb-3566-410c-9319-4a25f999727f, d36b93a2-56c2-462c-8ac3-de720cc890d4, dbf1a668-ff00-4a3a-ab16-7ff131610cdf, dd3ffa54-8054-4821-a2c8-b623f9a8515f, dfe8c313-ff73-441f-8ac0-639a8fe42a97, e31a9404-1e42-43fa-b6c0-029fcc6b76ab, e52ba2aa-3cfb-4cb0-8516-dff715c0eeaa, e7e18583-c68d-4a3e-a030-cd4fc2e54852, ebffeef5-48ce-4ad7-b452-44d62d35dc8e, eee3f1c8-97d1-4167-ab37-583d9d9d8e21]
125229 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_002.parquet
125239 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_002.parquet
125239 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_002.parquet => [013579a7-835e-4fc1-8b57-cf8d0c2d40a0, 01ea82d2-dfe7-4fe5-aa7b-d1691845aaa2, 090480df-bbfb-43c3-8750-e2b822d9260d, 1543a8a2-0cd0-42ba-8312-39900672c71a, 1e72280f-eb4c-441d-ab8c-369350c5e26b, 20929e4a-80e8-4776-8ef9-aad022d51b06, 21184043-a37a-47e7-b02e-b6e7d82bf10c, 295fba54-1fb8-48b8-a758-f83bedc6f7f5, 308ed0a9-43ae-44ac-ae69-857a9a5de8fa, 3ba7d052-02b3-4cbc-a529-bbfa1b2c9b9f, 3c6b332b-d90a-47dd-8a30-8aa39144061c, 3cb9f120-20b7-4508-b7c3-b520f7169997, 4130176e-a660-45c5-98c9-e0e32a071817, 4b6717c5-3912-43ea-8f28-f1b36728e929, 54d68f76-e57f-4db2-a4bb-016bc440da65, 54eef9d5-95da-4745-903d-9156e558aabb, 5bf0fadd-9185-4434-8a45-90751d12a0ea, 5d8418a2-c1ef-4cc8-b56b-bf4bee709076, 5df596cd-b6cf-4c1e-a127-41cd3b6b7a0d, 5ec5c3cb-a81c-41f0-92a2-bc267ab00918, 6196a3e0-92ec-4a20-962f-d2f5d5d36a29, 67cdf1ce-0100-4692-bbb2-a6e90f350ba0, 687f2e10-f9fc-478a-be18-171df1f4d02d, 6ad687e4-4d54-4962-b351-3c5c49fbc65c, 6b5c239a-c962-4df0-b317-fc7e9134bd9c, 6fd88142-f0f7-4eb0-9896-d71ade9d1ef7, 72d5f19c-ac2f-45d7-9a0f-50d890c13e8e, 74f60029-1cc8-4d32-b440-26c6504173c1, 77f038d4-76e1-4935-b558-e5fca1e4e757]
125272 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 27 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_002.parquet
125285 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_002.parquet
125285 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 27 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_002.parquet => [80518ef9-c050-4ea3-9f36-230b92a33586, 84e6651a-5f95-4c7e-a0df-12e5b7269d4a, 892fc762-2177-4a29-b69b-799a3489678c, 8c3da508-6a1a-4cd3-a1ee-70d94676b8ac, 94f54c74-e3ac-4124-8d4e-c098f0ad6156, 972c75fe-c7a1-45b9-897b-efc6442ff1dd, 9a037592-5a47-432c-92aa-6d31e8d08af7, 9bc8fc39-c27b-473e-9fdd-b9f3beb9fd57, 9dc52f20-14c6-48cc-a384-21d5e9a17586, 9ef274cf-01f4-4973-938b-5007ffd16c7c, a5a63c86-1719-4c7b-bbb5-46d6fb470eb0, a9a42894-5899-4a81-ad53-9656c775746a, aea2e94d-905a-42ad-8ae1-114488fa157b, af24bcf9-6a8e-4fdc-985a-b4613c6c975a, b7cf6665-5ac7-40eb-83c3-3fa8be93b76c, b82450b4-6128-401a-a1f1-a08e6b7a6e36, b96e0644-0ea5-47fa-a2ff-b085ad780c67, bc45af31-b60c-4bb3-ba4d-6bc1ca992439, d1a5f169-bf69-48ae-bfdd-3a83f6e0f37e, d537ccbc-9fdd-4cbd-90b9-dcf5dfbbe87b, d83de30c-f91e-4540-9502-331064b2573a, d9e2d563-98c5-4a96-9408-bcdcd89078aa, e2034d3e-2c0f-406d-a1f6-0ed109d08988, ea215ff4-7e26-4b4f-a14c-252068b7fdc8, eff10e4c-f862-40b7-8d19-a6146ff79347, f3139028-0726-4153-9395-52d454f8ef13, fa292beb-bdb9-4dea-aa3c-44272f058cbe]
125302 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 81 for /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_002.parquet
125311 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 81 row keys from /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_002.parquet
125311 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 81 results, for file /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_002.parquet => [052455df-eb2b-4561-9853-95b2a3cee6e8, 07885b59-8d32-4df6-8d34-3ae315902959, 0a808a8f-42dd-46f0-a0ff-f71bd310cf8e, 0bef0af8-a6b4-49c0-aac8-10afcc5c8c07, 0c91ba1c-efb4-4535-8f53-c8afdbf25c24, 1159c25e-f18d-485a-883b-779138d59b80, 116c0f47-139b-4269-a307-9c006dce6cd7, 18020a36-3f81-41fb-a4b5-221f9fee62d8, 1bcb2d76-e949-410d-aae0-0c4caf7973d7, 1eadc47f-2cfe-4741-aeed-b95e706f357e, 214509c3-edc8-47d5-bf93-3f54d459a70c, 2b199bf6-d195-4916-95df-251156c75fd9, 3c101e6a-7985-42be-9834-14a8c35ab4b6, 3d26cf6f-df68-4318-a434-2a9778cb81ef, 44332f3f-c532-42ce-82eb-3cbc74f28a99, 45acbc30-52ca-49a2-b2ed-63533ee6ea92, 4e21372e-4e4a-4357-9d47-1b55ad50e9a2, 51797452-eb67-4b2f-b5ee-62c2d255f08d, 537cf838-563a-431a-8d86-46d48ac9bda7, 56ceda6b-7d2d-4fec-8dbf-0e1945f5a0dd, 570631ab-8024-4111-9339-4b84f469ab17, 571b8bb8-82f3-47cb-b7ca-81451ed9d23d, 575d9904-0733-43c9-924a-b619ac09cda1, 57c610f3-fc0f-4254-bcf6-a571cc8d6057, 5e95da04-fc93-47a1-a521-394183817e3e, 61602e44-2a48-4c93-8a9c-ae31c3a82cf8, 61614252-ce11-4855-95f2-71e93d6c6ee2, 682a9f3f-d865-4a3f-aa5c-d0ce2ff62b3a, 68908573-1779-43d8-9e31-0d4b316d19d3, 68a7eb92-3cba-4cc4-986e-6a6b098a137b, 6ff15af0-6e39-4ac3-a3dd-eb71a334ccec, 717123ee-f90d-4d09-a3d6-21b099a64628, 7621fd15-d89b-4df2-b73c-07818c8c7098, 772da660-5355-4da7-affd-eabba459d163, 79a1e73a-5350-4266-89e8-c7225873c8a4, 7ae9072c-7ef5-4af4-beaa-dc565ed74c28, 7e699b96-b196-4237-923b-fb022f02010b, 82526a74-c36f-4ec5-9d34-5e377895649c, 82826283-9569-4476-88b2-b7cb913cba22, 84c554e0-5270-4652-8fb3-2b1b56fa6b83, 88925ddb-f4a7-4a35-8744-dc5d0cd066fa, 8918ef7a-d98a-4797-847f-064ce8ef32b9, 89377385-025f-4ee1-81f8-4123e2678cf3, 89435e10-b2a2-4577-b1f9-2a31a5ddd18e, 8a064233-e531-4e53-b1ed-6e0cdc2f8cac, 8c296cc4-5a30-4447-bf08-4207cbeafe38, 8dd5ff9b-af06-4350-8972-121af3eefb9f, 90118a4f-6c24-4589-a066-e6cc7687add1, 902d96d8-458d-4d4e-80d3-b07df272d123, 99e9a052-6eae-4ddd-97fd-07555bbc409e, 9aa73fbf-283b-4da7-8575-48bbee1d382d, 9af0e637-488f-42c5-813a-f940fc21f82c, 9c1a4423-cfd5-4f1c-ada9-a5e8c31e4be4, 9f2d5a98-1654-4170-8241-5b343d1581b6, a3249b26-9c17-408f-a577-e62c3c5716b3, a4634a41-3f39-44c8-a2fe-1a92e96b8c0b, a8afb1aa-c794-4f89-afb2-9ef26d0e79a3, a9bf476a-9b48-4821-9847-e6c1b80b66e5, b4dc8202-1dc3-44dc-a9dc-b574169b271a, b9a38647-8d0a-4e1e-af86-6be8c61b0df4, bc4b1ae0-24aa-4f2e-a911-239105e7da11, c4250122-118c-4809-9e54-de9603cbc6c6, c5696213-107c-4651-beef-e79a6cc4da37, c75b63b0-4d4f-4a6c-82d8-804388441681, c9e4fa0c-c67f-4993-b753-0815dc501190, cb21d449-8e19-4e5e-9ec7-9867b9f503de, cb6ab4ae-8337-4b15-830e-b06b56c79ff1, ce28e247-affb-47d8-89d7-d73c0b90d390, d3c28961-76ca-4ed4-9609-26050e75b1e8, d4203916-01d7-43c9-a09f-1b8aa87fdf0b, d66e50f2-89ce-43d9-b273-50230c9d8add, d80cbfc7-db4d-4f43-992f-b05a92d0195c, dc5e52d7-4e36-4cf0-9474-2ae5fa497f5a, e18fe628-597f-470f-9784-0cbf15654c4f, e8241bde-2366-440d-940c-0dc229290ca4, eef6005f-855a-44c1-bd4b-3fe72061cf49, f01a1f20-ad0c-4e12-b220-88f275cf4761, f029ccb6-5990-43dc-bac4-6fadd6164f2a, f15b587d-1ce3-42f5-84c8-71b2e9e3806f, f86d4a4d-a17f-4f96-b82e-eb93eaa6d7d5, f8bc2632-e0ae-43f8-b5ff-dc68b03cb8fa]
125450 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=56}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=81}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=63}}}
125462 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
125462 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=c0927ade-4885-4f0b-867f-59099273f276}, 1=BucketInfo {bucketType=UPDATE, fileLoc=dfed181e-f8b2-4034-aeba-1cfad57c7437}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{c0927ade-4885-4f0b-867f-59099273f276=0, dfed181e-f8b2-4034-aeba-1cfad57c7437=1, 8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d=2}
125477 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
125478 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
s, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 158B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,510B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,411B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,505B for [_row_key] BINARY: 63 values, 2,520B raw, 1,406B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 63
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 81
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,750
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 193B for [_hoodie_commit_seqno] BINARY: 56 values, 790B raw, 146B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,357B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,258B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,350B for [_row_key] BINARY: 56 values, 2,240B raw, 1,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKE125874 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
D]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 81
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 15,500
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_commit_seqno] BINARY: 81 values, 1,141B raw, 190B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,913B for [_hoodie_record_key] BINARY: 81 values, 3,247B raw, 1,813B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,904B for [_row_key] BINARY: 81 values, 3,240B raw, 1,804B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [fare] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 63
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 158B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,510B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,411B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,505B for [_row_key] BINARY: 63 values, 2,520B raw, 1,406B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 125977 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
125977 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
126119 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
126119 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
126310 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
126317 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
126318 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
126399 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
126413 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
126839 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
126840 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
127072 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 63 for /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_003.parquet
127086 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_003.parquet
127086 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 63 results, for file /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_003.parquet => [00d88660-89da-4423-affb-90a28d357f33, 07b99bef-0761-4908-b1df-ddea116b8cb1, 0b92f262-f0e0-499b-8946-d00806b49169, 0db303be-c841-499f-9bdd-5c60fc8b208f, 0f0d109a-f166-4ed0-9294-bae10caaed0d, 1065220a-591a-499d-a57d-8475fcc8b49d, 11fa3aa5-ccd1-4697-81f4-4178c9a9a3d9, 123f49b1-bc47-4858-b503-7dba194ab059, 18c24250-0371-4253-9576-be19ea98c1f0, 1bcc4be5-d9b9-414b-a880-3faea176d80a, 2ac3a513-ce82-4a76-a9e6-ecdd1a79a907, 2ace23dc-4106-4dca-be17-b560d1e9f6b4, 2e641553-071c-4149-9c96-ce4d2c8ca2b1, 307782b8-52d6-4a30-9688-4c8b4696ba8d, 30fdcebf-f63a-4e1d-a09a-07991cab02be, 369bdaa1-2f46-4aea-82af-0aebbca07dff, 409be3ad-1c61-42b7-9f57-9e926bb66094, 4228058c-208a-4593-adad-f1f18cc47b43, 43754ffd-1afe-44a9-b5c5-0d8e65d843db, 46da2242-904c-4659-bd5f-32288de37ab6, 4d948c8a-6f2a-4647-95f7-eea2c8dde369, 5122bd2e-3000-47aa-ac55-c770247f96cc, 5ef567a9-1a7e-4df8-8c62-62017d3e181c, 60e2cefa-19c8-49af-bc9d-4cf2502bf6b8, 68061fcf-b75a-4fe1-a37f-8366727dda6f, 694e2883-82c7-474f-9996-020f4c0d402e, 6b3e5459-70bc-4ecd-a6fd-fc756e9b7ab9, 6c131bfa-5658-4298-91ac-755967d9ccd5, 6ed9eadc-44b8-404e-8027-0f0b639bb84e, 7698cce0-0927-461d-90b2-ae1fdcebbc60, 8647e913-6a10-43bd-a32f-8660c2cb0ac1, 8c73a627-4480-4424-9ce7-bd7edbc274f8, 8f0c76d7-ab49-47cb-8af8-574cecadb742, 9163d9fc-0d64-4c72-ac03-a8c400a8ebe1, 93945806-db23-48ca-ba70-cddcbe89d9a4, 9b04a5f8-5206-43f4-9735-edf721a2972f, 9d68ef6b-01aa-4124-a46c-258596e5f3ea, a1441588-6bb4-4bec-80b9-707fca89c7ac, a29b5870-d8d6-47b4-8fac-09d2dcb71d75, a2fbad0c-649e-403d-9a18-8a9faa10bfbe, a49fd3c5-4ab5-4f71-929c-577f790e54b3, ab066755-7ed0-411c-aef0-b329a5b90f9c, aceefc56-345f-4930-a37d-6ed3afbf0704, ad00aca6-049a-4201-8c69-379a4b42db46, b3dcbe47-c32e-4db2-a2e0-1bca81ee1c59, b88da126-cd7c-4820-a476-c31b568d41d5, ba6f3cac-2d7f-4c61-8b89-25cd71d4b371, bf80b2c2-9e7d-427c-a2c9-c30f53923c67, c40524b8-9cbe-4227-830c-730a816f04fb, ca1ff377-1173-464a-bada-630966cf2237, cc918b30-cc7d-4ade-8f5d-0a358db2b126, cd34c3d9-3d0e-437c-bb46-fbdb3a79f485, ce007441-a690-42ff-b21a-fd869dee8d29, cfa83edb-3566-410c-9319-4a25f999727f, d36b93a2-56c2-462c-8ac3-de720cc890d4, dbf1a668-ff00-4a3a-ab16-7ff131610cdf, dd3ffa54-8054-4821-a2c8-b623f9a8515f, dfe8c313-ff73-441f-8ac0-639a8fe42a97, e31a9404-1e42-43fa-b6c0-029fcc6b76ab, e52ba2aa-3cfb-4cb0-8516-dff715c0eeaa, e7e18583-c68d-4a3e-a030-cd4fc2e54852, ebffeef5-48ce-4ad7-b452-44d62d35dc8e, eee3f1c8-97d1-4167-ab37-583d9d9d8e21]
127105 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 44 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
127109 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
127118 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
127118 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 44 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet => [013579a7-835e-4fc1-8b57-cf8d0c2d40a0, 01ea82d2-dfe7-4fe5-aa7b-d1691845aaa2, 090480df-bbfb-43c3-8750-e2b822d9260d, 1543a8a2-0cd0-42ba-8312-39900672c71a, 1e72280f-eb4c-441d-ab8c-369350c5e26b, 20929e4a-80e8-4776-8ef9-aad022d51b06, 21184043-a37a-47e7-b02e-b6e7d82bf10c, 295fba54-1fb8-48b8-a758-f83bedc6f7f5, 308ed0a9-43ae-44ac-ae69-857a9a5de8fa, 3ba7d052-02b3-4cbc-a529-bbfa1b2c9b9f, 3c6b332b-d90a-47dd-8a30-8aa39144061c, 3cb9f120-20b7-4508-b7c3-b520f7169997, 4130176e-a660-45c5-98c9-e0e32a071817, 4b6717c5-3912-43ea-8f28-f1b36728e929, 54d68f76-e57f-4db2-a4bb-016bc440da65, 54eef9d5-95da-4745-903d-9156e558aabb, 5bf0fadd-9185-4434-8a45-90751d12a0ea, 5d8418a2-c1ef-4cc8-b56b-bf4bee709076, 5df596cd-b6cf-4c1e-a127-41cd3b6b7a0d, 5ec5c3cb-a81c-41f0-92a2-bc267ab00918, 6196a3e0-92ec-4a20-962f-d2f5d5d36a29, 67cdf1ce-0100-4692-bbb2-a6e90f350ba0, 687f2e10-f9fc-478a-be18-171df1f4d02d, 6ad687e4-4d54-4962-b351-3c5c49fbc65c, 6b5c239a-c962-4df0-b317-fc7e9134bd9c, 6fd88142-f0f7-4eb0-9896-d71ade9d1ef7, 72d5f19c-ac2f-45d7-9a0f-50d890c13e8e, 74f60029-1cc8-4d32-b440-26c6504173c1, 77f038d4-76e1-4935-b558-e5fca1e4e757, 80518ef9-c050-4ea3-9f36-230b92a33586, 84e6651a-5f95-4c7e-a0df-12e5b7269d4a, 892fc762-2177-4a29-b69b-799a3489678c, 8c3da508-6a1a-4cd3-a1ee-70d94676b8ac, 94f54c74-e3ac-4124-8d4e-c098f0ad6156, 972c75fe-c7a1-45b9-897b-efc6442ff1dd, 9a037592-5a47-432c-92aa-6d31e8d08af7, 9bc8fc39-c27b-473e-9fdd-b9f3beb9fd57, 9dc52f20-14c6-48cc-a384-21d5e9a17586, 9ef274cf-01f4-4973-938b-5007ffd16c7c, a5a63c86-1719-4c7b-bbb5-46d6fb470eb0, a9a42894-5899-4a81-ad53-9656c775746a, aea2e94d-905a-42ad-8ae1-114488fa157b, af24bcf9-6a8e-4fdc-985a-b4613c6c975a, b7cf6665-5ac7-40eb-83c3-3fa8be93b76c]
127152 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
127164 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
127164 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet => [b82450b4-6128-401a-a1f1-a08e6b7a6e36, b96e0644-0ea5-47fa-a2ff-b085ad780c67, bc45af31-b60c-4bb3-ba4d-6bc1ca992439, d1a5f169-bf69-48ae-bfdd-3a83f6e0f37e, d537ccbc-9fdd-4cbd-90b9-dcf5dfbbe87b, d83de30c-f91e-4540-9502-331064b2573a, d9e2d563-98c5-4a96-9408-bcdcd89078aa, e2034d3e-2c0f-406d-a1f6-0ed109d08988, ea215ff4-7e26-4b4f-a14c-252068b7fdc8, eff10e4c-f862-40b7-8d19-a6146ff79347, f3139028-0726-4153-9395-52d454f8ef13, fa292beb-bdb9-4dea-aa3c-44272f058cbe]
127180 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 81 for /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_003.parquet
127191 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 81 row keys from /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_003.parquet
127191 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 81 results, for file /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_003.parquet => [052455df-eb2b-4561-9853-95b2a3cee6e8, 07885b59-8d32-4df6-8d34-3ae315902959, 0a808a8f-42dd-46f0-a0ff-f71bd310cf8e, 0bef0af8-a6b4-49c0-aac8-10afcc5c8c07, 0c91ba1c-efb4-4535-8f53-c8afdbf25c24, 1159c25e-f18d-485a-883b-779138d59b80, 116c0f47-139b-4269-a307-9c006dce6cd7, 18020a36-3f81-41fb-a4b5-221f9fee62d8, 1bcb2d76-e949-410d-aae0-0c4caf7973d7, 1eadc47f-2cfe-4741-aeed-b95e706f357e, 214509c3-edc8-47d5-bf93-3f54d459a70c, 2b199bf6-d195-4916-95df-251156c75fd9, 3c101e6a-7985-42be-9834-14a8c35ab4b6, 3d26cf6f-df68-4318-a434-2a9778cb81ef, 44332f3f-c532-42ce-82eb-3cbc74f28a99, 45acbc30-52ca-49a2-b2ed-63533ee6ea92, 4e21372e-4e4a-4357-9d47-1b55ad50e9a2, 51797452-eb67-4b2f-b5ee-62c2d255f08d, 537cf838-563a-431a-8d86-46d48ac9bda7, 56ceda6b-7d2d-4fec-8dbf-0e1945f5a0dd, 570631ab-8024-4111-9339-4b84f469ab17, 571b8bb8-82f3-47cb-b7ca-81451ed9d23d, 575d9904-0733-43c9-924a-b619ac09cda1, 57c610f3-fc0f-4254-bcf6-a571cc8d6057, 5e95da04-fc93-47a1-a521-394183817e3e, 61602e44-2a48-4c93-8a9c-ae31c3a82cf8, 61614252-ce11-4855-95f2-71e93d6c6ee2, 682a9f3f-d865-4a3f-aa5c-d0ce2ff62b3a, 68908573-1779-43d8-9e31-0d4b316d19d3, 68a7eb92-3cba-4cc4-986e-6a6b098a137b, 6ff15af0-6e39-4ac3-a3dd-eb71a334ccec, 717123ee-f90d-4d09-a3d6-21b099a64628, 7621fd15-d89b-4df2-b73c-07818c8c7098, 772da660-5355-4da7-affd-eabba459d163, 79a1e73a-5350-4266-89e8-c7225873c8a4, 7ae9072c-7ef5-4af4-beaa-dc565ed74c28, 7e699b96-b196-4237-923b-fb022f02010b, 82526a74-c36f-4ec5-9d34-5e377895649c, 82826283-9569-4476-88b2-b7cb913cba22, 84c554e0-5270-4652-8fb3-2b1b56fa6b83, 88925ddb-f4a7-4a35-8744-dc5d0cd066fa, 8918ef7a-d98a-4797-847f-064ce8ef32b9, 89377385-025f-4ee1-81f8-4123e2678cf3, 89435e10-b2a2-4577-b1f9-2a31a5ddd18e, 8a064233-e531-4e53-b1ed-6e0cdc2f8cac, 8c296cc4-5a30-4447-bf08-4207cbeafe38, 8dd5ff9b-af06-4350-8972-121af3eefb9f, 90118a4f-6c24-4589-a066-e6cc7687add1, 902d96d8-458d-4d4e-80d3-b07df272d123, 99e9a052-6eae-4ddd-97fd-07555bbc409e, 9aa73fbf-283b-4da7-8575-48bbee1d382d, 9af0e637-488f-42c5-813a-f940fc21f82c, 9c1a4423-cfd5-4f1c-ada9-a5e8c31e4be4, 9f2d5a98-1654-4170-8241-5b343d1581b6, a3249b26-9c17-408f-a577-e62c3c5716b3, a4634a41-3f39-44c8-a2fe-1a92e96b8c0b, a8afb1aa-c794-4f89-afb2-9ef26d0e79a3, a9bf476a-9b48-4821-9847-e6c1b80b66e5, b4dc8202-1dc3-44dc-a9dc-b574169b271a, b9a38647-8d0a-4e1e-af86-6be8c61b0df4, bc4b1ae0-24aa-4f2e-a911-239105e7da11, c4250122-118c-4809-9e54-de9603cbc6c6, c5696213-107c-4651-beef-e79a6cc4da37, c75b63b0-4d4f-4a6c-82d8-804388441681, c9e4fa0c-c67f-4993-b753-0815dc501190, cb21d449-8e19-4e5e-9ec7-9867b9f503de, cb6ab4ae-8337-4b15-830e-b06b56c79ff1, ce28e247-affb-47d8-89d7-d73c0b90d390, d3c28961-76ca-4ed4-9609-26050e75b1e8, d4203916-01d7-43c9-a09f-1b8aa87fdf0b, d66e50f2-89ce-43d9-b273-50230c9d8add, d80cbfc7-db4d-4f43-992f-b05a92d0195c, dc5e52d7-4e36-4cf0-9474-2ae5fa497f5a, e18fe628-597f-470f-9784-0cbf15654c4f, e8241bde-2366-440d-940c-0dc229290ca4, eef6005f-855a-44c1-bd4b-3fe72061cf49, f01a1f20-ad0c-4e12-b220-88f275cf4761, f029ccb6-5990-43dc-bac4-6fadd6164f2a, f15b587d-1ce3-42f5-84c8-71b2e9e3806f, f86d4a4d-a17f-4f96-b82e-eb93eaa6d7d5, f8bc2632-e0ae-43f8-b5ff-dc68b03cb8fa]
127326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=56}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=81}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=63}}}
127339 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
127339 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=c0927ade-4885-4f0b-867f-59099273f276}, 1=BucketInfo {bucketType=UPDATE, fileLoc=dfed181e-f8b2-4034-aeba-1cfad57c7437}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{c0927ade-4885-4f0b-867f-59099273f276=0, dfed181e-f8b2-4034-aeba-1cfad57c7437=1, 8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d=2}
127356 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
127356 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
127648 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
entries, 14B raw, 1B comp}
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:32:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 63
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:00 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 81
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,750
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 56 values, 790B raw, 147B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,357B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,258B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,350B for [_row_key] BINARY: 56 values, 2,240B raw, 1,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9127839 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
127839 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
127980 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
127980 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
128179 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
128186 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
128186 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
128463 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepoint 002 deleted
128464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [004]
128464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [004]
128709 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
128710 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_004.parquet	true
128710 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
128711 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_004.parquet	true
128711 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
128711 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_004.parquet	true
128716 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [004]
128716 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [004]
128722 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [004] rollback is complete
128722 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
128722 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
128782 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
128789 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
129219 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
129219 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
129489 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 63 for /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_003.parquet
129501 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_003.parquet
129502 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 63 results, for file /tmp/junit9062793193198639961/2015/03/17/8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d_2_003.parquet => [00d88660-89da-4423-affb-90a28d357f33, 07b99bef-0761-4908-b1df-ddea116b8cb1, 0b92f262-f0e0-499b-8946-d00806b49169, 0db303be-c841-499f-9bdd-5c60fc8b208f, 0f0d109a-f166-4ed0-9294-bae10caaed0d, 1065220a-591a-499d-a57d-8475fcc8b49d, 11fa3aa5-ccd1-4697-81f4-4178c9a9a3d9, 123f49b1-bc47-4858-b503-7dba194ab059, 18c24250-0371-4253-9576-be19ea98c1f0, 1bcc4be5-d9b9-414b-a880-3faea176d80a, 2ac3a513-ce82-4a76-a9e6-ecdd1a79a907, 2ace23dc-4106-4dca-be17-b560d1e9f6b4, 2e641553-071c-4149-9c96-ce4d2c8ca2b1, 307782b8-52d6-4a30-9688-4c8b4696ba8d, 30fdcebf-f63a-4e1d-a09a-07991cab02be, 369bdaa1-2f46-4aea-82af-0aebbca07dff, 409be3ad-1c61-42b7-9f57-9e926bb66094, 4228058c-208a-4593-adad-f1f18cc47b43, 43754ffd-1afe-44a9-b5c5-0d8e65d843db, 46da2242-904c-4659-bd5f-32288de37ab6, 4d948c8a-6f2a-4647-95f7-eea2c8dde369, 5122bd2e-3000-47aa-ac55-c770247f96cc, 5ef567a9-1a7e-4df8-8c62-62017d3e181c, 60e2cefa-19c8-49af-bc9d-4cf2502bf6b8, 68061fcf-b75a-4fe1-a37f-8366727dda6f, 694e2883-82c7-474f-9996-020f4c0d402e, 6b3e5459-70bc-4ecd-a6fd-fc756e9b7ab9, 6c131bfa-5658-4298-91ac-755967d9ccd5, 6ed9eadc-44b8-404e-8027-0f0b639bb84e, 7698cce0-0927-461d-90b2-ae1fdcebbc60, 8647e913-6a10-43bd-a32f-8660c2cb0ac1, 8c73a627-4480-4424-9ce7-bd7edbc274f8, 8f0c76d7-ab49-47cb-8af8-574cecadb742, 9163d9fc-0d64-4c72-ac03-a8c400a8ebe1, 93945806-db23-48ca-ba70-cddcbe89d9a4, 9b04a5f8-5206-43f4-9735-edf721a2972f, 9d68ef6b-01aa-4124-a46c-258596e5f3ea, a1441588-6bb4-4bec-80b9-707fca89c7ac, a29b5870-d8d6-47b4-8fac-09d2dcb71d75, a2fbad0c-649e-403d-9a18-8a9faa10bfbe, a49fd3c5-4ab5-4f71-929c-577f790e54b3, ab066755-7ed0-411c-aef0-b329a5b90f9c, aceefc56-345f-4930-a37d-6ed3afbf0704, ad00aca6-049a-4201-8c69-379a4b42db46, b3dcbe47-c32e-4db2-a2e0-1bca81ee1c59, b88da126-cd7c-4820-a476-c31b568d41d5, ba6f3cac-2d7f-4c61-8b89-25cd71d4b371, bf80b2c2-9e7d-427c-a2c9-c30f53923c67, c40524b8-9cbe-4227-830c-730a816f04fb, ca1ff377-1173-464a-bada-630966cf2237, cc918b30-cc7d-4ade-8f5d-0a358db2b126, cd34c3d9-3d0e-437c-bb46-fbdb3a79f485, ce007441-a690-42ff-b21a-fd869dee8d29, cfa83edb-3566-410c-9319-4a25f999727f, d36b93a2-56c2-462c-8ac3-de720cc890d4, dbf1a668-ff00-4a3a-ab16-7ff131610cdf, dd3ffa54-8054-4821-a2c8-b623f9a8515f, dfe8c313-ff73-441f-8ac0-639a8fe42a97, e31a9404-1e42-43fa-b6c0-029fcc6b76ab, e52ba2aa-3cfb-4cb0-8516-dff715c0eeaa, e7e18583-c68d-4a3e-a030-cd4fc2e54852, ebffeef5-48ce-4ad7-b452-44d62d35dc8e, eee3f1c8-97d1-4167-ab37-583d9d9d8e21]
129523 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 42 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 81
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 15,500
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 238B for [_hoodie_commit_seqno] BINARY: 81 values, 1,141B raw, 190B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,913B for [_hoodie_record_key] BINARY: 81 values, 3,247B raw, 1,813B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,904B for [_row_key] BINARY: 81 values, 3,240B raw, 1,804B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [fare] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 63
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 158B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,510B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,411B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,505B for [_row_key] BINARY: 63 values, 2,520B raw, 1,406B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:01 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 63
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 foo129536 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
129536 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 42 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet => [013579a7-835e-4fc1-8b57-cf8d0c2d40a0, 01ea82d2-dfe7-4fe5-aa7b-d1691845aaa2, 090480df-bbfb-43c3-8750-e2b822d9260d, 1543a8a2-0cd0-42ba-8312-39900672c71a, 1e72280f-eb4c-441d-ab8c-369350c5e26b, 20929e4a-80e8-4776-8ef9-aad022d51b06, 21184043-a37a-47e7-b02e-b6e7d82bf10c, 295fba54-1fb8-48b8-a758-f83bedc6f7f5, 308ed0a9-43ae-44ac-ae69-857a9a5de8fa, 3ba7d052-02b3-4cbc-a529-bbfa1b2c9b9f, 3c6b332b-d90a-47dd-8a30-8aa39144061c, 3cb9f120-20b7-4508-b7c3-b520f7169997, 4130176e-a660-45c5-98c9-e0e32a071817, 4b6717c5-3912-43ea-8f28-f1b36728e929, 54d68f76-e57f-4db2-a4bb-016bc440da65, 54eef9d5-95da-4745-903d-9156e558aabb, 5bf0fadd-9185-4434-8a45-90751d12a0ea, 5d8418a2-c1ef-4cc8-b56b-bf4bee709076, 5df596cd-b6cf-4c1e-a127-41cd3b6b7a0d, 5ec5c3cb-a81c-41f0-92a2-bc267ab00918, 6196a3e0-92ec-4a20-962f-d2f5d5d36a29, 67cdf1ce-0100-4692-bbb2-a6e90f350ba0, 687f2e10-f9fc-478a-be18-171df1f4d02d, 6ad687e4-4d54-4962-b351-3c5c49fbc65c, 6b5c239a-c962-4df0-b317-fc7e9134bd9c, 6fd88142-f0f7-4eb0-9896-d71ade9d1ef7, 72d5f19c-ac2f-45d7-9a0f-50d890c13e8e, 74f60029-1cc8-4d32-b440-26c6504173c1, 77f038d4-76e1-4935-b558-e5fca1e4e757, 80518ef9-c050-4ea3-9f36-230b92a33586, 84e6651a-5f95-4c7e-a0df-12e5b7269d4a, 892fc762-2177-4a29-b69b-799a3489678c, 8c3da508-6a1a-4cd3-a1ee-70d94676b8ac, 94f54c74-e3ac-4124-8d4e-c098f0ad6156, 972c75fe-c7a1-45b9-897b-efc6442ff1dd, 9a037592-5a47-432c-92aa-6d31e8d08af7, 9bc8fc39-c27b-473e-9fdd-b9f3beb9fd57, 9dc52f20-14c6-48cc-a384-21d5e9a17586, 9ef274cf-01f4-4973-938b-5007ffd16c7c, a5a63c86-1719-4c7b-bbb5-46d6fb470eb0, a9a42894-5899-4a81-ad53-9656c775746a, aea2e94d-905a-42ad-8ae1-114488fa157b]
129570 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
129582 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet
129582 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit9062793193198639961/2016/03/15/c0927ade-4885-4f0b-867f-59099273f276_0_003.parquet => [af24bcf9-6a8e-4fdc-985a-b4613c6c975a, b7cf6665-5ac7-40eb-83c3-3fa8be93b76c, b82450b4-6128-401a-a1f1-a08e6b7a6e36, b96e0644-0ea5-47fa-a2ff-b085ad780c67, bc45af31-b60c-4bb3-ba4d-6bc1ca992439, d1a5f169-bf69-48ae-bfdd-3a83f6e0f37e, d537ccbc-9fdd-4cbd-90b9-dcf5dfbbe87b, d83de30c-f91e-4540-9502-331064b2573a, d9e2d563-98c5-4a96-9408-bcdcd89078aa, e2034d3e-2c0f-406d-a1f6-0ed109d08988, ea215ff4-7e26-4b4f-a14c-252068b7fdc8, eff10e4c-f862-40b7-8d19-a6146ff79347, f3139028-0726-4153-9395-52d454f8ef13, fa292beb-bdb9-4dea-aa3c-44272f058cbe]
129600 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 81 for /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_003.parquet
129613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 81 row keys from /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_003.parquet
129613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 81 results, for file /tmp/junit9062793193198639961/2015/03/16/dfed181e-f8b2-4034-aeba-1cfad57c7437_1_003.parquet => [052455df-eb2b-4561-9853-95b2a3cee6e8, 07885b59-8d32-4df6-8d34-3ae315902959, 0a808a8f-42dd-46f0-a0ff-f71bd310cf8e, 0bef0af8-a6b4-49c0-aac8-10afcc5c8c07, 0c91ba1c-efb4-4535-8f53-c8afdbf25c24, 1159c25e-f18d-485a-883b-779138d59b80, 116c0f47-139b-4269-a307-9c006dce6cd7, 18020a36-3f81-41fb-a4b5-221f9fee62d8, 1bcb2d76-e949-410d-aae0-0c4caf7973d7, 1eadc47f-2cfe-4741-aeed-b95e706f357e, 214509c3-edc8-47d5-bf93-3f54d459a70c, 2b199bf6-d195-4916-95df-251156c75fd9, 3c101e6a-7985-42be-9834-14a8c35ab4b6, 3d26cf6f-df68-4318-a434-2a9778cb81ef, 44332f3f-c532-42ce-82eb-3cbc74f28a99, 45acbc30-52ca-49a2-b2ed-63533ee6ea92, 4e21372e-4e4a-4357-9d47-1b55ad50e9a2, 51797452-eb67-4b2f-b5ee-62c2d255f08d, 537cf838-563a-431a-8d86-46d48ac9bda7, 56ceda6b-7d2d-4fec-8dbf-0e1945f5a0dd, 570631ab-8024-4111-9339-4b84f469ab17, 571b8bb8-82f3-47cb-b7ca-81451ed9d23d, 575d9904-0733-43c9-924a-b619ac09cda1, 57c610f3-fc0f-4254-bcf6-a571cc8d6057, 5e95da04-fc93-47a1-a521-394183817e3e, 61602e44-2a48-4c93-8a9c-ae31c3a82cf8, 61614252-ce11-4855-95f2-71e93d6c6ee2, 682a9f3f-d865-4a3f-aa5c-d0ce2ff62b3a, 68908573-1779-43d8-9e31-0d4b316d19d3, 68a7eb92-3cba-4cc4-986e-6a6b098a137b, 6ff15af0-6e39-4ac3-a3dd-eb71a334ccec, 717123ee-f90d-4d09-a3d6-21b099a64628, 7621fd15-d89b-4df2-b73c-07818c8c7098, 772da660-5355-4da7-affd-eabba459d163, 79a1e73a-5350-4266-89e8-c7225873c8a4, 7ae9072c-7ef5-4af4-beaa-dc565ed74c28, 7e699b96-b196-4237-923b-fb022f02010b, 82526a74-c36f-4ec5-9d34-5e377895649c, 82826283-9569-4476-88b2-b7cb913cba22, 84c554e0-5270-4652-8fb3-2b1b56fa6b83, 88925ddb-f4a7-4a35-8744-dc5d0cd066fa, 8918ef7a-d98a-4797-847f-064ce8ef32b9, 89377385-025f-4ee1-81f8-4123e2678cf3, 89435e10-b2a2-4577-b1f9-2a31a5ddd18e, 8a064233-e531-4e53-b1ed-6e0cdc2f8cac, 8c296cc4-5a30-4447-bf08-4207cbeafe38, 8dd5ff9b-af06-4350-8972-121af3eefb9f, 90118a4f-6c24-4589-a066-e6cc7687add1, 902d96d8-458d-4d4e-80d3-b07df272d123, 99e9a052-6eae-4ddd-97fd-07555bbc409e, 9aa73fbf-283b-4da7-8575-48bbee1d382d, 9af0e637-488f-42c5-813a-f940fc21f82c, 9c1a4423-cfd5-4f1c-ada9-a5e8c31e4be4, 9f2d5a98-1654-4170-8241-5b343d1581b6, a3249b26-9c17-408f-a577-e62c3c5716b3, a4634a41-3f39-44c8-a2fe-1a92e96b8c0b, a8afb1aa-c794-4f89-afb2-9ef26d0e79a3, a9bf476a-9b48-4821-9847-e6c1b80b66e5, b4dc8202-1dc3-44dc-a9dc-b574169b271a, b9a38647-8d0a-4e1e-af86-6be8c61b0df4, bc4b1ae0-24aa-4f2e-a911-239105e7da11, c4250122-118c-4809-9e54-de9603cbc6c6, c5696213-107c-4651-beef-e79a6cc4da37, c75b63b0-4d4f-4a6c-82d8-804388441681, c9e4fa0c-c67f-4993-b753-0815dc501190, cb21d449-8e19-4e5e-9ec7-9867b9f503de, cb6ab4ae-8337-4b15-830e-b06b56c79ff1, ce28e247-affb-47d8-89d7-d73c0b90d390, d3c28961-76ca-4ed4-9609-26050e75b1e8, d4203916-01d7-43c9-a09f-1b8aa87fdf0b, d66e50f2-89ce-43d9-b273-50230c9d8add, d80cbfc7-db4d-4f43-992f-b05a92d0195c, dc5e52d7-4e36-4cf0-9474-2ae5fa497f5a, e18fe628-597f-470f-9784-0cbf15654c4f, e8241bde-2366-440d-940c-0dc229290ca4, eef6005f-855a-44c1-bd4b-3fe72061cf49, f01a1f20-ad0c-4e12-b220-88f275cf4761, f029ccb6-5990-43dc-bac4-6fadd6164f2a, f15b587d-1ce3-42f5-84c8-71b2e9e3806f, f86d4a4d-a17f-4f96-b82e-eb93eaa6d7d5, f8bc2632-e0ae-43f8-b5ff-dc68b03cb8fa]
129702 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=56}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=81}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=63}}}
129718 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
129718 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=c0927ade-4885-4f0b-867f-59099273f276}, 1=BucketInfo {bucketType=UPDATE, fileLoc=dfed181e-f8b2-4034-aeba-1cfad57c7437}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{c0927ade-4885-4f0b-867f-59099273f276=0, dfed181e-f8b2-4034-aeba-1cfad57c7437=1, 8ea0caf9-8d37-4243-aa1f-f3db9ac2fb4d=2}
129736 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
129736 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
129888 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
ters
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 56
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 56
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 81
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 56 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 56
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,750
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 56 values, 790B raw, 147B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,357B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,258B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,350B for [_row_key] BINARY: 56 values, 2,240B raw, 1,251B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 81 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 81
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 15,500
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 239B for [_hoodie_commit_seqno] BINARY: 81 values, 1,141B raw, 191B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,913B for [_hoodie_record_key] BINARY: 81 values, 3,247B raw, 1,813B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 81 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,904B for [_row_key] BINA130330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
130330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
130493 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
130493 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
130494 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
130707 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
130713 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
130713 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
130927 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01], with policy KEEP_LATEST_FILE_VERSIONS
130927 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
131194 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
131290 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=70, numUpdates=0}}}
131295 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 70, totalInsertBuckets => 1, recordsPerBucket => 500000
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
131296 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
131314 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
131314 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
131355 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
131462 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
131462 [pool-1313-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
131491 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
131491 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
RY: 81 values, 3,240B raw, 1,804B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 81 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [begin_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lat] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [end_lon] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 715B for [fare] DOUBLE: 81 values, 648B raw, 671B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 63
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 205B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 158B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,510B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,411B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,505B for [_row_key] BINARY: 63 values, 2,520B raw, 1,406B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:03 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,460
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 211B for [_hoodie_commit_seqno] BINARY: 65 values, 917B raw, 163B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,556B for [_hoodie_record_key] BINARY: 65 values, 2,607B raw, 1,456B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,546B for [_row_key] BINARY: 65 values, 2,600B raw, 1,446B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
Mar 16, 2018 9:33:05 AM INFO: org.apac131515 [pool-1313-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
131533 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
131533 [pool-1314-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
131558 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
131558 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
131587 [pool-1314-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
131603 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
131603 [pool-1315-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
131628 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
131628 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
131645 [pool-1315-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
131660 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
131660 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
131738 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
131738 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
131920 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
131958 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
131966 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
131966 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
132487 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
133535 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
134517 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
135493 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
136308 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
136609 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
137963 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
138076 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
138076 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
138235 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
138238 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 70 for /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_001.parquet
138253 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_001.parquet
138253 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 70 results, for file /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_001.parquet => [05993a9b-8922-4857-bb7f-a7c3ccf18b51, 09bcd5e9-43e0-4235-8695-d874fc749c86, 10d70cdc-ede3-4e35-8b75-9b8782b8d562, 1a879000-7ab3-4224-8846-5f19f88578df, 1d0de974-b16a-4f63-9489-6b9620adcb91, 2797be80-89e6-444e-991a-2b1fd15dec44, 2b45d3f8-359f-4aea-880a-06563f4fe9c0, 331323bf-85bc-4922-beed-2c542b17d268, 364a2c28-d9d5-4ad1-a044-d5695959e985, 3799879f-1307-4b3d-b9ef-bc23a72cf892, 37b28978-cab6-48a0-a711-e30f65cbb73a, 37cd5062-8bb0-4535-95ad-dc522fc408c2, 39bcde6d-c5d1-4c73-8976-a123d47526b4, 3a24d78f-25b3-4809-8333-18ca2556a405, 462a5117-cd7b-4390-842f-f08a1f80783d, 47717f9b-68f9-444a-9ad7-afa0c075b93a, 487e53f5-b4e7-46d6-96e6-473fed96c2fc, 4a9a9e1f-fe39-492f-87b7-3828dcc8fa60, 4c4cb266-bdcc-414a-918c-c160cdf655ab, 4c6d23ea-a1c8-4962-bf7a-449457efe791, 4ea100ea-7232-438f-b55e-56199069bae4, 4ea49757-6004-4e47-9d95-e9c5a5656a95, 5744be49-1a36-43da-8645-6b6172606d7a, 5abbf25f-4e14-4d89-b715-ba5ade54b1a2, 5bbf8dc1-8327-4ce8-b049-234032c83e55, 5cbedba1-1638-4cb4-a8e6-92be1671445b, 60636943-e295-46cd-a410-c47be6a29b76, 665a4b61-03e5-4ec4-88fd-4ba0978f0172, 6985d4b1-6bca-4f1f-98b1-b87931753b6f, 6bb05d11-a4f2-4f22-a8b3-75502ac2e867, 6cad9f0d-14ff-4e4d-a867-62c55b6ffd38, 6ecf1a0d-a4ca-4c55-b05f-b7e4cbf88fb2, 6efc6cb5-22f1-443a-a116-ee70823a6877, 6f73f42d-401e-4c3d-818e-597e4cda0e78, 6fa505c7-01b2-4bed-986d-9920aef47e65, 725952e6-c25f-4708-84df-42ce989eb22d, 74542bfa-9e73-42e6-8d6e-e277573b63ff, 76704c01-6f07-4a31-81ce-549d8e178fe1, 7713676c-b4e5-4424-8eeb-ca559823416c, 8128286b-0f77-4299-93f9-24d6ed924bb6, 88425bf7-ed27-4bf6-9da2-22ff6f509ec6, 8e2b7d98-385a-49a6-a6bd-604025d6cedc, 9465835e-940b-4183-b97e-bdeecdae81b8, 994f6c28-9188-4187-8abb-c70a3375194a, 9ae0f3bf-86fa-4a81-928a-f24205a35e89, 9ae90376-92ed-4183-a82a-d5f2556559c4, 9c889043-d95a-45d2-84d6-880ede5cb134, 9d80901e-2fbd-45dd-8b16-a5e57ce42b1c, 9ed6637c-c791-44c9-af7d-76011626b802, 9f7854b0-7789-4275-b411-be9152af1d13, a1d21d8a-10d6-4820-8552-813118938e3d, a2f0464c-7bf7-48f0-8c08-22a23106fc16, a3eed1fa-607d-4ecf-95cd-0597b9c311ab, a455f625-82cb-448b-b815-d40d2a307e4b, a9975bb1-5c6f-4fe4-b0a0-d1e6766a3db5, aad716bd-a17f-41f2-8fa2-65222306c677, b08bbeb4-696f-4045-9752-7192d4d0268a, b6e3179c-659c-4b42-a5ed-cb4c07f82ff1, c075a979-d832-4178-a7db-ab0bc421e924, cc139577-fb0b-454d-9976-9ef5020f0451, cda5870b-6ee9-4f1f-b0be-f3aba813c605, dc4369f0-3897-4559-9ad2-57d33726bf3a, de3509c5-5c04-497c-916d-f593f49e3e47, debdbefd-de98-4605-b291-febd941b5943, e133a59e-4a23-4eab-acfa-ee4710a963de, e26e07c6-066c-4f0d-bfce-1fc59f236bdd, e2db4609-1368-4836-a827-66c9654f7c50, e8de1f1f-c44a-495b-8c23-1a1d661cfc6c, ec1a3cf7-e5b0-41d5-9a09-cc96987e4925, f39d14af-efeb-4584-b66b-b41a9a787087]
138275 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 65 for /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet
138290 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet
138290 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 65 results, for file /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet => [021d3fd1-dbd4-4c0d-9d23-3f1f1552e04f, 02afae7b-bcb1-47a1-aa66-db871176f86a, 07932dc3-8675-4ba3-aa4d-f9807b991d80, 0903a96d-8e86-4224-af4c-d7d22a037877, 0f2bf123-8e12-42b9-8d42-7375b4b32c93, 0fe33592-319f-482e-81f7-42f3b608b824, 10f155ff-20c3-4b37-b50a-b989491fbf49, 11007e25-4939-49d4-9ea7-9d52fa1df6c0, 12a3f7c8-dbd4-44f6-a010-196495aa9016, 13a21ca8-7572-47aa-894a-2dba0a2c6794, 16babfde-9799-4de2-af5b-3f53cf266adb, 1d17bee3-54a9-4a41-beb7-cc0949772a77, 1d4acaf5-4ed4-425e-87dc-a182eabb2abe, 216749fb-9ac5-4bb9-870d-809ff26a34ae, 26a349d3-7660-4319-88a4-ce3edbc04854, 29d640d7-7b0e-4b80-b8fc-f9108067239c, 305001f4-961e-4b5b-a431-66d5bbe3462e, 30c1ab7a-c913-4cd8-9c3b-d5967a18d81d, 3147f409-4f20-4294-bd23-825acc47b6c6, 3a277cc9-a5d0-4777-a312-3073314821ca, 3ca0e82d-35dc-4f4a-b9af-a0eca390f9c6, 3ce49009-d2e7-4db4-836a-06a4956caf93, 416e4590-534e-4474-800d-0e4c05a48521, 48dacae7-e09b-472c-b16e-c908f8cad081, 49a2e105-9e89-4bfd-a41f-8fa7a06df554, 4c1f6cd4-6b0a-4f62-8c5d-dd90dea0f784, 54f0077f-eda8-4539-936e-684babf4d5b2, 5711085e-7da7-4067-8572-c4852537f5a8, 5f225a28-ebef-4961-b98d-d7559036ca3a, 621509ee-17c1-4d69-9bbf-88e196f770c7, 68751b1d-05c6-4945-9151-b72ad31c0d68, 6b15505a-fa5b-4e2d-818f-b19ae8453791, 708859a7-edcd-4f0a-b28f-a71261f7568e, 75c3bb3f-31d9-4a9e-aa18-60f2b9d7bfac, 7755e408-1113-4d1e-8bb4-63f22e430466, 79b82fa1-ca40-48dc-b2c2-b1f6ffc10bf2, 7b78689b-7b7d-4583-bf0c-66cfb5e2a85b, 7ea626b9-7570-42ec-b3f6-61d9b2732937, 7ee8ac31-2c14-48bd-a81a-0f8c6c796531, 7fbbce73-c5b8-42d8-bf60-e5f0753c45be, 80e9622e-6f88-4bd5-bc4f-9b94e62ee6ff, 97c66bdc-87fa-4b62-8ac7-0db3181588d1, 9927854f-9c89-4709-bac2-45dff654e460, 9d932921-0ac3-4151-b4ac-994ca5f37039, 9ec15db0-e1b7-4557-a93b-ac03af965e83, 9fe8aaf1-53bc-4e34-b795-f7770e1d2d0d, aa8b2925-c26e-468d-b3e3-eca70fea29a2, bf9616bc-0919-460a-b791-52a28d2edcdf, bfe05a54-ebea-4840-898d-e14f9001782c, c50d6ce6-ad2f-4a54-bba3-5a8cec802ad6, cc7131e0-8b2e-40a3-8743-78927ea7952a, ce35a106-1f1f-4311-9339-acf7641205c9, d795c3b7-877d-4396-aa86-10f06374f37b, e3efc543-9777-4e60-8a9d-f99072737b40, e427aada-71ab-49e5-89ca-740ae9ff28e5, e4f964d8-5910-4217-b34e-c5fb1570b4b3, e772907e-3a1f-4ca5-8751-d4981c5b2e8a, eca1d76d-4714-490f-8639-e3ea4789aed9, ecb4b0a4-0976-4d10-aa64-01b70fafe029, f1051a59-3651-4ef9-a646-a996ac7aebc9, f1ecd6f8-35e6-43ec-bac0-9d3075ed55a1, fa8771aa-406c-43ce-b94e-79d7935f4053, fb314702-5256-47b7-b6e9-111a7461851e, fcbd36d4-85e1-4339-b719-54c0b048d7c7, ffcacc8f-dfbd-4483-afc7-8ecd531f3dc1]
138307 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 65 for /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_1_001.parquet
138321 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_1_001.parquet
138321 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 65 results, for file /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_1_001.parquet => [015c6b13-341a-437a-82dc-641f7e9e5b28, 034fd385-e9a6-4a7b-bf05-858dc8b44ac8, 03cfdb2e-3ca3-4818-a49a-1adf7dcb7edc, 04027036-6109-4aed-a8ad-f428afdd2bd6, 044cdb2f-71c8-4a42-b51f-d9a850ffa568, 06b77b39-9765-48f8-af06-c227899184ef, 0790f304-4dcd-4100-a5cf-cabcbc487e9f, 0a5aeea6-1ffe-4b9c-a26f-ebfe34c993fe, 166f9f69-a55f-4fbf-9f77-aa401d226c7c, 22c13a6d-9118-42a0-ab66-9f002fc12112, 240c9463-a74b-461c-af8e-5d0aab2fa320, 258a0ee2-4d40-448e-b8be-ea3e14661ab1, 27bae7a9-11fb-4b38-ba40-ef5b58808496, 29a533a5-8c8e-4375-9dc4-0b0460937a49, 3aebf64a-ba26-4a61-9143-69ac559ac78a, 3c0ca29d-fd82-4208-90d9-72bb21df8da3, 3e0c7259-88f7-4bee-9b87-2f666ca5c97a, 44ed5b49-32e1-4fad-abfa-56c2eaa689ab, 4b3da0c5-f34f-44d7-a78d-9c676186ee20, 5398cf5a-0bf4-48cf-b27d-28ab3228a773, 59f17608-3beb-4e45-bf7e-01c27ab4f73e, 61f6ccf2-540c-48d4-a42d-b0c030901b3f, 63a9ad30-4a62-4c5a-b9dd-59de4ddd5d71, 6606f0b1-3aa9-40d1-b19e-619335f01638, 66d8f700-af99-44d7-abb5-4fef24036e33, 6a8bf02a-c5ab-49c9-b1c7-4bdc94cf7657, 6ba38791-0fa6-487e-b050-1f6ffd5be0b6, 7a5defe4-401f-4df0-8b73-7e8d44d1387b, 7aac28ad-75a5-4b95-9fb3-2ef144b06565, 7d4ce3ae-c1a3-46d2-828a-f686ef64fd0e, 883571d5-3f56-4b33-8b0d-285f30fc632a, 88c06133-e13b-4b7d-8af9-d96e62092060, 8920635e-ca58-4014-a558-3629189eb22e, 8f11f233-e837-49b4-bfac-dcea27b615ad, 9a5712b2-2b5f-491f-8f16-a76f84c663ab, a0203cb8-1dbb-40c1-b13c-3d31ab8ebeb7, a71d893b-b14b-4a7a-a79b-3601ad88f56c, a8fb8b56-ecf8-469c-8398-413b73c4266a, a92417a7-1de2-44dd-aba4-9b533733936d, aab44491-b4bb-4801-b6b2-6d835361631e, ab7f8fd5-5a2e-4096-ab2f-4457e5715234, abc0ff53-81a2-4380-b870-409b1994579f, b0f86ad3-9a0c-4571-98fb-75f9d0e72016, b0fe7aa3-f902-42b5-a68e-74785a62e216, b1ba8bf4-0f7b-4e62-a6c1-07133f60e7c8, b1f4bd40-7921-4683-ad79-5251f05fdec4, b74686a0-25ce-4bd9-899f-871699d597ae, b78e68f2-29a6-4f0f-afe1-7f83de849fd6, bb99a25b-198a-4737-bc3e-f28114a4e239, bfa966b9-d7df-4b68-8b17-353213a2650f, c3f6f3de-09b6-4843-af78-a98c8473ec9f, c5dd96f9-0eba-4f7b-8530-8a14b8c713a3, c8e5071e-243c-4485-9750-caed121e3869, d09be942-af4d-424a-8b52-fcc258ec7d3d, d52c72b2-c07d-488f-8a1b-da834e5e3111, dcfa2812-a79a-4660-9ee3-844b9e5e5382, e06d1d9d-c29e-4a5c-a17e-97e9b54edace, e2f0c29a-094f-445d-8b48-dfa84b23c4dd, e7b3a106-54a3-4101-8d68-b1e9963a3750, eb9a8200-0768-4bd3-8bf1-8afd235888e0, f46fcbb3-98cc-4ddd-8630-5998c3092b4e, f6786d47-332c-4202-9cd1-add13d3a403a, f712deea-e66f-408d-983d-dd35477099ff, f8143a55-4a90-453b-a1bc-8406c717b650, fd0e1752-3c51-4880-bb5d-2ef9498ff018]
138375 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
138765 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 72
138765 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
138957 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_001.parquet
138966 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_001.parquet
138967 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_001.parquet => [05993a9b-8922-4857-bb7f-a7c3ccf18b51, 09bcd5e9-43e0-4235-8695-d874fc749c86, 1d0de974-b16a-4f63-9489-6b9620adcb91, 37b28978-cab6-48a0-a711-e30f65cbb73a, 39bcde6d-c5d1-4c73-8976-a123d47526b4, 3a24d78f-25b3-4809-8333-18ca2556a405, 487e53f5-b4e7-46d6-96e6-473fed96c2fc, 4c4cb266-bdcc-414a-918c-c160cdf655ab, 4c6d23ea-a1c8-4962-bf7a-449457efe791, 5abbf25f-4e14-4d89-b715-ba5ade54b1a2, 665a4b61-03e5-4ec4-88fd-4ba0978f0172, 6efc6cb5-22f1-443a-a116-ee70823a6877, 6f73f42d-401e-4c3d-818e-597e4cda0e78, 76704c01-6f07-4a31-81ce-549d8e178fe1, 7713676c-b4e5-4424-8eeb-ca559823416c, 88425bf7-ed27-4bf6-9da2-22ff6f509ec6, 8e2b7d98-385a-49a6-a6bd-604025d6cedc, 994f6c28-9188-4187-8abb-c70a3375194a, 9ae90376-92ed-4183-a82a-d5f2556559c4, 9f7854b0-7789-4275-b411-be9152af1d13, a3eed1fa-607d-4ecf-95cd-0597b9c311ab, a9975bb1-5c6f-4fe4-b0a0-d1e6766a3db5, aad716bd-a17f-41f2-8fa2-65222306c677, b08bbeb4-696f-4045-9752-7192d4d0268a, b6e3179c-659c-4b42-a5ed-cb4c07f82ff1, cc139577-fb0b-454d-9976-9ef5020f0451, cda5870b-6ee9-4f1f-b0be-f3aba813c605, dc4369f0-3897-4559-9ad2-57d33726bf3a, de3509c5-5c04-497c-916d-f593f49e3e47]
138983 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet
138991 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet
138992 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet => [021d3fd1-dbd4-4c0d-9d23-3f1f1552e04f, 07932dc3-8675-4ba3-aa4d-f9807b991d80, 0903a96d-8e86-4224-af4c-d7d22a037877, 0fe33592-319f-482e-81f7-42f3b608b824, 10f155ff-20c3-4b37-b50a-b989491fbf49, 12a3f7c8-dbd4-44f6-a010-196495aa9016, 1d17bee3-54a9-4a41-beb7-cc0949772a77]
139018 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet
139026 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet
139026 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_0_001.parquet => [29d640d7-7b0e-4b80-b8fc-f9108067239c, 3147f409-4f20-4294-bd23-825acc47b6c6, 3ca0e82d-35dc-4f4a-b9af-a0eca390f9c6, 49a2e105-9e89-4bfd-a41f-8fa7a06df554, 5711085e-7da7-4067-8572-c4852537f5a8, 6b15505a-fa5b-4e2d-818f-b19ae8453791, 75c3bb3f-31d9-4a9e-aa18-60f2b9d7bfac, aa8b2925-c26e-468d-b3e3-eca70fea29a2, bf9616bc-0919-460a-b791-52a28d2edcdf, c50d6ce6-ad2f-4a54-bba3-5a8cec802ad6, f1051a59-3651-4ef9-a646-a996ac7aebc9, fcbd36d4-85e1-4339-b719-54c0b048d7c7]
139042 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_1_001.parquet
139052 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_1_001.parquet
139052 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_1_001.parquet => [166f9f69-a55f-4fbf-9f77-aa401d226c7c, 22c13a6d-9118-42a0-ab66-9f002fc12112, 240c9463-a74b-461c-af8e-5d0aab2fa320, 4b3da0c5-f34f-44d7-a78d-9c676186ee20, 59f17608-3beb-4e45-bf7e-01c27ab4f73e, 63a9ad30-4a62-4c5a-b9dd-59de4ddd5d71, 66d8f700-af99-44d7-abb5-4fef24036e33, 6a8bf02a-c5ab-49c9-b1c7-4bdc94cf7657, 6ba38791-0fa6-487e-b050-1f6ffd5be0b6, 7a5defe4-401f-4df0-8b73-7e8d44d1387b, 7aac28ad-75a5-4b95-9fb3-2ef144b06565, 7d4ce3ae-c1a3-46d2-828a-f686ef64fd0e, 8920635e-ca58-4014-a558-3629189eb22e, a92417a7-1de2-44dd-aba4-9b533733936d, ab7f8fd5-5a2e-4096-ab2f-4457e5715234, b0fe7aa3-f902-42b5-a68e-74785a62e216, b1f4bd40-7921-4683-ad79-5251f05fdec4, b78e68f2-29a6-4f0f-afe1-7f83de849fd6, c5dd96f9-0eba-4f7b-8530-8a14b8c713a3, c8e5071e-243c-4485-9750-caed121e3869, e7b3a106-54a3-4101-8d68-b1e9963a3750, eb9a8200-0768-4bd3-8bf1-8afd235888e0, f8143a55-4a90-453b-a1bc-8406c717b650, fd0e1752-3c51-4880-bb5d-2ef9498ff018]
139150 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=72}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=19}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=24}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=29}}}
139161 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
139161 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=9aad69d1-6efe-4229-afb4-183c845e81cb}, 1=BucketInfo {bucketType=UPDATE, fileLoc=983f3e2b-55cc-4f63-8ef5-83e04fc90f08}, 2=BucketInfo {bucketType=UPDATE, fileLoc=643a41ce-f715-415a-8848-c0dcd7360b4e}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{9aad69d1-6efe-4229-afb4-183c845e81cb=0, 983f3e2b-55cc-4f63-8ef5-83e04fc90f08=1, 643a41ce-f715-415a-8848-c0dcd7360b4e=2}
139173 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
139173 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
139498 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
139498 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
139543 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
139609 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
139609 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
139746 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
139770 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
139776 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
139776 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
140075 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 72
140075 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
140192 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_004.parquet
140205 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_004.parquet
140205 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit7224379144322867772/2015/03/17/643a41ce-f715-415a-8848-c0dcd7360b4e_2_004.parquet => [05993a9b-8922-4857-bb7f-a7c3ccf18b51, 09bcd5e9-43e0-4235-8695-d874fc749c86, 1d0de974-b16a-4f63-9489-6b9620adcb91, 37b28978-cab6-48a0-a711-e30f65cbb73a, 39bcde6d-c5d1-4c73-8976-a123d47526b4, 3a24d78f-25b3-4809-8333-18ca2556a405, 487e53f5-b4e7-46d6-96e6-473fed96c2fc, 4c4cb266-bdcc-414a-918c-c160cdf655ab, 4c6d23ea-a1c8-4962-bf7a-449457efe791, 5abbf25f-4e14-4d89-b715-ba5ade54b1a2, 665a4b61-03e5-4ec4-88fd-4ba0978f0172, 6efc6cb5-22f1-443a-a116-ee70823a6877, 6f73f42d-401e-4c3d-818e-597e4cda0e78, 76704c01-6f07-4a31-81ce-549d8e178fe1, 7713676c-b4e5-4424-8eeb-ca559823416c, 88425bf7-ed27-4bf6-9da2-22ff6f509ec6, 8e2b7d98-385a-49a6-a6bd-604025d6cedc, 994f6c28-9188-4187-8abb-c70a3375194a, 9ae90376-92ed-4183-a82a-d5f2556559c4, 9f7854b0-7789-4275-b411-be9152af1d13, a3eed1fa-607d-4ecf-95cd-0597b9c311ab, a9975bb1-5c6f-4fe4-b0a0-d1e6766a3db5, aad716bd-a17f-41f2-8fa2-65222306c677, b08bbeb4-696f-4045-9752-7192d4d0268a, b6e3179c-659c-4b42-a5ed-cb4c07f82ff1, cc139577-fb0b-454d-9976-9ef5020f0451, cda5870b-6ee9-4f1f-b0be-f3aba813c605, dc4369f0-3897-4559-9ad2-57d33726bf3a, de3509c5-5c04-497c-916d-f593f49e3e47]
140226 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_1_004.parquet
140238 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_1_004.parquet
140238 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit7224379144322867772/2016/03/15/983f3e2b-55cc-4f63-8ef5-83e04fc90f08_1_004.parquet => [021d3fd1-dbd4-4c0d-9d23-3f1f1552e04f, 07932dc3-8675-4ba3-aa4d-f9807b991d80, 0903a96d-8e86-4224-af4c-d7d22a037877, 0fe33592-319f-482e-81f7-42f3b608b824, 10f155ff-20c3-4b37-b50a-b989491fbf49, 12a3f7c8-dbd4-44f6-a010-196495aa9016, 1d17bee3-54a9-4a41-beb7-cc0949772a77, 29d640d7-7b0e-4b80-b8fc-f9108067239c, 3147f409-4f20-4294-bd23-825acc47b6c6, 3ca0e82d-35dc-4f4a-b9af-a0eca390f9c6, 49a2e105-9e89-4bfd-a41f-8fa7a06df554, 5711085e-7da7-4067-8572-c4852537f5a8, 6b15505a-fa5b-4e2d-818f-b19ae8453791, 75c3bb3f-31d9-4a9e-aa18-60f2b9d7bfac, aa8b2925-c26e-468d-b3e3-eca70fea29a2, bf9616bc-0919-460a-b791-52a28d2edcdf, c50d6ce6-ad2f-4a54-bba3-5a8cec802ad6, f1051a59-3651-4ef9-a646-a996ac7aebc9, fcbd36d4-85e1-4339-b719-54c0b048d7c7]
140254 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_0_004.parquet
140266 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_0_004.parquet
140266 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit7224379144322867772/2015/03/16/9aad69d1-6efe-4229-afb4-183c845e81cb_0_004.parquet => [166f9f69-a55f-4fbf-9f77-aa401d226c7c, 22c13a6d-9118-42a0-ab66-9f002fc12112, 240c9463-a74b-461c-af8e-5d0aab2fa320, 4b3da0c5-f34f-44d7-a78d-9c676186ee20, 59f17608-3beb-4e45-bf7e-01c27ab4f73e, 63a9ad30-4a62-4c5a-b9dd-59de4ddd5d71, 66d8f700-af99-44d7-abb5-4fef24036e33, 6a8bf02a-c5ab-49c9-b1c7-4bdc94cf7657, 6ba38791-0fa6-487e-b050-1f6ffd5be0b6, 7a5defe4-401f-4df0-8b73-7e8d44d1387b, 7aac28ad-75a5-4b95-9fb3-2ef144b06565, 7d4ce3ae-c1a3-46d2-828a-f686ef64fd0e, 8920635e-ca58-4014-a558-3629189eb22e, a92417a7-1de2-44dd-aba4-9b533733936d, ab7f8fd5-5a2e-4096-ab2f-4457e5715234, b0fe7aa3-f902-42b5-a68e-74785a62e216, b1f4bd40-7921-4683-ad79-5251f05fdec4, b78e68f2-29a6-4f0f-afe1-7f83de849fd6, c5dd96f9-0eba-4f7b-8530-8a14b8c713a3, c8e5071e-243c-4485-9750-caed121e3869, e7b3a106-54a3-4101-8d68-b1e9963a3750, eb9a8200-0768-4bd3-8bf1-8afd235888e0, f8143a55-4a90-453b-a1bc-8406c717b650, fd0e1752-3c51-4880-bb5d-2ef9498ff018]
141278 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
141529 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
141610 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
141610 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
141754 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
141754 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
141897 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
141897 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
142009 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
142009 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
142247 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
142247 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
142307 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
142596 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
142596 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
142728 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
142787 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
142796 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
142796 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
142797 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
142797 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
142797 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
142825 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
142825 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
142956 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
142956 [pool-1393-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
143006 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
143006 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
143022 [pool-1393-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
143044 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
143044 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
143095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
143095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
143188 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
143216 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
143223 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
143223 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
143322 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
143634 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 140
143635 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
143800 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 48 for /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_001.parquet
143811 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 100 row keys from /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_001.parquet
143811 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 48 results, for file /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_001.parquet => [07738e9f-ea61-46cc-a43a-6efdedf0999b, 0a7d72ed-5a85-4e23-a33e-74ccd7f19df9, 0af19c11-f857-4596-8a33-71dd9191dedd, 0f7ac02d-cec5-422d-a5f2-f107a393b1d2, 11b7be29-3bd9-4fde-b24d-0ba3882e1ce5, 156659e6-c626-4d08-afd0-82862a44c07f, 1793de45-d71c-449a-a3ab-dacf03fb397b, 17ea3e5b-dc6e-4690-aae5-a45275b88700, 2197835b-36ba-45f2-b9e0-8432c945c104, 21d19808-f7ae-45dc-81db-654a3710f936, 247d7f10-3967-4cd6-bb7a-966296df5c8f, 24ccd1fd-dedb-4345-a4e2-9dc3fbcf28cd, 2a5a08c4-4122-4741-afa3-9442e5b5b86e, 2b198c21-30ad-47d4-ac57-1d8c70d46d7e, 2b9d95b4-9c0f-44d0-9abe-e918b4679a09, 2fb33010-81ee-4bc2-a6a1-ac9255c14df7, 32c8f169-6310-43b7-9825-92ef5f8b6cb3, 33c3f176-f84b-4991-91d3-bb931b36bf30, 3b05ada5-aeb3-482d-b4f8-ebff27c04b64, 3bcd32c4-dd99-4f09-ac59-2c356cad7840, 4fb81097-5a8d-4c56-9ffc-1f6cce1efd0d, 502c9c8e-fea3-43fc-8ecb-2518d9495504, 571575e6-aa14-4374-a0fc-44fdff66382c, 59de14ec-c747-413f-b796-72b6ac70a70a, 6712a6b1-d2d3-4add-8a3b-4763b5efa017, 6c1f35da-5094-4669-8906-631912e6f97f, 6dd4c6f7-2358-477c-8b0e-cb17b75a7c65, 6e9533c3-0717-4348-9efe-ade597efaaf8, 6ef81fd3-ef7f-4087-8d16-d8dbe8d8dde3, 715cac7d-d75a-42fb-8b5b-5f4595257b1a, 736e0ee8-30e3-49e1-9cd8-a0b3d972bb60, 73efbabe-1a0a-4ba0-9913-6b1721d04db6, 761d3885-dbc3-46a0-9d6b-0d1c396039de, 76c16c2a-1f67-4463-8590-28766c583c76, 782b5bdc-d1bb-434d-9200-2cdf26765730, 7a593b32-598e-496b-8f02-83d4626be149, 7ae147a9-1450-431c-80e9-a563f02cb2cf, 80a1c733-3a09-41bc-9253-ab052f7abb8f, 812574d8-5310-4cbb-9c29-753ca4b677fa, 8574a821-40f8-4580-b6d1-5494cb3bbd0a, 85891966-465c-4b28-90cf-e38dbb856d17, 88bca80d-837c-4129-b120-ce7f59a3fe76, 8fe0c416-167f-4236-b671-5f8d61b2ed4b, 93fd7d47-d902-4040-980f-565b078a22fb, 949137ea-3c99-4e0a-9245-bd3b91a725cb, 9bb3e7d3-a36c-4b44-9bc5-b66b2ac33c37, 9dc055d6-d8a8-4c5c-a938-cd68619c76ec, a0642695-a79c-4045-9b3f-0587e53b988b]
143842 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 52 for /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_001.parquet
143854 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 100 row keys from /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_001.parquet
143854 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 52 results, for file /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_001.parquet => [a1569b64-1562-4aeb-bca9-b6f318eaa8d0, a23f9ace-8c1c-4f30-b1e3-8ebc2ce8a808, ab0b7087-c931-4c2a-ba74-1030ed1326f7, ac040e3f-2692-4b97-9bef-423a5977c78b, ad79585d-431a-46f5-8b87-527f5db86d2c, ade3c1dd-0393-4e84-875b-996c9b3c4b79, b01429ac-cc04-40a6-bb04-a94658094769, b0a717c8-f0c4-4b51-9aa5-0b456aec2070, b30776f0-ae8b-47e9-bbb5-af633f0d6d83, b646b821-0f60-4114-8191-a67b9d9b091c, b681f6be-bc00-4988-b21f-a4d5dac52e6c, b69cf61f-b92c-406a-a353-22017967171c, b7e1c612-7dce-40ad-a7f6-3817636fe3ec, ba11eb69-c463-4481-9138-bd5024da538a, bcb7df84-a27e-4ea3-999d-9b84bcd126a7, bd643d19-76e6-4b60-83ea-9c27fdbb08b5, bd848cf6-dea0-437c-8bff-903234afe88d, bdfe2840-ef43-4959-87dd-9c6caff31f20, be4eed2d-061a-47d4-9d0c-422de4aa0513, c73083c5-40d8-4637-8ce6-08695f2df7a2, cdc3ace5-9ae9-4fa4-9f52-05eaa0ee8d3a, ced7d469-d284-4971-b67e-0eef3b963ff7, d59aa37a-560c-4677-8aa2-a970d9c4a384, d68d932e-089e-403e-9cf0-79179dfbfe33, d6fa5bee-d384-4f96-8310-8af0033f590e, d9d63ee1-2c0b-4e02-a80c-8634f33d522c, da10f70f-f140-44bf-8e10-050bd4d049a1, db7c1b8c-04a0-4e01-b7ee-790f38c4b0c3, dc2ba441-ad28-4efb-8036-9f534aa96135, df9098d5-28f0-455c-a9d0-b8f0e7b89a5a, e0cb4e5f-b72c-430c-9991-c1dd88535bba, e1aae039-ac6b-4491-b384-75fb127e6ac6, e54df455-c8e5-44a2-ab8b-cf48de9a83f5, e6091c4b-2ff3-402f-8a48-5ad495224a1e, e705c94d-ca17-4eaf-93d7-1198fde7e587, e857119f-be5b-4c3c-be2c-5dd6a13c74c2, e9e0b46c-bd84-4dcf-9ab6-8502237b23e9, e9e16e0b-80ca-4ab5-9a1e-811b6d5f3829, eb16f28b-3de7-43bc-90a0-4e6e1a875fd3, eddcbdcc-65a1-4206-a8cc-684bfa25d763, eee0ceb1-3ce6-4a00-9b5e-5af15939b56d, f0a76c92-b1ff-44e2-832d-a139fe3bb8be, f1740574-efad-462f-ae71-789dfbc5d0ed, f1aa1391-ca53-499e-9324-77ef2afe24ce, f2056e10-258e-4ee8-94de-1b3019934992, f23f8d09-279d-4243-af19-31bdd5437cb2, f59ac8be-8b74-4d81-9dae-6a676d1b119d, f6910b64-d110-49f0-aa2a-332e5b1aa180, f84077f6-46e0-4d67-9951-e66f4ac4fc2b, fee8f1c9-f429-49a6-a882-a4779af6749e, ff0e37b3-2680-4bf7-83a1-2698d1deb9ad, ff186225-b044-4f3a-ae15-6159f6cd59b9]
143929 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=40, numUpdates=100}, partitionStat={2016/09/26=WorkloadStat {numInserts=40, numUpdates=100}}}
143939 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 4438
143939 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=44d1d158-f324-40eb-9a7e-fd810e86bbee}, sizeBytes=443800}]
143939 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 40 inserts to existing update bucket 0
143939 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
143939 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=44d1d158-f324-40eb-9a7e-fd810e86bbee}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{44d1d158-f324-40eb-9a7e-fd810e86bbee=0}
143951 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
143951 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
143971 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
144247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
144247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
144310 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
144310 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
144404 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
144410 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
144410 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
144511 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
144816 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 231
144816 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
145088 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_002.parquet
145100 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 140 row keys from /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_002.parquet
145100 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_002.parquet => [09425301-50ee-4877-afd0-cd57b16bfe72, 105fb2b6-eb0c-4392-af1f-67112357fce2, 142077cd-225a-403e-bfc7-23d5fa4c4e75, 14f96a2e-e9bf-420b-8b5a-8e3c137d9717, 19ebd814-da31-4033-a111-a8667cf3539d, 21ef28a1-cc5b-4287-9252-03c790826122, 281477bc-f243-42a4-9b89-3a6ecb970892, 33f08628-ea10-4250-97b2-910772dc5ff4, 54a03669-2ef7-4733-a0f2-77c0564bb85b, 590ed4e3-05d6-4621-9eef-ead712bd7759, 59564d6f-f8f5-43e6-bdc2-d16d9b81db38, 5c1c7ce4-2fe2-4c5e-9903-762b324726d4, 618b2ae2-41ad-4afa-bb2b-7540242da450, 63b2dc9d-16d6-4a7f-9431-0d3140142a09, 63fd9416-fa8f-4d19-94dd-9500d366aa2a, 654a71ea-be49-4dc7-ba5d-3d0ffd3030b0, 7a297cb3-b5b5-459d-850b-731fcbfc8eca, 7c178c21-0523-4877-992a-5cbb675fcc85, 7f3db897-260d-47b9-996f-bd990c847f51, 7f8a5113-c3d9-4868-816d-c11644cd537a]
145151 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_002.parquet
145163 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 140 row keys from /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_002.parquet
145163 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit4432915146319383015/2016/09/26/44d1d158-f324-40eb-9a7e-fd810e86bbee_0_002.parquet => [86a4d22f-dd50-4205-b2b0-226115c7098c, 87a19ff1-10de-4a23-96fd-14765089a434, 9af6da97-355a-48ad-b3f4-278645ba06b2, ada954ab-fc3f-447e-b982-25616fdd6999, b4072b36-a99a-44cf-9fe2-6121a83f1ec6, b5725f9e-a14a-4d1e-8c4f-470ae110be0c, b5b195f4-5512-4314-97fe-91f729126dd0, b6c42807-ca91-48c9-9b8e-9b22aa8072a3, b7d95925-6d77-4da8-83bb-6d0a97dd967e, bbc5aea6-24ea-4c93-9d72-c3e2184dc37b, bd55c027-3603-4ac7-80cb-31bc4d6d6b03, ca07d519-2c19-4ad1-bf29-91bd4dba9993, cfed0b40-995d-4ed6-81dc-42dccc912745, d4a85f05-229e-450d-b4c0-9b5b903eb016, e12c2301-f6c7-477f-8a63-109e304dcb89, e31c085d-c4b4-4ee2-a8c7-0865040c462d, e9c74ccd-240b-4323-83e4-337e6b6839b4, f07d83b9-1893-4998-99eb-1dc02df49285, f944305f-3a06-40d4-bf73-4806b90cb162, fa58dd1e-69be-4c06-b92e-9ab1b7b91156]
145168 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
145257 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=40}, partitionStat={2016/09/26=WorkloadStat {numInserts=200, numUpdates=40}}}
145267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 3195
145267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=002, fileId=44d1d158-f324-40eb-9a7e-fd810e86bbee}, sizeBytes=447205}]
145267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 180 inserts to existing update bucket 0
145267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 20, totalInsertBuckets => 1, recordsPerBucket => 100
145267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]
145267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :2, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=44d1d158-f324-40eb-9a7e-fd810e86bbee}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]}, 
UpdateLocations mapped to buckets =>{44d1d158-f324-40eb-9a7e-fd810e86bbee=0}
145279 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
145279 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
145411 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
145613 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
145613 [pool-1412-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
145621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
145621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
145639 [pool-1412-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
145655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
145655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
145734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
145734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
145823 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
145828 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
145828 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
145970 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
146058 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=71, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=66, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=63, numUpdates=0}}}
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 71, totalInsertBuckets => 1, recordsPerBucket => 500000
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 66, totalInsertBuckets => 1, recordsPerBucket => 500000
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
146065 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
146083 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
146083 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
146182 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
146182 [pool-1432-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
146206 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
146206 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
146230 [pool-1432-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
146246 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
146246 [pool-1433-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
146269 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
146269 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
146296 [pool-1433-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
146311 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
146311 [pool-1434-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
146333 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
146333 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
146362 [pool-1434-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
146641 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
146953 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit7927201792572685150/.hoodie/.temp/2016-03-15_49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001_3_3.parquet to /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet
147411 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
147501 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit7927201792572685150/.hoodie/.temp/2015-03-16_59b22167-9973-4bfe-aa8a-5afb1c105883_1_001_3_4.parquet to /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
148031 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit7927201792572685150/.hoodie/.temp/2015-03-17_d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001_3_5.parquet to /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet
148481 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
148767 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
149809 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
149810 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
149810 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
149906 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
149906 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
150730 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
150736 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
150736 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
150794 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
151381 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
151381 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
151545 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 71 for /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet
151560 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 71 row keys from /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet
151560 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 71 results, for file /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet => [0120dc3c-0b93-41c9-a850-b2db462085f6, 09577b82-66f0-4e1a-8d83-53360d290266, 0d203d2c-b965-4f86-8b5f-fb7ce7f1dd0e, 0fcc5d3e-c71a-4f9f-afa1-5d2996b0fca5, 11a14bbd-834e-458b-b3c9-1c70b91d8b49, 1425b282-b0a3-41bd-a956-9e16a5d6c651, 19de91bc-1caf-4d87-b261-da13efc504d6, 20ecb651-c7ec-4644-994b-829ad5ab6efe, 29de027b-6aa6-402e-a040-3a405777bbea, 2ddd5427-e551-40fb-bfb2-58c43a027c08, 2eda176b-2183-4b28-b752-b7558bc9bffa, 30c8fed3-237c-4700-a3b7-e0084f3d36bb, 3218147f-0ab0-41f4-9e9b-f85e54c2f5bc, 36022132-2543-44db-8df7-b32d252a5820, 3ab5c61d-3787-40a3-9543-196dce1d41cc, 3ca9397c-d5f0-4d13-a3c8-6afd014c1aca, 3e944dce-d5b4-48eb-83b2-6e37d0647667, 438f3fc6-ffa6-44e2-a259-613931bf8a17, 49b2761a-5303-4fc5-a4dd-ba6c2112e526, 4b0591a4-3365-4693-ac44-27ab795c4243, 5255b905-98b0-49a9-8904-4bbba6b680a3, 5684d0bc-27c3-4f52-9348-f7f03f0caa9d, 5adac9a0-e129-4480-abe1-c564b543981c, 61c131df-0432-4a6c-8e1f-66838929eeca, 66c8c6a9-9698-4d79-a9a7-e2beacd50528, 6871f576-bc99-4c2a-b252-b9e27fd8c6d4, 6d488b16-e5db-4732-a397-afe1e5d2617c, 7512ab3f-e421-44c0-983b-2c01e7129aae, 78470ad7-f2c9-4d76-8cce-a98e7d26275b, 786deee4-2a8d-47c7-af0e-1e06077ae12d, 79ccfb86-040f-445d-b969-9eae31554ab4, 7acfac96-407d-49ff-873b-428514678c5e, 7b0969de-b02f-4021-a1b3-7c80bb3fb88d, 7ca1d48c-10be-4dde-90eb-f6122e20abec, 7ff58111-96d0-41c2-ba0b-99e404f6440b, 81553189-ba16-4249-8449-47c5f2294b28, 856f3c0e-3188-49d6-96c1-3e0d9044e373, 8ece6fc0-75e6-4d1f-94eb-914333d0cee6, 905b22f8-8287-425d-822d-3307846541dc, 907cea78-0973-4545-a15e-cc1366661ae3, 921f696c-5545-40f8-9a7d-fefc1db11926, 93a81295-3d14-4775-910b-3f054d602a7b, 9ce22b3e-2557-4687-9197-22273666f88c, a6ad2455-8a23-4574-9008-31b06af309e8, a74772aa-b0ec-4529-99fa-9164a49a6962, a86930ea-b8d0-49ab-93b4-8163dec2617c, af2be4de-ee05-4089-851a-405c762391d9, b243e7df-4de8-4a40-8e6b-6eb85ee0ea2e, b28bc171-6479-44a2-8cc2-f4b657a24bb1, b4c3ed79-5d22-46b5-90d7-72ad843d7eb3, b7259a8e-4758-4998-99a3-049eece37819, b9710178-0307-4022-b850-8968927bbcb3, bbfa4c2a-35ae-4e32-8514-c2b9bfe14737, c6704856-4bba-46ee-8f9c-f9bdf96c5a36, c7b565cb-c686-4653-90ff-f55dcc445af5, c7fc6de8-78e6-4ee9-bc7a-331ce5fa6e6e, c89216b6-5c0e-4f07-84eb-3600680b6b17, c8cb9c8f-b3e7-4706-8a48-c83d0b91d9ea, c9a33896-7284-4ef5-958d-c172159b20ee, ceeb5b30-e3c1-43f5-8bd6-8707f7b4cca7, cf0e6fd2-da11-4d92-b68c-70f073ef2af1, d01daa0f-381e-47c7-ba7d-60ee46d1cd28, d2f36069-ed15-408a-ac4d-7496473f2948, d473d2b8-062b-4b6e-b962-7f9a7411b719, db57901a-37d2-42ac-8c47-6fa1adcb58d6, eb3d1369-b68e-447a-88c1-e6f07bece9b0, ed1120f4-9fdb-4785-8cba-acd8770434fc, f242db19-09d7-4f7a-b259-0cade39c1896, f3bdeaf1-d224-4aef-b9db-7cb965ef6539, f61da714-d9cd-4b99-8672-76b7a7e2c52f, fe65c8a1-8bcb-4f23-814d-379883cd5ec8]
151579 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 66 for /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
151591 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
151591 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 66 results, for file /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet => [026b8239-f8bc-4400-a8c5-bc299f1ab103, 08c269de-a74a-40c2-a386-a52803a89854, 09a5bd46-3a9d-4a74-a560-c932cfabde8a, 0ddf9db0-c9d2-4326-b111-9ef7a824db48, 0e2938fa-0bb2-4e95-b01c-bd854c6e4233, 1b0971ef-8026-4c96-8e46-5aa73f207897, 26855f00-8261-4f68-a957-024db03283f0, 30c6578c-3cbd-4663-8373-456ab9ae2be9, 35a30690-3a00-46f2-87c9-41b2c0e2fe21, 35d111df-6df5-4945-807d-2a544b19c0ec, 375a4791-4384-48dc-910e-6498bdca2751, 38057544-8bf7-4338-adc8-79b856c460b5, 3908e373-7cbe-4d26-911e-75bf124c51b5, 3b278e28-530f-4fb4-9456-2034dedf33ab, 44f18936-70d0-4582-bb62-c9eb9d67884b, 4a1385b2-22c0-4c04-a8fe-9d644781f0c2, 4e64c02f-5a97-44ce-a699-abb2a3aeccb2, 58a40042-00ba-4b67-bf80-c65e795d6685, 5f383074-d168-4458-a247-430225e7c56d, 5fdfa597-1c0a-4ea9-b8f6-bbad038b28e3, 66115908-c031-4ec8-8af5-bebe076b2db6, 6ea204c0-a23a-434b-8afb-5de36fbace07, 77fd0bbc-78dc-415a-8e64-df983e4fb901, 787cd706-6a7a-4395-895a-78f843fbd545, 7dcd7c97-e061-4019-ae96-3dbdca5c4980, 7e5f16d4-059c-4cde-b30c-7b119d32873a, 7ef0812d-e7ca-4cd8-b9dc-dc229125643e, 8388ab25-60a9-43a4-9709-b636271d28a2, 882a8c46-0705-48e0-a060-012ba2126d88, 8832dc87-0b0c-4083-99d5-4925c556786c, 88c3d13c-b26b-4460-87cf-dda4d3f2ab7b, 88fb110b-fbf4-4f84-96e4-d2fe549572b8, 8f78dffb-8e4f-4fa4-a9e6-e3f51f3d826d, 92f5a3a6-2759-4474-959b-031bcd386728, 95ad9524-9e8b-485a-be6e-f534a851b0d5, 98568cb1-9055-4596-a462-a9fae9fe4393, 98a08563-f048-425f-8e55-6ae80791cdfd, 9a1c2967-0d5c-4042-a433-948da07ea933, 9e344112-4d9f-4a2b-8501-6876810a4ebe, a0e58682-072f-4163-9850-8fba96ef8eef, ac5370db-7836-44f8-b9be-663a663f1df0, af74b8a2-9c7a-4026-ab82-5f8ef3d728c4, bbe61ccc-b5a4-41a7-9567-1fa7025be598, bbf8d6fd-df99-4292-9ba3-7f315b77cba1, c13fbb27-9621-476b-8a60-53e727dc31d1, c758507f-5f69-458f-ae39-d35fd5d10b63, c953c508-a685-4014-a648-f961b28aa309, ca1165c9-9b95-4375-a4a6-aa9be9f32cd1, caedb4a4-8369-461d-882f-ad3af7ece490, cfc76ddf-83d3-4cd9-b8d1-78de46d7e1d9, d014eaa2-9148-403e-98e3-f98e3ccc0f94, d2f9ae1b-68c9-4bba-9d34-0cb42f80a2ae, d7234b3a-3fc9-44de-8787-ad8ca502ab66, dc439606-b237-4fb4-a9c1-27a8d82e6d2a, e5f0fffc-dc78-43ae-81ba-27ebd4da392e, e6789306-98ef-4e01-afad-5ccc0487ee8e, e91e51ea-7cdf-4875-8713-6cdb48add95c, e9f17ff1-32ad-462b-9ec6-1ec3dff2cf21, eba6b7ae-cb71-41b9-96e7-855955f8bc67, ece81e11-48b8-4cce-ae7c-993207f3dd56, ece94204-2930-4f01-ad1e-d2590fb82c55, f3e1c8d0-88ac-428a-a6d6-86826a8fed48, f6e3bfb5-b7be-4dda-a2a5-05a615036813, fa09130c-edba-4bc3-9153-3660819619ae, fcba26c1-8f89-437b-a6eb-0766eca9fe06, ffc16ab0-a738-4be2-955c-27e47dd98bf6]
151610 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 63 for /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet
151615 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
151621 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet
151621 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 63 results, for file /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet => [007d4aab-bfaa-4aff-96c4-bd9ce6b5e7f0, 00f8a296-42ec-475f-96f5-742993359fee, 0869528b-c5be-4890-83d4-50683d38cbd0, 150b5802-6344-4d5a-af9e-78ee2f7add15, 1a8e3c64-7da2-4b94-8ff9-a737f1337f4d, 22451668-c29e-4f65-b7de-189eabd69969, 2616809a-7359-44f0-a113-d81c79bff25a, 281cc05c-641c-4717-b0db-0cf615b85f52, 28e810f7-a91c-4e9f-863a-e33e8e0b2ff3, 2ef28dd6-a10c-4ad0-b069-18727fd1efe4, 3043ff93-944d-446b-a4c8-fa96d07638be, 33f5c26d-2946-4d78-9769-75465cd787d3, 38a9df3d-30c8-43e5-8544-6bec9b287c86, 40d375b7-4159-4720-8731-6a88491cd117, 43e43f07-ec6a-4a41-b5c4-6e86c2b085a8, 4aebbebe-efb6-4981-8e1f-caa3937dc24f, 4b1ed25f-b12c-41ae-9096-10269eea54df, 4bbcb9af-bb36-400f-8bc0-62db77ac9b34, 4e4e5aff-88c6-4c0d-b4b6-953593eae84e, 4f281495-40cd-492a-9f00-33b4b091a0e4, 4f6f9b0b-025f-492a-9dcd-a8afbb848e58, 521818fa-85ad-40ef-adb0-7de741b90f54, 5245caea-d1ae-4564-870c-cb0d5fd3d921, 529d3ba0-9cf6-4917-8e26-c613d84c6044, 52cb728a-7b3e-417d-bcf5-05eba43c1a52, 57c3ea96-7df1-468e-9650-444d5c908e74, 5c31fc8d-b4a6-4a09-a716-41243a811e85, 5e35803f-ecdc-4d88-a0a8-b1f92d955385, 60e31a6d-c6ba-4e84-adc0-9ee4b824cd22, 6320d813-4123-40c6-b019-8ecc7f4c304b, 63df94c5-8a8f-445e-b0c0-a813d312eb7a, 69cf87fc-7dfb-4d7a-8c7d-08d302b77d52, 6dd47051-9347-4d87-bfba-e291093395eb, 71d5d36c-eeea-4069-9b95-cc93fc313cc3, 79e5d549-f8fa-4e7c-b2b1-535319f5d159, 86631f82-5a61-453a-a3ca-401668cc49f9, 98c2f7f3-08b6-42c0-bb4b-5038df933fc3, 9b86fcf2-ff76-4b84-99a3-65d533be4d60, 9eb764ac-8f85-4264-882b-289fc8dcca18, a13d2a41-4413-4d76-ac31-4ac54aa80b31, a2410620-bcc3-4c2a-ac96-dc6b55fad7e6, a2d1477d-552c-4a3e-88d8-c364f72f56e7, a7964453-e760-4040-a689-676db9d9f14c, a98120ed-53ab-427a-a1e4-0776f66f81e3, aa014a33-9248-4c4d-bd86-ef7405d58886, ab6342c7-79ca-40ca-8d16-367e7ede3272, ae2a9cf6-2270-4c59-bebd-741d97123700, b1901918-5e50-4396-805d-2dab6d385ef2, b6ecb2ab-5952-4dd3-bd82-d5aba0094181, b6f6fd78-058c-479f-afe3-5eb5bd99dba5, bdcd0355-94fe-4ad6-826d-0116c823e938, c04fdad3-08a2-4f42-b370-b0a0a3ecebee, c1e44087-5926-46e5-bcfd-54e71c882f12, c555808a-6b19-4ce4-b387-b84d8164427d, ca4969cf-ff3d-4912-8553-d51731f359f7, cfce0061-2a21-4226-b3b0-d3432e7249fc, d15b48a3-e89c-4fb8-a78d-8efe61b420c1, d313cf6f-c1d3-4547-8f6c-f2b67ae89696, d9386596-72b2-4fb9-a2b5-4f385835899b, e8dfeefb-8199-45bf-858c-77f7c196fc21, ea506b09-bbc1-4402-8683-64cd63703e8b, f8349193-a4d3-4e1f-ad87-ddad85d777af, fff4986b-f00f-4d71-8fca-a1eca7b04da6]
151670 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
152020 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 80
152020 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
152133 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
152196 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet
152206 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 71 row keys from /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet
152206 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_0_001.parquet => [0fcc5d3e-c71a-4f9f-afa1-5d2996b0fca5, 30c8fed3-237c-4700-a3b7-e0084f3d36bb, 3ab5c61d-3787-40a3-9543-196dce1d41cc, 3ca9397c-d5f0-4d13-a3c8-6afd014c1aca, 3e944dce-d5b4-48eb-83b2-6e37d0647667, 4b0591a4-3365-4693-ac44-27ab795c4243, 61c131df-0432-4a6c-8e1f-66838929eeca, 786deee4-2a8d-47c7-af0e-1e06077ae12d, 7acfac96-407d-49ff-873b-428514678c5e, 7ca1d48c-10be-4dde-90eb-f6122e20abec, 81553189-ba16-4249-8449-47c5f2294b28, 905b22f8-8287-425d-822d-3307846541dc, 907cea78-0973-4545-a15e-cc1366661ae3, 921f696c-5545-40f8-9a7d-fefc1db11926, 9ce22b3e-2557-4687-9197-22273666f88c, a6ad2455-8a23-4574-9008-31b06af309e8, a86930ea-b8d0-49ab-93b4-8163dec2617c, b28bc171-6479-44a2-8cc2-f4b657a24bb1, c7b565cb-c686-4653-90ff-f55dcc445af5, c7fc6de8-78e6-4ee9-bc7a-331ce5fa6e6e, c89216b6-5c0e-4f07-84eb-3600680b6b17, c8cb9c8f-b3e7-4706-8a48-c83d0b91d9ea, cf0e6fd2-da11-4d92-b68c-70f073ef2af1, d2f36069-ed15-408a-ac4d-7496473f2948, ed1120f4-9fdb-4785-8cba-acd8770434fc, f61da714-d9cd-4b99-8672-76b7a7e2c52f]
152223 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
152235 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
152235 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet => [3b278e28-530f-4fb4-9456-2034dedf33ab, 5fdfa597-1c0a-4ea9-b8f6-bbad038b28e3, 66115908-c031-4ec8-8af5-bebe076b2db6, 7dcd7c97-e061-4019-ae96-3dbdca5c4980, 7ef0812d-e7ca-4cd8-b9dc-dc229125643e, 8832dc87-0b0c-4083-99d5-4925c556786c, 88fb110b-fbf4-4f84-96e4-d2fe549572b8, 8f78dffb-8e4f-4fa4-a9e6-e3f51f3d826d, 92f5a3a6-2759-4474-959b-031bcd386728, 98a08563-f048-425f-8e55-6ae80791cdfd, 9e344112-4d9f-4a2b-8501-6876810a4ebe, a0e58682-072f-4163-9850-8fba96ef8eef, ac5370db-7836-44f8-b9be-663a663f1df0, c13fbb27-9621-476b-8a60-53e727dc31d1]
152261 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
152269 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet
152269 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_1_001.parquet => [c758507f-5f69-458f-ae39-d35fd5d10b63, c953c508-a685-4014-a648-f961b28aa309, caedb4a4-8369-461d-882f-ad3af7ece490, cfc76ddf-83d3-4cd9-b8d1-78de46d7e1d9, d014eaa2-9148-403e-98e3-f98e3ccc0f94, d2f9ae1b-68c9-4bba-9d34-0cb42f80a2ae, e5f0fffc-dc78-43ae-81ba-27ebd4da392e, e91e51ea-7cdf-4875-8713-6cdb48add95c, eba6b7ae-cb71-41b9-96e7-855955f8bc67, f3e1c8d0-88ac-428a-a6d6-86826a8fed48, fa09130c-edba-4bc3-9153-3660819619ae, ffc16ab0-a738-4be2-955c-27e47dd98bf6]
152286 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet
152295 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet
152295 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_2_001.parquet => [0869528b-c5be-4890-83d4-50683d38cbd0, 1a8e3c64-7da2-4b94-8ff9-a737f1337f4d, 40d375b7-4159-4720-8731-6a88491cd117, 4aebbebe-efb6-4981-8e1f-caa3937dc24f, 4e4e5aff-88c6-4c0d-b4b6-953593eae84e, 529d3ba0-9cf6-4917-8e26-c613d84c6044, 57c3ea96-7df1-468e-9650-444d5c908e74, 5e35803f-ecdc-4d88-a0a8-b1f92d955385, 6320d813-4123-40c6-b019-8ecc7f4c304b, 63df94c5-8a8f-445e-b0c0-a813d312eb7a, 6dd47051-9347-4d87-bfba-e291093395eb, 86631f82-5a61-453a-a3ca-401668cc49f9, 9b86fcf2-ff76-4b84-99a3-65d533be4d60, a2410620-bcc3-4c2a-ac96-dc6b55fad7e6, a2d1477d-552c-4a3e-88d8-c364f72f56e7, a98120ed-53ab-427a-a1e4-0776f66f81e3, aa014a33-9248-4c4d-bd86-ef7405d58886, b1901918-5e50-4396-805d-2dab6d385ef2, c04fdad3-08a2-4f42-b370-b0a0a3ecebee, c555808a-6b19-4ce4-b387-b84d8164427d, ca4969cf-ff3d-4912-8553-d51731f359f7, cfce0061-2a21-4226-b3b0-d3432e7249fc, d15b48a3-e89c-4fb8-a78d-8efe61b420c1, d9386596-72b2-4fb9-a2b5-4f385835899b, e8dfeefb-8199-45bf-858c-77f7c196fc21, ea506b09-bbc1-4402-8683-64cd63703e8b, f8349193-a4d3-4e1f-ad87-ddad85d777af, fff4986b-f00f-4d71-8fca-a1eca7b04da6]
152357 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=80}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=26}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=26}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=28}}}
152368 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6615
152368 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=59b22167-9973-4bfe-aa8a-5afb1c105883}, 1=BucketInfo {bucketType=UPDATE, fileLoc=d48a5590-0b74-4321-aeb3-ad81fa36efb0}, 2=BucketInfo {bucketType=UPDATE, fileLoc=49a30b19-96e6-4b58-bfe4-ebd44b086614}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{59b22167-9973-4bfe-aa8a-5afb1c105883=0, d48a5590-0b74-4321-aeb3-ad81fa36efb0=1, 49a30b19-96e6-4b58-bfe4-ebd44b086614=2}
152383 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
152383 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
152794 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
153281 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit7927201792572685150/.hoodie/.temp/2015-03-16_59b22167-9973-4bfe-aa8a-5afb1c105883_0_004_43_461.parquet to /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_0_004.parquet
153762 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit7927201792572685150/.hoodie/.temp/2015-03-17_d48a5590-0b74-4321-aeb3-ad81fa36efb0_1_004_43_462.parquet to /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_1_004.parquet
154082 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
154254 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit7927201792572685150/.hoodie/.temp/2016-03-15_49a30b19-96e6-4b58-bfe4-ebd44b086614_2_004_43_463.parquet to /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_2_004.parquet
154273 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
155730 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
155730 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
155780 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
155851 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
155851 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
156010 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
156016 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
156017 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
156079 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
156410 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 80
156410 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
156522 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_2_004.parquet
156535 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 71 row keys from /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_2_004.parquet
156535 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit7927201792572685150/2016/03/15/49a30b19-96e6-4b58-bfe4-ebd44b086614_2_004.parquet => [0fcc5d3e-c71a-4f9f-afa1-5d2996b0fca5, 30c8fed3-237c-4700-a3b7-e0084f3d36bb, 3ab5c61d-3787-40a3-9543-196dce1d41cc, 3ca9397c-d5f0-4d13-a3c8-6afd014c1aca, 3e944dce-d5b4-48eb-83b2-6e37d0647667, 4b0591a4-3365-4693-ac44-27ab795c4243, 61c131df-0432-4a6c-8e1f-66838929eeca, 786deee4-2a8d-47c7-af0e-1e06077ae12d, 7acfac96-407d-49ff-873b-428514678c5e, 7ca1d48c-10be-4dde-90eb-f6122e20abec, 81553189-ba16-4249-8449-47c5f2294b28, 905b22f8-8287-425d-822d-3307846541dc, 907cea78-0973-4545-a15e-cc1366661ae3, 921f696c-5545-40f8-9a7d-fefc1db11926, 9ce22b3e-2557-4687-9197-22273666f88c, a6ad2455-8a23-4574-9008-31b06af309e8, a86930ea-b8d0-49ab-93b4-8163dec2617c, b28bc171-6479-44a2-8cc2-f4b657a24bb1, c7b565cb-c686-4653-90ff-f55dcc445af5, c7fc6de8-78e6-4ee9-bc7a-331ce5fa6e6e, c89216b6-5c0e-4f07-84eb-3600680b6b17, c8cb9c8f-b3e7-4706-8a48-c83d0b91d9ea, cf0e6fd2-da11-4d92-b68c-70f073ef2af1, d2f36069-ed15-408a-ac4d-7496473f2948, ed1120f4-9fdb-4785-8cba-acd8770434fc, f61da714-d9cd-4b99-8672-76b7a7e2c52f]
156549 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_0_004.parquet
156561 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_0_004.parquet
156561 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit7927201792572685150/2015/03/16/59b22167-9973-4bfe-aa8a-5afb1c105883_0_004.parquet => [3b278e28-530f-4fb4-9456-2034dedf33ab, 5fdfa597-1c0a-4ea9-b8f6-bbad038b28e3, 66115908-c031-4ec8-8af5-bebe076b2db6, 7dcd7c97-e061-4019-ae96-3dbdca5c4980, 7ef0812d-e7ca-4cd8-b9dc-dc229125643e, 8832dc87-0b0c-4083-99d5-4925c556786c, 88fb110b-fbf4-4f84-96e4-d2fe549572b8, 8f78dffb-8e4f-4fa4-a9e6-e3f51f3d826d, 92f5a3a6-2759-4474-959b-031bcd386728, 98a08563-f048-425f-8e55-6ae80791cdfd, 9e344112-4d9f-4a2b-8501-6876810a4ebe, a0e58682-072f-4163-9850-8fba96ef8eef, ac5370db-7836-44f8-b9be-663a663f1df0, c13fbb27-9621-476b-8a60-53e727dc31d1, c758507f-5f69-458f-ae39-d35fd5d10b63, c953c508-a685-4014-a648-f961b28aa309, caedb4a4-8369-461d-882f-ad3af7ece490, cfc76ddf-83d3-4cd9-b8d1-78de46d7e1d9, d014eaa2-9148-403e-98e3-f98e3ccc0f94, d2f9ae1b-68c9-4bba-9d34-0cb42f80a2ae, e5f0fffc-dc78-43ae-81ba-27ebd4da392e, e91e51ea-7cdf-4875-8713-6cdb48add95c, eba6b7ae-cb71-41b9-96e7-855955f8bc67, f3e1c8d0-88ac-428a-a6d6-86826a8fed48, fa09130c-edba-4bc3-9153-3660819619ae, ffc16ab0-a738-4be2-955c-27e47dd98bf6]
156575 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_1_004.parquet
156585 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_1_004.parquet
156585 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit7927201792572685150/2015/03/17/d48a5590-0b74-4321-aeb3-ad81fa36efb0_1_004.parquet => [0869528b-c5be-4890-83d4-50683d38cbd0, 1a8e3c64-7da2-4b94-8ff9-a737f1337f4d, 40d375b7-4159-4720-8731-6a88491cd117, 4aebbebe-efb6-4981-8e1f-caa3937dc24f, 4e4e5aff-88c6-4c0d-b4b6-953593eae84e, 529d3ba0-9cf6-4917-8e26-c613d84c6044, 57c3ea96-7df1-468e-9650-444d5c908e74, 5e35803f-ecdc-4d88-a0a8-b1f92d955385, 6320d813-4123-40c6-b019-8ecc7f4c304b, 63df94c5-8a8f-445e-b0c0-a813d312eb7a, 6dd47051-9347-4d87-bfba-e291093395eb, 86631f82-5a61-453a-a3ca-401668cc49f9, 9b86fcf2-ff76-4b84-99a3-65d533be4d60, a2410620-bcc3-4c2a-ac96-dc6b55fad7e6, a2d1477d-552c-4a3e-88d8-c364f72f56e7, a98120ed-53ab-427a-a1e4-0776f66f81e3, aa014a33-9248-4c4d-bd86-ef7405d58886, b1901918-5e50-4396-805d-2dab6d385ef2, c04fdad3-08a2-4f42-b370-b0a0a3ecebee, c555808a-6b19-4ce4-b387-b84d8164427d, ca4969cf-ff3d-4912-8553-d51731f359f7, cfce0061-2a21-4226-b3b0-d3432e7249fc, d15b48a3-e89c-4fb8-a78d-8efe61b420c1, d9386596-72b2-4fb9-a2b5-4f385835899b, e8dfeefb-8199-45bf-858c-77f7c196fc21, ea506b09-bbc1-4402-8683-64cd63703e8b, f8349193-a4d3-4e1f-ad87-ddad85d777af, fff4986b-f00f-4d71-8fca-a1eca7b04da6]
157399 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
157917 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02, 2016/06/02], with policy KEEP_LATEST_COMMITS
157918 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
158066 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
158231 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093331
158436 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=173, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=166, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=161, numUpdates=0}}}
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 173, totalInsertBuckets => 1, recordsPerBucket => 500000
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 166, totalInsertBuckets => 1, recordsPerBucket => 500000
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 161, totalInsertBuckets => 1, recordsPerBucket => 500000
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
158444 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
158464 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093331
158464 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093331
158664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
158664 [pool-1497-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
158739 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
158740 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
158769 [pool-1497-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
158793 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
158793 [pool-1498-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
158869 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
158881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
158881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
158902 [pool-1498-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
158922 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
158922 [pool-1499-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
159005 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
159005 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
159029 [pool-1499-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
159048 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
159048 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
159151 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
159151 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
159319 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
159326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093331 as complete
159326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093331
159451 [dispatcher-event-loop-15] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 9 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
159726 [dispatcher-event-loop-10] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 13 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
159741 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
159745 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 500
159745 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
159796 [dispatcher-event-loop-18] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 14 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
159824 [dispatcher-event-loop-22] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 16 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
159891 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 173 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093331.parquet
159905 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093331.parquet
159906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 173 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093331.parquet => [00947f36-996f-4800-9e5c-4de0fff53a59, 02829aaf-e14e-4205-9d4b-a2536e0e8884, 03ac37ee-2823-43b9-9f8f-8f73b4e273d6, 067adc1c-9f1f-49ed-852a-5686a8e16f70, 0680902f-224b-40a1-99f9-e08eb8faef95, 07bf05cb-8abd-4ac3-bd8e-0f40ad4fc12f, 0805aff8-9539-4bd8-a1c4-c1d39a23da39, 0919f33f-6e58-40b1-99d6-5707d74b2009, 09e9c685-b818-4ed4-a7b7-89fff6b555bb, 0ce050b7-e135-4085-b15c-16c1d8cb0133, 0e6eec3c-a57c-475d-a3d2-f4986d3b77cb, 138d5c5a-8986-4c21-830e-72f3d51ced1f, 14396242-64d4-418e-8c5c-148364cd16d5, 17d8a377-8881-4be7-8b83-32109e493c6c, 192c7aee-d6e9-4690-82cf-43fa05b95324, 19c229a8-2d5a-40f7-bd9e-5e82ce94b31d, 1adb9483-2a41-4fd6-9a01-4a9fb126bbee, 1b14c61e-f23b-4b97-99d4-6cfd29dde6c4, 1b9eca29-dbbe-4545-8a82-87a3233587c0, 1d70b098-2ebb-4527-be26-59e170dd1d01, 1e137ba5-cefb-42ce-b986-83e0dc87b5c8, 1ee2dcd1-e1f5-4e06-b0c9-f87af05e8e69, 1f63cd5b-f7ee-412a-809a-df268286b6a1, 1f76f142-389b-49b0-81bb-96cfcdf8b04e, 1fba7035-9881-4562-ad39-fa233febde32, 21480d9d-4595-4893-a316-8e627e782186, 260f105d-847c-4159-990c-a93e144f75a0, 264c86a0-aba7-4d43-8df3-14c6dda62e0e, 271a0887-9b3a-44de-8377-bcd7f0575871, 27fd9609-72dc-4c2c-a566-f500ee2a2140, 2e45a3f0-a101-44b5-a0ad-0426305da749, 2e4fa341-827f-403b-8ede-435bf0219c8e, 2f730510-927c-4a41-9916-2e36540c0e1c, 2fac5a0f-d280-4827-8fe3-24380673d5a7, 302e49a2-8606-4ca1-b524-1587955b15fb, 33a74b5a-a549-45a7-b2d1-2ac0b81e6efe, 33ad9c06-32c8-496c-a029-354697cb5300, 359c880c-8f08-4426-8365-3fc26d083804, 364be7c8-f02f-4000-9ff5-73e1d47f9b76, 3787b123-6bb8-4f6b-9683-6e4a84537a95, 37b98416-ca90-49dd-ae1a-5c944b3566cd, 3884822c-531d-4463-87df-da7187dd723b, 38c8565e-9930-4345-9a47-e9665c4d9257, 41544492-0d1d-4b27-9991-7f885605157a, 416ed4f8-3720-44b4-8f11-e41a806c8c55, 449f8dc4-5a0f-4bc4-b20d-af5aeab1eb23, 451273b7-7398-45a0-a788-98dd313720a5, 4518e849-c1ea-42a7-952c-44c924ed129f, 46bbdb76-3416-410d-aa01-3379b25948da, 46e2976b-a563-41a2-a0f1-82c71bd5f945, 48e79645-4b35-4787-a777-87441cfb1d5e, 4a819c9f-4a47-41ea-ade8-4fc3995eadeb, 4b152c16-ecd7-4b66-a034-ddff3a2aa01e, 4d011e6a-1a3f-4b40-a66f-f837b46666e8, 4d8146b9-65e4-40cc-ba82-aa73bd27cb04, 4e98c8fc-3d45-4e4f-979f-500510c52d96, 50b334b4-65f4-4b8b-bb3e-9f1698747cdf, 513d8f98-ce13-437d-8524-19fae72945ae, 58399a92-9f75-4314-987e-01cae13cb237, 587bb0c7-74d8-469b-ad99-cfae857bb4f2, 5a67b167-5d34-4849-ab04-47ee58d375e1, 5c4c647f-12c3-48cb-addc-f749e7fef17b, 5d41f325-61c2-47be-8fbc-ece6a6b337f6, 61ae38bf-f32a-4b6e-90e7-91d477d98163, 61f02619-7ca8-436a-99d9-934720d68e4d, 626175f2-74e1-40da-ab8a-8da62d4409fe, 685d2328-ebce-4645-b730-80ee3103bffb, 6986df5e-136d-435b-8db1-9611799d002f, 6a89de45-794a-41d4-855c-f5c0d7d8d9b1, 6ae58b57-bcbe-40a3-bb5d-9b8bc3bdd93d, 6ae90a75-4918-468f-a970-cf2086a97187, 6d42376e-1715-4d7c-a411-b7d0b6420291, 6e11d424-72e6-4b22-b631-41140b006161, 6f2fe1bd-5071-4059-b992-4c3be78a28f6, 718b4d74-38ae-4dcb-a0d6-79ba1ab4ca47, 73a52b6f-cf70-44a3-abea-4488c7b76b1a, 759ab3a0-8d68-444a-a443-57c38321870a, 79f4aa30-b7ea-4848-9f40-6ed638fe900c, 7a7c5400-9402-44f5-94bb-807bd414b28a, 7b1489fc-f5ff-43d8-bce5-23aa76168fe9, 7b6f06b9-649e-4294-90e9-a2c493ee9bbb, 7bf52ffd-c049-4d6f-b99e-cc9df3d4af9e, 7c6af481-d161-4016-9992-0fdbc47fea76, 7d326b62-6677-4c7d-8d77-cda46b5f7945, 7ee83d2e-a4d1-4395-9a90-e6770829f64b, 802fee72-1c3c-4cec-b944-86b9c0923d03, 8170f283-bb7f-40fc-bae6-352d54e1670e, 825a92c0-5d7f-4991-bc5d-b1719642146d, 84f7f934-5631-4495-9435-05c13aeff6ca, 85fba49c-55a5-445b-9f6e-60d107b3b92f, 86544f55-c4c3-4583-b025-f7fa133a12b3, 89354362-487f-4f0e-8a58-55264212ba0c, 8e943bfa-110f-4593-afc8-6499f3aac480, 8f07f7a2-53fd-428a-acdc-2162d1089376, 93b99003-3fba-4ece-b82c-6e5cc0aa85e7, 9751a411-e1b9-45ba-9941-0f4dd11077d3, 97b5a949-870c-45ab-85bd-61183acecc7d, 981efbea-d130-40bf-9064-83cd9892b1eb, 983a1441-b104-42dd-ac21-2bc8b7a5bfee, 992c00bb-8945-416c-94f5-3c3bd1327bdf, 9942cfb3-bf22-4d1f-bdaa-2bf31b39cc14, 9a0a43ba-01b5-451c-b561-d664271cfcfe, 9ae1485e-58b6-47b1-9072-0d3e5774f60d, 9b2b6f2e-d152-4cd2-8a65-d9761772f00e, 9ba7c944-1f1e-404d-975a-f47e083f3003, 9c526792-c804-41f5-8613-07e5250e857f, 9c8437af-12b5-48aa-a0f3-4f85f89957b0, 9ef55d9a-f4bb-40c0-a75d-87c83507a692, a19ca5f5-cced-4a84-a8e3-cb1125c7ac28, a1b44f3e-c733-473f-80f4-a415748801af, a2cadf61-eac7-4caa-8f73-6235bed3108e, a6d352d5-c3de-44e3-9fb5-e78aafe7b564, a7b2fad3-3bb6-49fe-8bf1-9acdff032b45, a9676da9-67e7-434e-90f4-6baf43052440, ac7039d2-d6b2-46c7-9ff8-7ae6916bbff9, aee6e254-30a4-4a70-b23a-a716140dd1c4, af8ca64a-3fa8-472a-bcfd-98c8121519ef, b244fc5a-f398-4188-a74a-6694cb57ba79, b27675ad-ad0d-47e9-8106-e06658301d09, b2b8c87c-6654-49ee-b191-d5692f5157dc, b34c607f-3e1a-4d5b-8c70-403b548b345d, b4c29552-6554-49b3-95d7-714ccd66ce4f, b4e5e16f-2f7f-413b-b57b-5c961cb49498, b7897b4b-9024-4382-a5d0-fd0e3d85d2de, b8e05635-5d74-473b-b934-658202ba14a0, bc02501f-0bd2-4f3f-b69e-0ec603669caa, bc605f3a-6d9a-43dd-89aa-7aae099910af, bd80543d-262e-45da-8f7b-b6526893451d, be0947eb-7ac3-4e93-8895-828837fbc9ef, bfee649a-c28c-4255-98d6-c27cf546d755, c7bc0491-204a-4dc0-9ef5-4c641dfee689, c98f44ca-0c57-4144-8d73-a4842486ef73, cb91c470-85bf-4416-878e-8a263aefe7f0, cee877bb-2d79-4ed3-bd27-64c2c67f3caf, cef1698a-ef6e-458b-9cb2-a45b63014503, cf428a07-1cb2-495c-bed5-36a93936bf91, cf4b1a19-445d-4629-a271-d81a6138f1f2, d0a21f25-93ce-4517-a4ae-e2d37d31aa74, d1130d89-5343-4a24-b6d9-9dc38b5fc611, d14378cd-2d96-436e-af98-cb2866daa28f, d1c65ccc-8445-4252-9ca8-01535ff29589, d46edada-38f6-44fa-aef2-1be7dd3fc1e3, d4c2d966-caf7-4b85-b45a-27d6ae2bea1a, d56b63ac-c13a-4832-b903-4ea076224edf, d7c0b099-0df3-4690-a831-8434ec19e738, d83b9fa1-9fa4-4da5-9f9d-251470f3fca4, d8f3166d-f127-4e66-9858-c02bbaca966b, da1620f5-06f0-491b-9f89-5a3b0c915d91, da318180-b9a9-436c-a290-a240d6dba2d7, da334aaa-e51e-4fda-b8d2-aff6ca8a47d7, db676924-138d-4471-b3dc-c8d1e3e2fd64, dcb6d591-31d8-4ac0-ac1f-a02db07b833e, de0669fe-8296-422d-8c90-0e6b81d165aa, de7ba323-799c-4f64-8f6d-ddc698e931af, e0722894-3ad2-430a-9e84-148e141126ed, e0e5fe5c-419c-44c1-b519-71575c46c630, e2c8d1f8-76e9-49b5-be51-bf7bd56727eb, e374a13b-807c-4515-ad8f-5be515b55a8a, e522133e-faf9-487d-86ff-fd2350606b36, e69164f6-b2e0-4c54-b7bf-725b87352306, e7c2707f-6899-4f49-aa6e-cfe12e1df3cd, ec512908-eed1-453b-96ac-0fe2e2de4a43, edb29e97-12fe-48bc-82c0-f64254ef1df5, f2eed523-773f-4da4-ac02-b5d4ccada49e, f7d50688-40b8-4209-8085-48eb39281140, f96f0837-72c8-4e2a-9593-6ae081c4d498, fa2946da-9fe1-4c27-9b0b-c9c5847c36ea, fab71880-7f73-4939-a213-e454b8bf15a0, fbe69434-ba31-4957-a1cd-52699c369823, fcd63c53-f717-4fec-b9e7-a3e8f5e49aad, fdaebb1f-d5c0-48ee-b2b9-96be2de0c2be, fe1f26f3-6637-43d2-af88-eeffbca1d08e, fe451c76-02fa-4c96-9688-61b502d7c8da]
159921 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 161 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet
159933 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet
159933 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 161 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet => [024d09de-b6f2-4708-b9ce-943c6218564b, 07495283-b628-4e5a-bfbe-5c03a7e6c83c, 090fc2eb-1b33-468a-8724-3884a88db263, 0aabf14c-d755-4ffd-a0d3-23319ce0ed06, 0d02f4ee-4bef-452b-9675-5274a5661a92, 0dd76f3f-824b-4449-927c-d4b9ddf34346, 0e3108b1-6f37-4428-b0bf-4118cd869495, 11744edc-223d-4be8-9cee-2cda2c489b70, 121de7f0-ca19-49f5-ad58-63373dfcd701, 123f684a-6de3-4e13-b9b4-4692fe592d00, 140ce555-7af4-4efc-9d43-f2281711a387, 16aeac83-91eb-4714-986c-0383f13eb1c6, 178903a9-b0b6-4683-a24c-854a4bdbb17a, 1a21f8bf-c534-4348-8e39-1ca7c5121f52, 1a9e8127-9a1c-465b-921b-f22f36638162, 1c3299d3-628f-497b-9063-27801528beaf, 1e9b84ac-8c84-4cfe-aa19-ae1fa2e5d2a6, 1ec74c00-d87d-45f5-bff3-2767e80a4dee, 1ee0ef65-1274-464e-8f21-5fd337f32995, 1f9098a0-502c-4b0c-bfa0-ffd32ca3e783, 20afeeba-4219-464e-b264-06c45aa7bcbd, 21a28133-495b-4796-a120-3bcaaa21d702, 22728ac1-6c89-4e6a-a5eb-f9c16fac84d1, 2316eefd-704d-41c3-b269-cbfdfa044910, 29967ba8-c4e9-4fe8-b93f-c309f86242cf, 3030830a-d3ca-4f49-8cf0-499f4f88a8ec, 31d13c9d-92b9-4a57-baa1-8942b8d9cc1c, 32797904-8add-4728-a428-14f7c1e1db70, 32f111ad-5691-4f45-b9e3-c264b111f305, 34045d54-685b-44d9-9588-cabd79757f5f, 3627898e-c8ca-491b-ba76-9c2513b77c0c, 362ec986-71d6-4e7e-aa44-9fbf9ba2e607, 365ed998-5c3c-4cd9-9eef-56c31b1089f9, 36ac1717-a52b-41e0-9385-5707a678540d, 372711be-5c7f-470f-9a32-11e0d89a6f2f, 37d012c2-3f5d-4330-adc2-20ee5ec5f172, 38ec2fd5-d35b-4330-b387-f53fca4ee655, 39b08c5c-c27a-4d9d-8db7-12ace8c6553e, 3b02ec09-1fc8-42bf-97eb-c75790abb8a2, 3b9879dd-a6a5-4d30-a381-6903fc1f1f69, 3d25f25a-4dd0-41b9-89ba-c2f606d8c056, 3d372cb6-bae3-4936-8f99-9713cf5e78ac, 3fd2935e-c7ae-4ad1-8333-3783ea8bebb5, 40e8c122-0fef-450c-aa59-40562f2b1812, 445c04b1-e33e-4f92-9572-1c7b6df66c7c, 453c7a0a-7ee0-40e7-b461-9a2ab624b5d1, 47e93875-b60b-44c9-98d4-c2b64de6c279, 485dcc79-3df2-4930-bec4-a18ac07b1e07, 4a079e60-2009-4120-95fb-e3d25ab4b5a7, 4b152ee0-f7d4-4887-a04f-5e88896a4539, 4b5ea1b5-4720-40b2-9ce7-6225c49ae5d2, 4b91f1c0-8f7a-4f03-9a47-7d922ab8d1dd, 4caf3ef7-f497-4c5e-a567-f5df9e0455e6, 4cd1ce43-e857-4eb2-8ab4-af5134eb91c7, 4e89208a-e682-4219-8c90-284d39b10879, 4ee4f4d6-7850-452f-b75a-5c49422a0390, 4f213b3f-de37-4619-91e4-a95198939124, 530ff09c-ddf7-4012-b31c-c3b72ccc9fc0, 54ae818f-d567-42f2-b0cc-b19632c648d8, 56becf07-c8c3-4f5a-9a9c-c3b9ae1165fa, 57a08795-1fd2-4742-bc78-a3099aff4a47, 582df0ff-a523-4b9d-9049-9bef07be9666, 58cf8183-c8e0-4675-824b-872f494758ec, 5aff5d68-8a07-4859-80e3-1c0af21d4ea9, 5b2c3f08-96ff-4f08-8572-55257dee75a0, 5c0d3f04-fc39-4774-ab6d-4a5484584053, 5d5ba628-5d0d-4570-85fb-046817717c7f, 5ddd61a4-44d8-4327-89a5-a2a2682163a4, 5f5c6fd7-e8e4-4fbc-b660-0708328213a0, 608f809b-2528-4c0b-9ea8-83a4a077115e, 665252e7-131b-4aee-8f69-01f45f8bf236, 6754f2b5-f2d3-4680-96df-db5714c55c9b, 69983a9f-f68f-45f5-ac2b-05791f7d0f7b, 6a07d179-190a-4e9f-ad01-224da7ff649a, 6a537695-6fa6-4fd9-8136-d2b3bb2c7a3d, 6a76e931-ebd4-4709-8f02-dda88a684f9c, 739f0dab-1ba2-4bf1-9eed-26a17b3e49aa, 74abb8e4-544a-420c-9aab-73ed572f2486, 74beac4c-034c-418e-9d39-b656d05d7e92, 75047d3a-6dcf-4729-b02e-d9e4123d9aac, 77995c2e-cf2f-43e7-be48-5926afc1b0f7, 7833ad21-f4de-430e-b29e-73039bef20c1, 7850eca5-7192-4101-9a1b-2f416b1d644e, 79647ae0-a24e-437e-baea-4aef2ed415f1, 7af7fe8e-53b9-451f-9065-4f2527f673c2, 7c4b1099-a9ba-4503-8845-8fa5d3d70ee5, 7cf19c5a-da6b-4303-b06d-a4afa6d3ce75, 7d777c37-a6c1-45d7-a2e6-f9c03195a6b8, 7f0e853a-7336-44b9-bb41-b6e10634b3bd, 7f33227c-b6f7-4570-a939-cfe0d18fe54b, 7f512ff7-61c7-4cdc-8e0b-2491b1a34514, 80dc4aab-d322-4492-aaa8-a9fe7b063140, 851d950f-df08-45b1-9ae1-ffc4fbd718bf, 88740679-bbe4-4af6-91c0-5be02099fa77, 8947ba3d-1a35-4d27-a324-562dcb05d57a, 8c53900e-e992-4942-810c-4624d8c8582b, 8cda699e-63de-4bfe-9795-c0412f5a4f86, 8e63d271-6d8b-4dec-9a48-a2f87ff77ab5, 920e55f9-fe40-459f-bac3-6e912aeec201, 92829cc2-29b4-422f-9685-42bb1cbcd39d, 9374b3a6-c1c9-432e-8d80-677b1e8358ec, 9529cc94-01ac-4cab-b0f0-16eac00f0150, 981ad32f-78a3-4ddd-bc66-cc8a450fcfab, 983913f8-05a0-4cea-b8c1-16a6d6034193, 98cac094-04cb-4571-9cb9-7cff74feeaf4, 9aa72902-c260-4abb-96f6-8c365126df16, 9b163649-298e-4369-ab69-ce69fdaaedd1, 9cbf5627-00f1-40b1-a0aa-9229c82472cd, 9d1d00cc-fb15-45ee-a6bf-0ad51ae0e3d7, 9f518c03-e8e6-4aca-8972-b64dc33c080b, a1c46bd8-e774-4813-9af9-c4325b7988ab, a50e4b57-69ad-49ed-8bcf-2c84f9e7e848, a678460d-5a41-4367-92db-90f097c1c003, a8e81bb2-cd3b-4050-8da5-b2e0242f930f, a933b402-56ae-4340-bc01-6b575834e39c, acb3bdcd-9817-40cc-be72-491ea32c3cc9, ae7f3f73-c412-4f13-94ed-5d2668c7ffb0, af144552-1c63-4801-b867-db2add520014, b07eb804-81be-4a58-95dc-e79fabbe3777, b11aff12-333a-4e07-976a-4f940ce9fbf3, b14fa368-5fee-421d-bbf9-839a7059b49f, b1ece490-1b9d-4c93-8802-563debc8ef78, b25e9bf8-3036-4268-a751-13d185926873, b3812142-f9f5-4da1-8a00-a4fdf05221fb, b80a914d-dc6a-4091-9559-9e1b533674da, b832dcc0-5f2b-4d41-98a0-0e42df684a4a, b864f7f5-2046-4db5-b463-48b32d9d719b, b8d5cacf-b9f9-4622-9cc2-69f913cd4cde, b8e83928-151e-4163-8ea9-ec7dce2de51a, bac32b51-be45-438b-af84-d43013bb50be, bb63629a-56fb-4487-a84c-e44b5dbd2b66, c768bd2e-9d49-4222-8a73-aad29b57b39c, c8c709f6-bf9a-401f-87d9-a574625bdd49, c9feba8f-72ae-4337-a851-134062d09a33, cecea963-4602-4dc8-81fe-b04919a83fda, ceef1098-f30f-4d4f-84aa-e3a04f737b36, cf9216a3-2bc1-41ed-8238-c3bba653eddb, d21a912c-7d5a-4c18-9d05-2db5358508fa, d22c3a08-9b6e-46e5-b661-f12c668a1edc, d56832e5-17ab-4565-8c49-8a21af83fbc8, d750f530-8f68-4be0-9f85-ccc62357812a, da0cb566-b69f-44ce-854e-eff0c0979f59, da12949a-7927-45d4-8b1f-c46137511da7, dc9faee5-02eb-4ccf-86e9-a15036b437f9, dd225c40-4f04-413f-be39-7ef4308bbf26, e06778d6-a4ad-400b-aee4-0c56cfcc9444, e0812ccd-3df1-4e45-9db9-6b8a0b405423, e7f30649-ab2f-459c-96f1-c3a8ad7bff90, eab55e44-8177-4007-9c0a-f711fc68767c, eafd206a-d52f-43de-baa5-16347371d348, eb479f21-982b-4bc3-bf89-1a789863c87e, ec0516ba-bd1d-4442-a66e-597432c4c9de, edd80db7-bc32-45da-8975-3802e8ce8319, f0bce441-bf48-4b59-97af-04c2a1a6ec37, f11af746-2266-4040-958f-9d799e6c405b, f230d16d-6436-4a91-9f1d-b5853f28551c, f3f538a9-2854-4abe-b9d7-a487ad0a49d6, f6d79100-6a55-440e-8200-7c3957e80b90, f791aaa8-36a0-4ce0-8a75-83f2886345d1, fc8bc0a3-feed-4a29-b162-45eb3aa3be1e, fee59702-1564-4173-a733-dfb96cce335c]
159947 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 166 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093331.parquet
159959 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093331.parquet
159959 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 166 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093331.parquet => [01f383f1-71b0-4bd0-938b-d7e461489314, 034b2558-edd2-4537-949b-0e3c2147350f, 04ae0e5d-f42b-4f20-a17c-41558c3dbd4c, 07258b45-79c0-4c3a-9a43-0563c2dd39a8, 078dad8a-fcdb-4e04-97cf-f60d1dded828, 08b40233-c5b5-4354-92fe-7886bc5844bd, 096a194c-5d7b-494f-9944-85a1c73ab00b, 096fe4f4-8fd3-4196-bd86-fd278b39e76f, 09e10984-c042-4cd2-b1d8-2e5ccd28a59d, 0a6243f5-669e-4fb5-95cb-110f7b36d7c8, 0b04fc50-dbe0-4a8b-ad6e-315dcc853c16, 0b98ad02-5ebc-489e-9626-f0e9d70b77fb, 0c01d759-0be0-4b05-8683-19859e972471, 0fdf536f-3615-47a6-92bd-d67842b640fe, 11c18f31-927e-451e-a219-e64c04e05523, 11ef8106-129f-48f5-bdeb-ead2a25801f6, 122bf41e-41c6-447c-ad7a-abca7d8fc95f, 130a21d2-a010-4e71-a98b-ec030174f9e5, 13d6786b-9280-4b30-855d-4c3018500c12, 171dc029-6c43-4619-8d9b-b8117b383ba4, 1aab0b51-4681-4aa5-9433-eac048293ab2, 1d3a1e3e-7db8-4178-ab90-6cc08bd707d0, 1df0955e-7649-41ac-bf90-69fc893cd8d2, 1e7aa634-52e7-491c-a81e-e00c8a46307a, 1eff65ab-7758-4e6c-8f37-f20c56c7f255, 1f3a5089-6aaf-4576-a77f-8862e40b40e1, 20172980-2863-4d0a-a381-98e2d6ca9a1d, 217a6a02-980a-4f62-b673-cee4c3fce248, 22a9fca4-7921-4a98-87c7-daa0fead5136, 25268b8f-93a2-4f11-8a7e-bd9527a08211, 2672c6e3-310a-40f4-9c43-8b69dd9ee9e8, 26745f76-b88b-42af-9df5-a742bcad5f87, 2681f190-b5ec-4ace-ac66-d9a2c44d2c79, 29415dca-5622-4f42-a0c0-43468e8ed8ea, 2994d8d1-51e0-4e8f-87df-ef32d9a9b2c7, 299c8fe9-279f-42ed-9aa6-ff48edbe16ef, 2acb7159-4484-4823-ad42-76bd0514b13b, 2ae3cc9e-8d7e-4ef2-9399-c6e7f4c27fee, 32263ae6-848c-4573-9005-481d4fd63e56, 32b8f7a4-3535-4d25-870e-aae9004de40a, 37f015ef-ac9e-41dd-87b3-3967ca5c25ba, 388cf170-9efc-42e9-88c6-ea255b31472f, 392bf4f2-a94e-4698-b818-48b4f558f4bc, 39e567e6-d894-40c0-ab5d-b42391eb141e, 3bccf9d8-a6c8-4c8f-9cc8-a023718b7b38, 3d49de59-cf72-4455-a16e-70d7c08e146f, 3e7ef3ea-962d-4626-bb36-b419774c9274, 3fa2352b-db70-43a9-a6c6-91aafe5ab233, 4662816c-d7a5-4d0b-a5e9-ac4d48a4d07e, 470be392-23a6-4727-a79c-223b5eed90f3, 4b886131-70d7-45ca-ad42-625fcfca71fd, 4de92e51-5858-4b27-a509-9aeb69a5cbc9, 4f993390-875b-4bb8-8634-b644e640759b, 4fa41102-6eaf-41bd-9952-b73a35b572c2, 4fe6bbb2-e60e-4e91-bdc1-fe6975f4683e, 51343225-1865-4d3e-b5ed-1a018d0893af, 526f8b84-2b79-4b96-a1b2-4e375785ed20, 53d49e83-0c4b-490a-b855-01d7e333aff6, 545ac36f-b12e-4b76-bd9f-3f61e142f740, 5463f98c-b959-40e2-90ae-d8caaa16fcea, 589ae407-0616-4da9-9ef2-fb76df8e2d7d, 589fd441-276e-4851-8ed4-6c9c2ada8376, 5a123ae8-fd5b-4f35-9ac3-183d5d690a64, 5c4775d2-d6b7-4133-9263-4bc19636bf36, 60576512-3ba3-4887-8b6f-8cbd75620440, 622144ba-2f33-4930-b9ab-0d05c8c99c4b, 62a87ff3-30df-4892-bd85-a46875546a18, 6300762e-2461-4f87-8954-d5a795b9a04b, 663af841-1a9a-47d8-aba4-a17b444fabdb, 6713078a-0773-40d8-b354-7551f515b7fd, 6b51afcd-194a-4bde-8be1-c47c686936ab, 6ca60082-d6ca-43d3-bf50-55eec762d8ba, 6d02257d-2a68-42fb-8c0b-46e7e84ecbbb, 6ed76575-a7df-4cec-b987-fab5896d46b6, 6f2de487-0120-4219-a72c-3a6fba0bba2a, 755ccc4d-f9d7-4719-813a-b96ef28222f0, 77dda440-45c5-4bb7-86d2-e9ff7de37f71, 79325980-ae38-4d98-afb7-14afa5c15780, 79b9b28c-125f-4630-9429-b5b299703795, 7a385e05-4e59-453e-b1eb-2b62cde47551, 7afe3079-b0ea-45af-9f4f-ba8010cfb838, 7b4f368e-2456-4178-9d12-c363960a37c3, 7bd36650-e301-4cdd-9881-e82d7eb9cd89, 80719764-31f2-4418-811f-64a5c10550f7, 84467a4c-def7-4fa1-b29d-6d91e03bf774, 8514bfc7-9aa9-4cfc-9579-20a216cec883, 897e9e95-3436-46b5-8065-7f96c6fc624a, 8e230d46-a460-49b1-87df-55f831004f93, 901e3980-bf4e-41bc-a734-74349517bfbf, 915d6ef0-17e0-4409-b1ae-ac5287d35d5d, 94de2bfc-1476-4b1c-aae1-5ed0f9219f41, 95531bc8-e5e4-4379-9679-4e52f8e5106e, 95c49ec8-b3aa-46ad-9039-061cc65931b9, 97a4539a-e874-472b-a8a5-86d67effc922, 97aad77a-b918-4e64-a5cd-f612b329668b, 994f26d0-8162-43e9-b0a1-763973ed6508, 9a7cbc7a-77f8-4a78-9224-6b133116d15b, 9c1530bc-29ad-4f7f-a14d-1864824211b3, a19c6a3f-f44c-49e7-b645-3d6a4c8fb616, a1e7b2ad-3d7f-4504-a6d0-c425b0dbdac6, a2412de9-c6d9-4c7d-b1c9-c967389b303e, a2aa6f48-2a19-46d9-b972-e0bc6ffcfd9b, a6bf3cd4-d702-4da3-a06d-f868efb7510c, a9988efa-a7b0-4338-b75c-4bd086fb705d, a9d4d152-8b17-4ff6-b07a-9812ebf7909f, ac892707-21b2-46d4-bdac-9cb6787c481f, add428cd-8017-4a66-af91-5a91324783ea, adf56004-d961-4e73-b2f6-ada615de48c0, ae0893ef-bf11-4ba6-bb7f-e0788d88e776, ae41e0cd-e875-4d44-8e22-a7445c956e8c, b0d75e47-1c43-49ed-a785-2d6c407ca8da, b5bc28d9-d3d4-4b7f-814c-10cd9214ee21, b67ec158-b447-49e6-924b-2b696bb4e855, b745c012-7100-4b46-9418-109e3cd735a6, b8537577-c5f3-461d-b6de-9368bb06503a, b9c0ee45-fc08-49ee-82cb-6ffbdd8d1b88, ba97cad3-7769-46cc-8979-7e96e5de97bf, bb725a68-fdf6-4fe5-8ff7-e27a02508621, bf7e6a2a-6bb4-4de0-9ed2-8c6a42c9d53c, bf80945e-bb8d-4555-a764-c99535760e7e, bfee4a68-713d-452e-88ea-d3f6302cb9de, c165d994-19ef-4715-90ed-e47b05ea2e6d, c1e2e392-52aa-4a91-af36-51217b4d66fc, c24a2ac5-eb6a-4cf1-b426-8ef43d1993eb, c44e66e9-c5f2-4de2-a2fb-39330e503093, c5c49918-8ca0-4cbd-a5c1-d6d6b88a48c1, c73246a0-195e-4ef9-8945-c0ade94969ff, c733c946-c817-4195-9a73-636941ba84ab, c79248fd-c704-40e8-9f34-e9aa5000881a, c797fc45-a21e-4c00-be1e-a03ff9b7e022, c860320b-615c-4d5a-b615-ae9897525df5, c8c08fd2-628f-4651-8e59-6c57412c44be, c9732d9c-765c-41bd-a77d-9fc573eb935c, cb778594-4fbc-4bac-ba3d-e4af5bd49909, ce559e8c-16e0-41f3-a716-e8892e691000, cf6f7f83-20c9-4cdf-9336-6da6b53b27ac, d04c97ca-fc2e-4732-9972-62fad97534da, d0b5ac05-d67f-4cfb-a7bb-556213f905e6, d18816b2-e073-41ca-a275-a44b590170fb, d1bf195f-ff15-49c1-9b37-017000572fbc, d1d43428-3470-4c37-af55-8397dab5b73d, d2d821f7-c970-4b59-ba00-18861dba92d5, d2f45337-69a9-4718-8c06-01bfba9a4ce5, d4e60c92-3c3a-40fd-a4c0-f2b7f1bbc919, d53d8820-6ec1-4bef-a44f-b2162a4da8e3, d557db10-a81a-4aea-87cd-79299cd3797e, d5f8aa9c-728e-45ac-b906-3c1e3f258f7c, d6f60643-1a23-4eff-9586-71894c822038, d7fa0947-0d45-4f9c-8b29-3961dfe2fc53, d808a19a-1b52-407b-889c-e50cf64d3309, dab2f10a-e531-40e3-a63e-d695afc7a909, ddbd77c2-9e1a-424c-8975-7936930c5f0e, ded7f615-67b8-4171-9c6d-37daa87ef4be, dff8587c-39dc-44b7-a3d8-39bc3677db92, e117ae59-0034-42a5-ad87-19df8ded6656, e563b812-6e6f-41b7-ad5e-afbedaef0aff, e9ba917f-491e-4e59-bd64-fa56fe45f5c9, f1fac871-d06d-44bd-8140-ff9fe0fcc05f, f322ac76-d2f5-42d7-a070-3e9ddbaa3f39, f47c7025-8638-473e-9389-e84336bce0d3, f563554a-6940-4c39-aaa4-826609db5c7e, f7baf6bc-5229-42c4-a9c4-4d9d72560d55, f8a4e28e-776d-4ea0-a910-8dd64bfd4e22, f8af1895-fd8c-4b68-b7c0-1b0e65b0e01b, fa49493a-d356-4702-a827-69a2ffcd2c99, fdbd615d-6a48-4378-a66a-16a59a088735]
160261 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161158 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093334
161544 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 92
161544 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
161661 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161746 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093331.parquet
161758 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093331.parquet
161758 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093331.parquet => [03ac37ee-2823-43b9-9f8f-8f73b4e273d6, 14396242-64d4-418e-8c5c-148364cd16d5, 192c7aee-d6e9-4690-82cf-43fa05b95324, 1adb9483-2a41-4fd6-9a01-4a9fb126bbee, 1b14c61e-f23b-4b97-99d4-6cfd29dde6c4, 1f63cd5b-f7ee-412a-809a-df268286b6a1, 1f76f142-389b-49b0-81bb-96cfcdf8b04e, 264c86a0-aba7-4d43-8df3-14c6dda62e0e, 27fd9609-72dc-4c2c-a566-f500ee2a2140, 359c880c-8f08-4426-8365-3fc26d083804, 37b98416-ca90-49dd-ae1a-5c944b3566cd, 50b334b4-65f4-4b8b-bb3e-9f1698747cdf, 5c4c647f-12c3-48cb-addc-f749e7fef17b, 61f02619-7ca8-436a-99d9-934720d68e4d, 8170f283-bb7f-40fc-bae6-352d54e1670e, 981efbea-d130-40bf-9064-83cd9892b1eb, 9942cfb3-bf22-4d1f-bdaa-2bf31b39cc14, 9a0a43ba-01b5-451c-b561-d664271cfcfe, 9ba7c944-1f1e-404d-975a-f47e083f3003, 9c526792-c804-41f5-8613-07e5250e857f, a1b44f3e-c733-473f-80f4-a415748801af, a6d352d5-c3de-44e3-9fb5-e78aafe7b564, b4c29552-6554-49b3-95d7-714ccd66ce4f, c7bc0491-204a-4dc0-9ef5-4c641dfee689, cb91c470-85bf-4416-878e-8a263aefe7f0, cef1698a-ef6e-458b-9cb2-a45b63014503, d8f3166d-f127-4e66-9858-c02bbaca966b, da1620f5-06f0-491b-9f89-5a3b0c915d91, e522133e-faf9-487d-86ff-fd2350606b36, e69164f6-b2e0-4c54-b7bf-725b87352306, e7c2707f-6899-4f49-aa6e-cfe12e1df3cd, f2eed523-773f-4da4-ac02-b5d4ccada49e, fab71880-7f73-4939-a213-e454b8bf15a0, fcd63c53-f717-4fec-b9e7-a3e8f5e49aad, fe1f26f3-6637-43d2-af88-eeffbca1d08e]
161778 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet
161788 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet
161788 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet => [16aeac83-91eb-4714-986c-0383f13eb1c6, 1ee0ef65-1274-464e-8f21-5fd337f32995, 1f9098a0-502c-4b0c-bfa0-ffd32ca3e783, 20afeeba-4219-464e-b264-06c45aa7bcbd, 3030830a-d3ca-4f49-8cf0-499f4f88a8ec, 38ec2fd5-d35b-4330-b387-f53fca4ee655, 39b08c5c-c27a-4d9d-8db7-12ace8c6553e, 3b9879dd-a6a5-4d30-a381-6903fc1f1f69, 3d372cb6-bae3-4936-8f99-9713cf5e78ac, 40e8c122-0fef-450c-aa59-40562f2b1812, 54ae818f-d567-42f2-b0cc-b19632c648d8]
161813 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 15 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet
161823 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet
161823 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 15 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093331.parquet => [582df0ff-a523-4b9d-9049-9bef07be9666, 6754f2b5-f2d3-4680-96df-db5714c55c9b, 74beac4c-034c-418e-9d39-b656d05d7e92, 79647ae0-a24e-437e-baea-4aef2ed415f1, 7d777c37-a6c1-45d7-a2e6-f9c03195a6b8, 8947ba3d-1a35-4d27-a324-562dcb05d57a, 920e55f9-fe40-459f-bac3-6e912aeec201, a1c46bd8-e774-4813-9af9-c4325b7988ab, b11aff12-333a-4e07-976a-4f940ce9fbf3, b3812142-f9f5-4da1-8a00-a4fdf05221fb, c8c709f6-bf9a-401f-87d9-a574625bdd49, d56832e5-17ab-4565-8c49-8a21af83fbc8, d750f530-8f68-4be0-9f85-ccc62357812a, e0812ccd-3df1-4e45-9db9-6b8a0b405423, edd80db7-bc32-45da-8975-3802e8ce8319]
161838 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093331.parquet
161850 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093331.parquet
161850 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093331.parquet => [096a194c-5d7b-494f-9944-85a1c73ab00b, 0c01d759-0be0-4b05-8683-19859e972471, 1aab0b51-4681-4aa5-9433-eac048293ab2, 1e7aa634-52e7-491c-a81e-e00c8a46307a, 299c8fe9-279f-42ed-9aa6-ff48edbe16ef, 2acb7159-4484-4823-ad42-76bd0514b13b, 3d49de59-cf72-4455-a16e-70d7c08e146f, 4662816c-d7a5-4d0b-a5e9-ac4d48a4d07e, 470be392-23a6-4727-a79c-223b5eed90f3, 5463f98c-b959-40e2-90ae-d8caaa16fcea, 589ae407-0616-4da9-9ef2-fb76df8e2d7d, 589fd441-276e-4851-8ed4-6c9c2ada8376, 5a123ae8-fd5b-4f35-9ac3-183d5d690a64, 62a87ff3-30df-4892-bd85-a46875546a18, 6b51afcd-194a-4bde-8be1-c47c686936ab, 79325980-ae38-4d98-afb7-14afa5c15780, 95c49ec8-b3aa-46ad-9039-061cc65931b9, 97aad77a-b918-4e64-a5cd-f612b329668b, 994f26d0-8162-43e9-b0a1-763973ed6508, 9a7cbc7a-77f8-4a78-9224-6b133116d15b, a1e7b2ad-3d7f-4504-a6d0-c425b0dbdac6, a2412de9-c6d9-4c7d-b1c9-c967389b303e, a9988efa-a7b0-4338-b75c-4bd086fb705d, a9d4d152-8b17-4ff6-b07a-9812ebf7909f, b67ec158-b447-49e6-924b-2b696bb4e855, b745c012-7100-4b46-9418-109e3cd735a6, bfee4a68-713d-452e-88ea-d3f6302cb9de, c5c49918-8ca0-4cbd-a5c1-d6d6b88a48c1, ce559e8c-16e0-41f3-a716-e8892e691000, d53d8820-6ec1-4bef-a44f-b2162a4da8e3, ded7f615-67b8-4171-9c6d-37daa87ef4be]
161918 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=92}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=26}}}
161928 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161931 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2699
161931 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
161950 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093334
161950 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093334
162313 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
162313 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
162434 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
162434 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
162592 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
162598 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093334 as complete
162598 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093334
163720 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
163817 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093337
163822 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
164191 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 96
164191 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
164339 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 41 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093334.parquet
164351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093334.parquet
164351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 41 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093334.parquet => [07bf05cb-8abd-4ac3-bd8e-0f40ad4fc12f, 0919f33f-6e58-40b1-99d6-5707d74b2009, 0ce050b7-e135-4085-b15c-16c1d8cb0133, 19c229a8-2d5a-40f7-bd9e-5e82ce94b31d, 1adb9483-2a41-4fd6-9a01-4a9fb126bbee, 1ee2dcd1-e1f5-4e06-b0c9-f87af05e8e69, 21480d9d-4595-4893-a316-8e627e782186, 260f105d-847c-4159-990c-a93e144f75a0, 3884822c-531d-4463-87df-da7187dd723b, 41544492-0d1d-4b27-9991-7f885605157a, 416ed4f8-3720-44b4-8f11-e41a806c8c55, 449f8dc4-5a0f-4bc4-b20d-af5aeab1eb23, 4518e849-c1ea-42a7-952c-44c924ed129f, 48e79645-4b35-4787-a777-87441cfb1d5e, 4a819c9f-4a47-41ea-ade8-4fc3995eadeb, 4d8146b9-65e4-40cc-ba82-aa73bd27cb04, 5c4c647f-12c3-48cb-addc-f749e7fef17b, 685d2328-ebce-4645-b730-80ee3103bffb, 6986df5e-136d-435b-8db1-9611799d002f, 6d42376e-1715-4d7c-a411-b7d0b6420291, 7a7c5400-9402-44f5-94bb-807bd414b28a, 7d326b62-6677-4c7d-8d77-cda46b5f7945, 8e943bfa-110f-4593-afc8-6499f3aac480, 93b99003-3fba-4ece-b82c-6e5cc0aa85e7, 97b5a949-870c-45ab-85bd-61183acecc7d, 981efbea-d130-40bf-9064-83cd9892b1eb, 9942cfb3-bf22-4d1f-bdaa-2bf31b39cc14, 9c526792-c804-41f5-8613-07e5250e857f, a2cadf61-eac7-4caa-8f73-6235bed3108e, b4c29552-6554-49b3-95d7-714ccd66ce4f, bc02501f-0bd2-4f3f-b69e-0ec603669caa, bd80543d-262e-45da-8f7b-b6526893451d, c7bc0491-204a-4dc0-9ef5-4c641dfee689, cef1698a-ef6e-458b-9cb2-a45b63014503, cf428a07-1cb2-495c-bed5-36a93936bf91, cf4b1a19-445d-4629-a271-d81a6138f1f2, d1130d89-5343-4a24-b6d9-9dc38b5fc611, d1c65ccc-8445-4252-9ca8-01535ff29589, d4c2d966-caf7-4b85-b45a-27d6ae2bea1a, d8f3166d-f127-4e66-9858-c02bbaca966b, ec512908-eed1-453b-96ac-0fe2e2de4a43]
164369 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093334.parquet
164382 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093334.parquet
164383 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093334.parquet => [07495283-b628-4e5a-bfbe-5c03a7e6c83c, 090fc2eb-1b33-468a-8724-3884a88db263, 121de7f0-ca19-49f5-ad58-63373dfcd701, 1a21f8bf-c534-4348-8e39-1ca7c5121f52, 1c3299d3-628f-497b-9063-27801528beaf, 1f9098a0-502c-4b0c-bfa0-ffd32ca3e783, 21a28133-495b-4796-a120-3bcaaa21d702]
164409 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093334.parquet
164421 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093334.parquet
164421 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093334.parquet => [32797904-8add-4728-a428-14f7c1e1db70, 365ed998-5c3c-4cd9-9eef-56c31b1089f9, 38ec2fd5-d35b-4330-b387-f53fca4ee655, 3b9879dd-a6a5-4d30-a381-6903fc1f1f69, 3d25f25a-4dd0-41b9-89ba-c2f606d8c056, 4b5ea1b5-4720-40b2-9ce7-6225c49ae5d2, 4f213b3f-de37-4619-91e4-a95198939124, 57a08795-1fd2-4742-bc78-a3099aff4a47, 665252e7-131b-4aee-8f69-01f45f8bf236, 6754f2b5-f2d3-4680-96df-db5714c55c9b, 69983a9f-f68f-45f5-ac2b-05791f7d0f7b, 7cf19c5a-da6b-4303-b06d-a4afa6d3ce75, 7f33227c-b6f7-4570-a939-cfe0d18fe54b, 92829cc2-29b4-422f-9685-42bb1cbcd39d, 9d1d00cc-fb15-45ee-a6bf-0ad51ae0e3d7, 9f518c03-e8e6-4aca-8972-b64dc33c080b, acb3bdcd-9817-40cc-be72-491ea32c3cc9, da0cb566-b69f-44ce-854e-eff0c0979f59, da12949a-7927-45d4-8b1f-c46137511da7, e7f30649-ab2f-459c-96f1-c3a8ad7bff90, eab55e44-8177-4007-9c0a-f711fc68767c, eafd206a-d52f-43de-baa5-16347371d348, eb479f21-982b-4bc3-bf89-1a789863c87e, ec0516ba-bd1d-4442-a66e-597432c4c9de, edd80db7-bc32-45da-8975-3802e8ce8319, f791aaa8-36a0-4ce0-8a75-83f2886345d1]
164436 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 22 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093334.parquet
164448 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093334.parquet
164448 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 22 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093334.parquet => [0a6243f5-669e-4fb5-95cb-110f7b36d7c8, 122bf41e-41c6-447c-ad7a-abca7d8fc95f, 1df0955e-7649-41ac-bf90-69fc893cd8d2, 20172980-2863-4d0a-a381-98e2d6ca9a1d, 388cf170-9efc-42e9-88c6-ea255b31472f, 39e567e6-d894-40c0-ab5d-b42391eb141e, 545ac36f-b12e-4b76-bd9f-3f61e142f740, 6d02257d-2a68-42fb-8c0b-46e7e84ecbbb, 6f2de487-0120-4219-a72c-3a6fba0bba2a, 79b9b28c-125f-4630-9429-b5b299703795, 8514bfc7-9aa9-4cfc-9579-20a216cec883, 95531bc8-e5e4-4379-9679-4e52f8e5106e, 97a4539a-e874-472b-a8a5-86d67effc922, 994f26d0-8162-43e9-b0a1-763973ed6508, ac892707-21b2-46d4-bdac-9cb6787c481f, c24a2ac5-eb6a-4cf1-b426-8ef43d1993eb, cf6f7f83-20c9-4cdf-9336-6da6b53b27ac, d04c97ca-fc2e-4732-9972-62fad97534da, d18816b2-e073-41ca-a275-a44b590170fb, d1bf195f-ff15-49c1-9b37-017000572fbc, d2d821f7-c970-4b59-ba00-18861dba92d5, e9ba917f-491e-4e59-bd64-fa56fe45f5c9]
164513 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=96}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=41}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=22}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=33}}}
164526 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2700
164526 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
164544 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093337
164544 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093337
164959 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
164970 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
164970 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
165137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
165137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
165322 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
165328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093337 as complete
165328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093337
165833 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
166262 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
166527 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093340
166875 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 88
166875 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
167032 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093337.parquet
167044 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093337.parquet
167044 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093337.parquet => [02829aaf-e14e-4205-9d4b-a2536e0e8884, 07bf05cb-8abd-4ac3-bd8e-0f40ad4fc12f, 1e137ba5-cefb-42ce-b986-83e0dc87b5c8, 1f63cd5b-f7ee-412a-809a-df268286b6a1, 37b98416-ca90-49dd-ae1a-5c944b3566cd, 416ed4f8-3720-44b4-8f11-e41a806c8c55, 449f8dc4-5a0f-4bc4-b20d-af5aeab1eb23, 46bbdb76-3416-410d-aa01-3379b25948da, 58399a92-9f75-4314-987e-01cae13cb237, 6a89de45-794a-41d4-855c-f5c0d7d8d9b1, 73a52b6f-cf70-44a3-abea-4488c7b76b1a, 759ab3a0-8d68-444a-a443-57c38321870a, 7ee83d2e-a4d1-4395-9a90-e6770829f64b, 85fba49c-55a5-445b-9f6e-60d107b3b92f, 89354362-487f-4f0e-8a58-55264212ba0c, 9751a411-e1b9-45ba-9941-0f4dd11077d3, 981efbea-d130-40bf-9064-83cd9892b1eb, 9ae1485e-58b6-47b1-9072-0d3e5774f60d, 9ba7c944-1f1e-404d-975a-f47e083f3003, 9c8437af-12b5-48aa-a0f3-4f85f89957b0, b34c607f-3e1a-4d5b-8c70-403b548b345d, bd80543d-262e-45da-8f7b-b6526893451d, c7bc0491-204a-4dc0-9ef5-4c641dfee689, c98f44ca-0c57-4144-8d73-a4842486ef73, cf428a07-1cb2-495c-bed5-36a93936bf91, d0a21f25-93ce-4517-a4ae-e2d37d31aa74, d14378cd-2d96-436e-af98-cb2866daa28f, da318180-b9a9-436c-a290-a240d6dba2d7, e0e5fe5c-419c-44c1-b519-71575c46c630, e2c8d1f8-76e9-49b5-be51-bf7bd56727eb, e69164f6-b2e0-4c54-b7bf-725b87352306, f96f0837-72c8-4e2a-9593-6ae081c4d498, fdaebb1f-d5c0-48ee-b2b9-96be2de0c2be]
167060 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093337.parquet
167072 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093337.parquet
167072 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093337.parquet => [090fc2eb-1b33-468a-8724-3884a88db263, 1f9098a0-502c-4b0c-bfa0-ffd32ca3e783, 4a079e60-2009-4120-95fb-e3d25ab4b5a7, 4caf3ef7-f497-4c5e-a567-f5df9e0455e6, 530ff09c-ddf7-4012-b31c-c3b72ccc9fc0, 608f809b-2528-4c0b-9ea8-83a4a077115e, 665252e7-131b-4aee-8f69-01f45f8bf236, 6a76e931-ebd4-4709-8f02-dda88a684f9c, 739f0dab-1ba2-4bf1-9eed-26a17b3e49aa, 77995c2e-cf2f-43e7-be48-5926afc1b0f7, 79647ae0-a24e-437e-baea-4aef2ed415f1]
167101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 13 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093337.parquet
167114 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093337.parquet
167114 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 13 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093337.parquet => [7cf19c5a-da6b-4303-b06d-a4afa6d3ce75, 7f33227c-b6f7-4570-a939-cfe0d18fe54b, 88740679-bbe4-4af6-91c0-5be02099fa77, 9374b3a6-c1c9-432e-8d80-677b1e8358ec, 9529cc94-01ac-4cab-b0f0-16eac00f0150, 98cac094-04cb-4571-9cb9-7cff74feeaf4, 9b163649-298e-4369-ab69-ce69fdaaedd1, a1c46bd8-e774-4813-9af9-c4325b7988ab, a8e81bb2-cd3b-4050-8da5-b2e0242f930f, b80a914d-dc6a-4091-9559-9e1b533674da, ceef1098-f30f-4d4f-84aa-e3a04f737b36, da12949a-7927-45d4-8b1f-c46137511da7, ec0516ba-bd1d-4442-a66e-597432c4c9de]
167130 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093337.parquet
167144 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093337.parquet
167144 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093337.parquet => [07258b45-79c0-4c3a-9a43-0563c2dd39a8, 078dad8a-fcdb-4e04-97cf-f60d1dded828, 08b40233-c5b5-4354-92fe-7886bc5844bd, 11ef8106-129f-48f5-bdeb-ead2a25801f6, 13d6786b-9280-4b30-855d-4c3018500c12, 20172980-2863-4d0a-a381-98e2d6ca9a1d, 29415dca-5622-4f42-a0c0-43468e8ed8ea, 32263ae6-848c-4573-9005-481d4fd63e56, 37f015ef-ac9e-41dd-87b3-3967ca5c25ba, 388cf170-9efc-42e9-88c6-ea255b31472f, 3e7ef3ea-962d-4626-bb36-b419774c9274, 3fa2352b-db70-43a9-a6c6-91aafe5ab233, 4b886131-70d7-45ca-ad42-625fcfca71fd, 51343225-1865-4d3e-b5ed-1a018d0893af, 545ac36f-b12e-4b76-bd9f-3f61e142f740, 589ae407-0616-4da9-9ef2-fb76df8e2d7d, 6b51afcd-194a-4bde-8be1-c47c686936ab, 6d02257d-2a68-42fb-8c0b-46e7e84ecbbb, 77dda440-45c5-4bb7-86d2-e9ff7de37f71, 7a385e05-4e59-453e-b1eb-2b62cde47551, 7b4f368e-2456-4178-9d12-c363960a37c3, 84467a4c-def7-4fa1-b29d-6d91e03bf774, 95531bc8-e5e4-4379-9679-4e52f8e5106e, ae41e0cd-e875-4d44-8e22-a7445c956e8c, b745c012-7100-4b46-9418-109e3cd735a6, c73246a0-195e-4ef9-8945-c0ade94969ff, d1d43428-3470-4c37-af55-8397dab5b73d, d2d821f7-c970-4b59-ba00-18861dba92d5, d5f8aa9c-728e-45ac-b906-3c1e3f258f7c, ddbd77c2-9e1a-424c-8975-7936930c5f0e, e563b812-6e6f-41b7-ad5e-afbedaef0aff]
167207 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=88}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=33}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=24}}}
167220 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
167220 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
167239 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093340
167239 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093340
167283 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
167627 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
167627 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
167795 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
167795 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
167971 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
167977 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093340 as complete
167977 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093340
168323 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
168784 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
169185 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093342
169783 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 88
169783 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
169841 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
170019 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093340.parquet
170034 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093340.parquet
170034 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093340.parquet => [0680902f-224b-40a1-99f9-e08eb8faef95, 07bf05cb-8abd-4ac3-bd8e-0f40ad4fc12f, 0e6eec3c-a57c-475d-a3d2-f4986d3b77cb, 19c229a8-2d5a-40f7-bd9e-5e82ce94b31d, 1adb9483-2a41-4fd6-9a01-4a9fb126bbee, 264c86a0-aba7-4d43-8df3-14c6dda62e0e, 302e49a2-8606-4ca1-b524-1587955b15fb, 3787b123-6bb8-4f6b-9683-6e4a84537a95, 3884822c-531d-4463-87df-da7187dd723b, 4518e849-c1ea-42a7-952c-44c924ed129f, 4b152c16-ecd7-4b66-a034-ddff3a2aa01e, 50b334b4-65f4-4b8b-bb3e-9f1698747cdf, 6a89de45-794a-41d4-855c-f5c0d7d8d9b1, 6ae58b57-bcbe-40a3-bb5d-9b8bc3bdd93d, 6d42376e-1715-4d7c-a411-b7d0b6420291, 79f4aa30-b7ea-4848-9f40-6ed638fe900c, 84f7f934-5631-4495-9435-05c13aeff6ca, 86544f55-c4c3-4583-b025-f7fa133a12b3, 93b99003-3fba-4ece-b82c-6e5cc0aa85e7, 9b2b6f2e-d152-4cd2-8a65-d9761772f00e, 9ef55d9a-f4bb-40c0-a75d-87c83507a692, a2cadf61-eac7-4caa-8f73-6235bed3108e, a7b2fad3-3bb6-49fe-8bf1-9acdff032b45, a9676da9-67e7-434e-90f4-6baf43052440, af8ca64a-3fa8-472a-bcfd-98c8121519ef, b244fc5a-f398-4188-a74a-6694cb57ba79, b4e5e16f-2f7f-413b-b57b-5c961cb49498, cee877bb-2d79-4ed3-bd27-64c2c67f3caf, d46edada-38f6-44fa-aef2-1be7dd3fc1e3, da1620f5-06f0-491b-9f89-5a3b0c915d91, da318180-b9a9-436c-a290-a240d6dba2d7, edb29e97-12fe-48bc-82c0-f64254ef1df5, f7d50688-40b8-4209-8085-48eb39281140, fab71880-7f73-4939-a213-e454b8bf15a0, fbe69434-ba31-4957-a1cd-52699c369823]
170053 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093340.parquet
170067 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093340.parquet
170067 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093340.parquet => [090fc2eb-1b33-468a-8724-3884a88db263, 140ce555-7af4-4efc-9d43-f2281711a387, 1ec74c00-d87d-45f5-bff3-2767e80a4dee, 1ee0ef65-1274-464e-8f21-5fd337f32995, 1f9098a0-502c-4b0c-bfa0-ffd32ca3e783, 21a28133-495b-4796-a120-3bcaaa21d702, 31d13c9d-92b9-4a57-baa1-8942b8d9cc1c, 34045d54-685b-44d9-9588-cabd79757f5f, 362ec986-71d6-4e7e-aa44-9fbf9ba2e607]
170097 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093340.parquet
170108 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093340.parquet
170108 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093340.parquet => [4b152ee0-f7d4-4887-a04f-5e88896a4539, 4f213b3f-de37-4619-91e4-a95198939124, 56becf07-c8c3-4f5a-9a9c-c3b9ae1165fa, 5d5ba628-5d0d-4570-85fb-046817717c7f, 5f5c6fd7-e8e4-4fbc-b660-0708328213a0, 75047d3a-6dcf-4729-b02e-d9e4123d9aac, 7af7fe8e-53b9-451f-9065-4f2527f673c2, 88740679-bbe4-4af6-91c0-5be02099fa77, 8c53900e-e992-4942-810c-4624d8c8582b, 981ad32f-78a3-4ddd-bc66-cc8a450fcfab, 9b163649-298e-4369-ab69-ce69fdaaedd1, 9cbf5627-00f1-40b1-a0aa-9229c82472cd, b07eb804-81be-4a58-95dc-e79fabbe3777, b1ece490-1b9d-4c93-8802-563debc8ef78, c768bd2e-9d49-4222-8a73-aad29b57b39c, cf9216a3-2bc1-41ed-8238-c3bba653eddb, d22c3a08-9b6e-46e5-b661-f12c668a1edc, eafd206a-d52f-43de-baa5-16347371d348, f791aaa8-36a0-4ce0-8a75-83f2886345d1]
170125 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093340.parquet
170136 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093340.parquet
170136 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093340.parquet => [04ae0e5d-f42b-4f20-a17c-41558c3dbd4c, 096fe4f4-8fd3-4196-bd86-fd278b39e76f, 122bf41e-41c6-447c-ad7a-abca7d8fc95f, 171dc029-6c43-4619-8d9b-b8117b383ba4, 1d3a1e3e-7db8-4178-ab90-6cc08bd707d0, 1eff65ab-7758-4e6c-8f37-f20c56c7f255, 1f3a5089-6aaf-4576-a77f-8862e40b40e1, 29415dca-5622-4f42-a0c0-43468e8ed8ea, 32263ae6-848c-4573-9005-481d4fd63e56, 62a87ff3-30df-4892-bd85-a46875546a18, 6d02257d-2a68-42fb-8c0b-46e7e84ecbbb, 7bd36650-e301-4cdd-9881-e82d7eb9cd89, 994f26d0-8162-43e9-b0a1-763973ed6508, 9c1530bc-29ad-4f7f-a14d-1864824211b3, a1e7b2ad-3d7f-4504-a6d0-c425b0dbdac6, a6bf3cd4-d702-4da3-a06d-f868efb7510c, add428cd-8017-4a66-af91-5a91324783ea, b0d75e47-1c43-49ed-a785-2d6c407ca8da, ba97cad3-7769-46cc-8979-7e96e5de97bf, bf80945e-bb8d-4555-a764-c99535760e7e, c73246a0-195e-4ef9-8945-c0ade94969ff, cb778594-4fbc-4bac-ba3d-e4af5bd49909, d4e60c92-3c3a-40fd-a4c0-f2b7f1bbc919, d6f60643-1a23-4eff-9586-71894c822038, ddbd77c2-9e1a-424c-8975-7936930c5f0e]
170228 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=88}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=28}}}
170243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
170243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
170302 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093342
170302 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093342
170716 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
170716 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
170816 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
170907 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
170908 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
171102 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
171107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093342 as complete
171107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093342
171195 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
172393 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093346
172526 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
172947 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
173010 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 90
173010 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
173243 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093342.parquet
173255 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093342.parquet
173255 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093342.parquet => [00947f36-996f-4800-9e5c-4de0fff53a59, 067adc1c-9f1f-49ed-852a-5686a8e16f70, 0919f33f-6e58-40b1-99d6-5707d74b2009, 0ce050b7-e135-4085-b15c-16c1d8cb0133, 138d5c5a-8986-4c21-830e-72f3d51ced1f, 19c229a8-2d5a-40f7-bd9e-5e82ce94b31d, 1e137ba5-cefb-42ce-b986-83e0dc87b5c8, 1f63cd5b-f7ee-412a-809a-df268286b6a1, 2f730510-927c-4a41-9916-2e36540c0e1c, 302e49a2-8606-4ca1-b524-1587955b15fb, 416ed4f8-3720-44b4-8f11-e41a806c8c55, 4d8146b9-65e4-40cc-ba82-aa73bd27cb04, 5c4c647f-12c3-48cb-addc-f749e7fef17b, 626175f2-74e1-40da-ab8a-8da62d4409fe, 6a89de45-794a-41d4-855c-f5c0d7d8d9b1, 6ae58b57-bcbe-40a3-bb5d-9b8bc3bdd93d, 6ae90a75-4918-468f-a970-cf2086a97187, 718b4d74-38ae-4dcb-a0d6-79ba1ab4ca47, 85fba49c-55a5-445b-9f6e-60d107b3b92f, 89354362-487f-4f0e-8a58-55264212ba0c, 8e943bfa-110f-4593-afc8-6499f3aac480, 981efbea-d130-40bf-9064-83cd9892b1eb, 9942cfb3-bf22-4d1f-bdaa-2bf31b39cc14, 9a0a43ba-01b5-451c-b561-d664271cfcfe, 9ba7c944-1f1e-404d-975a-f47e083f3003, 9c8437af-12b5-48aa-a0f3-4f85f89957b0, a2cadf61-eac7-4caa-8f73-6235bed3108e, a7b2fad3-3bb6-49fe-8bf1-9acdff032b45, a9676da9-67e7-434e-90f4-6baf43052440, b34c607f-3e1a-4d5b-8c70-403b548b345d, b7897b4b-9024-4382-a5d0-fd0e3d85d2de, bc02501f-0bd2-4f3f-b69e-0ec603669caa, cef1698a-ef6e-458b-9cb2-a45b63014503, d1130d89-5343-4a24-b6d9-9dc38b5fc611, de0669fe-8296-422d-8c90-0e6b81d165aa, e0722894-3ad2-430a-9e84-148e141126ed, e0e5fe5c-419c-44c1-b519-71575c46c630, f96f0837-72c8-4e2a-9593-6ae081c4d498]
173270 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093342.parquet
173281 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093342.parquet
173281 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093342.parquet => [0e3108b1-6f37-4428-b0bf-4118cd869495, 121de7f0-ca19-49f5-ad58-63373dfcd701, 123f684a-6de3-4e13-b9b4-4692fe592d00, 1c3299d3-628f-497b-9063-27801528beaf, 40e8c122-0fef-450c-aa59-40562f2b1812, 445c04b1-e33e-4f92-9572-1c7b6df66c7c, 4e89208a-e682-4219-8c90-284d39b10879]
173308 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093342.parquet
173321 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093342.parquet
173321 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093342.parquet => [54ae818f-d567-42f2-b0cc-b19632c648d8, 57a08795-1fd2-4742-bc78-a3099aff4a47, 5aff5d68-8a07-4859-80e3-1c0af21d4ea9, 665252e7-131b-4aee-8f69-01f45f8bf236, 7cf19c5a-da6b-4303-b06d-a4afa6d3ce75, 981ad32f-78a3-4ddd-bc66-cc8a450fcfab, a8e81bb2-cd3b-4050-8da5-b2e0242f930f, b832dcc0-5f2b-4d41-98a0-0e42df684a4a, b8e83928-151e-4163-8ea9-ec7dce2de51a, c768bd2e-9d49-4222-8a73-aad29b57b39c, c8c709f6-bf9a-401f-87d9-a574625bdd49, d750f530-8f68-4be0-9f85-ccc62357812a, e7f30649-ab2f-459c-96f1-c3a8ad7bff90, fee59702-1564-4173-a733-dfb96cce335c]
173336 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093342.parquet
173348 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093342.parquet
173348 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093342.parquet => [096a194c-5d7b-494f-9944-85a1c73ab00b, 09e10984-c042-4cd2-b1d8-2e5ccd28a59d, 0a6243f5-669e-4fb5-95cb-110f7b36d7c8, 11c18f31-927e-451e-a219-e64c04e05523, 1aab0b51-4681-4aa5-9433-eac048293ab2, 1e7aa634-52e7-491c-a81e-e00c8a46307a, 1f3a5089-6aaf-4576-a77f-8862e40b40e1, 22a9fca4-7921-4a98-87c7-daa0fead5136, 3d49de59-cf72-4455-a16e-70d7c08e146f, 470be392-23a6-4727-a79c-223b5eed90f3, 526f8b84-2b79-4b96-a1b2-4e375785ed20, 589fd441-276e-4851-8ed4-6c9c2ada8376, 5a123ae8-fd5b-4f35-9ac3-183d5d690a64, 6713078a-0773-40d8-b354-7551f515b7fd, 6b51afcd-194a-4bde-8be1-c47c686936ab, 6d02257d-2a68-42fb-8c0b-46e7e84ecbbb, 7b4f368e-2456-4178-9d12-c363960a37c3, 901e3980-bf4e-41bc-a734-74349517bfbf, 94de2bfc-1476-4b1c-aae1-5ed0f9219f41, 95531bc8-e5e4-4379-9679-4e52f8e5106e, a2aa6f48-2a19-46d9-b972-e0bc6ffcfd9b, a9d4d152-8b17-4ff6-b07a-9812ebf7909f, ac892707-21b2-46d4-bdac-9cb6787c481f, b5bc28d9-d3d4-4b7f-814c-10cd9214ee21, c73246a0-195e-4ef9-8945-c0ade94969ff, c8c08fd2-628f-4651-8e59-6c57412c44be, c9732d9c-765c-41bd-a77d-9fc573eb935c, d2d821f7-c970-4b59-ba00-18861dba92d5, d7fa0947-0d45-4f9c-8b29-3961dfe2fc53, d808a19a-1b52-407b-889c-e50cf64d3309, f1fac871-d06d-44bd-8140-ff9fe0fcc05f]
173416 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=90}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=38}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=21}}}
173426 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
173427 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
173465 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093346
173465 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093346
173895 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
173896 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
174067 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
174067 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
174117 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
174961 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
175001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
175007 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093346 as complete
175007 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093346
175961 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
176252 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093349
176560 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
176609 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 93
176609 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
176756 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093346.parquet
176769 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093346.parquet
176769 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093346.parquet => [17d8a377-8881-4be7-8b83-32109e493c6c, 1b9eca29-dbbe-4545-8a82-87a3233587c0, 27fd9609-72dc-4c2c-a566-f500ee2a2140, 2e4fa341-827f-403b-8ede-435bf0219c8e, 302e49a2-8606-4ca1-b524-1587955b15fb, 364be7c8-f02f-4000-9ff5-73e1d47f9b76, 3787b123-6bb8-4f6b-9683-6e4a84537a95, 3884822c-531d-4463-87df-da7187dd723b, 46e2976b-a563-41a2-a0f1-82c71bd5f945, 5c4c647f-12c3-48cb-addc-f749e7fef17b, 61ae38bf-f32a-4b6e-90e7-91d477d98163, 61f02619-7ca8-436a-99d9-934720d68e4d, 759ab3a0-8d68-444a-a443-57c38321870a, 7b1489fc-f5ff-43d8-bce5-23aa76168fe9, 802fee72-1c3c-4cec-b944-86b9c0923d03, 825a92c0-5d7f-4991-bc5d-b1719642146d, 8e943bfa-110f-4593-afc8-6499f3aac480, 9942cfb3-bf22-4d1f-bdaa-2bf31b39cc14, 9a0a43ba-01b5-451c-b561-d664271cfcfe, 9b2b6f2e-d152-4cd2-8a65-d9761772f00e, a19ca5f5-cced-4a84-a8e3-cb1125c7ac28, a2cadf61-eac7-4caa-8f73-6235bed3108e, a7b2fad3-3bb6-49fe-8bf1-9acdff032b45, b27675ad-ad0d-47e9-8106-e06658301d09, b7897b4b-9024-4382-a5d0-fd0e3d85d2de, bd80543d-262e-45da-8f7b-b6526893451d, cb91c470-85bf-4416-878e-8a263aefe7f0, cee877bb-2d79-4ed3-bd27-64c2c67f3caf, d1130d89-5343-4a24-b6d9-9dc38b5fc611, d83b9fa1-9fa4-4da5-9f9d-251470f3fca4, e0e5fe5c-419c-44c1-b519-71575c46c630, ec512908-eed1-453b-96ac-0fe2e2de4a43, fbe69434-ba31-4957-a1cd-52699c369823, fcd63c53-f717-4fec-b9e7-a3e8f5e49aad]
176786 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 13 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093346.parquet
176799 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093346.parquet
176799 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 13 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093346.parquet => [0dd76f3f-824b-4449-927c-d4b9ddf34346, 121de7f0-ca19-49f5-ad58-63373dfcd701, 140ce555-7af4-4efc-9d43-f2281711a387, 3627898e-c8ca-491b-ba76-9c2513b77c0c, 47e93875-b60b-44c9-98d4-c2b64de6c279, 56becf07-c8c3-4f5a-9a9c-c3b9ae1165fa, 582df0ff-a523-4b9d-9049-9bef07be9666, 5aff5d68-8a07-4859-80e3-1c0af21d4ea9, 5c0d3f04-fc39-4774-ab6d-4a5484584053, 5d5ba628-5d0d-4570-85fb-046817717c7f, 6754f2b5-f2d3-4680-96df-db5714c55c9b, 6a07d179-190a-4e9f-ad01-224da7ff649a, 6a76e931-ebd4-4709-8f02-dda88a684f9c]
176829 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093346.parquet
176841 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093346.parquet
176841 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093346.parquet => [74beac4c-034c-418e-9d39-b656d05d7e92, 7d777c37-a6c1-45d7-a2e6-f9c03195a6b8, 7f33227c-b6f7-4570-a939-cfe0d18fe54b, 8947ba3d-1a35-4d27-a324-562dcb05d57a, 8e63d271-6d8b-4dec-9a48-a2f87ff77ab5, a1c46bd8-e774-4813-9af9-c4325b7988ab, a933b402-56ae-4340-bc01-6b575834e39c, b07eb804-81be-4a58-95dc-e79fabbe3777, b11aff12-333a-4e07-976a-4f940ce9fbf3, b25e9bf8-3036-4268-a751-13d185926873, b3812142-f9f5-4da1-8a00-a4fdf05221fb, c768bd2e-9d49-4222-8a73-aad29b57b39c, ceef1098-f30f-4d4f-84aa-e3a04f737b36, f11af746-2266-4040-958f-9d799e6c405b, f230d16d-6436-4a91-9f1d-b5853f28551c, f791aaa8-36a0-4ce0-8a75-83f2886345d1, fc8bc0a3-feed-4a29-b162-45eb3aa3be1e, fee59702-1564-4173-a733-dfb96cce335c]
176858 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093346.parquet
176870 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093346.parquet
176871 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093346.parquet => [13d6786b-9280-4b30-855d-4c3018500c12, 1d3a1e3e-7db8-4178-ab90-6cc08bd707d0, 217a6a02-980a-4f62-b673-cee4c3fce248, 22a9fca4-7921-4a98-87c7-daa0fead5136, 2681f190-b5ec-4ace-ac66-d9a2c44d2c79, 32263ae6-848c-4573-9005-481d4fd63e56, 3fa2352b-db70-43a9-a6c6-91aafe5ab233, 4662816c-d7a5-4d0b-a5e9-ac4d48a4d07e, 4fe6bbb2-e60e-4e91-bdc1-fe6975f4683e, 5463f98c-b959-40e2-90ae-d8caaa16fcea, 589ae407-0616-4da9-9ef2-fb76df8e2d7d, 6713078a-0773-40d8-b354-7551f515b7fd, 6b51afcd-194a-4bde-8be1-c47c686936ab, 84467a4c-def7-4fa1-b29d-6d91e03bf774, 901e3980-bf4e-41bc-a734-74349517bfbf, 915d6ef0-17e0-4409-b1ae-ac5287d35d5d, 94de2bfc-1476-4b1c-aae1-5ed0f9219f41, ae41e0cd-e875-4d44-8e22-a7445c956e8c, b0d75e47-1c43-49ed-a785-2d6c407ca8da, b8537577-c5f3-461d-b6de-9368bb06503a, bf7e6a2a-6bb4-4de0-9ed2-8c6a42c9d53c, bf80945e-bb8d-4555-a764-c99535760e7e, c79248fd-c704-40e8-9f34-e9aa5000881a, c8c08fd2-628f-4651-8e59-6c57412c44be, d1d43428-3470-4c37-af55-8397dab5b73d, d4e60c92-3c3a-40fd-a4c0-f2b7f1bbc919, dab2f10a-e531-40e3-a63e-d695afc7a909, f563554a-6940-4c39-aaa4-826609db5c7e]
176937 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=93}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=34}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=31}}}
176949 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
176949 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
176968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093349
176968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093349
177308 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
177308 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
177500 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
177500 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
177664 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
177670 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093349 as complete
177670 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093349
177754 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
178125 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
178889 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093352
179223 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 87
179223 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
179412 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093349.parquet
179425 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093349.parquet
179425 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093349.parquet => [07bf05cb-8abd-4ac3-bd8e-0f40ad4fc12f, 14396242-64d4-418e-8c5c-148364cd16d5, 21480d9d-4595-4893-a316-8e627e782186, 2e4fa341-827f-403b-8ede-435bf0219c8e, 449f8dc4-5a0f-4bc4-b20d-af5aeab1eb23, 451273b7-7398-45a0-a788-98dd313720a5, 4518e849-c1ea-42a7-952c-44c924ed129f, 4b152c16-ecd7-4b66-a034-ddff3a2aa01e, 4d011e6a-1a3f-4b40-a66f-f837b46666e8, 61ae38bf-f32a-4b6e-90e7-91d477d98163, 685d2328-ebce-4645-b730-80ee3103bffb, 6d42376e-1715-4d7c-a411-b7d0b6420291, 79f4aa30-b7ea-4848-9f40-6ed638fe900c, 7d326b62-6677-4c7d-8d77-cda46b5f7945, 8170f283-bb7f-40fc-bae6-352d54e1670e, 8f07f7a2-53fd-428a-acdc-2162d1089376, 9ba7c944-1f1e-404d-975a-f47e083f3003, a7b2fad3-3bb6-49fe-8bf1-9acdff032b45, b2b8c87c-6654-49ee-b191-d5692f5157dc, b34c607f-3e1a-4d5b-8c70-403b548b345d, bc02501f-0bd2-4f3f-b69e-0ec603669caa, d0a21f25-93ce-4517-a4ae-e2d37d31aa74, da1620f5-06f0-491b-9f89-5a3b0c915d91, da318180-b9a9-436c-a290-a240d6dba2d7, db676924-138d-4471-b3dc-c8d1e3e2fd64, de7ba323-799c-4f64-8f6d-ddc698e931af, e69164f6-b2e0-4c54-b7bf-725b87352306, e7c2707f-6899-4f49-aa6e-cfe12e1df3cd, fdaebb1f-d5c0-48ee-b2b9-96be2de0c2be]
179440 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 15 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093349.parquet
179450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093349.parquet
179450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 15 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093349.parquet => [07495283-b628-4e5a-bfbe-5c03a7e6c83c, 0dd76f3f-824b-4449-927c-d4b9ddf34346, 123f684a-6de3-4e13-b9b4-4692fe592d00, 16aeac83-91eb-4714-986c-0383f13eb1c6, 37d012c2-3f5d-4330-adc2-20ee5ec5f172, 3b9879dd-a6a5-4d30-a381-6903fc1f1f69, 445c04b1-e33e-4f92-9572-1c7b6df66c7c, 4b5ea1b5-4720-40b2-9ce7-6225c49ae5d2, 4cd1ce43-e857-4eb2-8ab4-af5134eb91c7, 4e89208a-e682-4219-8c90-284d39b10879, 57a08795-1fd2-4742-bc78-a3099aff4a47, 5c0d3f04-fc39-4774-ab6d-4a5484584053, 665252e7-131b-4aee-8f69-01f45f8bf236, 77995c2e-cf2f-43e7-be48-5926afc1b0f7, 79647ae0-a24e-437e-baea-4aef2ed415f1]
179474 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 15 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093349.parquet
179484 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093349.parquet
179484 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 15 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093349.parquet => [88740679-bbe4-4af6-91c0-5be02099fa77, 8cda699e-63de-4bfe-9795-c0412f5a4f86, 981ad32f-78a3-4ddd-bc66-cc8a450fcfab, a1c46bd8-e774-4813-9af9-c4325b7988ab, a678460d-5a41-4367-92db-90f097c1c003, ae7f3f73-c412-4f13-94ed-5d2668c7ffb0, b8e83928-151e-4163-8ea9-ec7dce2de51a, c768bd2e-9d49-4222-8a73-aad29b57b39c, ceef1098-f30f-4d4f-84aa-e3a04f737b36, d22c3a08-9b6e-46e5-b661-f12c668a1edc, da0cb566-b69f-44ce-854e-eff0c0979f59, eb479f21-982b-4bc3-bf89-1a789863c87e, ec0516ba-bd1d-4442-a66e-597432c4c9de, f791aaa8-36a0-4ce0-8a75-83f2886345d1, fc8bc0a3-feed-4a29-b162-45eb3aa3be1e]
179499 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093349.parquet
179507 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093349.parquet
179507 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093349.parquet => [13d6786b-9280-4b30-855d-4c3018500c12, 171dc029-6c43-4619-8d9b-b8117b383ba4, 1f3a5089-6aaf-4576-a77f-8862e40b40e1, 22a9fca4-7921-4a98-87c7-daa0fead5136, 25268b8f-93a2-4f11-8a7e-bd9527a08211, 2672c6e3-310a-40f4-9c43-8b69dd9ee9e8, 3fa2352b-db70-43a9-a6c6-91aafe5ab233, 4f993390-875b-4bb8-8634-b644e640759b, 4fa41102-6eaf-41bd-9952-b73a35b572c2, 60576512-3ba3-4887-8b6f-8cbd75620440, 6b51afcd-194a-4bde-8be1-c47c686936ab, 6f2de487-0120-4219-a72c-3a6fba0bba2a, 7afe3079-b0ea-45af-9f4f-ba8010cfb838, 80719764-31f2-4418-811f-64a5c10550f7, 84467a4c-def7-4fa1-b29d-6d91e03bf774, 994f26d0-8162-43e9-b0a1-763973ed6508, a19c6a3f-f44c-49e7-b645-3d6a4c8fb616, a9988efa-a7b0-4338-b75c-4bd086fb705d, ba97cad3-7769-46cc-8979-7e96e5de97bf, c44e66e9-c5f2-4de2-a2fb-39330e503093, c8c08fd2-628f-4651-8e59-6c57412c44be, cb778594-4fbc-4bac-ba3d-e4af5bd49909, d04c97ca-fc2e-4732-9972-62fad97534da, d0b5ac05-d67f-4cfb-a7bb-556213f905e6, dff8587c-39dc-44b7-a3d8-39bc3677db92, f47c7025-8638-473e-9389-e84336bce0d3, f8a4e28e-776d-4ea0-a910-8dd64bfd4e22, f8af1895-fd8c-4b68-b7c0-1b0e65b0e01b]
179567 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=87}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=30}}}
179580 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
179580 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
179599 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093352
179599 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093352
179727 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
179960 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
179960 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
179989 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
180155 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
180155 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
180283 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
180288 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093352 as complete
180288 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093352
181411 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
181484 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093355
181848 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 88
181848 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
181989 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 41 for /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093352.parquet
181998 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 173 row keys from /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093352.parquet
181998 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 41 results, for file /tmp/junit5083366197362681842/2016/03/15/0f5f96dd-101a-4d9c-a140-a59d58585e56_0_20180316093352.parquet => [0919f33f-6e58-40b1-99d6-5707d74b2009, 19c229a8-2d5a-40f7-bd9e-5e82ce94b31d, 1d70b098-2ebb-4527-be26-59e170dd1d01, 264c86a0-aba7-4d43-8df3-14c6dda62e0e, 2e45a3f0-a101-44b5-a0ad-0426305da749, 3787b123-6bb8-4f6b-9683-6e4a84537a95, 3884822c-531d-4463-87df-da7187dd723b, 449f8dc4-5a0f-4bc4-b20d-af5aeab1eb23, 451273b7-7398-45a0-a788-98dd313720a5, 48e79645-4b35-4787-a777-87441cfb1d5e, 61ae38bf-f32a-4b6e-90e7-91d477d98163, 626175f2-74e1-40da-ab8a-8da62d4409fe, 685d2328-ebce-4645-b730-80ee3103bffb, 6986df5e-136d-435b-8db1-9611799d002f, 6ae58b57-bcbe-40a3-bb5d-9b8bc3bdd93d, 6ae90a75-4918-468f-a970-cf2086a97187, 6f2fe1bd-5071-4059-b992-4c3be78a28f6, 7b1489fc-f5ff-43d8-bce5-23aa76168fe9, 8170f283-bb7f-40fc-bae6-352d54e1670e, 8e943bfa-110f-4593-afc8-6499f3aac480, 983a1441-b104-42dd-ac21-2bc8b7a5bfee, 9c8437af-12b5-48aa-a0f3-4f85f89957b0, a6d352d5-c3de-44e3-9fb5-e78aafe7b564, a9676da9-67e7-434e-90f4-6baf43052440, aee6e254-30a4-4a70-b23a-a716140dd1c4, b2b8c87c-6654-49ee-b191-d5692f5157dc, b34c607f-3e1a-4d5b-8c70-403b548b345d, b4e5e16f-2f7f-413b-b57b-5c961cb49498, bc02501f-0bd2-4f3f-b69e-0ec603669caa, bc605f3a-6d9a-43dd-89aa-7aae099910af, be0947eb-7ac3-4e93-8895-828837fbc9ef, c98f44ca-0c57-4144-8d73-a4842486ef73, cf4b1a19-445d-4629-a271-d81a6138f1f2, d7c0b099-0df3-4690-a831-8434ec19e738, d8f3166d-f127-4e66-9858-c02bbaca966b, e0e5fe5c-419c-44c1-b519-71575c46c630, e2c8d1f8-76e9-49b5-be51-bf7bd56727eb, e522133e-faf9-487d-86ff-fd2350606b36, e69164f6-b2e0-4c54-b7bf-725b87352306, fbe69434-ba31-4957-a1cd-52699c369823, fe1f26f3-6637-43d2-af88-eeffbca1d08e]
182012 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093352.parquet
182019 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
182020 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093352.parquet
182020 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093352.parquet => [0e3108b1-6f37-4428-b0bf-4118cd869495, 140ce555-7af4-4efc-9d43-f2281711a387, 178903a9-b0b6-4683-a24c-854a4bdbb17a]
182044 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 21 for /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093352.parquet
182052 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093352.parquet
182052 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 21 results, for file /tmp/junit5083366197362681842/2015/03/17/d3d72157-a9c8-4797-aba8-ce26c80c1734_2_20180316093352.parquet => [22728ac1-6c89-4e6a-a5eb-f9c16fac84d1, 37d012c2-3f5d-4330-adc2-20ee5ec5f172, 3b02ec09-1fc8-42bf-97eb-c75790abb8a2, 3b9879dd-a6a5-4d30-a381-6903fc1f1f69, 3d25f25a-4dd0-41b9-89ba-c2f606d8c056, 3fd2935e-c7ae-4ad1-8333-3783ea8bebb5, 40e8c122-0fef-450c-aa59-40562f2b1812, 4a079e60-2009-4120-95fb-e3d25ab4b5a7, 4b152ee0-f7d4-4887-a04f-5e88896a4539, 4f213b3f-de37-4619-91e4-a95198939124, 56becf07-c8c3-4f5a-9a9c-c3b9ae1165fa, 5b2c3f08-96ff-4f08-8572-55257dee75a0, 608f809b-2528-4c0b-9ea8-83a4a077115e, 7af7fe8e-53b9-451f-9065-4f2527f673c2, 7cf19c5a-da6b-4303-b06d-a4afa6d3ce75, 8cda699e-63de-4bfe-9795-c0412f5a4f86, b80a914d-dc6a-4091-9559-9e1b533674da, c8c709f6-bf9a-401f-87d9-a574625bdd49, d21a912c-7d5a-4c18-9d05-2db5358508fa, e06778d6-a4ad-400b-aee4-0c56cfcc9444, e0812ccd-3df1-4e45-9db9-6b8a0b405423]
182070 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 23 for /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093352.parquet
182078 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 166 row keys from /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093352.parquet
182079 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 23 results, for file /tmp/junit5083366197362681842/2015/03/16/ea314e36-6e19-4c55-a88f-b4c60bb8928c_1_20180316093352.parquet => [0a6243f5-669e-4fb5-95cb-110f7b36d7c8, 3e7ef3ea-962d-4626-bb36-b419774c9274, 4fe6bbb2-e60e-4e91-bdc1-fe6975f4683e, 589fd441-276e-4851-8ed4-6c9c2ada8376, 6d02257d-2a68-42fb-8c0b-46e7e84ecbbb, 6ed76575-a7df-4cec-b987-fab5896d46b6, 7a385e05-4e59-453e-b1eb-2b62cde47551, 8e230d46-a460-49b1-87df-55f831004f93, 901e3980-bf4e-41bc-a734-74349517bfbf, 94de2bfc-1476-4b1c-aae1-5ed0f9219f41, 994f26d0-8162-43e9-b0a1-763973ed6508, a2412de9-c6d9-4c7d-b1c9-c967389b303e, ac892707-21b2-46d4-bdac-9cb6787c481f, add428cd-8017-4a66-af91-5a91324783ea, b5bc28d9-d3d4-4b7f-814c-10cd9214ee21, b67ec158-b447-49e6-924b-2b696bb4e855, c1e2e392-52aa-4a91-af36-51217b4d66fc, c73246a0-195e-4ef9-8945-c0ade94969ff, c860320b-615c-4d5a-b615-ae9897525df5, d557db10-a81a-4aea-87cd-79299cd3797e, f1fac871-d06d-44bd-8140-ff9fe0fcc05f, f322ac76-d2f5-42d7-a070-3e9ddbaa3f39, f7baf6bc-5229-42c4-a9c4-4d9d72560d55]
182135 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=88}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=41}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=23}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=24}}}
182147 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
182147 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0f5f96dd-101a-4d9c-a140-a59d58585e56}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ea314e36-6e19-4c55-a88f-b4c60bb8928c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d3d72157-a9c8-4797-aba8-ce26c80c1734}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0f5f96dd-101a-4d9c-a140-a59d58585e56=0, ea314e36-6e19-4c55-a88f-b4c60bb8928c=1, d3d72157-a9c8-4797-aba8-ce26c80c1734=2}
182165 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093355
182165 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093355
182510 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
182510 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
182714 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
182714 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
182867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
182872 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093355 as complete
182872 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093355
182930 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
183105 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
183105 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
183282 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
183283 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/01/id31_1_20160506030611.parquet	true
183283 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
183283 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/02/id32_1_20160506030611.parquet	true
183283 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
183284 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/06/id33_1_20160506030611.parquet	true
183290 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
183290 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
183297 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
183298 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
183298 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
183460 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
183460 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
183460 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
183466 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
183466 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
183471 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
183472 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
183472 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
183639 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
183640 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/01/id21_1_20160502020601.parquet	true
183640 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
183640 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/02/id22_1_20160502020601.parquet	true
183640 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
183641 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/06/id23_1_20160502020601.parquet	true
183646 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
183646 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
183653 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
183655 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
183655 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
183836 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
183836 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/01/id21_1_20160502020601.parquet	true
183837 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
183837 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/02/id22_1_20160502020601.parquet	true
183837 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
183837 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/06/id23_1_20160502020601.parquet	true
183843 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
183843 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
183851 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
183852 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160501010101]
183852 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160501010101]
183995 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
184007 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
184008 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/01/id11_1_20160501010101.parquet	true
184008 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
184009 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/02/id12_1_20160501010101.parquet	true
184009 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
184009 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1560010445291476092/2016/05/06/id13_1_20160501010101.parquet	true
184012 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160501010101]
184012 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160501010101]
184018 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160501010101] rollback is complete
184076 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
184438 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
184438 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
184446 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
184718 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=300, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=93, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=100, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=107, numUpdates=0}}}
184725 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 93, totalInsertBuckets => 1, recordsPerBucket => 500000
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 500000
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 107, totalInsertBuckets => 1, recordsPerBucket => 500000
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
184726 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
184744 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
184744 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
184836 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
184836 [pool-1648-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
184857 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
184857 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
184877 [pool-1648-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
184892 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
184892 [pool-1649-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
184917 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
184917 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
184943 [pool-1649-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
184959 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
184959 [pool-1650-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
184984 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
184984 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
185000 [pool-1650-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
185017 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
185017 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
185112 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
185112 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
185247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
185254 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
185254 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
185765 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
185765 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
185785 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
185911 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 61 for /tmp/junit1851847279769451296/2016/03/15/28fbf26e-a5a4-4124-ae01-184c578c4bce_0_001.parquet
185923 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 61 row keys from /tmp/junit1851847279769451296/2016/03/15/28fbf26e-a5a4-4124-ae01-184c578c4bce_0_001.parquet
185923 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 61 results, for file /tmp/junit1851847279769451296/2016/03/15/28fbf26e-a5a4-4124-ae01-184c578c4bce_0_001.parquet => [0852ad3c-3467-41d4-810f-1580ec7ebc10, 091a6fbc-9050-4efc-b097-c8713f841d85, 114b4f38-7059-44fb-a1a5-e455a02b5645, 11a6d156-10d3-41a9-9ec7-f691ffe55631, 15d97866-647a-470c-82b3-90ff768cde7f, 1d078fd5-8511-4a16-99cd-ca6c495e2325, 218fdea4-ae90-41e6-84b9-0582a286297f, 23445181-c82f-4d41-99bb-6ed767424d9f, 28f57f58-a02f-455c-b2a9-5fc7ecc31e17, 2a9a7a6a-7942-4f5e-adc6-a66c1f9a3590, 2beaae0c-e7cc-4972-9a1e-ab95c567a0a9, 367a3ce6-ee42-4154-8b6b-04710c66a6aa, 3a66ad59-191f-4324-a813-669265607446, 3f1396be-6abd-46b4-90d3-aaf10afa8cd9, 413f2025-135e-4b6a-865d-04de6785f82c, 41df4d92-7003-44a6-aad7-ce187d2f30ad, 445262ef-8d50-4c22-a93f-55ea4cbeb486, 46bbb44c-c7c7-4203-9caa-939f5310e842, 46ed7e32-e998-4f1f-bfac-016423bb6339, 47045837-87b9-4be7-a572-bd3b66f950f8, 47409fed-2036-46c8-9b69-3be41944fe37, 4a5fe69a-8751-454d-9930-56e9f4548a75, 4afa0bc0-df20-49e7-b3a8-1d79e2a80c89, 4aff3564-d4e6-403e-a3d0-22e69643480a, 63608b22-4f5d-4351-a856-6de65db1425e, 74b21293-344d-4c3c-ba4d-e320dec45f13, 753315ac-d29e-4fda-9e25-2964c2160bfc, 7664e10c-21f4-48a5-a249-73fa5e0241d6, 78722f4c-5dc8-4aa0-a944-7f496a180a8e, 81f2e6ed-ba19-4c8d-bcd8-7435527937a0, 853cec72-86aa-4a4d-8ac7-bc166d6fef66, 890b4786-7bdd-4a57-8200-f415223c1ecc, 8c497c03-060f-49a3-a549-60e51ffa6fcb, 8ff6fad0-021b-4292-9e07-a79b1ada5eaf, 91700456-8569-410d-a69e-ebd3631dd40a, 92c09d12-de04-4508-9b61-924fedad8c88, 963e3c82-eb20-4e44-b681-61ef56c80f70, 98f9f897-59f5-4e9d-a8ae-ceb968d987ab, 9dc54064-65da-4977-8825-0eb2f68dd6d7, a4735bbe-6246-48cf-ad23-2b665f982e7d, aece8e73-5add-4adf-9c66-464f5f7a6609, aedff64e-ebfc-4480-80d0-42f55e530b3c, b94b3a05-62ea-4eb1-823a-f4895b563675, ba348a7a-eb1a-4f99-b382-ade22e47d54d, c9dcb71b-5323-4b1b-854c-572ff78d12e2, ca1859b2-c5bf-42ab-969a-c9b276623f85, da517d2d-b7ff-4f1d-80a2-1faea2b24f1c, dac6f5c5-ba75-4994-a63d-4c0bf9ee8306, dd85bd27-aebf-4dc4-8523-2ccb1b9b23c6, e12344a3-e601-4321-b55d-31144beab9a7, e574aa76-c875-4710-9fc7-65aef1fde35a, e6c1fcdd-ca22-4376-aa4d-b3169e822548, e755919c-83e8-4c2a-ac9f-613d419c3ecb, e87b8e84-7f21-447a-a4d3-7ac4e91ec30d, ead20acc-4270-483d-a229-7b7f3b08c24d, f04ba809-2eb5-4002-8c61-a66f93812c4a, f4c9ba20-c490-407f-9a8e-8a9fd784fbea, f656823e-11ef-4395-b6fd-5fb84c2a6dd9, fb7e1f59-2d8c-4468-ac4c-0125de0237ac, fedc2d66-87c4-4780-ae9d-c76d98e7a663, fee5b4b0-3e3e-4fad-ab68-c41d6c0e8a9b]
185938 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 70 for /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet
185947 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet
185947 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 70 results, for file /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet => [05da6b67-e983-41ea-8028-f8a3f536ac36, 088b9690-7c45-4742-a8b5-19fce58f27e3, 104cb600-982a-413f-a52a-c4576f8aa3ec, 13922fa0-dc1f-49f0-8d21-611448e62b7d, 14f32d84-0db7-4bd6-93b1-b01e54ef8bff, 18e47efc-ba0f-43d3-a852-9e552eb9bfaa, 197e8a75-1e14-47ab-9c8f-cc59ecd80272, 1dfc2b6f-fb7f-4dbb-ad75-8e211af66391, 1ecaa3c5-3385-442d-b22b-d520a7889709, 25f0875d-bf88-4c2e-a45a-c7fc7f390e2c, 27fc53ec-8385-4789-a442-b7ce7aeb3d72, 2a9beac5-adfe-49ef-a8c1-bcfccf28f983, 2fb7b835-5795-4cf8-a004-30acc18657f7, 33ea2ca2-cba2-41c8-839a-cd9e6cc815cb, 392b8ad8-c7a8-4a9f-bdf0-33166ec04e53, 3e537c7f-aa5c-4fc4-9cd3-4c393b35d27a, 3f9be1ba-f06e-47f6-8ee5-0db39e353c33, 3fc9191c-c4ee-40ac-a937-0649bf2c8d9f, 410e1ba2-a874-4777-ad6e-832edc72ddfd, 4480891f-dad0-4d08-8df5-90ca573bfadd, 47750f8e-9648-437a-a41d-d9c65565b112, 4cd44cbe-bab6-47cf-b3f1-5de1365282c3, 4e901074-bb3e-4f60-a22a-f1d26febd4c9, 513023a5-938d-4f20-8550-ac657180f784, 561e52ed-b37b-40ea-98a2-b2994bc38f3a, 5b079880-fe0b-4b74-8294-665c68566c50, 5d6196cb-ba85-4da0-bbb9-ea389cf7743a, 6052a982-02f7-48c6-a838-83a8712d0449, 63045704-fedb-418f-bbc0-381c81487f42, 68ecedc3-864f-4788-bdd9-3c51122d8aeb, 6da1c1dc-1cfe-4e38-a4e8-b3718a4c307d, 6e1a656f-5b8e-464f-a6a9-1611a0aa8d2a, 7522784c-c356-4da9-992c-a1d538bd2d5a, 79913a2d-20ee-405d-9da2-6031cdc6fd39, 7f98a1d9-9d93-45d1-bae5-4071b8c89463, 7fcbe85c-fadb-4cef-96f9-b385e0b7b3fd, 82148152-2fc9-4674-aaeb-74fef6f4bf37, 8acb3a21-d9df-450d-9b62-94a99d0ccd67, 8c5442a6-9e85-4341-83e5-24065b92eb2d, 919e7e79-48eb-4c5c-826d-f472ea8d7da5, 91a7e5d0-dbf1-4dfe-8cab-f88b34d54787, 9270153f-b1b3-4579-98b0-0aa3a9ec79d1, 973e1d87-1748-432c-9b45-e5a17eccb266, 98456042-a967-40e5-bee6-8e2ba89090ac, 99e8f72f-0466-48e8-a918-7471396238da, 9f0ec310-f313-49c4-bcd9-823668249bbe, a16d1f7d-232b-458d-9a61-93be237225fa, a743c02f-5750-4f41-b03b-471d66904398, a7fb8647-3cb8-472c-910d-b2d6a98f1179, afafba96-e18b-4271-a1ad-f57d74cf278a, b01ab6a0-3bf6-4dc7-a617-cf6922fec730, b1012394-7c8e-4f8b-ba22-3c040300ba9b, b3479560-3e3c-4cfe-8cf7-78303bd8f309, b677a75b-c772-4f16-b98d-44b55e7cd741, bd07c1e8-cae6-41ed-9c45-c7d23b9ca67c, c13ba4d2-e100-42ea-af10-42e9b5cddf87, c586c62d-4ad6-4703-b310-2a949bfc14d4, cf471275-a2e9-4ad7-b0d7-64575d7ffc51, d2bbd635-3d94-4c09-91f7-1ff5b6cef26f, d446def3-60c8-4050-8aa9-66352b71a8eb, d7322de8-1415-4950-92ca-230872b006de, d90b8832-ae97-4a09-8b0e-6c52e7c63577, dbb6dfc0-0337-4d23-b2b8-06bb722f73c9, debe88b1-b1bb-47d1-81eb-1aa8d11599c4, df8f5b7b-adf6-4ca1-a180-385b004e52bc, e04eea1f-72a6-4147-89bb-9d39eaa48e35, e9895255-8ee1-4548-b927-0f405aabcf91, eed5fcfd-f4bd-4613-b0e7-072660af6cf1, fb16efaa-c3e8-4b6e-851f-0f0f86861d60, fcc95602-8cfb-4b62-8992-4a4f1647a88c]
185960 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 69 for /tmp/junit1851847279769451296/2015/03/16/d3aeb829-289a-4d35-a3f3-bc271e10142c_1_001.parquet
185966 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
185971 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit1851847279769451296/2015/03/16/d3aeb829-289a-4d35-a3f3-bc271e10142c_1_001.parquet
185972 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 69 results, for file /tmp/junit1851847279769451296/2015/03/16/d3aeb829-289a-4d35-a3f3-bc271e10142c_1_001.parquet => [0cc2c1c2-6451-48e6-b247-f0789b140b62, 0e20dd8f-7b33-40fe-9720-6c0ee4fe420f, 1237d960-45b6-4452-bcb7-34892a4c1880, 15213466-54d1-4c8c-ad3d-4dd2b1e26109, 16081892-a7a6-4c67-9b64-ca30ae13fdf2, 19aecb9f-3a20-4d14-a047-1a554ad61292, 1d5d42f9-c775-4d96-8979-22e467891bfa, 1f54bf7e-fe6a-44a7-a49e-49a70dd91da5, 21a40030-f3c4-4b0c-b219-d422a98e17b6, 2240254b-433e-4e7f-9809-11f6d7582d60, 25819226-937c-4eae-824a-6c833794f5bc, 259a02f7-1e75-4a81-90d6-244a8d988aa5, 28b4652f-655c-4533-a657-e4c18cedec9d, 28ccfe21-89b2-46d9-b13e-c236a35ab8e6, 29d4ef61-cb2c-4c9a-9359-acad25823e7e, 30b1ffea-f62b-46a9-a4db-0de9ac86b94a, 3314b273-15f0-46f4-9fd2-31a11e40b6e4, 335694ba-99a7-43c6-86f0-d619b3c1c170, 36d28f2c-c1d8-4987-9801-62ed0db8f9fd, 37ac4914-6113-4d94-bd30-5d720b3b9a2f, 424d7aa6-f86c-427c-a6bd-40b0fcba6f5b, 431292d2-cb52-4a57-858b-e217d3a75919, 461663b2-4af4-4437-a245-d1e7a60aa8ea, 52144f98-efd8-4e02-b4f5-4e7a71b6ddd7, 53bb5d1f-14fa-4651-ad4e-635122ca523c, 55674a3b-d6a2-40bd-8381-7af479381819, 59b80c57-d5d3-4b97-9ca8-975ccae88a8c, 5d6d8256-9e1d-4780-a4e1-a7baf1bd7eb2, 5dba739c-4192-4788-b1f2-95e71905bdf8, 5df17bd7-dc08-4196-830f-f0422f6fb673, 6e5e0b95-eeb2-4aad-9326-307e728f2ef4, 83987687-3171-42c5-adcc-f2edb43717c3, 84017734-3f9e-4bb6-81cb-d45365d64218, 874e9148-426e-456a-a222-701a70901933, 87ee0c89-eaa9-4e9e-8a65-0c13fbc85065, 895bf6c5-7ae2-43a6-847a-35f9a07c940c, 8f545782-2ed4-40a6-b0be-9fa08c62c293, 944d9cf1-2742-4be0-8ecd-0e100f437435, 972c9bf4-bbdd-4666-ba73-358339a5eab3, 98d03447-72c6-4427-a95f-aaec7fe798a0, 99332489-4906-4c72-bea3-854eef124b01, 9adddb8c-83fb-4355-957f-ebf9c462e60a, a3938e62-45ae-4015-b63c-a7d0aea5497d, a5b2c6ad-d2ac-43c3-ae1c-a77c1e00b275, ac465332-a874-4747-aeaf-ded0867b0671, ad33734f-ca98-4575-bb65-5bd324ece0b7, b24abb33-a0b0-4364-a78d-f0f53c523c11, b37fe1c2-ad5a-4980-9ea8-cb24c2815fdc, c63f2ed7-ae9e-453a-854f-9b7761b774f3, c82e0500-6207-476b-abc8-5dfd11d25a93, c93476da-34e9-4e23-bd92-c3e6865b53b8, ca528c73-de46-4bbb-8173-7a21be099b9e, d3ea228a-ccd5-4238-882b-3e2e361e6da4, d4ca5f82-0b99-40fe-a2fa-e3deceb90b3b, d5397baf-eb42-40c0-a8df-7426643ed8ce, d9647a01-311d-4557-9975-84600feb19ec, dcdea48f-13b6-452f-8119-bd9845aee776, dd132549-0818-4e5d-90f7-4148dacebce4, e13347e1-f3cc-43d7-afbf-8fc420144c2d, e1b03c7c-000c-4854-a1cc-cdd0762c7ffb, e6697a8c-b946-4f75-b459-1330d0c489f7, e8895c56-441f-4b69-9bbc-0d2245e4cfe0, eaf6cee0-0714-4ef9-8e28-08d332ed2e06, f4227186-29e0-4700-84f8-0d7342c857ab, f636058a-1528-4330-8e48-11fb87dddea6, f8905012-62df-416a-847e-678f5288fb8c, fb1814bb-156d-4264-a80d-b6a5ef2ed898, fbbfe0f6-d5e2-4dbb-a4cf-ba623fd819d1, fd58fd1a-2a11-443e-80a3-fca15f04ca0c]
186018 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
186284 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
186284 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
186456 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 36 for /tmp/junit1851847279769451296/2016/03/15/28fbf26e-a5a4-4124-ae01-184c578c4bce_0_001.parquet
186465 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 61 row keys from /tmp/junit1851847279769451296/2016/03/15/28fbf26e-a5a4-4124-ae01-184c578c4bce_0_001.parquet
186465 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 36 results, for file /tmp/junit1851847279769451296/2016/03/15/28fbf26e-a5a4-4124-ae01-184c578c4bce_0_001.parquet => [0852ad3c-3467-41d4-810f-1580ec7ebc10, 114b4f38-7059-44fb-a1a5-e455a02b5645, 15d97866-647a-470c-82b3-90ff768cde7f, 218fdea4-ae90-41e6-84b9-0582a286297f, 2beaae0c-e7cc-4972-9a1e-ab95c567a0a9, 367a3ce6-ee42-4154-8b6b-04710c66a6aa, 3f1396be-6abd-46b4-90d3-aaf10afa8cd9, 413f2025-135e-4b6a-865d-04de6785f82c, 46ed7e32-e998-4f1f-bfac-016423bb6339, 47045837-87b9-4be7-a572-bd3b66f950f8, 4a5fe69a-8751-454d-9930-56e9f4548a75, 4afa0bc0-df20-49e7-b3a8-1d79e2a80c89, 4aff3564-d4e6-403e-a3d0-22e69643480a, 63608b22-4f5d-4351-a856-6de65db1425e, 753315ac-d29e-4fda-9e25-2964c2160bfc, 7664e10c-21f4-48a5-a249-73fa5e0241d6, 78722f4c-5dc8-4aa0-a944-7f496a180a8e, 81f2e6ed-ba19-4c8d-bcd8-7435527937a0, 890b4786-7bdd-4a57-8200-f415223c1ecc, 8c497c03-060f-49a3-a549-60e51ffa6fcb, 91700456-8569-410d-a69e-ebd3631dd40a, 92c09d12-de04-4508-9b61-924fedad8c88, 98f9f897-59f5-4e9d-a8ae-ceb968d987ab, 9dc54064-65da-4977-8825-0eb2f68dd6d7, da517d2d-b7ff-4f1d-80a2-1faea2b24f1c, dd85bd27-aebf-4dc4-8523-2ccb1b9b23c6, e6c1fcdd-ca22-4376-aa4d-b3169e822548, e755919c-83e8-4c2a-ac9f-613d419c3ecb, e87b8e84-7f21-447a-a4d3-7ac4e91ec30d, ead20acc-4270-483d-a229-7b7f3b08c24d, f04ba809-2eb5-4002-8c61-a66f93812c4a, f4c9ba20-c490-407f-9a8e-8a9fd784fbea, f656823e-11ef-4395-b6fd-5fb84c2a6dd9, fb7e1f59-2d8c-4468-ac4c-0125de0237ac, fedc2d66-87c4-4780-ae9d-c76d98e7a663, fee5b4b0-3e3e-4fad-ab68-c41d6c0e8a9b]
186481 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet
186493 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet
186493 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet => [05da6b67-e983-41ea-8028-f8a3f536ac36, 13922fa0-dc1f-49f0-8d21-611448e62b7d, 14f32d84-0db7-4bd6-93b1-b01e54ef8bff, 1dfc2b6f-fb7f-4dbb-ad75-8e211af66391, 1ecaa3c5-3385-442d-b22b-d520a7889709, 27fc53ec-8385-4789-a442-b7ce7aeb3d72, 392b8ad8-c7a8-4a9f-bdf0-33166ec04e53, 3f9be1ba-f06e-47f6-8ee5-0db39e353c33, 410e1ba2-a874-4777-ad6e-832edc72ddfd, 4480891f-dad0-4d08-8df5-90ca573bfadd, 47750f8e-9648-437a-a41d-d9c65565b112, 4e901074-bb3e-4f60-a22a-f1d26febd4c9, 561e52ed-b37b-40ea-98a2-b2994bc38f3a, 5b079880-fe0b-4b74-8294-665c68566c50]
186520 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 21 for /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet
186533 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet
186533 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 21 results, for file /tmp/junit1851847279769451296/2015/03/17/a00a0083-c552-44b6-a08a-1e3d5c0d4477_2_001.parquet => [6052a982-02f7-48c6-a838-83a8712d0449, 63045704-fedb-418f-bbc0-381c81487f42, 68ecedc3-864f-4788-bdd9-3c51122d8aeb, 6e1a656f-5b8e-464f-a6a9-1611a0aa8d2a, 7f98a1d9-9d93-45d1-bae5-4071b8c89463, 82148152-2fc9-4674-aaeb-74fef6f4bf37, 8acb3a21-d9df-450d-9b62-94a99d0ccd67, 919e7e79-48eb-4c5c-826d-f472ea8d7da5, 91a7e5d0-dbf1-4dfe-8cab-f88b34d54787, 973e1d87-1748-432c-9b45-e5a17eccb266, 98456042-a967-40e5-bee6-8e2ba89090ac, a16d1f7d-232b-458d-9a61-93be237225fa, a743c02f-5750-4f41-b03b-471d66904398, afafba96-e18b-4271-a1ad-f57d74cf278a, b3479560-3e3c-4cfe-8cf7-78303bd8f309, bd07c1e8-cae6-41ed-9c45-c7d23b9ca67c, c13ba4d2-e100-42ea-af10-42e9b5cddf87, debe88b1-b1bb-47d1-81eb-1aa8d11599c4, df8f5b7b-adf6-4ca1-a180-385b004e52bc, e04eea1f-72a6-4147-89bb-9d39eaa48e35, eed5fcfd-f4bd-4613-b0e7-072660af6cf1]
186548 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit1851847279769451296/2015/03/16/d3aeb829-289a-4d35-a3f3-bc271e10142c_1_001.parquet
186561 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit1851847279769451296/2015/03/16/d3aeb829-289a-4d35-a3f3-bc271e10142c_1_001.parquet
186561 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit1851847279769451296/2015/03/16/d3aeb829-289a-4d35-a3f3-bc271e10142c_1_001.parquet => [0cc2c1c2-6451-48e6-b247-f0789b140b62, 1237d960-45b6-4452-bcb7-34892a4c1880, 15213466-54d1-4c8c-ad3d-4dd2b1e26109, 16081892-a7a6-4c67-9b64-ca30ae13fdf2, 1f54bf7e-fe6a-44a7-a49e-49a70dd91da5, 21a40030-f3c4-4b0c-b219-d422a98e17b6, 2240254b-433e-4e7f-9809-11f6d7582d60, 25819226-937c-4eae-824a-6c833794f5bc, 259a02f7-1e75-4a81-90d6-244a8d988aa5, 29d4ef61-cb2c-4c9a-9359-acad25823e7e, 3314b273-15f0-46f4-9fd2-31a11e40b6e4, 335694ba-99a7-43c6-86f0-d619b3c1c170, 431292d2-cb52-4a57-858b-e217d3a75919, 52144f98-efd8-4e02-b4f5-4e7a71b6ddd7, 5dba739c-4192-4788-b1f2-95e71905bdf8, 5df17bd7-dc08-4196-830f-f0422f6fb673, 6e5e0b95-eeb2-4aad-9326-307e728f2ef4, 83987687-3171-42c5-adcc-f2edb43717c3, 874e9148-426e-456a-a222-701a70901933, 8f545782-2ed4-40a6-b0be-9fa08c62c293, 98d03447-72c6-4427-a95f-aaec7fe798a0, 99332489-4906-4c72-bea3-854eef124b01, a3938e62-45ae-4015-b63c-a7d0aea5497d, c63f2ed7-ae9e-453a-854f-9b7761b774f3, c82e0500-6207-476b-abc8-5dfd11d25a93, ca528c73-de46-4bbb-8173-7a21be099b9e, e8895c56-441f-4b69-9bbc-0d2245e4cfe0, fb1814bb-156d-4264-a80d-b6a5ef2ed898, fd58fd1a-2a11-443e-80a3-fca15f04ca0c]
186650 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=100}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=36}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=35}}}
186661 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
186661 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=d3aeb829-289a-4d35-a3f3-bc271e10142c}, 1=BucketInfo {bucketType=UPDATE, fileLoc=a00a0083-c552-44b6-a08a-1e3d5c0d4477}, 2=BucketInfo {bucketType=UPDATE, fileLoc=28fbf26e-a5a4-4124-ae01-184c578c4bce}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{d3aeb829-289a-4d35-a3f3-bc271e10142c=0, a00a0083-c552-44b6-a08a-1e3d5c0d4477=1, 28fbf26e-a5a4-4124-ae01-184c578c4bce=2}
186679 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
186679 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
187052 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
187052 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
187183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
187183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
187337 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
187344 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
187344 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
187784 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
187922 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
188297 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
188297 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
188440 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
188440 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
188587 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
188587 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
188721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
188721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
188893 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
188893 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
189160 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
189364 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
189414 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
189543 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
189543 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
189807 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=54, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=62, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=84, numUpdates=0}}}
189814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
189814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
189814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 54, totalInsertBuckets => 1, recordsPerBucket => 500000
189814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
189814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
189815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 62, totalInsertBuckets => 1, recordsPerBucket => 500000
189815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
189815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
189815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 84, totalInsertBuckets => 1, recordsPerBucket => 500000
189815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
189815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
189831 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
189831 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
189954 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
189954 [pool-1706-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
189978 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
189978 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
190003 [pool-1706-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
190026 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
190026 [pool-1707-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
190061 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
190061 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
190078 [pool-1707-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
190100 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
190100 [pool-1708-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
190147 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
190147 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
190171 [pool-1708-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
190190 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
190190 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
190304 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
190304 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
190445 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
190452 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
190452 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
190538 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
190767 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
191168 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
191168 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
191378 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
191420 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 54 for /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_0_001.parquet
191434 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_0_001.parquet
191434 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 54 results, for file /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_0_001.parquet => [0dbaf990-11a1-42cf-912b-a5f236cec880, 119e2e6b-df29-4985-b7a5-74dee30c88a1, 18adb7b0-e1fe-417a-9d00-cb99aed45a91, 1a0a95a6-9694-46ec-838f-c254494c11d8, 1da77d30-d54f-43df-865f-02c64210c8d8, 2ad79f12-d408-4aba-8650-7fc82c6b5457, 2d04b4c7-d5a6-4371-b2db-4c5df466f7b5, 2f3d75e6-9699-426e-8df0-fb23a94f6b64, 306058f6-5d35-4b92-b682-a545846fa43e, 31b61fe9-2cbe-466f-8a45-40ebf7aae0f3, 369152d0-7562-47a5-b850-5d810db2b400, 38cb2bfd-8ab8-4e69-96f6-4b1f4ba16fb0, 3b318de0-d15b-4864-8d2a-ba5b89299fb1, 462e5afa-52b3-4683-b77e-e04b6bd09c36, 47a61168-91a8-40c0-a861-9fb58aa1d8e4, 519ae528-fffc-4b37-9e97-b33ecef53d0f, 54be29fe-d517-4c48-8409-a3123e7179e9, 550695aa-f184-47de-b451-4e3e55399f39, 57ffed01-33a7-4954-a952-7653c091d32c, 5a6359c8-6ec3-4f28-a3a2-97408a1f33f6, 5d7c6862-b0d0-4213-b0c6-b6f785b011ab, 5d9fed14-7a65-4852-938a-e550e73d115b, 5ec65a41-98b3-44ac-92a8-c21c1dfdb083, 636886ba-a1da-42f7-9b47-4488b0092f55, 6408d55d-d965-4101-b201-0b88e1faaa3e, 66358b1c-f1b9-4c02-b037-8ca5ec17c234, 695118e9-7823-448c-9f91-0bc07d3733b1, 6a3e6df1-fe03-428d-8c2a-bd02b813987c, 70453bfe-f5d3-42d1-824e-e916225312f2, 738fb0cf-a9cf-45d7-b458-d9619bb6757c, 86cf0712-6bda-47ab-a67d-962c4661ace3, 8c4d031c-2488-4829-9cbb-b928778025e1, 905aadd1-6e7a-48e8-b0c3-9dd5917fbedd, 91acc773-3592-4806-9e2f-5cebba2eb6dc, a08c9953-a9ff-4777-946a-ba6409310a62, a44135d6-a346-4504-a601-70c9814d8238, a6bb88a1-3ea7-4abc-aa19-af540ae6d905, b05fa0a5-e68b-47cf-872e-73bd722eea20, c183df6b-e46e-4a47-a52e-6156f3a44c2f, c2984195-3860-410b-9f09-97e5a3868e26, ca65f250-eb54-4b0c-95eb-2c38c017fdd2, d3b9df9b-2251-4616-bdab-6a92098ce8d4, d4259edd-027d-458c-8bd6-541526b54c0c, da343e25-03d7-4d8e-997a-353ee2423c45, da4e14db-28bb-4fd2-b10e-22270ecb460d, dfeee4cf-3cf4-489c-8b6e-641e5fa19d2a, ec2c1a35-d4e1-4346-8993-04d2a5d39626, ed769104-4cf4-4fcb-b775-652576181ce6, ee355ba7-cd4c-446e-90ac-476803f88f43, ee5b0345-7328-4e05-b78f-e5c710986647, ef33d004-8a32-4a2d-abd4-def11a8b9d3a, f2d82f28-281a-4ca0-89ff-118bf7966bc3, f37d336b-c3bf-4487-8073-417fd07ae315, f54f1aec-d7e4-44b2-b0b5-485dd81788c7]
191454 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 37 for /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_1_001.parquet
191470 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 62 row keys from /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_1_001.parquet
191470 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 37 results, for file /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_1_001.parquet => [02c34c5f-17ef-4e78-9a3f-5a4e4208fdea, 093922a8-a75d-4e51-a8ef-9bc2f139a978, 0bd41b9f-12c4-429f-a1a8-2ad4fde33142, 0d4db3f1-2cdd-4865-87ae-3601532c74f0, 11d28a3d-70e0-46ea-85af-b042915136b1, 13865402-377c-4a7f-bf73-f4e31a64f151, 1aabd278-b76c-4c91-a7e8-91b6ddc21702, 1bc87635-57ac-40e0-856c-f67f01ed7874, 20c72b70-fd4a-45db-a73a-447ed3b5573e, 21a8c27b-56e4-4184-a8d3-a312bac28447, 2344751b-202b-4494-9753-c9553a909745, 2617f2b1-43f6-42cc-8c6f-2d04e6b6475c, 2aadd268-8c17-46d6-9794-622c21fab557, 2ec5e860-900e-42f6-85c1-bd04d6398121, 332cc370-2e9b-4856-9a9a-7d9bbd151eef, 34b27174-f3fc-4a8f-a2b5-f21b328cc1b2, 36bdcbf1-c14a-410b-87bc-9ffa06f312ea, 471da1d5-df53-4b55-8572-924e31ce6065, 475bda76-fbb8-4932-b76c-a5b0c0bbbc55, 51fb0d34-e31f-496c-bf03-f13aa341f9b2, 56aeebb1-f54c-4833-9b66-9e30c7df5626, 5acc0325-f173-4436-86d2-5e35892e4dda, 5bd3c0b3-77d4-4b49-839b-926394f1a769, 5d5da3fa-03b3-479b-9889-51fcfb7780a1, 5d809042-f08c-4ea9-8022-59eee558cced, 5e616964-c5a7-45a0-aade-c06a71d0c3d4, 634b0ade-96c2-497c-a256-e3815db41311, 66f985cd-5b72-4629-b1be-7570fee043de, 6aa07d6f-ad5e-4fc6-a3b2-87efeff013d7, 6ad50be1-942c-48f1-bd09-04b5ab57c437, 714c7633-4ab8-450a-a241-4ae4c3e59b8c, 71e5721c-c481-4bc9-8d4e-bc834873303e, 7f04fafb-77d4-4d37-a1fe-8e681a57fbe4, 80d80a4a-9168-4131-b618-54e31d4e2322, 8993978d-a3dd-4352-ac05-b9743624636b, 90bf7bac-1b9a-4492-8b6d-22ee6c67db23, 963d928c-4944-4c6d-ae79-bd072f7dc8a7]
191521 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_1_001.parquet
191535 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 62 row keys from /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_1_001.parquet
191535 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_1_001.parquet => [9644003f-8aad-45c6-bdba-24c7192533d2, 9733fa97-2c53-4c54-a05f-fc4b69511cf4, 9808392e-16ab-4c77-a3c0-93e57a900462, 9dcf99da-7146-443d-885b-b54c0a343596, a0d9f885-4372-45d1-b998-679e47c640bd, a73ff970-6619-4483-98c4-8e4a79a30314, a7f3cf98-ada8-4c75-a5a6-58ec58842422, b2984f45-2aa3-4467-b0ec-c79feb7272b6, bc12cfdf-16d9-4000-a91d-102451233d91, c9b479f8-cb78-4db4-bdce-34b3ca50b821, ca7e94d1-3f85-4e50-97d3-acea9b03259f, cf073dd5-c0c2-4402-9873-10ab596dcf3a, d14dc6f7-96f4-4da5-b563-9008ccec7a34, d20bd071-01d4-476a-a945-eecdf0149e22, d549964b-3d58-4496-bb6b-4963ed38c621, d57f560e-4cff-4869-86ed-6adfd634ab56, d6bdacc1-f47a-4852-8823-24f26a04e685, d7dbf624-0492-49d4-935d-d97dd05cba4c, e446272f-e016-4fea-b99b-e19189366ad2, e9a19773-8818-40da-8f95-02621bf921f8, ef832b7a-fc56-4d56-8e1d-eb4e79fe151b, f00fc7e1-743a-45de-8627-c16076bfd28f, f39a647d-52b0-4728-801c-832517cea866, fa6273bc-f47b-4282-aa9a-54f82bbb7297, ff92d401-aa95-43fd-a92d-e54a1ecdaf48]
191555 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 84 for /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_001.parquet
191568 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 84 row keys from /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_001.parquet
191568 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 84 results, for file /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_001.parquet => [00bc19d4-5f64-4938-9e52-de99566511f4, 00d1d9c6-ce43-4a21-8748-a428f5db970f, 049dc209-e18e-472b-91fd-14f089d40509, 054a5ca8-b540-4157-b24a-66e50d4e0f13, 07637e30-642b-4ab2-9abb-1c59d05bd753, 0cf9135f-f41b-4186-adf2-c3a7a269adbc, 0e24eb59-edf4-4052-be6f-973394a30551, 147897ca-01e2-42a2-8406-05b155026161, 158bcd77-8136-4440-80ad-9a082f9725ec, 184991a5-0f28-405b-80c9-a925a8cbf410, 19c0c648-1c7e-46c1-8d82-f950f7831f99, 22474b77-3fed-4043-9148-d77278fde68b, 26b1f3a0-238a-4a6a-bb7c-71412b6ca773, 280c42ec-3eaa-4af7-a3f9-286e31d8d07b, 2922ed51-c25c-4c3e-98a9-7d21f97a759e, 2b4d25b4-7db7-427c-aebf-d1590633f10f, 3463d2ce-72c1-42b2-ac55-fc91b36e42fe, 35300edb-3492-47c5-a1ed-8613abe04133, 35680009-403f-4472-b1ae-3040209cf6e8, 366c4dcc-0416-4324-bad5-7d787e64a00e, 383ecf3a-a51c-4a87-9fa6-21b385591c66, 3a9b5f35-c255-4624-83cb-7bb31cb5f006, 3b65b84a-86de-4a38-9e38-2d1cd897bc44, 42b64223-be3c-45a8-b63b-9a40226e4494, 4cc469d3-ea4c-499f-a16b-174cc66b7608, 5014fa44-629f-4f45-aba4-8c86532babf2, 52680904-ae87-47db-95e0-6f1269ac56a8, 564306a3-c0d8-4718-bb1e-1324bb7a9f4a, 56862dd4-1311-4b8a-be86-08d89c01d0b7, 5817faf5-9965-49e3-a308-6d64566ace77, 59b63a13-0e06-41ef-861b-67f674628554, 5b65be6c-a3ef-4419-8ff2-dda0defaa9be, 5ce35e13-a6cf-4114-a398-512404a59e06, 6204bdd6-eafc-4cfa-b6fa-0aa9b9cd6ca3, 63b57f6f-1ba8-494c-a93c-2970d3132baa, 68698cd7-4048-4560-8886-a2bf81552148, 68b7da57-45b1-42de-a7b6-0c5ac507f75a, 692a6095-7f8e-4b6e-91c3-29395a8820bf, 69365c11-c6fc-4fa4-826f-df0af3554a60, 73b23002-090d-4960-91cf-6f10e14a4c48, 7597993f-f527-4d86-92f3-edb3df5e6207, 77f7ac15-576e-41c1-90dc-fc2ce97d5572, 780bdbc0-ceee-4432-95aa-4bb9b8b42c9e, 7f2b42ae-eaa7-4ba4-a447-c0e452178690, 86725f90-e1da-4455-8ab7-3f80817910e2, 8b2e1a1b-dd7f-4407-bf2c-3fc9b45d79fc, 96a35361-dec6-473e-9262-c39f8fa45e93, 97935bba-79c5-4748-a703-b0f7c905cc34, 9a1a493f-31ea-442b-a6ce-3fadd384e6ba, 9fb20966-0049-45e7-b5a5-f6937d475b46, a2f456c2-567d-4a4a-b821-ddbeade6e6b1, a67217a6-3a18-435c-b57d-c57f03c13373, addb61d7-c5a7-4f9e-8c06-7e20fe3c1c1e, af723071-b719-418d-bd22-2be23e22d37d, b33f6eac-eb37-4f2b-adfd-fc2bbdd4a247, b39c7f17-941e-491f-b66c-9a7ac772adb9, b42654a1-6264-40ca-a4ec-1ba66d993825, b42d129e-5cf5-4558-8038-56c6ac8d634d, b5d06f3e-1136-4145-82b8-95c63caf6725, b6987eef-89d6-437f-98fc-81e0f7ee1eb2, b78323b7-8851-47f8-85ab-3e4a5c410905, b83ddae4-d0d8-44bb-a303-d1e7f1af5978, b8d4cfb3-0219-46c9-9798-1b0071452ace, b90e5fdb-09e4-4f49-9a19-33869cae1381, be4046c8-9ace-42ae-ac37-24be041725bd, bf362e54-8406-4c91-9b2c-9626d275add8, c3a5e91b-531e-4cf1-8264-39d029332cf9, c5e016b7-d533-46aa-b155-71cb340b89ef, c74c2a62-71f6-4450-b6c2-a2b55b295e8b, cc5bd3fa-dd09-4f96-8753-0dcf6493db28, ce68e6fa-692d-4c75-8958-8b2f5a350ee5, ce6e70a8-b1c3-45a4-910a-6a0f3ff75017, d60e843c-6758-496a-9b5d-e8e9b7703ccf, d646ca6a-4f78-4c61-b978-eacd638d01f6, e4172159-ec1d-4ae5-8851-9052f2625a69, e4cfa018-27bf-45ff-affb-c48e573b54a3, e60a7706-0802-4c99-892f-a1a470247c57, e6539c15-e777-4f73-afaf-a2823f8fd7ce, e8554207-f207-446e-9a03-89482bc75d94, f1c619ed-a1f0-48af-b2e2-9ba6a3f2c987, f47c09dd-3f35-4236-a57b-c89d24a82ac1, f6250065-4dfc-4ba9-9ca1-679a5bd60222, fea0ae4f-f39c-414e-99ed-43c6116939d5, ff0eef24-320e-40cf-82a5-875469df2ea7]
191709 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=54}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=62}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=84}}}
191720 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
191720 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=626030cb-22ae-45a1-8ae4-437f1d4aac24}, 1=BucketInfo {bucketType=UPDATE, fileLoc=27bc806e-c63e-4d32-8efa-45f7ba70a983}, 2=BucketInfo {bucketType=UPDATE, fileLoc=cb85a420-b139-45e0-b5fc-219f25531612}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{626030cb-22ae-45a1-8ae4-437f1d4aac24=0, 27bc806e-c63e-4d32-8efa-45f7ba70a983=1, cb85a420-b139-45e0-b5fc-219f25531612=2}
191734 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
191734 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
192148 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
192148 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
192258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
192258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
192411 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
192418 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
192418 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
192504 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepointing latest commit 002
192675 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
192702 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
192707 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2016/03/15
192708 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/16
192708 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/17
192721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepoint 002 created
192721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
193355 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
193355 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
193586 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 54 for /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_002.parquet
193599 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_002.parquet
193599 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 54 results, for file /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_002.parquet => [0dbaf990-11a1-42cf-912b-a5f236cec880, 119e2e6b-df29-4985-b7a5-74dee30c88a1, 18adb7b0-e1fe-417a-9d00-cb99aed45a91, 1a0a95a6-9694-46ec-838f-c254494c11d8, 1da77d30-d54f-43df-865f-02c64210c8d8, 2ad79f12-d408-4aba-8650-7fc82c6b5457, 2d04b4c7-d5a6-4371-b2db-4c5df466f7b5, 2f3d75e6-9699-426e-8df0-fb23a94f6b64, 306058f6-5d35-4b92-b682-a545846fa43e, 31b61fe9-2cbe-466f-8a45-40ebf7aae0f3, 369152d0-7562-47a5-b850-5d810db2b400, 38cb2bfd-8ab8-4e69-96f6-4b1f4ba16fb0, 3b318de0-d15b-4864-8d2a-ba5b89299fb1, 462e5afa-52b3-4683-b77e-e04b6bd09c36, 47a61168-91a8-40c0-a861-9fb58aa1d8e4, 519ae528-fffc-4b37-9e97-b33ecef53d0f, 54be29fe-d517-4c48-8409-a3123e7179e9, 550695aa-f184-47de-b451-4e3e55399f39, 57ffed01-33a7-4954-a952-7653c091d32c, 5a6359c8-6ec3-4f28-a3a2-97408a1f33f6, 5d7c6862-b0d0-4213-b0c6-b6f785b011ab, 5d9fed14-7a65-4852-938a-e550e73d115b, 5ec65a41-98b3-44ac-92a8-c21c1dfdb083, 636886ba-a1da-42f7-9b47-4488b0092f55, 6408d55d-d965-4101-b201-0b88e1faaa3e, 66358b1c-f1b9-4c02-b037-8ca5ec17c234, 695118e9-7823-448c-9f91-0bc07d3733b1, 6a3e6df1-fe03-428d-8c2a-bd02b813987c, 70453bfe-f5d3-42d1-824e-e916225312f2, 738fb0cf-a9cf-45d7-b458-d9619bb6757c, 86cf0712-6bda-47ab-a67d-962c4661ace3, 8c4d031c-2488-4829-9cbb-b928778025e1, 905aadd1-6e7a-48e8-b0c3-9dd5917fbedd, 91acc773-3592-4806-9e2f-5cebba2eb6dc, a08c9953-a9ff-4777-946a-ba6409310a62, a44135d6-a346-4504-a601-70c9814d8238, a6bb88a1-3ea7-4abc-aa19-af540ae6d905, b05fa0a5-e68b-47cf-872e-73bd722eea20, c183df6b-e46e-4a47-a52e-6156f3a44c2f, c2984195-3860-410b-9f09-97e5a3868e26, ca65f250-eb54-4b0c-95eb-2c38c017fdd2, d3b9df9b-2251-4616-bdab-6a92098ce8d4, d4259edd-027d-458c-8bd6-541526b54c0c, da343e25-03d7-4d8e-997a-353ee2423c45, da4e14db-28bb-4fd2-b10e-22270ecb460d, dfeee4cf-3cf4-489c-8b6e-641e5fa19d2a, ec2c1a35-d4e1-4346-8993-04d2a5d39626, ed769104-4cf4-4fcb-b775-652576181ce6, ee355ba7-cd4c-446e-90ac-476803f88f43, ee5b0345-7328-4e05-b78f-e5c710986647, ef33d004-8a32-4a2d-abd4-def11a8b9d3a, f2d82f28-281a-4ca0-89ff-118bf7966bc3, f37d336b-c3bf-4487-8073-417fd07ae315, f54f1aec-d7e4-44b2-b0b5-485dd81788c7]
193616 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 50 for /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_002.parquet
193627 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 62 row keys from /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_002.parquet
193627 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 50 results, for file /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_002.parquet => [02c34c5f-17ef-4e78-9a3f-5a4e4208fdea, 093922a8-a75d-4e51-a8ef-9bc2f139a978, 0bd41b9f-12c4-429f-a1a8-2ad4fde33142, 0d4db3f1-2cdd-4865-87ae-3601532c74f0, 11d28a3d-70e0-46ea-85af-b042915136b1, 13865402-377c-4a7f-bf73-f4e31a64f151, 1aabd278-b76c-4c91-a7e8-91b6ddc21702, 1bc87635-57ac-40e0-856c-f67f01ed7874, 20c72b70-fd4a-45db-a73a-447ed3b5573e, 21a8c27b-56e4-4184-a8d3-a312bac28447, 2344751b-202b-4494-9753-c9553a909745, 2617f2b1-43f6-42cc-8c6f-2d04e6b6475c, 2aadd268-8c17-46d6-9794-622c21fab557, 2ec5e860-900e-42f6-85c1-bd04d6398121, 332cc370-2e9b-4856-9a9a-7d9bbd151eef, 34b27174-f3fc-4a8f-a2b5-f21b328cc1b2, 36bdcbf1-c14a-410b-87bc-9ffa06f312ea, 471da1d5-df53-4b55-8572-924e31ce6065, 475bda76-fbb8-4932-b76c-a5b0c0bbbc55, 51fb0d34-e31f-496c-bf03-f13aa341f9b2, 56aeebb1-f54c-4833-9b66-9e30c7df5626, 5acc0325-f173-4436-86d2-5e35892e4dda, 5bd3c0b3-77d4-4b49-839b-926394f1a769, 5d5da3fa-03b3-479b-9889-51fcfb7780a1, 5d809042-f08c-4ea9-8022-59eee558cced, 5e616964-c5a7-45a0-aade-c06a71d0c3d4, 634b0ade-96c2-497c-a256-e3815db41311, 66f985cd-5b72-4629-b1be-7570fee043de, 6aa07d6f-ad5e-4fc6-a3b2-87efeff013d7, 6ad50be1-942c-48f1-bd09-04b5ab57c437, 714c7633-4ab8-450a-a241-4ae4c3e59b8c, 71e5721c-c481-4bc9-8d4e-bc834873303e, 7f04fafb-77d4-4d37-a1fe-8e681a57fbe4, 80d80a4a-9168-4131-b618-54e31d4e2322, 8993978d-a3dd-4352-ac05-b9743624636b, 90bf7bac-1b9a-4492-8b6d-22ee6c67db23, 963d928c-4944-4c6d-ae79-bd072f7dc8a7, 9644003f-8aad-45c6-bdba-24c7192533d2, 9733fa97-2c53-4c54-a05f-fc4b69511cf4, 9808392e-16ab-4c77-a3c0-93e57a900462, 9dcf99da-7146-443d-885b-b54c0a343596, a0d9f885-4372-45d1-b998-679e47c640bd, a73ff970-6619-4483-98c4-8e4a79a30314, a7f3cf98-ada8-4c75-a5a6-58ec58842422, b2984f45-2aa3-4467-b0ec-c79feb7272b6, bc12cfdf-16d9-4000-a91d-102451233d91, c9b479f8-cb78-4db4-bdce-34b3ca50b821, ca7e94d1-3f85-4e50-97d3-acea9b03259f, cf073dd5-c0c2-4402-9873-10ab596dcf3a, d14dc6f7-96f4-4da5-b563-9008ccec7a34]
193658 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_002.parquet
193669 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 62 row keys from /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_002.parquet
193669 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_002.parquet => [d20bd071-01d4-476a-a945-eecdf0149e22, d549964b-3d58-4496-bb6b-4963ed38c621, d57f560e-4cff-4869-86ed-6adfd634ab56, d6bdacc1-f47a-4852-8823-24f26a04e685, d7dbf624-0492-49d4-935d-d97dd05cba4c, e446272f-e016-4fea-b99b-e19189366ad2, e9a19773-8818-40da-8f95-02621bf921f8, ef832b7a-fc56-4d56-8e1d-eb4e79fe151b, f00fc7e1-743a-45de-8627-c16076bfd28f, f39a647d-52b0-4728-801c-832517cea866, fa6273bc-f47b-4282-aa9a-54f82bbb7297, ff92d401-aa95-43fd-a92d-e54a1ecdaf48]
193685 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 84 for /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_002.parquet
193694 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 84 row keys from /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_002.parquet
193695 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 84 results, for file /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_002.parquet => [00bc19d4-5f64-4938-9e52-de99566511f4, 00d1d9c6-ce43-4a21-8748-a428f5db970f, 049dc209-e18e-472b-91fd-14f089d40509, 054a5ca8-b540-4157-b24a-66e50d4e0f13, 07637e30-642b-4ab2-9abb-1c59d05bd753, 0cf9135f-f41b-4186-adf2-c3a7a269adbc, 0e24eb59-edf4-4052-be6f-973394a30551, 147897ca-01e2-42a2-8406-05b155026161, 158bcd77-8136-4440-80ad-9a082f9725ec, 184991a5-0f28-405b-80c9-a925a8cbf410, 19c0c648-1c7e-46c1-8d82-f950f7831f99, 22474b77-3fed-4043-9148-d77278fde68b, 26b1f3a0-238a-4a6a-bb7c-71412b6ca773, 280c42ec-3eaa-4af7-a3f9-286e31d8d07b, 2922ed51-c25c-4c3e-98a9-7d21f97a759e, 2b4d25b4-7db7-427c-aebf-d1590633f10f, 3463d2ce-72c1-42b2-ac55-fc91b36e42fe, 35300edb-3492-47c5-a1ed-8613abe04133, 35680009-403f-4472-b1ae-3040209cf6e8, 366c4dcc-0416-4324-bad5-7d787e64a00e, 383ecf3a-a51c-4a87-9fa6-21b385591c66, 3a9b5f35-c255-4624-83cb-7bb31cb5f006, 3b65b84a-86de-4a38-9e38-2d1cd897bc44, 42b64223-be3c-45a8-b63b-9a40226e4494, 4cc469d3-ea4c-499f-a16b-174cc66b7608, 5014fa44-629f-4f45-aba4-8c86532babf2, 52680904-ae87-47db-95e0-6f1269ac56a8, 564306a3-c0d8-4718-bb1e-1324bb7a9f4a, 56862dd4-1311-4b8a-be86-08d89c01d0b7, 5817faf5-9965-49e3-a308-6d64566ace77, 59b63a13-0e06-41ef-861b-67f674628554, 5b65be6c-a3ef-4419-8ff2-dda0defaa9be, 5ce35e13-a6cf-4114-a398-512404a59e06, 6204bdd6-eafc-4cfa-b6fa-0aa9b9cd6ca3, 63b57f6f-1ba8-494c-a93c-2970d3132baa, 68698cd7-4048-4560-8886-a2bf81552148, 68b7da57-45b1-42de-a7b6-0c5ac507f75a, 692a6095-7f8e-4b6e-91c3-29395a8820bf, 69365c11-c6fc-4fa4-826f-df0af3554a60, 73b23002-090d-4960-91cf-6f10e14a4c48, 7597993f-f527-4d86-92f3-edb3df5e6207, 77f7ac15-576e-41c1-90dc-fc2ce97d5572, 780bdbc0-ceee-4432-95aa-4bb9b8b42c9e, 7f2b42ae-eaa7-4ba4-a447-c0e452178690, 86725f90-e1da-4455-8ab7-3f80817910e2, 8b2e1a1b-dd7f-4407-bf2c-3fc9b45d79fc, 96a35361-dec6-473e-9262-c39f8fa45e93, 97935bba-79c5-4748-a703-b0f7c905cc34, 9a1a493f-31ea-442b-a6ce-3fadd384e6ba, 9fb20966-0049-45e7-b5a5-f6937d475b46, a2f456c2-567d-4a4a-b821-ddbeade6e6b1, a67217a6-3a18-435c-b57d-c57f03c13373, addb61d7-c5a7-4f9e-8c06-7e20fe3c1c1e, af723071-b719-418d-bd22-2be23e22d37d, b33f6eac-eb37-4f2b-adfd-fc2bbdd4a247, b39c7f17-941e-491f-b66c-9a7ac772adb9, b42654a1-6264-40ca-a4ec-1ba66d993825, b42d129e-5cf5-4558-8038-56c6ac8d634d, b5d06f3e-1136-4145-82b8-95c63caf6725, b6987eef-89d6-437f-98fc-81e0f7ee1eb2, b78323b7-8851-47f8-85ab-3e4a5c410905, b83ddae4-d0d8-44bb-a303-d1e7f1af5978, b8d4cfb3-0219-46c9-9798-1b0071452ace, b90e5fdb-09e4-4f49-9a19-33869cae1381, be4046c8-9ace-42ae-ac37-24be041725bd, bf362e54-8406-4c91-9b2c-9626d275add8, c3a5e91b-531e-4cf1-8264-39d029332cf9, c5e016b7-d533-46aa-b155-71cb340b89ef, c74c2a62-71f6-4450-b6c2-a2b55b295e8b, cc5bd3fa-dd09-4f96-8753-0dcf6493db28, ce68e6fa-692d-4c75-8958-8b2f5a350ee5, ce6e70a8-b1c3-45a4-910a-6a0f3ff75017, d60e843c-6758-496a-9b5d-e8e9b7703ccf, d646ca6a-4f78-4c61-b978-eacd638d01f6, e4172159-ec1d-4ae5-8851-9052f2625a69, e4cfa018-27bf-45ff-affb-c48e573b54a3, e60a7706-0802-4c99-892f-a1a470247c57, e6539c15-e777-4f73-afaf-a2823f8fd7ce, e8554207-f207-446e-9a03-89482bc75d94, f1c619ed-a1f0-48af-b2e2-9ba6a3f2c987, f47c09dd-3f35-4236-a57b-c89d24a82ac1, f6250065-4dfc-4ba9-9ca1-679a5bd60222, fea0ae4f-f39c-414e-99ed-43c6116939d5, ff0eef24-320e-40cf-82a5-875469df2ea7]
193792 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
193793 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=54}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=62}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=84}}}
193807 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
193807 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=626030cb-22ae-45a1-8ae4-437f1d4aac24}, 1=BucketInfo {bucketType=UPDATE, fileLoc=27bc806e-c63e-4d32-8efa-45f7ba70a983}, 2=BucketInfo {bucketType=UPDATE, fileLoc=cb85a420-b139-45e0-b5fc-219f25531612}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{626030cb-22ae-45a1-8ae4-437f1d4aac24=0, 27bc806e-c63e-4d32-8efa-45f7ba70a983=1, cb85a420-b139-45e0-b5fc-219f25531612=2}
193867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
193867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
194386 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
194417 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
194417 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
194548 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
194548 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
194834 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
194841 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
194841 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
195128 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
195715 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
195771 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
195771 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
196033 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 54 for /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_003.parquet
196047 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_003.parquet
196047 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 54 results, for file /tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_003.parquet => [0dbaf990-11a1-42cf-912b-a5f236cec880, 119e2e6b-df29-4985-b7a5-74dee30c88a1, 18adb7b0-e1fe-417a-9d00-cb99aed45a91, 1a0a95a6-9694-46ec-838f-c254494c11d8, 1da77d30-d54f-43df-865f-02c64210c8d8, 2ad79f12-d408-4aba-8650-7fc82c6b5457, 2d04b4c7-d5a6-4371-b2db-4c5df466f7b5, 2f3d75e6-9699-426e-8df0-fb23a94f6b64, 306058f6-5d35-4b92-b682-a545846fa43e, 31b61fe9-2cbe-466f-8a45-40ebf7aae0f3, 369152d0-7562-47a5-b850-5d810db2b400, 38cb2bfd-8ab8-4e69-96f6-4b1f4ba16fb0, 3b318de0-d15b-4864-8d2a-ba5b89299fb1, 462e5afa-52b3-4683-b77e-e04b6bd09c36, 47a61168-91a8-40c0-a861-9fb58aa1d8e4, 519ae528-fffc-4b37-9e97-b33ecef53d0f, 54be29fe-d517-4c48-8409-a3123e7179e9, 550695aa-f184-47de-b451-4e3e55399f39, 57ffed01-33a7-4954-a952-7653c091d32c, 5a6359c8-6ec3-4f28-a3a2-97408a1f33f6, 5d7c6862-b0d0-4213-b0c6-b6f785b011ab, 5d9fed14-7a65-4852-938a-e550e73d115b, 5ec65a41-98b3-44ac-92a8-c21c1dfdb083, 636886ba-a1da-42f7-9b47-4488b0092f55, 6408d55d-d965-4101-b201-0b88e1faaa3e, 66358b1c-f1b9-4c02-b037-8ca5ec17c234, 695118e9-7823-448c-9f91-0bc07d3733b1, 6a3e6df1-fe03-428d-8c2a-bd02b813987c, 70453bfe-f5d3-42d1-824e-e916225312f2, 738fb0cf-a9cf-45d7-b458-d9619bb6757c, 86cf0712-6bda-47ab-a67d-962c4661ace3, 8c4d031c-2488-4829-9cbb-b928778025e1, 905aadd1-6e7a-48e8-b0c3-9dd5917fbedd, 91acc773-3592-4806-9e2f-5cebba2eb6dc, a08c9953-a9ff-4777-946a-ba6409310a62, a44135d6-a346-4504-a601-70c9814d8238, a6bb88a1-3ea7-4abc-aa19-af540ae6d905, b05fa0a5-e68b-47cf-872e-73bd722eea20, c183df6b-e46e-4a47-a52e-6156f3a44c2f, c2984195-3860-410b-9f09-97e5a3868e26, ca65f250-eb54-4b0c-95eb-2c38c017fdd2, d3b9df9b-2251-4616-bdab-6a92098ce8d4, d4259edd-027d-458c-8bd6-541526b54c0c, da343e25-03d7-4d8e-997a-353ee2423c45, da4e14db-28bb-4fd2-b10e-22270ecb460d, dfeee4cf-3cf4-489c-8b6e-641e5fa19d2a, ec2c1a35-d4e1-4346-8993-04d2a5d39626, ed769104-4cf4-4fcb-b775-652576181ce6, ee355ba7-cd4c-446e-90ac-476803f88f43, ee5b0345-7328-4e05-b78f-e5c710986647, ef33d004-8a32-4a2d-abd4-def11a8b9d3a, f2d82f28-281a-4ca0-89ff-118bf7966bc3, f37d336b-c3bf-4487-8073-417fd07ae315, f54f1aec-d7e4-44b2-b0b5-485dd81788c7]
196068 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 62 for /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_003.parquet
196080 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 62 row keys from /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_003.parquet
196080 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 62 results, for file /tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_003.parquet => [02c34c5f-17ef-4e78-9a3f-5a4e4208fdea, 093922a8-a75d-4e51-a8ef-9bc2f139a978, 0bd41b9f-12c4-429f-a1a8-2ad4fde33142, 0d4db3f1-2cdd-4865-87ae-3601532c74f0, 11d28a3d-70e0-46ea-85af-b042915136b1, 13865402-377c-4a7f-bf73-f4e31a64f151, 1aabd278-b76c-4c91-a7e8-91b6ddc21702, 1bc87635-57ac-40e0-856c-f67f01ed7874, 20c72b70-fd4a-45db-a73a-447ed3b5573e, 21a8c27b-56e4-4184-a8d3-a312bac28447, 2344751b-202b-4494-9753-c9553a909745, 2617f2b1-43f6-42cc-8c6f-2d04e6b6475c, 2aadd268-8c17-46d6-9794-622c21fab557, 2ec5e860-900e-42f6-85c1-bd04d6398121, 332cc370-2e9b-4856-9a9a-7d9bbd151eef, 34b27174-f3fc-4a8f-a2b5-f21b328cc1b2, 36bdcbf1-c14a-410b-87bc-9ffa06f312ea, 471da1d5-df53-4b55-8572-924e31ce6065, 475bda76-fbb8-4932-b76c-a5b0c0bbbc55, 51fb0d34-e31f-496c-bf03-f13aa341f9b2, 56aeebb1-f54c-4833-9b66-9e30c7df5626, 5acc0325-f173-4436-86d2-5e35892e4dda, 5bd3c0b3-77d4-4b49-839b-926394f1a769, 5d5da3fa-03b3-479b-9889-51fcfb7780a1, 5d809042-f08c-4ea9-8022-59eee558cced, 5e616964-c5a7-45a0-aade-c06a71d0c3d4, 634b0ade-96c2-497c-a256-e3815db41311, 66f985cd-5b72-4629-b1be-7570fee043de, 6aa07d6f-ad5e-4fc6-a3b2-87efeff013d7, 6ad50be1-942c-48f1-bd09-04b5ab57c437, 714c7633-4ab8-450a-a241-4ae4c3e59b8c, 71e5721c-c481-4bc9-8d4e-bc834873303e, 7f04fafb-77d4-4d37-a1fe-8e681a57fbe4, 80d80a4a-9168-4131-b618-54e31d4e2322, 8993978d-a3dd-4352-ac05-b9743624636b, 90bf7bac-1b9a-4492-8b6d-22ee6c67db23, 963d928c-4944-4c6d-ae79-bd072f7dc8a7, 9644003f-8aad-45c6-bdba-24c7192533d2, 9733fa97-2c53-4c54-a05f-fc4b69511cf4, 9808392e-16ab-4c77-a3c0-93e57a900462, 9dcf99da-7146-443d-885b-b54c0a343596, a0d9f885-4372-45d1-b998-679e47c640bd, a73ff970-6619-4483-98c4-8e4a79a30314, a7f3cf98-ada8-4c75-a5a6-58ec58842422, b2984f45-2aa3-4467-b0ec-c79feb7272b6, bc12cfdf-16d9-4000-a91d-102451233d91, c9b479f8-cb78-4db4-bdce-34b3ca50b821, ca7e94d1-3f85-4e50-97d3-acea9b03259f, cf073dd5-c0c2-4402-9873-10ab596dcf3a, d14dc6f7-96f4-4da5-b563-9008ccec7a34, d20bd071-01d4-476a-a945-eecdf0149e22, d549964b-3d58-4496-bb6b-4963ed38c621, d57f560e-4cff-4869-86ed-6adfd634ab56, d6bdacc1-f47a-4852-8823-24f26a04e685, d7dbf624-0492-49d4-935d-d97dd05cba4c, e446272f-e016-4fea-b99b-e19189366ad2, e9a19773-8818-40da-8f95-02621bf921f8, ef832b7a-fc56-4d56-8e1d-eb4e79fe151b, f00fc7e1-743a-45de-8627-c16076bfd28f, f39a647d-52b0-4728-801c-832517cea866, fa6273bc-f47b-4282-aa9a-54f82bbb7297, ff92d401-aa95-43fd-a92d-e54a1ecdaf48]
196119 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 84 for /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_003.parquet
196133 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 84 row keys from /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_003.parquet
196133 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 84 results, for file /tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_003.parquet => [00bc19d4-5f64-4938-9e52-de99566511f4, 00d1d9c6-ce43-4a21-8748-a428f5db970f, 049dc209-e18e-472b-91fd-14f089d40509, 054a5ca8-b540-4157-b24a-66e50d4e0f13, 07637e30-642b-4ab2-9abb-1c59d05bd753, 0cf9135f-f41b-4186-adf2-c3a7a269adbc, 0e24eb59-edf4-4052-be6f-973394a30551, 147897ca-01e2-42a2-8406-05b155026161, 158bcd77-8136-4440-80ad-9a082f9725ec, 184991a5-0f28-405b-80c9-a925a8cbf410, 19c0c648-1c7e-46c1-8d82-f950f7831f99, 22474b77-3fed-4043-9148-d77278fde68b, 26b1f3a0-238a-4a6a-bb7c-71412b6ca773, 280c42ec-3eaa-4af7-a3f9-286e31d8d07b, 2922ed51-c25c-4c3e-98a9-7d21f97a759e, 2b4d25b4-7db7-427c-aebf-d1590633f10f, 3463d2ce-72c1-42b2-ac55-fc91b36e42fe, 35300edb-3492-47c5-a1ed-8613abe04133, 35680009-403f-4472-b1ae-3040209cf6e8, 366c4dcc-0416-4324-bad5-7d787e64a00e, 383ecf3a-a51c-4a87-9fa6-21b385591c66, 3a9b5f35-c255-4624-83cb-7bb31cb5f006, 3b65b84a-86de-4a38-9e38-2d1cd897bc44, 42b64223-be3c-45a8-b63b-9a40226e4494, 4cc469d3-ea4c-499f-a16b-174cc66b7608, 5014fa44-629f-4f45-aba4-8c86532babf2, 52680904-ae87-47db-95e0-6f1269ac56a8, 564306a3-c0d8-4718-bb1e-1324bb7a9f4a, 56862dd4-1311-4b8a-be86-08d89c01d0b7, 5817faf5-9965-49e3-a308-6d64566ace77, 59b63a13-0e06-41ef-861b-67f674628554, 5b65be6c-a3ef-4419-8ff2-dda0defaa9be, 5ce35e13-a6cf-4114-a398-512404a59e06, 6204bdd6-eafc-4cfa-b6fa-0aa9b9cd6ca3, 63b57f6f-1ba8-494c-a93c-2970d3132baa, 68698cd7-4048-4560-8886-a2bf81552148, 68b7da57-45b1-42de-a7b6-0c5ac507f75a, 692a6095-7f8e-4b6e-91c3-29395a8820bf, 69365c11-c6fc-4fa4-826f-df0af3554a60, 73b23002-090d-4960-91cf-6f10e14a4c48, 7597993f-f527-4d86-92f3-edb3df5e6207, 77f7ac15-576e-41c1-90dc-fc2ce97d5572, 780bdbc0-ceee-4432-95aa-4bb9b8b42c9e, 7f2b42ae-eaa7-4ba4-a447-c0e452178690, 86725f90-e1da-4455-8ab7-3f80817910e2, 8b2e1a1b-dd7f-4407-bf2c-3fc9b45d79fc, 96a35361-dec6-473e-9262-c39f8fa45e93, 97935bba-79c5-4748-a703-b0f7c905cc34, 9a1a493f-31ea-442b-a6ce-3fadd384e6ba, 9fb20966-0049-45e7-b5a5-f6937d475b46, a2f456c2-567d-4a4a-b821-ddbeade6e6b1, a67217a6-3a18-435c-b57d-c57f03c13373, addb61d7-c5a7-4f9e-8c06-7e20fe3c1c1e, af723071-b719-418d-bd22-2be23e22d37d, b33f6eac-eb37-4f2b-adfd-fc2bbdd4a247, b39c7f17-941e-491f-b66c-9a7ac772adb9, b42654a1-6264-40ca-a4ec-1ba66d993825, b42d129e-5cf5-4558-8038-56c6ac8d634d, b5d06f3e-1136-4145-82b8-95c63caf6725, b6987eef-89d6-437f-98fc-81e0f7ee1eb2, b78323b7-8851-47f8-85ab-3e4a5c410905, b83ddae4-d0d8-44bb-a303-d1e7f1af5978, b8d4cfb3-0219-46c9-9798-1b0071452ace, b90e5fdb-09e4-4f49-9a19-33869cae1381, be4046c8-9ace-42ae-ac37-24be041725bd, bf362e54-8406-4c91-9b2c-9626d275add8, c3a5e91b-531e-4cf1-8264-39d029332cf9, c5e016b7-d533-46aa-b155-71cb340b89ef, c74c2a62-71f6-4450-b6c2-a2b55b295e8b, cc5bd3fa-dd09-4f96-8753-0dcf6493db28, ce68e6fa-692d-4c75-8958-8b2f5a350ee5, ce6e70a8-b1c3-45a4-910a-6a0f3ff75017, d60e843c-6758-496a-9b5d-e8e9b7703ccf, d646ca6a-4f78-4c61-b978-eacd638d01f6, e4172159-ec1d-4ae5-8851-9052f2625a69, e4cfa018-27bf-45ff-affb-c48e573b54a3, e60a7706-0802-4c99-892f-a1a470247c57, e6539c15-e777-4f73-afaf-a2823f8fd7ce, e8554207-f207-446e-9a03-89482bc75d94, f1c619ed-a1f0-48af-b2e2-9ba6a3f2c987, f47c09dd-3f35-4236-a57b-c89d24a82ac1, f6250065-4dfc-4ba9-9ca1-679a5bd60222, fea0ae4f-f39c-414e-99ed-43c6116939d5, ff0eef24-320e-40cf-82a5-875469df2ea7]
196219 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=54}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=62}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=84}}}
196232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
196232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=626030cb-22ae-45a1-8ae4-437f1d4aac24}, 1=BucketInfo {bucketType=UPDATE, fileLoc=27bc806e-c63e-4d32-8efa-45f7ba70a983}, 2=BucketInfo {bucketType=UPDATE, fileLoc=cb85a420-b139-45e0-b5fc-219f25531612}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{626030cb-22ae-45a1-8ae4-437f1d4aac24=0, 27bc806e-c63e-4d32-8efa-45f7ba70a983=1, cb85a420-b139-45e0-b5fc-219f25531612=2}
196236 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
196282 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
196282 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
197235 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
197235 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
197402 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
197403 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
197571 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
197577 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
197577 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
197670 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Rolling back commits [003, 004]
197671 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [003, 004]
197671 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [003, 004]
197761 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
197827 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
197900 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
197901 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_003.parquet	true
197901 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8977063781363289417/2016/03/15/27bc806e-c63e-4d32-8efa-45f7ba70a983_1_004.parquet	true
197901 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
197902 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_003.parquet	true
197902 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8977063781363289417/2015/03/16/626030cb-22ae-45a1-8ae4-437f1d4aac24_0_004.parquet	true
197902 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
197903 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_003.parquet	true
197903 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8977063781363289417/2015/03/17/cb85a420-b139-45e0-b5fc-219f25531612_2_004.parquet	true
197908 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [003, 004]
197908 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [003, 004]
197915 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [003, 004] rollback is complete
197915 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
198026 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
198026 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
198172 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
198173 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8109413565877470551/2016/05/01/id31_1_20160506030611.parquet	true
198173 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
198173 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8109413565877470551/2016/05/02/id32_1_20160506030611.parquet	true
198173 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
198173 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8109413565877470551/2016/05/06/id33_1_20160506030611.parquet	true
198178 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
198178 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
198182 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
198183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
198183 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
198325 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
198326 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8109413565877470551/2016/05/01/id21_1_20160502020601.parquet	true
198326 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
198326 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8109413565877470551/2016/05/02/id22_1_20160502020601.parquet	true
198326 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
198326 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit8109413565877470551/2016/05/06/id23_1_20160502020601.parquet	true
198332 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
198332 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
198338 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
198415 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
198415 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
198553 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
198555 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
198555 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
198982 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/30a756d5-9e55-497e-826b-5e49bdd5e88c_1_000.parquet	true
199119 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/6c89856b-d73e-4621-8824-00a569b396f8_1_000.parquet	true
199190 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
199252 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/a7d041a1-4913-48f3-a326-66e678a19ccf_1_000.parquet	true
199270 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
199386 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/1783fd41-a890-4b3e-9235-1015b6d8377d_1_000.parquet	true
199516 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/08fa96e2-cd21-4d1b-ad8d-b4fc66eae48a_1_000.parquet	true
199650 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/911644cd-3f94-469c-ae21-d35723177305_1_000.parquet	true
199795 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/0cfaaa8f-66a3-4f3c-9bb2-77b86c43c173_1_000.parquet	true
199936 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/04426b91-1e4d-4428-8142-f5792be65fb4_1_000.parquet	true
200092 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/ec64a76f-970a-4679-9a6d-6dc468737267_1_000.parquet	true
200275 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3450882846581021568/.hoodie/.temp/ccc05101-80d7-43bc-a9b7-80c673f87856_1_000.parquet	true
200278 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
Tests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 144.371 sec - in com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
Running com.uber.hoodie.TestMultiFS
Formatting using clusterid: testClusterID
201130 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
201171 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
202309 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
202543 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
202855 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
202898 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
203812 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093417
203830 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting commit 20180316093417
203857 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
204121 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
204198 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
204198 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
204434 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=35, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=29, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=36, numUpdates=0}}}
204858 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
204858 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
204858 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 35, totalInsertBuckets => 1, recordsPerBucket => 500000
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 29, totalInsertBuckets => 1, recordsPerBucket => 500000
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 36, totalInsertBuckets => 1, recordsPerBucket => 500000
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
204859 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
204879 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093417
204879 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093417
205020 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
205020 [pool-1799-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
205039 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
205039 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
205869 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
205906 [pool-1799-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
205916 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
205930 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
205931 [pool-1800-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
205945 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
205946 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
206817 [pool-1800-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
206841 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
206841 [pool-1801-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
206862 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
206862 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
207253 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
207717 [pool-1801-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
207976 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
208163 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
208163 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
208189 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
208189 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
208342 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
208756 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093417 as complete
208756 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093417
208835 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
209099 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093422
209106 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting write commit 20180316093422
209115 [main] INFO  com.uber.hoodie.TestMultiFS  - Writing to path: file:///tmp/hoodie/sample-table
209397 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
209416 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
209416 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
209606 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=24, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=42, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=34, numUpdates=0}}}
209615 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
209615 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
209615 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 24, totalInsertBuckets => 1, recordsPerBucket => 500000
209615 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 42, totalInsertBuckets => 1, recordsPerBucket => 500000
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 34, totalInsertBuckets => 1, recordsPerBucket => 500000
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
209616 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
209634 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093422
209634 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093422
209753 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
209753 [pool-1803-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
209765 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
209765 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
209809 [pool-1803-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
209832 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
209833 [pool-1804-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
209855 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
209855 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
209885 [pool-1804-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
209904 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
209904 [pool-1805-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
209918 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
209918 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
209956 [pool-1805-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
209978 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
209978 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
210115 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
210115 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
210276 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
210285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093422 as complete
210285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093422
210285 [main] INFO  com.uber.hoodie.TestMultiFS  - Reading from path: file:///tmp/hoodie/sample-table
210570 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
210595 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
210700 [DataNode: [[[DISK]file:/tmp/1521189253968-0/dfs/data/data1/, [DISK]file:/tmp/1521189253968-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45302] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-79642948-172.17.0.3-1521189254072 (Datanode Uuid 2168394d-d8ed-4b7f-a33b-ed21c9fd1bfd) service to localhost/127.0.0.1:45302 interrupted
210701 [DataNode: [[[DISK]file:/tmp/1521189253968-0/dfs/data/data1/, [DISK]file:/tmp/1521189253968-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45302] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-79642948-172.17.0.3-1521189254072 (Datanode Uuid 2168394d-d8ed-4b7f-a33b-ed21c9fd1bfd) service to localhost/127.0.0.1:45302
210714 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@6d7f00ff] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.603 sec - in com.uber.hoodie.TestMultiFS
Running com.uber.hoodie.func.TestBufferedIterator
210947 [pool-1807-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
210958 [pool-1807-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
com.uber.hoodie.exception.HoodieException: operation has failed
	at com.uber.hoodie.func.BufferedIterator.throwExceptionIfFailed(BufferedIterator.java:195)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:163)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$2(TestBufferedIterator.java:155)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Failing it :)
	at com.uber.hoodie.func.TestBufferedIterator.testException(TestBufferedIterator.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
210973 [pool-1807-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
210975 [pool-1807-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
java.lang.RuntimeException: failing record reading
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$3(TestBufferedIterator.java:185)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
211224 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
212437 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
213318 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
214174 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
214478 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
215998 [pool-1808-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
216019 [pool-1808-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at com.uber.hoodie.func.BufferedIterator.insertRecord(BufferedIterator.java:126)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testMemoryLimitForBuffering$1(TestBufferedIterator.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
216044 [pool-1809-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
216139 [pool-1809-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.238 sec - in com.uber.hoodie.func.TestBufferedIterator
Running com.uber.hoodie.func.TestUpdateMapFunction
216165 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
216205 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
216237 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
216237 [pool-1810-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
216238 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
216238 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
216282 [pool-1810-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
980d39d5-92e9-4e26-8f78-baca6804279e
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.149 sec <<< FAILURE! - in com.uber.hoodie.func.TestUpdateMapFunction
testSchemaEvolutionOnUpdate(com.uber.hoodie.func.TestUpdateMapFunction)  Time elapsed: 0.149 sec  <<< ERROR!
java.lang.NullPointerException
	at com.uber.hoodie.func.TestUpdateMapFunction.testSchemaEvolutionOnUpdate(TestUpdateMapFunction.java:106)

Running com.uber.hoodie.table.TestCopyOnWriteTable
216422 [dispatcher-event-loop-13] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (152 KB). The maximum recommended task size is 100 KB.
216463 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
216464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
216464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
216464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
216464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]
216464 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]}, 
UpdateLocations mapped to buckets =>{file1=0}
216787 [dispatcher-event-loop-21] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 2 contains a task of very large size (752 KB). The maximum recommended task size is 100 KB.
216825 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
216826 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
216826 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
216826 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 2200, totalInsertBuckets => 2, recordsPerBucket => 1000
216826 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]
216826 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]}, 
UpdateLocations mapped to buckets =>{file1=0}
217089 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
217090 [pool-1841-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
217717 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
217749 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
217749 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
217775 [pool-1841-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
f7710408-fd23-48cd-bbb4-108aaf49244f_0_20180316093430.parquet-478507
22de788a-3748-460f-a869-2b4274796524_0_20180316093430.parquet-478547
ee070096-35d2-4648-ad79-4ce78bfdaccb_0_20180316093430.parquet-451853
217855 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
217855 [pool-1857-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
217859 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
217859 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
217921 [pool-1857-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
217923 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
217923 [pool-1858-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
217926 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
217926 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
218003 [pool-1858-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
218214 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
218215 [pool-1889-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
218218 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
218218 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
218237 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
218317 [pool-1889-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
218320 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
218320 [pool-1890-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
218327 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
218327 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
218402 [pool-1890-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
218610 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
218610 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
218611 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
218611 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]
218611 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]}, 
UpdateLocations mapped to buckets =>{file1=0}
218720 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
218720 [pool-1921-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
218721 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
218721 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
218782 [pool-1921-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
218884 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
218884 [pool-1937-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
218885 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
218885 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
218939 [pool-1937-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
219715 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.805 sec - in com.uber.hoodie.table.TestCopyOnWriteTable
Running com.uber.hoodie.table.TestMergeOnReadTable
Formatting using clusterid: testClusterID
220163 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
221566 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
221643 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
222320 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
222971 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
223090 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
223282 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
223883 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
224292 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
224292 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
224537 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
224562 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=71, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=66, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=63, numUpdates=0}}}
224815 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
224983 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 71, totalInsertBuckets => 1, recordsPerBucket => 500000
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 66, totalInsertBuckets => 1, recordsPerBucket => 500000
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
224984 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
225005 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
225005 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
225130 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
225130 [pool-1964-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
225163 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
225163 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
226000 [pool-1964-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
226023 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
226023 [pool-1965-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
226067 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
226067 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
226162 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
226851 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
226891 [pool-1965-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
226914 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
226914 [pool-1966-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
226944 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
226945 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
227450 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
227775 [pool-1966-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
228214 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
228214 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
228232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
228232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
228386 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
228468 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
228801 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
228801 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
228919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
229276 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 82
229276 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
229451 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 30 for /tmp/junit5212398239343190464/2016/03/15/817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38_0_001.parquet
229477 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 71 row keys from /tmp/junit5212398239343190464/2016/03/15/817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38_0_001.parquet
229477 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 30 results, for file /tmp/junit5212398239343190464/2016/03/15/817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38_0_001.parquet => [15f0e232-900e-4ab0-a2fd-fa7ed77612e9, 18e2e7f8-cf5d-444a-9bb6-fbdb9571b621, 1f5e0f28-e078-441a-b039-a262b403285a, 20bfad58-c48c-4ebb-9428-411912bb899f, 29cce7e0-849d-4d09-bc14-7831f2acdc43, 2fc63f46-731a-41aa-b12d-4d808be9f949, 31ce9203-11b5-43ce-b8fa-eaccf6932ce7, 3b6f3997-b42b-4a7d-9201-885c88261f02, 456cb6b4-accb-44c9-aa33-eb4b4bbee359, 4a692007-ff9f-4981-8aee-99a4403cbd3a, 5191e9f3-3868-4e56-9043-ef53b92c0d87, 561caee1-f521-4a0f-9d6b-a833f622d804, 614de7aa-6309-48fa-a83d-b77effd5309a, 62c82571-2051-4440-bf7f-e4a7c2637920, 72cd7616-4cda-41ad-ad9e-2739a22be704, 8492f0f3-16b9-4ce7-86ea-2e848d0f05ef, 867f69b7-d647-4708-bac8-475b9398c890, 8a651972-b717-4c6a-89bb-3ee3a631b752, 9bc0cc61-c6d9-4715-b26c-3091e6cb6a6f, 9f048ff5-8dfc-4df8-89ab-2f9d68fabc3c, aa3674bf-5bb8-4fc4-98d7-887814cbb80e, ae4f26a3-0d91-4d2b-9652-9fb8b497fc9a, b9e48da3-b198-4892-9723-a6130faf847c, bf3daa0c-3f3a-437f-80bb-02c505a25026, c9f32784-8eb5-47cd-be51-143995b19983, ca1488b0-4cd4-45d7-88f8-285b7d464f05, d05210d0-d813-4641-bc6e-6ed4832367ce, d7dd4211-2cbe-4834-b9ea-f0f4190fdb39, ef505649-7dcd-4f9b-bc13-32abfa452b76, f041a3e0-ea57-4470-b07a-c507ae91e5ae]
229512 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit5212398239343190464/2015/03/16/9d2603ec-14b6-423b-8c69-3230bb7622f5_1_001.parquet
229526 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
229531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit5212398239343190464/2015/03/16/9d2603ec-14b6-423b-8c69-3230bb7622f5_1_001.parquet
229531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit5212398239343190464/2015/03/16/9d2603ec-14b6-423b-8c69-3230bb7622f5_1_001.parquet => [0bcaa094-d090-4003-b106-57930d2f7469, 245bb323-e35c-4558-abd0-fd23af1da9bb, 2d47962b-3880-4c05-a584-4a9501cb9301, 3e64eeed-bf25-48bd-be87-52652ec129a8, 3ffa81b9-e3c4-4912-9cef-93fc3bb0c4ff, 555a2d03-9459-4cb4-9b84-78992eacd3dd, 562b95cd-fb12-4ee0-9563-c7ef4489dfa3, 60d3947b-4063-4afc-a73b-fca6adda95a1, 6df8bf78-d9fd-4b17-bc7f-0028a01508c5, 79f1754c-3227-4e6a-8923-59c66fa0c908, 858fcd9a-4773-4ec4-b834-5c4c609ebb3d]
229564 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit5212398239343190464/2015/03/16/9d2603ec-14b6-423b-8c69-3230bb7622f5_1_001.parquet
229582 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit5212398239343190464/2015/03/16/9d2603ec-14b6-423b-8c69-3230bb7622f5_1_001.parquet
229582 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit5212398239343190464/2015/03/16/9d2603ec-14b6-423b-8c69-3230bb7622f5_1_001.parquet => [898d5cce-5f3e-4183-b22b-1d5f80fc3d36, 974b7e82-0ba1-433d-aaa6-24cf7881a967, a175dac3-0a78-4681-a248-b78f84885dac, a2e6b8dd-0fba-4bd7-b298-df1ac43cb20c, aad35f6b-bf13-436f-82eb-1927b6dfcc91, ac8f3a5a-2ca6-4903-b85d-922df55e3981, bd9fb213-f83f-4df6-80d8-fc91317800a8, c3c21efa-330d-45bf-a72a-41c8651d3880, d91c6a3f-277b-4b05-86f2-c785da0bf654, edfc9982-11f2-4505-a279-5ff934970b4c, f11982ea-9de7-4611-8c20-6e3740bacc21, fe4cbb6a-4a8c-431c-b56e-0a9c13ddb309]
229601 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit5212398239343190464/2015/03/17/9f7801c8-4782-475e-968e-6339db5eaa01_2_001.parquet
229620 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit5212398239343190464/2015/03/17/9f7801c8-4782-475e-968e-6339db5eaa01_2_001.parquet
229620 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit5212398239343190464/2015/03/17/9f7801c8-4782-475e-968e-6339db5eaa01_2_001.parquet => [2084408d-f3fe-43f9-90b6-837448299132, 2a3c7338-7b6c-4cc8-8156-e72c0f32188c, 38703c35-5898-4522-840a-d733b1b46861, 3a413f58-e53c-4c9d-ad94-523eec5b36f3, 3f0b1a2b-f8c6-48c5-b505-988a889adea2, 419a4f02-72ff-44d1-81ef-89471751336c, 43330434-a787-4969-b1e1-a89bfbff7bf4, 47c95994-dca8-4351-b953-cb1325f6e981, 4b1fa252-a98a-4e7a-9147-23e79fcb8f0d, 57b60d5e-e48e-47e5-ad89-617f37292105, 7f2896e6-82f5-4175-9237-121387f71803, 84565cde-b61a-4cdd-be7f-946cf44fb3b6, 8823b898-458c-4a61-baf4-5ce0fbe0e167, 9d5e8ffb-1b15-4b24-ac0c-01cabc77a00d, acfa8e18-eac8-4738-907d-a0779a23082f, af288463-1efc-4a3e-89d0-08482e0a2cc8, b7978139-8c9b-4767-a584-43680fd61758, bd4aa551-a458-4c1c-a455-65adfa274f09, c785bc18-ff55-4567-a0c6-df7cea1c5b12, c7c48930-55f0-48be-8e40-2a896b9818cb, c7ea1b5b-73b6-49d9-81ba-7dc363e8aeb9, d26e1733-aefe-4113-91a9-d15cf4b74c9d, d5bec869-c1e2-4383-9160-0e3dc6586f09, dc27f456-f974-4d02-87c0-83858d461bfc, ebea2cc4-e23e-4f17-81c6-83a83c9a020b, f4699a44-aadd-47d1-b616-698416431924, f8ba72b8-1ff1-4f7e-98b7-69684d074d40, f9e1fa73-6c8b-4640-bef6-d98402950fbb, fc8e3549-220d-4cab-ba7c-ae3c619439e6]
229674 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=82}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=23}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=29}}}
230091 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
230091 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38}, 1=BucketInfo {bucketType=UPDATE, fileLoc=9d2603ec-14b6-423b-8c69-3230bb7622f5}, 2=BucketInfo {bucketType=UPDATE, fileLoc=9f7801c8-4782-475e-968e-6339db5eaa01}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38=0, 9d2603ec-14b6-423b-8c69-3230bb7622f5=1, 9f7801c8-4782-475e-968e-6339db5eaa01=2}
230121 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
230121 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
230246 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38
230545 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
230737 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 9d2603ec-14b6-423b-8c69-3230bb7622f5
231191 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 9f7801c8-4782-475e-968e-6339db5eaa01
231275 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
231656 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
231656 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
231669 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
231669 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
231801 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
232213 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
232213 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
232226 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
232298 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093445
232311 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Checking if compaction needs to be run on /tmp/junit5212398239343190464
232311 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Compacting merge on read table /tmp/junit5212398239343190464
232407 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093445
232407 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093445
232550 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
233606 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
233892 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
234378 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
234378 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
234393 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
234393 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
234549 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
234564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093445 as complete
234564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093445
235021 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
235679 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
235762 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
236128 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
236128 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
236379 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=69, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=66, numUpdates=0}}}
236500 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
236791 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 66, totalInsertBuckets => 1, recordsPerBucket => 500000
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
236792 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
236810 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
236810 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
236880 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
236956 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
236956 [pool-1997-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
236988 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
236988 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
237415 [pool-1997-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
237440 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
237440 [pool-1998-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
237474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
237474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
237614 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
238055 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
238308 [pool-1998-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
238328 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
238328 [pool-1999-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
238370 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
238370 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
239125 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
239178 [pool-1999-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
239203 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
239203 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
239212 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
239212 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
239354 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
239365 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
239365 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
239450 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
239656 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
239781 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
239781 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
239949 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 65 for /tmp/junit1130775108166109240/2016/03/15/0fc52315-21e6-429a-a77c-c0fce32682f6_0_001.parquet
239969 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit1130775108166109240/2016/03/15/0fc52315-21e6-429a-a77c-c0fce32682f6_0_001.parquet
239969 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 65 results, for file /tmp/junit1130775108166109240/2016/03/15/0fc52315-21e6-429a-a77c-c0fce32682f6_0_001.parquet => [0159e5cf-163e-4460-87a6-9c350fc4c81a, 01a73d03-9f00-443d-be9d-e0198ba7c39f, 054b2a67-798c-4f5b-8c15-f76ab0ceccdc, 0a570a41-3281-4cfa-9048-2745cf253700, 0ad36ac2-dbfe-4ff9-80e0-f9455c87cf86, 0ade94be-34f1-4d23-bb19-111904db049b, 0bca040d-e626-4026-b087-59f5b57bde9c, 1228d02f-ec9c-4185-9934-d44f53fda662, 13e29fa7-326f-4878-bf6a-48a34ae909c1, 14889f48-d34e-4362-a48f-1076333d448d, 15c972c1-aaed-4d36-895f-4be82127c2c6, 19dcff98-78fd-4f09-9741-2f8a44f327e4, 1aa3f7c2-d0bd-4e72-a0f6-48d1fc4eb390, 1b1e4a85-a7eb-4e14-bedc-e1d9530e6609, 1c929bfd-af34-49c5-8dfb-5e188a85d78b, 203de2a8-29bb-44e9-88f2-f025e1f1f90d, 2ce9801c-a533-4e92-b4d0-c808ad4af08a, 2db4ea31-6d83-44ff-9372-fe5dd14e8417, 323a716c-322b-4e43-9c56-57f08ff143cc, 349c29f2-2912-4ae7-9d2d-1f94d044912f, 3b45ee6d-1825-46dd-90fe-48cb731c85b1, 3bf11223-b1ac-4642-9eba-717946bfa673, 414bc834-0d35-4c5a-92ef-260f157b4f63, 445132a3-f1a0-4d86-8d44-d712d0df5c66, 48d42b92-f757-46a4-9823-a6960d5c8481, 4a13ccef-c8b3-4662-8edb-8f062bc8bd5c, 4d7d8b0d-4a3a-49b7-a9aa-bea0c4cd2d18, 51e8f26e-8501-4ef8-abd0-6be9b2745347, 52215b4c-32ab-47ed-b817-141537aaa2b9, 5905e02d-6ece-4a2c-aae9-3d3c0d34eef4, 590b45f6-9ffb-4441-b54b-782821d5cdd0, 65fc3f67-c773-47c6-887f-63f511fb88d4, 6b0266ff-def3-4173-9485-230f875c78f8, 6c2edd73-cfa2-47b9-8fb7-8a69b971bbbb, 75413aed-2ffb-48ae-acab-2386e42cc742, 77eec28e-5428-474e-829c-b680f85db197, 7c9a5995-65bc-4eaf-b7d0-d2bc0563779d, 828641a3-5387-4a38-aef8-61d9247b1be6, 9322678b-ff58-475b-b9ba-98acf1a531d5, 97b2c7c4-a3a8-4d8b-9225-b9f118e8664f, 999bb62d-ffc7-4a84-b13d-6f3f05fe3104, 9c38182e-f697-4d89-a94a-2240a4fa8fc9, 9c6753a4-3df6-4994-99d2-88b2209d80bb, a046052b-bce1-4e45-8e28-b5af0b9d61cc, a319af2f-bba5-4a48-9b0f-b198c9fac97a, a3a22020-01da-4dfa-8a3e-0820461f2383, af5a6e7c-eb5a-4bb6-92d0-d2fb904a7f86, c2381a0c-16dd-4e2f-b2b5-d63e7cbd1e32, c6b796e9-0c24-403b-ba52-7292c1f373bb, c7b8100e-4d7d-4366-8df5-2c52a3694164, c995bee6-30bb-43c6-874b-6e4050be79b8, ce400b46-80cf-4dc9-821d-ddd6ecf85983, d1dff6f8-4f6b-46ba-beae-875c7c324dda, d4885834-25fd-4cd5-b470-1d0a8774db4d, deefe284-4eb0-498f-a39a-c588e9e9eb3f, e3395790-8b06-4588-acaa-afe16c200700, e78c985d-6eb7-499b-97be-d9273c09aff1, e87ff104-279b-45bb-9634-68ce52569334, eafa12bb-5aa0-48e2-96f1-4205968537bd, eb02d630-235a-4057-9e99-4a3b323fe042, ed34ae9c-b015-4791-ad5f-8802af22bfac, f451164c-dea1-4275-9cf8-47d8475b8ae4, f4be01b8-c62a-4239-bb94-255c8566722c, f55d84cb-2339-4ec3-b9cb-df1bcd2efe44, f941f7d8-8b58-45b9-b07b-f926d03cd241]
239989 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_1_001.parquet
240010 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_1_001.parquet
240010 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_1_001.parquet => [02e7fe7f-39cd-4061-a5e1-b2c83ace3c76, 05981e3a-65d1-4065-b30c-771c3718823b, 0b919466-74a2-413d-a107-0815b9f4b554, 0edb6faa-c83e-4749-84de-466bfa8598a2, 1152bd77-1c51-47e9-aa60-0cb2e194a3c2, 14858680-b025-4320-bd93-81b3a4727bcb, 18ae7b91-907c-4689-9b9e-a946162cb164, 1cff2d3c-c942-465f-9e37-ad233b0c3dd6, 1faedcd7-9abc-49a5-b4d2-c0ae00784fb1, 21e980c4-abd9-479d-bb27-57a7d4252d50, 24446800-1fa1-4940-b044-97c546d093d8, 273d608f-f6f1-4172-9304-548fbab31fe3, 2839587c-1502-487e-aa0e-9e96d72bf624, 28748033-c25c-481b-9861-def9e6aab32a, 3dc4d3e1-3db9-4e50-b307-6dfa74c54d07, 3fc8f037-37b1-4096-a154-a01f20a2846a, 408f0a5d-d6cd-4ca0-940d-9293c1094361, 47431469-2bfa-4d48-86e4-f56972ddd337, 4f625388-bc4b-4f59-9bf1-0fe3e7838715, 547a66dd-bca5-4e85-ac7a-969805a54bc5, 5487c53e-39a6-46bf-999b-14640950973e, 55501b02-1cfc-4748-b515-14627bc99d2a, 5953c6be-0572-4921-b8a7-f5cdcac76c65, 5f0dd61d-5aec-40eb-b691-22f89c314a24, 5f719397-cc74-487c-a5f3-e5eb3c3b846a, 608f25ef-5370-449c-8782-d55c1b4c234c, 639a6124-85e5-4de3-9342-1cce26651dd4, 6e04f7fe-a86a-463d-8cb8-144fba5cdc86, 734f3cde-5d05-4c8b-9b21-5f507f529b5f, 75d76b89-b8fe-4674-bc92-53df0a664262, 76bbdd74-aef5-4ae4-9273-3ab31c928d4c, 7bbb326c-bfcc-47bb-9aba-57b0e571d487, 7f4f4722-81c7-4e65-a6a5-27564b429e2f, 80159101-2a23-4959-9295-93ba0f0ad368]
240046 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_1_001.parquet
240064 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_1_001.parquet
240064 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_1_001.parquet => [8033d480-3b7f-4713-bb07-8e2375a15c25, 804742ea-a8f3-42f2-b3f9-5ebc7c75923b, 880a1d2f-d2c6-4759-a31f-c77ec582c6be, 9144e940-ff88-486f-b423-cf5d30be76dc, 94308972-a807-4940-adc1-7b2e4196b519, 9b5d1162-92ab-4474-95ec-2edcc239ab58, 9bb840b8-e1c4-4056-8909-d3be28d84c5c, 9c4aef78-8231-4e45-8a65-e9fd055a2558, a661c43a-1435-4736-95a5-e5ff82fda163, a6d622fe-181b-4400-94b7-9c388d82898f, a9b116da-f0d7-449e-b334-209116660173, aaa57694-477b-4789-8dc6-a6aa5f1695d3, ab5de0f2-e1fc-4a25-ba13-0e132ea83687, ac58da64-213e-4e4a-b4ba-37c8265862eb, acbc939e-34e0-477a-b7fc-78629192935c, b6c58ccc-b92b-4f0b-b43f-d47f2a4548af, b7eba202-11e3-4cd5-aeca-e649592e4670, bde56712-0679-4a78-891b-9bc59b702bd5, c796d97d-e773-4f0c-83a1-6086589bc133, cbc93130-538b-4426-b4c1-6f703976fd82, d6ddab43-d29d-4ea2-a551-28a20722b948, d756a85c-1f97-4a98-a359-81d8b9632042, dcf4456d-ac2d-4984-98b7-a5817278b08c, dd07f37f-4166-476e-8001-3d01b17a7263, e014b72e-43fd-45ce-b0f7-9507d5f9318c, e6787ff8-4dac-44db-b56d-028d6c6e7840, e6f14449-853a-4b19-aa6a-89f51594ba8f, ea662f92-00a7-45ea-b0ff-f8faf5f2cde3, f11bc6f7-dc73-4cf9-ba12-e6b1f7687f35, f2a83c4f-37d3-48ad-8b95-d68694d267dc, f5c760af-6a31-49cf-8f90-f5a7a0d9e38a, f88cb0f9-9312-4376-8642-e9f38b08dd01, fc1380e6-5942-43ca-8561-8152cb1fe1e4, fd737206-1372-47f7-81be-a2d03b4f282e, fd819351-a6c0-4a94-bbae-75d0c24017e5]
240084 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 66 for /tmp/junit1130775108166109240/2015/03/17/f2ab9f2f-6191-490d-bd73-b89064fe77e1_2_001.parquet
240103 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit1130775108166109240/2015/03/17/f2ab9f2f-6191-490d-bd73-b89064fe77e1_2_001.parquet
240103 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 66 results, for file /tmp/junit1130775108166109240/2015/03/17/f2ab9f2f-6191-490d-bd73-b89064fe77e1_2_001.parquet => [02f1702d-8290-40b5-bf14-8c8b7d82d092, 0a915614-95f9-4197-93a8-01ca7a6b9338, 0c553a04-72ad-4848-9ac2-115de4c054c9, 0fb87048-c74c-4a4e-ba35-a8d9a28ae8dd, 1502c129-a395-46a0-a1d6-562731f16232, 176ea09f-e994-4862-81e1-c07ad675d442, 1a54a208-ae4e-47fe-a135-72bec9047903, 1da33fb9-add2-44fe-9a54-31616e3f5706, 1e2dbe6e-2668-438c-9c41-dcef4afeb699, 21d4e82a-0b09-4437-afb4-8debaf9f1961, 251a3b60-9058-4142-993a-b79963c9cddc, 277754df-cfbb-4c7c-bc88-df34be7342b1, 2cc85b0d-25cd-4835-bf7d-51432a3d009d, 2d97651a-aee8-4bd0-84ec-bb09434524ea, 30329ac4-76e7-451d-83a3-33b2df612b78, 31479ca7-7c42-4cfa-a051-d7c05698e95d, 357df93b-6280-47b7-bb78-dd64b5bb0fc6, 35bd0c77-d3d7-4adc-a404-61297ec8613e, 36910dd3-61de-481a-89df-90faec93f330, 3a7a24b6-d10b-47dc-94f6-0f606ea5754a, 3cc1adc7-7428-4854-9cb2-ebc5bc72210b, 3d58bf0f-9695-4b90-8361-579e6c345531, 3ee13eb5-ee0f-4283-9018-6a2dde45893e, 41a4c66f-489a-4962-b3b3-174613fbf97b, 422102a9-b78d-459a-8424-5a6cb4ee2ba0, 4444c4a7-043b-45e5-b3ba-86109ca72f1a, 48f71c3f-05d2-41b4-bc45-16b3acc87f92, 50f9b12c-5f57-46da-9b03-1d728d5ef9fe, 556da429-f255-447c-a724-1de7a333f860, 5678f74d-0c26-42ba-ac48-ae9969664df8, 5a5f1b9a-9a02-469f-aaf6-762f421e9801, 5cbbf8e1-1ddd-4a39-b8fa-f7e0abb244b3, 5e503608-9946-4030-890b-d60d25061c52, 65a8f79b-7204-4ecd-80dd-336351119a3a, 69b3dc7a-bb07-4c7c-8886-eb157b69a3e2, 76619358-9772-4f71-bf7a-167114b46663, 8087f404-2a3e-4694-ba3f-c69a30557fe0, 82f84ecb-4112-4720-b451-ea5014d75b77, 8392ed57-567a-4563-a743-3d67e54763bc, 869061d7-8801-4d6d-8aaf-a8fa5dffb71f, 884e0db2-958a-4d0e-8352-f6e5f4be0dee, 8d49a5a5-3546-4a12-acbb-6041e857931f, 9206b8a3-87ce-41c7-8b70-6f114acab0d2, 97adf131-6488-42f2-8f9c-e02eee590ddc, b3e5242b-c91d-47df-856e-3e76bf442e48, b71237cb-4cda-42ca-a668-7f9b34184a2b, b891787e-e095-427f-b4c4-3df0673ec4e4, bb6b47a9-ce2c-49d5-a1e8-28ddbd812ad5, bbf3aafc-9e65-4f25-84d2-71ac2952f6ad, be804534-8fd3-4848-89e0-633521a131ee, c1237f11-1133-4aa6-adf5-ec5994685f88, c70da7f5-e801-4bab-82a3-fae6967a0dbe, c94133a6-8cef-4dd2-b775-5772ca082290, c9a3b138-c4b1-4f05-867e-fa7f64cfd317, ceae2e7d-dd9a-487e-8e15-03ee6daa4d99, d47351de-1bc6-4bd9-a95b-f4886e0f5019, d4bc1cba-7ae5-4570-8231-38bcd90206cd, d7a38e70-c667-4d00-8b30-41481d95634d, dad4d2a5-b963-4553-8286-d741d6262f65, e0ba28d6-f6f4-44ca-814d-38e2588d1cf8, e13bf0e9-65c5-44f9-bae3-09349bbc83aa, e9407562-de5d-4931-917c-bf6f14854a4a, ec800802-152b-4376-8302-5b6eb74c6eef, eda0df4e-7eb0-4709-98e8-fc4d604e95e9, f24130f7-8bf0-4925-b3e7-f70dfcb1fb26, fbf85101-c8f7-429f-b6c0-47c09e5a689e]
240184 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=69}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=66}}}
240502 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
240606 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
240607 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0fc52315-21e6-429a-a77c-c0fce32682f6}, 1=BucketInfo {bucketType=UPDATE, fileLoc=f2ab9f2f-6191-490d-bd73-b89064fe77e1}, 2=BucketInfo {bucketType=UPDATE, fileLoc=33d91b83-1e19-44da-9d63-0d575849d022}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0fc52315-21e6-429a-a77c-c0fce32682f6=0, f2ab9f2f-6191-490d-bd73-b89064fe77e1=1, 33d91b83-1e19-44da-9d63-0d575849d022=2}
240624 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
240624 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
241547 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
242424 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
242424 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
242441 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
242441 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
242466 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
242607 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
243020 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
243020 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
243548 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
243562 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
243631 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
243633 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:44231/tmp/junit1130775108166109240/2015/03/16/33d91b83-1e19-44da-9d63-0d575849d022_2_002.parquet	true
243633 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
243635 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:44231/tmp/junit1130775108166109240/2015/03/17/f2ab9f2f-6191-490d-bd73-b89064fe77e1_1_002.parquet	true
243635 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
243637 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:44231/tmp/junit1130775108166109240/2016/03/15/0fc52315-21e6-429a-a77c-c0fce32682f6_0_002.parquet	true
243643 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
243675 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
244055 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
244056 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
244615 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
244902 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
244934 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
244934 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
245116 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=4, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=11, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=5, numUpdates=0}}}
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 4, totalInsertBuckets => 1, recordsPerBucket => 500000
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 11, totalInsertBuckets => 1, recordsPerBucket => 500000
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 5, totalInsertBuckets => 1, recordsPerBucket => 500000
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
245528 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
245545 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
245545 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
245650 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
245672 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
245673 [pool-2029-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
245675 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
245675 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
246208 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
246542 [pool-2029-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
246566 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
246566 [pool-2030-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
246573 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
246573 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
247010 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
247437 [pool-2030-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
247457 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
247457 [pool-2031-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
247460 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
247460 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
247643 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
248328 [pool-2031-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
248437 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
248765 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
248765 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
248782 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
248782 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
248949 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
248960 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
248960 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
248963 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
249064 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
249436 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
249436 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
249601 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
249619 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 11 row keys from /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
249619 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet => [046a227b-7bca-4fb3-9cfa-c2dca5eb37d8, 3bb09908-3a8a-4a3d-aa61-a49a223c7c2f, 3ee29e01-36aa-4abd-8a8b-07f2524143bf, 550ef307-d10b-424e-8d75-3b5ccbdab0ae, 5d6e8eb8-60fc-44af-88ef-debef6eb0722, 8a3d3064-cc45-4b1d-a575-378caa3aa7c2, 8f2218b9-d372-40f9-884b-d9d161ecd793, b425a60c-4aea-4059-a2bc-e424d1e87f1a, bf9d7c7b-4b3c-4a4f-a3b7-0d53070276d6, d4d63ed2-ca1d-4dc8-a05e-b4622265781b]
249621 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
249652 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
249669 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 11 row keys from /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
249669 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet => [f07e81b8-0e31-4280-a379-a1355835a609]
249688 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet
249703 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet
249703 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet => [82526be2-f326-4ae6-81b5-54ae11fac6fb, 85f4e832-e007-4ab3-9c3f-f460000f7ed5, 9bc086c7-e250-4c4f-9429-b6884cc3c6a7, a08583d2-f91f-4696-9598-3f53e06c1950, f1368893-d82d-4a62-a8e3-17ed7d5f6f63]
249723 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 4 for /tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet
249739 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 4 row keys from /tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet
249739 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 4 results, for file /tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet => [0b8d9e87-4bac-4c55-853a-3f4520e4560b, 1ed77724-f13d-48dd-8712-7fbf2b7e3ac7, 8c2967ab-cd4b-450f-9415-5b8479839233, 9713a0a7-60d2-4e95-8357-9329b536acb2]
249794 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=4}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=11}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=5}}}
250208 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
250208 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=eb662fe9-5939-4be1-a3e2-1e9c73809697}, 1=BucketInfo {bucketType=UPDATE, fileLoc=095658b7-3819-4d6d-b003-41e7adc209b7}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8f320fee-5964-41bd-ab4d-bdb05013a831}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{eb662fe9-5939-4be1-a3e2-1e9c73809697=0, 095658b7-3819-4d6d-b003-41e7adc209b7=1, 8f320fee-5964-41bd-ab4d-bdb05013a831=2}
250229 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
250229 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
250357 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file eb662fe9-5939-4be1-a3e2-1e9c73809697
250799 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 095658b7-3819-4d6d-b003-41e7adc209b7
251036 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
251104 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
251247 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 8f320fee-5964-41bd-ab4d-bdb05013a831
252107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
252107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
252122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
252123 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
252281 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
252294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
252294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
252380 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
252544 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
252739 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
252739 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
252919 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
252936 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 11 row keys from /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
252936 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet => [046a227b-7bca-4fb3-9cfa-c2dca5eb37d8, 3bb09908-3a8a-4a3d-aa61-a49a223c7c2f, 3ee29e01-36aa-4abd-8a8b-07f2524143bf, 550ef307-d10b-424e-8d75-3b5ccbdab0ae, 5d6e8eb8-60fc-44af-88ef-debef6eb0722, 8a3d3064-cc45-4b1d-a575-378caa3aa7c2, 8f2218b9-d372-40f9-884b-d9d161ecd793, b425a60c-4aea-4059-a2bc-e424d1e87f1a, bf9d7c7b-4b3c-4a4f-a3b7-0d53070276d6, d4d63ed2-ca1d-4dc8-a05e-b4622265781b]
252941 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
252970 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
252990 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 11 row keys from /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet
252990 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet => [f07e81b8-0e31-4280-a379-a1355835a609]
253009 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet
253027 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet
253027 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet => [82526be2-f326-4ae6-81b5-54ae11fac6fb, 85f4e832-e007-4ab3-9c3f-f460000f7ed5, 9bc086c7-e250-4c4f-9429-b6884cc3c6a7, a08583d2-f91f-4696-9598-3f53e06c1950, f1368893-d82d-4a62-a8e3-17ed7d5f6f63]
253054 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 4 for /tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet
253076 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 4 row keys from /tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet
253076 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 4 results, for file /tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet => [0b8d9e87-4bac-4c55-853a-3f4520e4560b, 1ed77724-f13d-48dd-8712-7fbf2b7e3ac7, 8c2967ab-cd4b-450f-9415-5b8479839233, 9713a0a7-60d2-4e95-8357-9329b536acb2]
253135 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=4}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=11}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=5}}}
253549 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
253549 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=eb662fe9-5939-4be1-a3e2-1e9c73809697}, 1=BucketInfo {bucketType=UPDATE, fileLoc=095658b7-3819-4d6d-b003-41e7adc209b7}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8f320fee-5964-41bd-ab4d-bdb05013a831}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{eb662fe9-5939-4be1-a3e2-1e9c73809697=0, 095658b7-3819-4d6d-b003-41e7adc209b7=1, 8f320fee-5964-41bd-ab4d-bdb05013a831=2}
253574 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
253574 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
253698 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file eb662fe9-5939-4be1-a3e2-1e9c73809697
254203 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 095658b7-3819-4d6d-b003-41e7adc209b7
254590 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
254625 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
254661 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 8f320fee-5964-41bd-ab4d-bdb05013a831
255532 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
255533 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
255546 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
255547 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
255729 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
255743 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
255743 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
255961 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit5778346105746453762
255967 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
255970 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
255972 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
255979 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit5778346105746453762
255989 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
255989 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
256011 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
256439 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
256545 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
256803 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
256823 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit5778346105746453762/2016/03/15/.eb662fe9-5939-4be1-a3e2-1e9c73809697_001.log.1] for base split hdfs://localhost:44231/tmp/junit5778346105746453762/2016/03/15/eb662fe9-5939-4be1-a3e2-1e9c73809697_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
256856 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit5778346105746453762
256863 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
256864 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
256865 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
256871 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit5778346105746453762
256879 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
256879 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
256891 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
256908 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
256918 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit5778346105746453762/2015/03/16/.095658b7-3819-4d6d-b003-41e7adc209b7_001.log.1] for base split hdfs://localhost:44231/tmp/junit5778346105746453762/2015/03/16/095658b7-3819-4d6d-b003-41e7adc209b7_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
256942 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit5778346105746453762
256950 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
256951 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
256952 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
256956 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit5778346105746453762
256973 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
256973 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
256983 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
256997 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
257007 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit5778346105746453762/2015/03/17/.8f320fee-5964-41bd-ab4d-bdb05013a831_001.log.1] for base split hdfs://localhost:44231/tmp/junit5778346105746453762/2015/03/17/8f320fee-5964-41bd-ab4d-bdb05013a831_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
257219 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
257548 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
257908 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
257908 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
258133 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=7, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=6, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=7, numUpdates=0}}}
258199 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
258454 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 7, totalInsertBuckets => 1, recordsPerBucket => 500000
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 6, totalInsertBuckets => 1, recordsPerBucket => 500000
258550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
258551 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
258551 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 7, totalInsertBuckets => 1, recordsPerBucket => 500000
258551 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
258551 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
258571 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
258571 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
258697 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
258697 [pool-2063-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
258701 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
258701 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
259565 [pool-2063-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
259588 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
259589 [pool-2064-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
259593 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
259593 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
259853 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
260087 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
260445 [pool-2064-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
260472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
260472 [pool-2065-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
260477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
260477 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
261243 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
261332 [pool-2065-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
261538 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
261767 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
261767 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
261783 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
261783 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
261955 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
262356 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
262371 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
262371 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
262489 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
262889 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 37
262889 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
262946 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
263085 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 6 for /tmp/junit7246681326311688461/2015/03/16/81ba9517-bbf8-458b-84b9-8473dc774c4a_1_001.parquet
263108 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 6 row keys from /tmp/junit7246681326311688461/2015/03/16/81ba9517-bbf8-458b-84b9-8473dc774c4a_1_001.parquet
263108 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 6 results, for file /tmp/junit7246681326311688461/2015/03/16/81ba9517-bbf8-458b-84b9-8473dc774c4a_1_001.parquet => [14ecd821-5f36-4308-8b80-54a55f2a0805, 3219c2ad-65bc-4765-9dda-33c773982c58, 7859ea2f-cd69-45e4-a659-207fed573753, 9fb53560-ca79-4a8a-8e01-fff63a51493c, a118cdb6-4a0a-4eff-bccd-22108414f46c, a4784b70-eec5-41f7-abe6-7240bf7d4ac9]
263132 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 4 for /tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_2_001.parquet
263154 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 7 row keys from /tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_2_001.parquet
263154 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 4 results, for file /tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_2_001.parquet => [023f97fb-3a79-45af-817d-c9c5479668eb, 2fbae62c-3347-41d2-bff8-3ff9be546f6c, 3c4f7833-2c60-4962-aeb5-4341b9762f7a, 7577b78d-fcfd-48a5-9329-028d46a736c2]
263188 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_2_001.parquet
263211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 7 row keys from /tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_2_001.parquet
263211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_2_001.parquet => [d4ee8d6d-661a-47e7-981d-99677a048631, d9b3cf76-040c-46a7-92ba-f424c67ad029, f163bf5f-9b57-466a-a1e9-05bd43e4989f]
263233 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit7246681326311688461/2016/03/15/91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb_0_001.parquet
263253 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 7 row keys from /tmp/junit7246681326311688461/2016/03/15/91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb_0_001.parquet
263253 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit7246681326311688461/2016/03/15/91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb_0_001.parquet => [019e35a6-d6bc-4b63-aef9-ed1d90ba5f2d, 40b9c943-59a8-4082-a0d1-b815b4044bad, 543770fe-e8bb-4e9b-9ebc-d3513937f87e, 74ad16f7-1c34-48be-9235-0f4eb300745e, 802846e2-9cf9-4490-8cdc-ce6759ef430d, a106b9ef-1fc2-4b31-817a-30833058e278, f3c9af1c-6ba6-41c4-897b-355033c67c6e]
263312 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=4, numUpdates=7}, 2015/03/16=WorkloadStat {numInserts=7, numUpdates=6}, 2015/03/17=WorkloadStat {numInserts=9, numUpdates=7}}}
263729 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
263737 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb}, sizeBytes=435780}]
263737 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 4 inserts to existing update bucket 1
263737 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=1, weight=1.0}]
263739 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=81ba9517-bbf8-458b-84b9-8473dc774c4a}, sizeBytes=435692}]
263739 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 7 inserts to existing update bucket 2
263739 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=2, weight=1.0}]
263741 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=86797c53-a8f2-4b1d-8bca-0a2c360be71c}, sizeBytes=435776}]
263741 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 9 inserts to existing update bucket 0
263741 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=0, weight=1.0}]
263741 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=86797c53-a8f2-4b1d-8bca-0a2c360be71c}, 1=BucketInfo {bucketType=UPDATE, fileLoc=91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb}, 2=BucketInfo {bucketType=UPDATE, fileLoc=81ba9517-bbf8-458b-84b9-8473dc774c4a}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=2, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{86797c53-a8f2-4b1d-8bca-0a2c360be71c=0, 91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb=1, 81ba9517-bbf8-458b-84b9-8473dc774c4a=2}
263761 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
263762 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
263890 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 86797c53-a8f2-4b1d-8bca-0a2c360be71c
263891 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file 86797c53-a8f2-4b1d-8bca-0a2c360be71c
264334 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
264351 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
264393 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb
264394 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file 91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb
264886 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 81ba9517-bbf8-458b-84b9-8473dc774c4a
264886 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file 81ba9517-bbf8-458b-84b9-8473dc774c4a
265499 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
265796 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
265796 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
265811 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
265811 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
265995 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
266301 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
266411 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
266411 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
266592 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit7246681326311688461
266597 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
266597 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
266599 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
266602 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit7246681326311688461
266610 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
266610 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
266619 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
266634 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
266642 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:44231/tmp/junit7246681326311688461/2015/03/17/86797c53-a8f2-4b1d-8bca-0a2c360be71c_0_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
266656 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit7246681326311688461
266659 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
266661 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
266662 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
266663 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
266666 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit7246681326311688461
266673 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
266673 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
266682 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
266697 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
266705 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:44231/tmp/junit7246681326311688461/2016/03/15/91c81cd7-8ccb-44a6-b56f-c0fb0a24fccb_1_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
266716 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit7246681326311688461
266720 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
266721 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
266722 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
266725 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit7246681326311688461
266731 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
266731 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
266739 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
266752 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
266761 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:44231/tmp/junit7246681326311688461/2015/03/16/81ba9517-bbf8-458b-84b9-8473dc774c4a_2_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
267741 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093521
268067 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
268067 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
268228 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
268228 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
268321 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=71, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=66, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=63, numUpdates=0}}}
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 71, totalInsertBuckets => 1, recordsPerBucket => 500000
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 66, totalInsertBuckets => 1, recordsPerBucket => 500000
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
268734 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
268753 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
268878 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
268878 [pool-2095-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
268919 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
268919 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
269391 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
269524 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
269745 [pool-2095-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
269774 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
269774 [pool-2096-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
269809 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
269809 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
270640 [pool-2096-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
270668 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
270668 [pool-2097-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
270707 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
270707 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
271133 [pool-2097-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
271428 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
271448 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
271658 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
271987 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
271987 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
272214 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=58, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=70, numUpdates=0}}}
272626 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
272626 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
272626 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 58, totalInsertBuckets => 1, recordsPerBucket => 500000
272626 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
272626 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
272627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
272627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
272627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
272627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 70, totalInsertBuckets => 1, recordsPerBucket => 500000
272627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
272627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
272645 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
272647 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
272647 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
272766 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
272766 [pool-2113-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
272789 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
272790 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
273089 [DataXceiver for client DFSClient_NONMAPREDUCE_1110464906_1 at /127.0.0.1:38880 [Receiving block BP-1086827514-172.17.0.3-1521189273870:blk_1073741910_1089]] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Slow flushOrSync took 310ms (threshold=300ms), isSync:true, flushTotalNanos=11458ns
273428 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
273873 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
273943 [pool-2113-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
273969 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
273969 [pool-2114-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
274013 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
274013 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
274308 [DataXceiver for client DFSClient_NONMAPREDUCE_1110464906_1 at /127.0.0.1:38888 [Receiving block BP-1086827514-172.17.0.3-1521189273870:blk_1073741912_1091]] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Slow flushOrSync took 323ms (threshold=300ms), isSync:true, flushTotalNanos=11778ns
274857 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
275160 [pool-2114-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
275193 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
275193 [pool-2115-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
275225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
275225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
275943 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
276301 [pool-2115-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
276497 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
276734 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
276734 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
276745 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
276745 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
276909 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
277048 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
277321 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
277321 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
277431 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
277836 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
277836 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
278036 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 58 for /tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet
278058 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet
278058 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 58 results, for file /tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet => [0132922d-e51c-4fde-b6b6-b9c42cc5179b, 0af179d9-b61a-48fe-94ca-ef36fd89a7bb, 0d163946-abbc-4fc2-bc0e-d145e7cd571a, 175cb073-2fe1-409d-ab31-952834267a9e, 187de3a1-9d4a-42b9-8670-eaad8329b84c, 1ee26af3-522f-440e-8fc1-18682422f9c6, 28cbb3c6-7645-41dd-b37e-92265ba6966e, 29f845ac-853f-43b9-a9a6-769b3c585b51, 2b99bbdf-c1ae-4be9-801e-2190d97c6625, 3099a4b0-edbe-496a-a468-25d8243020cd, 355b3c4d-f719-437a-b5e0-5d533b4657d3, 446a9039-f6a4-41c1-8fd2-d9c9970e0d5c, 45cfc743-66f8-4962-be3f-5473005b0d73, 50a42c57-9645-4ec3-9bf1-5767b4215550, 56941f94-3e55-47b3-a755-d7d9a5fba3c6, 58796dff-f1c6-422b-aac6-7427fa4cb25b, 5bc366fa-a8d7-4ce5-8619-72cc950f6dcb, 5bcea9fa-cc91-4017-b9b9-557557f6972a, 5c48b9ef-a3d5-41d4-98b6-780d0dbb4d53, 65ab9317-bce7-48ff-8d96-a7d9bee7b133, 6a371cd3-cc0d-4157-a975-21a4ce567b6a, 6ad3cb4f-c554-4e6a-88ae-38df67f7c1ee, 6b118239-9c58-417a-894f-145dffa1c1b2, 6fa25816-aef1-4643-aa47-5a21f08e0728, 71d1bb14-3f3b-4b62-9cd3-9bdfe2d0a739, 7389bc59-32a1-44ad-a3ae-a5be4d64e1d4, 756a3703-d0b2-4750-8b7c-958f087e1241, 76dbb139-752d-427e-9b1c-45ca2b9e6344, 81e91c17-a5f1-4cb3-8c4f-fc3e6577fd08, 838086c8-9ef8-44f2-95b6-0249656c26a7, 872e0f3b-b390-40af-94d4-ee9dbfdce7a3, 8ac9025f-2747-45b9-9cd7-803688fdbeb5, 8b5cfad5-a6b6-485e-b063-472a0256b7fe, 90c080c1-e337-476a-bfcc-e455796edd91, 978cac71-3f8d-46b2-b148-6ef308b893bc, 98cadbec-a6e2-4ea2-a7a7-a9aa0c25193f, 99b0e6a9-d23f-4066-b240-b6d89ba7377b, 9e7374cf-4d83-487f-8e48-401c606e4eff, aa535268-5f4a-4978-a0b5-7887bc458aab, ad957841-08e4-4f19-be8d-a4486eafb20d, b1b73f4d-1d3a-4d2e-94bc-a16ae2dd9d87, bbdaa492-2c3c-416b-8f8a-ab101fee938f, bda2f9d6-d176-484f-8953-c733ea972837, c0bfec72-0877-4789-b354-9cab6734b7e0, c77dd69e-54d5-44aa-80e2-41e90f2aa2ed, c8dcc0a1-1b10-44f7-8f00-de3d2606f949, d17c793f-6c3e-48e5-b935-bcac8da4863b, daef5f47-aa96-4d12-82c5-68186bff7512, dc490821-2344-429a-8568-9e5cf7ebbf1b, e117d280-71e0-414a-bd66-3900ec2888e7, e3553528-590e-4a93-a6f0-c3ed4e4efbfc, e682e3b2-71a5-4098-ad61-aa7c77a3a324, e6c0ef9e-70df-4830-8bd1-11f17d05eff0, ed027908-b79a-46a9-8f02-998b01ff972a, eee655bf-c5b8-4499-b09a-6a2041e926de, f247ccc1-dfd8-471a-9578-c63fdc41c93e, f6f2efe3-f495-4fe2-8105-4b165889e215, fa658513-ebf0-4d63-9d62-a6664576a660]
278092 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 47 for /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
278112 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
278112 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 47 results, for file /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet => [03a8568e-8802-4253-b36e-811a7eadf77f, 05ffb1a4-f715-4594-9964-21132d5703d7, 0a14baa2-1134-4c54-92f6-0beeeccccafa, 0e801886-db08-4f5e-b670-c58cae302614, 12f7f7b1-154c-42aa-b726-d9fcf4cae6ff, 145f5882-1688-4485-9c0d-5bd0f0a59501, 1695fa04-eeac-4039-8096-c47f766b542a, 2117ff57-0e98-4bb5-a533-dedbea4b2b88, 22c7e0f8-769e-4393-b990-5bab4ff7085b, 284208db-6529-4cc1-82a8-4609dca42b5b, 33105c2a-1dc2-40af-bda6-6916c9124142, 3566108e-3f6b-410f-8f75-57521c7fc234, 3d8d423a-b87e-43d0-ba2e-f27961f22a44, 3e61b6f2-24a8-4423-88af-d8a83fed7c25, 40ec0e56-dc26-446a-8a67-dc6507fbea08, 41947c3f-d812-454b-8daf-da9553e10ad4, 4d711a18-8153-45d0-abf2-544c87c1978e, 4df10d82-6156-4d95-8a18-2c88fffeab0e, 4ec6b925-cf2a-485f-8164-2b9f340ec4c7, 521e499c-4576-4fa9-a406-74474417be2c, 53bb98f7-1052-4306-bde4-afb3e209128e, 5645a4b4-ad50-4c0f-9fab-e791c1855c17, 56da4c18-e22c-4d5a-92b4-47cf9a0089b7, 5a774629-50e9-45f9-adbd-f2f22f2f9afe, 5ab18db5-829c-4df5-8da4-ebb5b05f8ab6, 5b3491ae-7e4a-427e-9ae4-7cc39e0c16b1, 5f68946d-a67e-480b-a4b1-48fe68cf7e98, 6a87aa3a-663c-4913-80e0-71298def1b7b, 6e743a2f-36e9-44b1-8704-81cdf9c7d383, 70ebb964-9ece-4067-a24b-10dd1fe9dadf, 7196e945-bbb6-4c5c-97a1-5d021c59c4a3, 73d08d3f-7ed8-47d1-be5f-615d8d0da562, 756786a6-0ec5-43f0-bc61-f848cc78dda1, 7d4df068-7843-4876-ae9a-92b944dc54f0, 7e4e68dd-048a-47a9-9e81-3977f6dceba3, 836734d9-8e35-4ca1-bebc-a4fe163e4d28, 87c4ac86-f88d-4115-90af-47311ac241c7, 900c77bf-1f7c-4e4d-a1f5-cbc41113d35d, 9038e662-200b-4e78-a9ef-bd09c20ee3ce, 927e845b-86c5-4df8-9d7b-b76611cf3e2b, 931d56f0-1218-4ce5-8c91-1c10ce1ca9dd, 961c6002-e7d4-42f5-8ffe-fa87b6005194, 983e4882-6a0d-42a2-a60f-a6eb609b9ce0, 9b66561c-1a0a-417b-834e-378dcdc1d23a, a09a25df-feba-4eba-bd81-272e6f543a0f, a1e9b9a7-4ee1-4546-88a7-9e8d5a40d2cd, a9277d29-57f6-4ebf-95a6-9561c3df332e]
278163 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 23 for /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
278183 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
278183 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 23 results, for file /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet => [abbc9718-3549-4942-8dfd-7974d86b7d71, ad8f23cb-38c0-470a-acca-dbc0e529158f, b113ff0a-c5d2-4a26-9536-9fddcaea5d2c, bd3a0fbe-ec25-4146-8963-97a30b212b51, be758a37-b07b-4587-9591-ec95add5c975, bfe854af-704c-470d-99f7-934d3009d760, c1e773b4-37ab-4d82-a672-1a9f41d88faa, c23416b2-71cb-4ab9-9623-16978aa2e3b1, c245ac9a-8888-466e-a117-1179eae28ed5, c4475eb2-32c9-43df-ad89-323e0c92d4f5, c9f7c9b6-a019-40ba-9cee-16487204d409, d17bb029-3391-45b3-8288-ce9bc28f06c7, d19e5187-d880-4c24-ac66-18fca65f340d, d452706f-b626-4a3a-98a0-2032fab88e13, de72e13c-00f5-4fd4-a764-c97b276faa1f, dffc7299-a2d0-40a9-b323-398de36c152a, e824ead4-764d-4b2a-bf2f-7120607cc10d, e8b87f06-7f69-4f90-9978-fff2a5e72ea5, ebb20e80-a801-4105-ba2b-68022d88d41b, ed86c837-03ce-4e10-9f35-379f49efb86b, effc5c5c-8688-4027-b7a0-3bdeb15a514f, f5512b7c-3401-4977-aebc-c6e0f68b5267, fd8b1604-abb0-4bc7-862e-d4a67e4c7d82]
278211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 72 for /tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet
278231 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet
278231 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 72 results, for file /tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet => [011a38b6-5d02-45b5-a6f8-db503739ae64, 0fa94e2f-3f59-42f1-a6f7-9dc936f25459, 104e066c-6034-4a35-9d6b-61d987e6d4c5, 14ed7d8b-103e-4717-a99b-e9765f0aab9c, 152c6fa7-3968-48fd-b41c-62ea7931bc9d, 16e1181b-7f51-4109-ada7-843683b88a5f, 1b60c682-be9a-4df7-833e-5560c6a2ccfd, 1ce5e9e4-dffb-44ac-ae33-5b638857b652, 1fe8675b-20da-4904-92cc-53839d1fed0d, 24d8f789-4e44-4cc6-9b30-02bd58c3603a, 31b22ca9-02af-4ad9-93fe-2473beddeb88, 3908d003-1068-4eed-8514-6305cf3f8492, 407d6840-0b1a-44d9-b29b-70c7d05276f1, 43998fdf-e98b-4e7e-9e4a-fc49aaa9fb8a, 462e1777-9c41-4ec6-993a-211430ce8f64, 4742bf9d-75bc-47d0-9e3b-f6faa9ad0ee0, 4ca5fe89-abe0-40a5-8bc5-8a18610d46f8, 4ee42cf2-c7e4-4c48-860a-02f9698b6e1e, 50350d97-1b3f-499a-a155-5adb1236e4fb, 513f4f7d-948f-43a2-8aac-47a97d6c39f3, 554fa8a9-900c-4131-b00f-2930a508899e, 5a963b32-60d3-4d89-b6ae-d1b3848d0066, 5b721f70-3859-438f-8f6a-6b3bddc4a194, 5cc9b4fe-b181-4e73-8e55-09a2d418ec12, 60787c3b-16fa-4ee6-bf85-571d3e66cedb, 625eccea-6d38-4eba-8be9-9baf0cf289ca, 62ab780e-07b6-4488-a745-327a6c08e22d, 6943d905-85c7-4585-8176-df38eb32491a, 72378e4a-4222-454a-990b-f7cbd7a732f0, 72eaab3a-c6a1-40e3-992c-5924a5570017, 733e8f78-c189-4101-a559-99755020ef62, 7893508c-255d-4866-889c-a91b985d89c7, 7f3ec73c-e356-49db-ab5b-c37100119faa, 813a6fca-4b5e-466a-b79a-f73c4c1e5ac7, 88756c4c-b6da-49ff-adcc-7ebbf79e55d0, 89c7b0f6-cbb3-4eda-ae79-113b6b6f482d, 8ccbf915-faa2-4918-9eaf-1aee5f4e9b24, 8f79c79f-c039-4f9e-94ea-973039efe3ed, 922377ff-4131-45c0-954a-fa6875f03bca, 9279d9ec-8463-4219-bda4-70e7ec293a57, 95b7d2a8-fdfd-404a-b336-883a9bf52540, a031782b-8b09-48a4-9bd3-f0289f04622b, a21bb7c4-1237-4fa1-8afe-7072432e8deb, a34f2148-5613-46a1-8aa8-7d59c0676070, a6ea603a-eab0-4487-989c-69222d99a4dd, ad08e6b9-f760-4eb7-898a-0713c14b00a3, afa0a2b1-ea05-468c-99e6-782134a6c55d, b43b86cd-2b89-4a35-81c0-8933ff05be87, b5ce999f-ddd3-4416-9745-6cdfe9dfba3a, b62141bd-5bf1-4ed4-88e1-e7a5256dcc25, b68b29d8-210f-49cb-991f-580b789b9505, b9dcb0a1-055a-44f3-a913-468df127d0fc, bb2db857-8b18-4d1a-b111-2aafe8efa8cb, bc39214a-fc1c-410c-9669-a85aca9dda30, bc5e99f6-5a40-4631-ab92-058a08ceddcc, c6a59ed7-9c24-4c1c-b6fe-e4ddb20330a5, d143ad07-fda0-44b4-991a-5dcaa92f011a, d19f47c8-c0d2-49e9-9d0d-f6a276d3f2d4, d4e79e9f-e8d0-48b4-acd4-5a1a5a9b03b6, d7718652-43be-4195-b1c9-15f57da68062, da6e82fc-9f8b-4d24-af9d-513a0302ead1, e1f681f3-4c55-4d0f-a2ac-fd3836cc30aa, e225a206-2378-410c-bf05-40317cd21ecd, e610bd17-95e4-4e7e-9968-31414a7517de, eb84a131-4a96-4362-95be-809fcd4a10b7, eeacb81c-7450-4756-9509-618db90f7665, ef54094b-354e-4530-9855-125ea8816a87, efa0e373-fe26-4f6c-b72f-aaefc38fb6cb, f10c2874-b41b-4d63-a7c3-9eab2a2b1964, f89f58e4-f4f0-4c3a-84d8-59521f696d7e, fb2c4b30-bc84-4b4a-b275-f93a85ec48d5, fcd9f819-3cb7-47b1-a0e6-5dbd2a27c541]
278324 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
278346 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=58}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=72}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=70}}}
278760 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
278760 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=60c6bf93-aa89-4717-8b6f-6120aed5076a}, 1=BucketInfo {bucketType=UPDATE, fileLoc=d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d6f06a16-810c-4c9a-9ea3-6c44968e0d1d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{60c6bf93-aa89-4717-8b6f-6120aed5076a=0, d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5=1, d6f06a16-810c-4c9a-9ea3-6c44968e0d1d=2}
278773 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
278773 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
278888 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 60c6bf93-aa89-4717-8b6f-6120aed5076a
279125 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
279348 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5
279763 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
279837 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file d6f06a16-810c-4c9a-9ea3-6c44968e0d1d
280738 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
280738 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
280751 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
280752 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
280927 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
281088 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
281207 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
281339 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
281339 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
281526 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
281532 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
281532 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
281533 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
281537 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
281544 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
281545 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
281556 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
281568 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
281577 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/.60c6bf93-aa89-4717-8b6f-6120aed5076a_001.log.1] for base split hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
281612 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
281619 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
281619 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
281620 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
281624 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
281631 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
281631 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
281639 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
281649 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
281656 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/.d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_001.log.1] for base split hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
281683 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
281688 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
281689 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
281690 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
281693 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
281701 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
281701 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
281707 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
281717 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
281725 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/.d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_001.log.1] for base split hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
281755 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
281832 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
282275 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
282581 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
282713 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
282972 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
283152 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
283570 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
283570 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
283644 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
283649 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
283650 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
283651 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
283654 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
283663 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
283663 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
283670 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
283683 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
283691 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/.60c6bf93-aa89-4717-8b6f-6120aed5076a_001.log.1] for base split hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
283702 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
283708 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
283708 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
283709 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
283713 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
283720 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
283720 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
283729 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
283740 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
283750 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/.d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_001.log.1] for base split hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
283767 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
283772 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
283775 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
283776 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
283779 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:44231/tmp/junit1829311128283141443
283786 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
283786 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
283797 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
283809 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
283816 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/.d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_001.log.1] for base split hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
283823 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
283908 [dispatcher-event-loop-24] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 61 contains a task of very large size (117 KB). The maximum recommended task size is 100 KB.
284222 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
284246 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 172
284246 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
284255 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
284437 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 52 for /tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet
284456 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet
284456 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 52 results, for file /tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_0_001.parquet => [0132922d-e51c-4fde-b6b6-b9c42cc5179b, 0af179d9-b61a-48fe-94ca-ef36fd89a7bb, 0d163946-abbc-4fc2-bc0e-d145e7cd571a, 175cb073-2fe1-409d-ab31-952834267a9e, 187de3a1-9d4a-42b9-8670-eaad8329b84c, 1ee26af3-522f-440e-8fc1-18682422f9c6, 28cbb3c6-7645-41dd-b37e-92265ba6966e, 29f845ac-853f-43b9-a9a6-769b3c585b51, 2b99bbdf-c1ae-4be9-801e-2190d97c6625, 3099a4b0-edbe-496a-a468-25d8243020cd, 355b3c4d-f719-437a-b5e0-5d533b4657d3, 446a9039-f6a4-41c1-8fd2-d9c9970e0d5c, 45cfc743-66f8-4962-be3f-5473005b0d73, 56941f94-3e55-47b3-a755-d7d9a5fba3c6, 58796dff-f1c6-422b-aac6-7427fa4cb25b, 5bc366fa-a8d7-4ce5-8619-72cc950f6dcb, 5bcea9fa-cc91-4017-b9b9-557557f6972a, 5c48b9ef-a3d5-41d4-98b6-780d0dbb4d53, 65ab9317-bce7-48ff-8d96-a7d9bee7b133, 6a371cd3-cc0d-4157-a975-21a4ce567b6a, 6ad3cb4f-c554-4e6a-88ae-38df67f7c1ee, 6fa25816-aef1-4643-aa47-5a21f08e0728, 7389bc59-32a1-44ad-a3ae-a5be4d64e1d4, 756a3703-d0b2-4750-8b7c-958f087e1241, 76dbb139-752d-427e-9b1c-45ca2b9e6344, 81e91c17-a5f1-4cb3-8c4f-fc3e6577fd08, 872e0f3b-b390-40af-94d4-ee9dbfdce7a3, 8ac9025f-2747-45b9-9cd7-803688fdbeb5, 8b5cfad5-a6b6-485e-b063-472a0256b7fe, 90c080c1-e337-476a-bfcc-e455796edd91, 978cac71-3f8d-46b2-b148-6ef308b893bc, 98cadbec-a6e2-4ea2-a7a7-a9aa0c25193f, 9e7374cf-4d83-487f-8e48-401c606e4eff, aa535268-5f4a-4978-a0b5-7887bc458aab, ad957841-08e4-4f19-be8d-a4486eafb20d, b1b73f4d-1d3a-4d2e-94bc-a16ae2dd9d87, bbdaa492-2c3c-416b-8f8a-ab101fee938f, bda2f9d6-d176-484f-8953-c733ea972837, c0bfec72-0877-4789-b354-9cab6734b7e0, c77dd69e-54d5-44aa-80e2-41e90f2aa2ed, c8dcc0a1-1b10-44f7-8f00-de3d2606f949, d17c793f-6c3e-48e5-b935-bcac8da4863b, daef5f47-aa96-4d12-82c5-68186bff7512, dc490821-2344-429a-8568-9e5cf7ebbf1b, e117d280-71e0-414a-bd66-3900ec2888e7, e3553528-590e-4a93-a6f0-c3ed4e4efbfc, e682e3b2-71a5-4098-ad61-aa7c77a3a324, e6c0ef9e-70df-4830-8bd1-11f17d05eff0, ed027908-b79a-46a9-8f02-998b01ff972a, eee655bf-c5b8-4499-b09a-6a2041e926de, f247ccc1-dfd8-471a-9578-c63fdc41c93e, fa658513-ebf0-4d63-9d62-a6664576a660]
284486 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
284505 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
284505 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet => [05ffb1a4-f715-4594-9964-21132d5703d7, 0a14baa2-1134-4c54-92f6-0beeeccccafa, 0e801886-db08-4f5e-b670-c58cae302614, 12f7f7b1-154c-42aa-b726-d9fcf4cae6ff, 145f5882-1688-4485-9c0d-5bd0f0a59501, 1695fa04-eeac-4039-8096-c47f766b542a, 2117ff57-0e98-4bb5-a533-dedbea4b2b88, 22c7e0f8-769e-4393-b990-5bab4ff7085b, 284208db-6529-4cc1-82a8-4609dca42b5b, 33105c2a-1dc2-40af-bda6-6916c9124142, 3566108e-3f6b-410f-8f75-57521c7fc234, 3d8d423a-b87e-43d0-ba2e-f27961f22a44, 40ec0e56-dc26-446a-8a67-dc6507fbea08, 41947c3f-d812-454b-8daf-da9553e10ad4, 4d711a18-8153-45d0-abf2-544c87c1978e, 4ec6b925-cf2a-485f-8164-2b9f340ec4c7, 521e499c-4576-4fa9-a406-74474417be2c, 53bb98f7-1052-4306-bde4-afb3e209128e, 5645a4b4-ad50-4c0f-9fab-e791c1855c17, 56da4c18-e22c-4d5a-92b4-47cf9a0089b7, 5a774629-50e9-45f9-adbd-f2f22f2f9afe, 5ab18db5-829c-4df5-8da4-ebb5b05f8ab6, 5f68946d-a67e-480b-a4b1-48fe68cf7e98, 6a87aa3a-663c-4913-80e0-71298def1b7b, 6e743a2f-36e9-44b1-8704-81cdf9c7d383, 70ebb964-9ece-4067-a24b-10dd1fe9dadf, 7196e945-bbb6-4c5c-97a1-5d021c59c4a3, 73d08d3f-7ed8-47d1-be5f-615d8d0da562, 7d4df068-7843-4876-ae9a-92b944dc54f0]
284551 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
284569 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet
284569 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_2_001.parquet => [7e4e68dd-048a-47a9-9e81-3977f6dceba3, 836734d9-8e35-4ca1-bebc-a4fe163e4d28, 87c4ac86-f88d-4115-90af-47311ac241c7, 900c77bf-1f7c-4e4d-a1f5-cbc41113d35d, 9038e662-200b-4e78-a9ef-bd09c20ee3ce, 927e845b-86c5-4df8-9d7b-b76611cf3e2b, 961c6002-e7d4-42f5-8ffe-fa87b6005194, 983e4882-6a0d-42a2-a60f-a6eb609b9ce0, 9b66561c-1a0a-417b-834e-378dcdc1d23a, a09a25df-feba-4eba-bd81-272e6f543a0f, a1e9b9a7-4ee1-4546-88a7-9e8d5a40d2cd, a9277d29-57f6-4ebf-95a6-9561c3df332e, abbc9718-3549-4942-8dfd-7974d86b7d71, b113ff0a-c5d2-4a26-9536-9fddcaea5d2c, bd3a0fbe-ec25-4146-8963-97a30b212b51, be758a37-b07b-4587-9591-ec95add5c975, bfe854af-704c-470d-99f7-934d3009d760, c1e773b4-37ab-4d82-a672-1a9f41d88faa, c23416b2-71cb-4ab9-9623-16978aa2e3b1, c245ac9a-8888-466e-a117-1179eae28ed5, c4475eb2-32c9-43df-ad89-323e0c92d4f5, c9f7c9b6-a019-40ba-9cee-16487204d409, d19e5187-d880-4c24-ac66-18fca65f340d, dffc7299-a2d0-40a9-b323-398de36c152a, e824ead4-764d-4b2a-bf2f-7120607cc10d, e8b87f06-7f69-4f90-9978-fff2a5e72ea5, ebb20e80-a801-4105-ba2b-68022d88d41b, effc5c5c-8688-4027-b7a0-3bdeb15a514f, fd8b1604-abb0-4bc7-862e-d4a67e4c7d82]
284598 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 62 for /tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet
284617 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet
284617 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 62 results, for file /tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_1_001.parquet => [011a38b6-5d02-45b5-a6f8-db503739ae64, 0fa94e2f-3f59-42f1-a6f7-9dc936f25459, 104e066c-6034-4a35-9d6b-61d987e6d4c5, 14ed7d8b-103e-4717-a99b-e9765f0aab9c, 152c6fa7-3968-48fd-b41c-62ea7931bc9d, 16e1181b-7f51-4109-ada7-843683b88a5f, 1ce5e9e4-dffb-44ac-ae33-5b638857b652, 1fe8675b-20da-4904-92cc-53839d1fed0d, 24d8f789-4e44-4cc6-9b30-02bd58c3603a, 31b22ca9-02af-4ad9-93fe-2473beddeb88, 3908d003-1068-4eed-8514-6305cf3f8492, 407d6840-0b1a-44d9-b29b-70c7d05276f1, 43998fdf-e98b-4e7e-9e4a-fc49aaa9fb8a, 462e1777-9c41-4ec6-993a-211430ce8f64, 4ca5fe89-abe0-40a5-8bc5-8a18610d46f8, 50350d97-1b3f-499a-a155-5adb1236e4fb, 513f4f7d-948f-43a2-8aac-47a97d6c39f3, 554fa8a9-900c-4131-b00f-2930a508899e, 5a963b32-60d3-4d89-b6ae-d1b3848d0066, 5b721f70-3859-438f-8f6a-6b3bddc4a194, 5cc9b4fe-b181-4e73-8e55-09a2d418ec12, 60787c3b-16fa-4ee6-bf85-571d3e66cedb, 625eccea-6d38-4eba-8be9-9baf0cf289ca, 62ab780e-07b6-4488-a745-327a6c08e22d, 72eaab3a-c6a1-40e3-992c-5924a5570017, 733e8f78-c189-4101-a559-99755020ef62, 7893508c-255d-4866-889c-a91b985d89c7, 7f3ec73c-e356-49db-ab5b-c37100119faa, 813a6fca-4b5e-466a-b79a-f73c4c1e5ac7, 88756c4c-b6da-49ff-adcc-7ebbf79e55d0, 89c7b0f6-cbb3-4eda-ae79-113b6b6f482d, 8ccbf915-faa2-4918-9eaf-1aee5f4e9b24, 8f79c79f-c039-4f9e-94ea-973039efe3ed, 9279d9ec-8463-4219-bda4-70e7ec293a57, 95b7d2a8-fdfd-404a-b336-883a9bf52540, a031782b-8b09-48a4-9bd3-f0289f04622b, a21bb7c4-1237-4fa1-8afe-7072432e8deb, a6ea603a-eab0-4487-989c-69222d99a4dd, ad08e6b9-f760-4eb7-898a-0713c14b00a3, afa0a2b1-ea05-468c-99e6-782134a6c55d, b43b86cd-2b89-4a35-81c0-8933ff05be87, b5ce999f-ddd3-4416-9745-6cdfe9dfba3a, b62141bd-5bf1-4ed4-88e1-e7a5256dcc25, b68b29d8-210f-49cb-991f-580b789b9505, b9dcb0a1-055a-44f3-a913-468df127d0fc, bb2db857-8b18-4d1a-b111-2aafe8efa8cb, bc39214a-fc1c-410c-9669-a85aca9dda30, c6a59ed7-9c24-4c1c-b6fe-e4ddb20330a5, d143ad07-fda0-44b4-991a-5dcaa92f011a, d19f47c8-c0d2-49e9-9d0d-f6a276d3f2d4, d4e79e9f-e8d0-48b4-acd4-5a1a5a9b03b6, da6e82fc-9f8b-4d24-af9d-513a0302ead1, e1f681f3-4c55-4d0f-a2ac-fd3836cc30aa, e225a206-2378-410c-bf05-40317cd21ecd, e610bd17-95e4-4e7e-9968-31414a7517de, eb84a131-4a96-4362-95be-809fcd4a10b7, eeacb81c-7450-4756-9509-618db90f7665, ef54094b-354e-4530-9855-125ea8816a87, efa0e373-fe26-4f6c-b72f-aaefc38fb6cb, f10c2874-b41b-4d63-a7c3-9eab2a2b1964, f89f58e4-f4f0-4c3a-84d8-59521f696d7e, fcd9f819-3cb7-47b1-a0e6-5dbd2a27c541]
284715 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=172}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=52}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=62}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=58}}}
285131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
285131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=60c6bf93-aa89-4717-8b6f-6120aed5076a}, 1=BucketInfo {bucketType=UPDATE, fileLoc=d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d6f06a16-810c-4c9a-9ea3-6c44968e0d1d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{60c6bf93-aa89-4717-8b6f-6120aed5076a=0, d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5=1, d6f06a16-810c-4c9a-9ea3-6c44968e0d1d=2}
285152 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
285152 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
285287 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 60c6bf93-aa89-4717-8b6f-6120aed5076a
285772 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5
285826 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
286214 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
286263 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file d6f06a16-810c-4c9a-9ea3-6c44968e0d1d
287168 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
287168 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
287181 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
287181 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
287365 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
287779 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
287779 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
287870 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316093541
287885 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Checking if compaction needs to be run on /tmp/junit1829311128283141443
287886 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Compacting merge on read table /tmp/junit1829311128283141443
287920 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
288001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316093541
288001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316093541
288272 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
289075 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
289388 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
289973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
289973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
289987 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
289987 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
290167 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
290427 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
290582 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316093541 as complete
290582 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316093541
290610 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [20180316093541]
290682 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
290684 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_0_20180316093541.parquet	true
290685 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
290687 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_1_20180316093541.parquet	true
290687 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
290689 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/60c6bf93-aa89-4717-8b6f-6120aed5076a_2_20180316093541.parquet	true
290693 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20180316093541]
291106 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20180316093541] rollback is complete
291106 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
291150 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
291278 [DataNode: [[[DISK]file:/tmp/1521189273775-0/dfs/data/data1/, [DISK]file:/tmp/1521189273775-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:44231] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1086827514-172.17.0.3-1521189273870 (Datanode Uuid 765d122c-38e9-4b32-8399-ee14b11b4713) service to localhost/127.0.0.1:44231 interrupted
291278 [DataNode: [[[DISK]file:/tmp/1521189273775-0/dfs/data/data1/, [DISK]file:/tmp/1521189273775-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:44231] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1086827514-172.17.0.3-1521189273870 (Datanode Uuid 765d122c-38e9-4b32-8399-ee14b11b4713) service to localhost/127.0.0.1:44231
291283 [spirals-vortex:40727.activeMasterManager-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
291305 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@58d455b8] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 7, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 71.417 sec - in com.uber.hoodie.table.TestMergeOnReadTable
Running com.uber.hoodie.config.HoodieWriteConfigTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.config.HoodieWriteConfigTest
291555 [Thread-4780] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit5778346105746453762/2015/03/17/.8f320fee-5964-41bd-ab4d-bdb05013a831_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291555 [Thread-4774] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit5778346105746453762/2015/03/16/.095658b7-3819-4d6d-b003-41e7adc209b7_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291562 [Thread-5132] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/.d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291562 [Thread-5085] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/.d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291555 [Thread-5033] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/.60c6bf93-aa89-4717-8b6f-6120aed5076a_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291562 [Thread-5045] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/17/.d6f06a16-810c-4c9a-9ea3-6c44968e0d1d_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291564 [Thread-4500] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit5212398239343190464/2016/03/15/.817cd1f3-ab1b-47cb-92f2-6fb7e0f5bc38_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291564 [Thread-5039] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/.d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291566 [Thread-4768] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit5778346105746453762/2016/03/15/.eb662fe9-5939-4be1-a3e2-1e9c73809697_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291568 [Thread-5124] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/.d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291569 [Thread-4508] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit5212398239343190464/2015/03/17/.9f7801c8-4782-475e-968e-6339db5eaa01_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291571 [Thread-4516] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit5212398239343190464/2015/03/16/.9d2603ec-14b6-423b-8c69-3230bb7622f5_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291577 [Thread-5073] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/.60c6bf93-aa89-4717-8b6f-6120aed5076a_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291578 [Thread-5079] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2015/03/16/.d70f5eb1-dbf2-4b0e-bf2c-bcb41fcc83e5_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291578 [Thread-5140] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:44231/tmp/junit1829311128283141443/2016/03/15/.60c6bf93-aa89-4717-8b6f-6120aed5076a_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
291925 [Executor task launch worker-0-SendThread(localhost:63778)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622def27c30009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)

Results :

Tests in error: 
  TestUpdateMapFunction.testSchemaEvolutionOnUpdate:106  NullPointer

Tests run: 66, Failures: 0, Errors: 1, Skipped: 1

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hoodie ............................................. SUCCESS [  0.053 s]
[INFO] hoodie-common ...................................... SUCCESS [01:16 min]
[INFO] hoodie-hadoop-mr ................................... SUCCESS [  5.140 s]
[INFO] hoodie-client ...................................... FAILURE [04:56 min]
[INFO] hoodie-spark ....................................... SKIPPED
[INFO] hoodie-hive ........................................ SKIPPED
[INFO] hoodie-utilities ................................... SKIPPED
[INFO] hoodie-cli ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 06:18 min
[INFO] Finished at: 2018-03-16T09:35:47+01:00
[INFO] Final Memory: 59M/1340M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hoodie-client: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/uber/hudi/354203634/hoodie-client/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hoodie-client
