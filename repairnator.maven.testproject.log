[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Apache SAMOA
[INFO] samoa-instances
[INFO] samoa-api
[INFO] samoa-test
[INFO] samoa-local
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache SAMOA 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa ---
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-api
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-instances
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-local
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-storm
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-flink
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-apex
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-samza
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-test
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/samoa-threads
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250252707/bin
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[INFO] Scan 383 files header done in 920.77ms.
[INFO] 
 * uptodate header on 373 files.
 * fail header on 10 files.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa ---
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-instances 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-instances ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-instances ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250252707/samoa-instances/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250252707/samoa-instances/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-instances ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250252707/samoa-instances/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom (3 KB at 4.6 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom (3 KB at 42.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar (67 KB at 948.5 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.instances.ArffLoaderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.023 sec - in org.apache.samoa.instances.ArffLoaderTest

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-api 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-api ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250252707/samoa-api/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-api ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
2017-07-05 12:33:20,277 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
2017-07-05 12:33:20,950 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
2017-07-05 12:33:21,137 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:21,171 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:21,223 INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1049)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2017-07-05 12:33:21,224 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:21,224 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:21,226 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:21,229 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:21
2017-07-05 12:33:21,232 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:21,232 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:21,234 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:21,235 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:21,285 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:21,286 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:21,286 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:21,288 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:21,288 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:21,289 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:21,289 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:21,289 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:21,289 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:21,298 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:21,298 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:21,299 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:21,299 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:21,302 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:21,428 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:21,428 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:21,429 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:21,429 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:21,436 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:21,448 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:21,449 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:21,449 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:21,450 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:21,454 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:21,454 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:21,454 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:21,456 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:21,456 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:21,459 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:21,460 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:21,460 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:21,460 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:21,467 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:21,468 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:21,468 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:21,534 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:21,568 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 12:33:21,592 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 12:33:21,798 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 12:33:21,828 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 12:33:21,861 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 12:33:21,958 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 12:33:21,959 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 12:33:21,962 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 12:33:22,002 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 12:33:22,064 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-07-05 12:33:22,070 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 12:33:22,086 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:22,090 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 12:33:22,090 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:22,122 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 12:33:22,124 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:22,145 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 43469
2017-07-05 12:33:22,145 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:22,187 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_43469_hdfs____.1actoe/webapp
2017-07-05 12:33:22,546 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43469
2017-07-05 12:33:22,565 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:22,566 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:22,568 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:22,568 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:22,569 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:22,570 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:22
2017-07-05 12:33:22,570 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:22,570 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:22,571 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:22,571 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:22,584 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:22,584 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:22,585 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:22,585 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:22,585 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:22,586 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:22,586 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:22,586 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:22,586 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:22,587 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:22,587 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:22,588 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:22,588 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:22,589 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:22,589 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:22,590 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:22,591 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:22,591 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:22,597 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:22,598 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:22,598 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:22,598 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:22,599 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:22,600 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:22,600 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:22,601 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:22,601 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:22,601 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:22,601 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:22,602 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:22,602 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:22,602 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:22,605 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:22,605 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:22,605 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:22,626 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:22,642 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:22,646 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 12:33:22,646 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 12:33:22,647 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 12:33:22,672 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 12:33:22,681 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 12:33:22,682 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 12:33:22,689 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 12:33:22,690 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 12:33:22,747 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 12:33:22,747 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 141 msecs
2017-07-05 12:33:23,285 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 12:33:23,297 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:23,313 INFO  [Socket Reader #1 for port 45390] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 45390
2017-07-05 12:33:23,352 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:45390 to access this namenode/service.
2017-07-05 12:33:23,358 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 12:33:23,377 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:23,378 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:23,378 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 12:33:23,379 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 12:33:23,380 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 12:33:23,380 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 12:33:23,395 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 12:33:23,395 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 12:33:23,395 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 12:33:23,395 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 12:33:23,396 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 12:33:23,396 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 17 msec
2017-07-05 12:33:23,420 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:23,420 INFO  [IPC Server listener on 45390] ipc.Server (Server.java:run(674)) - IPC Server listener on 45390: starting
2017-07-05 12:33:23,427 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:45390
2017-07-05 12:33:23,428 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 12:33:23,435 INFO  [CacheReplicationMonitor(2031463753)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 12:33:23,436 INFO  [CacheReplicationMonitor(2031463753)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2412092283 milliseconds
2017-07-05 12:33:23,438 INFO  [CacheReplicationMonitor(2031463753)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2017-07-05 12:33:23,443 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 12:33:23,443 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 12:33:23,500 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 12:33:23,501 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 12:33:23,506 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 12:33:23,513 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:48334
2017-07-05 12:33:23,516 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 12:33:23,516 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 12:33:23,524 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 12:33:23,525 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:23,526 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 12:33:23,526 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:23,530 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:23,531 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 48301
2017-07-05 12:33:23,531 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:23,536 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_48301_datanode____fjccwn/webapp
2017-07-05 12:33:23,663 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:48301
2017-07-05 12:33:23,667 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 12:33:23,667 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 12:33:23,682 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:23,683 INFO  [Socket Reader #1 for port 33110] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 33110
2017-07-05 12:33:23,690 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:33110
2017-07-05 12:33:23,700 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 12:33:23,703 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 12:33:23,713 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:45390 starting to offer service
2017-07-05 12:33:23,723 INFO  [IPC Server listener on 33110] ipc.Server (Server.java:run(674)) - IPC Server listener on 33110: starting
2017-07-05 12:33:23,723 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:24,031 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 12:33:24,045 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:24,046 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:24,046 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:24,078 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:24,078 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:24,078 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:24,164 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:24,165 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:24,166 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1805789181-172.17.0.8-1499250801482 is not formatted.
2017-07-05 12:33:24,166 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:24,166 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1805789181-172.17.0.8-1499250801482 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1805789181-172.17.0.8-1499250801482/current
2017-07-05 12:33:24,186 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:24,186 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1805789181-172.17.0.8-1499250801482 is not formatted.
2017-07-05 12:33:24,186 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:24,186 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1805789181-172.17.0.8-1499250801482 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1805789181-172.17.0.8-1499250801482/current
2017-07-05 12:33:24,204 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:24,205 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:24,227 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=301590330;bpid=BP-1805789181-172.17.0.8-1499250801482;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=301590330;c=0;bpid=BP-1805789181-172.17.0.8-1499250801482;dnuuid=null
2017-07-05 12:33:24,244 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:24,248 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:24,258 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID fdec1c88-009e-4027-ba8c-0d41eb54f2a4
2017-07-05 12:33:24,291 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 12:33:24,292 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 12:33:24,293 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 12:33:24,293 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 12:33:24,301 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 12:33:24,306 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499257989306 with interval 21600000
2017-07-05 12:33:24,306 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:24,307 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1805789181-172.17.0.8-1499250801482 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:24,307 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1805789181-172.17.0.8-1499250801482 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:24,327 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1805789181-172.17.0.8-1499250801482 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 20ms
2017-07-05 12:33:24,327 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1805789181-172.17.0.8-1499250801482 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 20ms
2017-07-05 12:33:24,329 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1805789181-172.17.0.8-1499250801482: 22ms
2017-07-05 12:33:24,329 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1805789181-172.17.0.8-1499250801482 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:24,329 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1805789181-172.17.0.8-1499250801482 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:24,330 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1805789181-172.17.0.8-1499250801482 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 12:33:24,330 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1805789181-172.17.0.8-1499250801482 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 12:33:24,331 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-05 12:33:24,334 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid null) service to localhost/127.0.0.1:45390 beginning handshake with NN
2017-07-05 12:33:24,345 INFO  [IPC Server handler 2 on 45390] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=fdec1c88-009e-4027-ba8c-0d41eb54f2a4, infoPort=48301, ipcPort=33110, storageInfo=lv=-56;cid=testClusterID;nsid=301590330;c=0) storage fdec1c88-009e-4027-ba8c-0d41eb54f2a4
2017-07-05 12:33:24,350 INFO  [IPC Server handler 2 on 45390] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:24,351 INFO  [IPC Server handler 2 on 45390] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:48334
2017-07-05 12:33:24,358 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid null) service to localhost/127.0.0.1:45390 successfully registered with NN
2017-07-05 12:33:24,359 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:45390 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 12:33:24,359 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2246)) - dn.getCapacity() == 0
2017-07-05 12:33:24,363 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:24,381 INFO  [IPC Server handler 4 on 45390] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:24,381 INFO  [IPC Server handler 4 on 45390] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-1e15d3d9-5be2-4259-9314-43addc4a0741 for DN 127.0.0.1:48334
2017-07-05 12:33:24,384 INFO  [IPC Server handler 4 on 45390] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-a6129912-6094-4a34-b904-c7584863a3ac for DN 127.0.0.1:48334
2017-07-05 12:33:24,404 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid fdec1c88-009e-4027-ba8c-0d41eb54f2a4) service to localhost/127.0.0.1:45390 trying to claim ACTIVE state with txid=1
2017-07-05 12:33:24,405 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid fdec1c88-009e-4027-ba8c-0d41eb54f2a4) service to localhost/127.0.0.1:45390
2017-07-05 12:33:24,420 INFO  [IPC Server handler 5 on 45390] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-a6129912-6094-4a34-b904-c7584863a3ac,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:24,423 INFO  [IPC Server handler 5 on 45390] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-a6129912-6094-4a34-b904-c7584863a3ac node DatanodeRegistration(127.0.0.1, datanodeUuid=fdec1c88-009e-4027-ba8c-0d41eb54f2a4, infoPort=48301, ipcPort=33110, storageInfo=lv=-56;cid=testClusterID;nsid=301590330;c=0), blocks: 0, hasStaleStorages: true, processing time: 5 msecs
2017-07-05 12:33:24,423 INFO  [IPC Server handler 5 on 45390] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-1e15d3d9-5be2-4259-9314-43addc4a0741,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:24,424 INFO  [IPC Server handler 5 on 45390] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-1e15d3d9-5be2-4259-9314-43addc4a0741 node DatanodeRegistration(127.0.0.1, datanodeUuid=fdec1c88-009e-4027-ba8c-0d41eb54f2a4, infoPort=48301, ipcPort=33110, storageInfo=lv=-56;cid=testClusterID;nsid=301590330;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 12:33:24,452 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 44 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@2f760fb2
2017-07-05 12:33:24,452 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:24,456 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 12:33:24,457 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:24,457 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 12:33:24,457 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 12:33:24,460 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:24,467 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1805789181-172.17.0.8-1499250801482 to blockPoolScannerMap, new size=1
2017-07-05 12:33:24,472 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:24,481 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:24,520 INFO  [IPC Server handler 2 on 45390] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 12:33:24,548 INFO  [IPC Server handler 3 on 45390] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:24,590 INFO  [IPC Server handler 4 on 45390] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1805789181-172.17.0.8-1499250801482 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-1e15d3d9-5be2-4259-9314-43addc4a0741:NORMAL:127.0.0.1:48334|RBW]]}
2017-07-05 12:33:24,705 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-2041758607_1 at /127.0.0.1:57568 [Receiving block BP-1805789181-172.17.0.8-1499250801482:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1805789181-172.17.0.8-1499250801482:blk_1073741825_1001 src: /127.0.0.1:57568 dest: /127.0.0.1:48334
2017-07-05 12:33:24,773 INFO  [PacketResponder: BP-1805789181-172.17.0.8-1499250801482:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:57568, dest: /127.0.0.1:48334, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2041758607_1, offset: 0, srvID: fdec1c88-009e-4027-ba8c-0d41eb54f2a4, blockid: BP-1805789181-172.17.0.8-1499250801482:blk_1073741825_1001, duration: 31459953
2017-07-05 12:33:24,774 INFO  [PacketResponder: BP-1805789181-172.17.0.8-1499250801482:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1805789181-172.17.0.8-1499250801482:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:24,795 INFO  [IPC Server handler 7 on 45390] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-1e15d3d9-5be2-4259-9314-43addc4a0741:NORMAL:127.0.0.1:48334|RBW]]} has not reached minimal replication 1
2017-07-05 12:33:24,796 INFO  [IPC Server handler 7 on 45390] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 12:33:24,796 INFO  [IPC Server handler 7 on 45390] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 12:33:24,798 INFO  [IPC Server handler 8 on 45390] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:48334 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-1e15d3d9-5be2-4259-9314-43addc4a0741:NORMAL:127.0.0.1:48334|RBW]]} size 1
2017-07-05 12:33:25,203 INFO  [IPC Server handler 9 on 45390] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-2041758607_1
2017-07-05 12:33:25,210 INFO  [IPC Server handler 5 on 45390] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 12:33:25,212 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 12:33:25,213 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 12:33:25,213 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37eeec90] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 12:33:25,213 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 12:33:25,241 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:25,341 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 12:33:25,342 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 33110
2017-07-05 12:33:25,345 INFO  [IPC Server listener on 33110] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 33110
2017-07-05 12:33:25,345 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:25,348 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid fdec1c88-009e-4027-ba8c-0d41eb54f2a4) service to localhost/127.0.0.1:45390 interrupted
2017-07-05 12:33:25,348 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid fdec1c88-009e-4027-ba8c-0d41eb54f2a4) service to localhost/127.0.0.1:45390
2017-07-05 12:33:25,349 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1805789181-172.17.0.8-1499250801482 (Datanode Uuid fdec1c88-009e-4027-ba8c-0d41eb54f2a4)
2017-07-05 12:33:25,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1805789181-172.17.0.8-1499250801482 from blockPoolScannerMap
2017-07-05 12:33:25,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:45390] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1805789181-172.17.0.8-1499250801482
2017-07-05 12:33:25,352 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@664a51b9] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 12:33:25,353 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 12:33:25,353 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 12:33:25,354 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 12:33:25,354 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 12:33:25,355 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 12:33:25,355 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:25,356 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 12:33:25,356 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@37f21974] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 12:33:25,357 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 1 2 
2017-07-05 12:33:25,358 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@6f80fafe] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 12:33:25,360 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-05 12:33:25,362 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-05 12:33:25,368 INFO  [CacheReplicationMonitor(2031463753)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 12:33:25,373 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 45390
2017-07-05 12:33:25,376 INFO  [IPC Server listener on 45390] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 45390
2017-07-05 12:33:25,376 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:25,377 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@13518f37] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 12:33:25,378 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@165b8a71] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 12:33:25,402 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:25,403 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 12:33:25,408 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:25,508 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 12:33:25,510 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 12:33:25,510 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 12:33:25,557 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 12:33:25,562 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:25,563 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:25,564 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:25,564 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:25,565 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:25,566 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:25
2017-07-05 12:33:25,566 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:25,566 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,567 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:25,567 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:25,687 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:25,688 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:25,688 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:25,688 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:25,688 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:25,689 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:25,689 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:25,689 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:25,689 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:25,689 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:25,690 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:25,690 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:25,690 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:25,690 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:25,691 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:25,691 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,691 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:25,691 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:25,697 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:25,697 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:25,697 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,698 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:25,698 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:25,699 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:25,700 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:25,700 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:25,700 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:25,700 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:25,700 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:25,700 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,701 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:25,701 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:25,702 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:25,702 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:25,702 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:25,704 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:25,736 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 12:33:25,755 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 12:33:25,801 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 12:33:25,803 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 12:33:25,804 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 12:33:25,806 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 12:33:25,806 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 12:33:25,807 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 12:33:25,812 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 12:33:25,813 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 12:33:25,814 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:25,815 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 12:33:25,815 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:25,816 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 12:33:25,816 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:25,818 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 58244
2017-07-05 12:33:25,818 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:25,824 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_58244_hdfs____6u5o49/webapp
2017-07-05 12:33:25,939 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:58244
2017-07-05 12:33:25,942 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:25,942 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:25,943 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:25,944 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:25,944 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:25,945 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:25
2017-07-05 12:33:25,945 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:25,946 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,948 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:25,948 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:25,968 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:25,968 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:25,969 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:25,969 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:25,969 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:25,970 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:25,970 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:25,970 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:25,971 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:25,971 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:25,971 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:25,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:25,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:25,973 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:25,973 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:25,974 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,974 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:25,975 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:25,990 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:25,990 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:25,991 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,991 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:25,992 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:25,996 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:25,996 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:25,996 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:25,997 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:25,997 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:25,997 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:25,998 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:25,998 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:25,999 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:26,001 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:26,001 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:26,002 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:26,012 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:26,026 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:26,028 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 12:33:26,029 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 12:33:26,030 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 12:33:26,031 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 12:33:26,032 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 12:33:26,033 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 12:33:26,033 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 12:33:26,034 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 12:33:26,063 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 12:33:26,064 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 62 msecs
2017-07-05 12:33:26,064 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 12:33:26,065 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:26,219 INFO  [Socket Reader #1 for port 35772] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 35772
2017-07-05 12:33:26,224 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:35772 to access this namenode/service.
2017-07-05 12:33:26,226 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 12:33:26,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:26,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:26,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 12:33:26,245 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 12:33:26,245 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 12:33:26,245 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 12:33:26,262 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 12:33:26,263 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 12:33:26,263 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 12:33:26,263 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 12:33:26,263 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 12:33:26,263 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 18 msec
2017-07-05 12:33:26,268 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:26,270 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:35772
2017-07-05 12:33:26,270 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 12:33:26,271 INFO  [IPC Server listener on 35772] ipc.Server (Server.java:run(674)) - IPC Server listener on 35772: starting
2017-07-05 12:33:26,271 INFO  [CacheReplicationMonitor(14954711)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 12:33:26,272 INFO  [CacheReplicationMonitor(14954711)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2412095120 milliseconds
2017-07-05 12:33:26,275 INFO  [CacheReplicationMonitor(14954711)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 12:33:26,279 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 12:33:26,279 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 12:33:26,303 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 12:33:26,304 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 12:33:26,304 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 12:33:26,306 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:45228
2017-07-05 12:33:26,306 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 12:33:26,306 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 12:33:26,308 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 12:33:26,309 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:26,310 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 12:33:26,310 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:26,311 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:26,312 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 44023
2017-07-05 12:33:26,312 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:26,318 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_44023_datanode____j498wo/webapp
2017-07-05 12:33:26,425 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44023
2017-07-05 12:33:26,426 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 12:33:26,426 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 12:33:26,427 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:26,428 INFO  [Socket Reader #1 for port 51391] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 51391
2017-07-05 12:33:26,430 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:51391
2017-07-05 12:33:26,433 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 12:33:26,434 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 12:33:26,435 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35772 starting to offer service
2017-07-05 12:33:26,435 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:26,435 INFO  [IPC Server listener on 51391] ipc.Server (Server.java:run(674)) - IPC Server listener on 51391: starting
2017-07-05 12:33:26,446 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 12:33:26,450 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:26,450 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:26,458 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:26,458 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:26,458 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:26,495 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:26,496 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:26,496 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:26,563 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:26,563 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:26,584 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:26,584 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:26,585 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-279280342-172.17.0.8-1499250805704 is not formatted.
2017-07-05 12:33:26,585 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:26,585 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-279280342-172.17.0.8-1499250805704 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-279280342-172.17.0.8-1499250805704/current
2017-07-05 12:33:26,595 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:26,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-279280342-172.17.0.8-1499250805704 is not formatted.
2017-07-05 12:33:26,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:26,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-279280342-172.17.0.8-1499250805704 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-279280342-172.17.0.8-1499250805704/current
2017-07-05 12:33:26,603 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:26,603 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:26,626 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1758065087;bpid=BP-279280342-172.17.0.8-1499250805704;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1758065087;c=0;bpid=BP-279280342-172.17.0.8-1499250805704;dnuuid=null
2017-07-05 12:33:26,658 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID b8bfb684-f64a-437b-b2a2-c1139afa7ada
2017-07-05 12:33:26,659 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 12:33:26,660 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 12:33:26,660 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 12:33:26,660 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 12:33:26,665 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 12:33:26,666 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499266162666 with interval 21600000
2017-07-05 12:33:26,667 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:26,669 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:26,667 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:26,672 INFO  [Thread-142] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-279280342-172.17.0.8-1499250805704 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:26,674 INFO  [Thread-143] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-279280342-172.17.0.8-1499250805704 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:26,680 INFO  [Thread-143] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-279280342-172.17.0.8-1499250805704 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 6ms
2017-07-05 12:33:26,684 INFO  [Thread-142] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-279280342-172.17.0.8-1499250805704 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 11ms
2017-07-05 12:33:26,684 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-279280342-172.17.0.8-1499250805704: 16ms
2017-07-05 12:33:26,685 INFO  [Thread-146] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-279280342-172.17.0.8-1499250805704 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:26,685 INFO  [Thread-146] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-279280342-172.17.0.8-1499250805704 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 1ms
2017-07-05 12:33:26,689 INFO  [Thread-147] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-279280342-172.17.0.8-1499250805704 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:26,689 INFO  [Thread-147] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-279280342-172.17.0.8-1499250805704 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 12:33:26,689 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 5ms
2017-07-05 12:33:26,690 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid null) service to localhost/127.0.0.1:35772 beginning handshake with NN
2017-07-05 12:33:26,694 INFO  [IPC Server handler 4 on 35772] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=b8bfb684-f64a-437b-b2a2-c1139afa7ada, infoPort=44023, ipcPort=51391, storageInfo=lv=-56;cid=testClusterID;nsid=1758065087;c=0) storage b8bfb684-f64a-437b-b2a2-c1139afa7ada
2017-07-05 12:33:26,694 INFO  [IPC Server handler 4 on 35772] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:26,695 INFO  [IPC Server handler 4 on 35772] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:45228
2017-07-05 12:33:26,696 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid null) service to localhost/127.0.0.1:35772 successfully registered with NN
2017-07-05 12:33:26,697 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:35772 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 12:33:26,700 INFO  [IPC Server handler 3 on 35772] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:26,701 INFO  [IPC Server handler 3 on 35772] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-b19d0bc9-b182-47c8-b245-570ddb49486c for DN 127.0.0.1:45228
2017-07-05 12:33:26,701 INFO  [IPC Server handler 3 on 35772] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2 for DN 127.0.0.1:45228
2017-07-05 12:33:26,702 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid b8bfb684-f64a-437b-b2a2-c1139afa7ada) service to localhost/127.0.0.1:35772 trying to claim ACTIVE state with txid=1
2017-07-05 12:33:26,702 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid b8bfb684-f64a-437b-b2a2-c1139afa7ada) service to localhost/127.0.0.1:35772
2017-07-05 12:33:26,705 INFO  [IPC Server handler 7 on 35772] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-b19d0bc9-b182-47c8-b245-570ddb49486c,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:26,706 INFO  [IPC Server handler 7 on 35772] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-b19d0bc9-b182-47c8-b245-570ddb49486c node DatanodeRegistration(127.0.0.1, datanodeUuid=b8bfb684-f64a-437b-b2a2-c1139afa7ada, infoPort=44023, ipcPort=51391, storageInfo=lv=-56;cid=testClusterID;nsid=1758065087;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 12:33:26,706 INFO  [IPC Server handler 7 on 35772] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:26,706 INFO  [IPC Server handler 7 on 35772] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2 node DatanodeRegistration(127.0.0.1, datanodeUuid=b8bfb684-f64a-437b-b2a2-c1139afa7ada, infoPort=44023, ipcPort=51391, storageInfo=lv=-56;cid=testClusterID;nsid=1758065087;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 12:33:26,707 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 4 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4cbc7d3f
2017-07-05 12:33:26,708 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:26,708 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 12:33:26,708 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:26,709 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 12:33:26,709 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 12:33:26,712 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:26,714 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-279280342-172.17.0.8-1499250805704 to blockPoolScannerMap, new size=1
2017-07-05 12:33:26,775 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:26,783 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:26,788 INFO  [IPC Server handler 0 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 12:33:26,793 INFO  [IPC Server handler 2 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:26,802 INFO  [IPC Server handler 1 on 35772] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-279280342-172.17.0.8-1499250805704 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|RBW]]}
2017-07-05 12:33:26,808 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1307375831_1 at /127.0.0.1:38605 [Receiving block BP-279280342-172.17.0.8-1499250805704:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-279280342-172.17.0.8-1499250805704:blk_1073741825_1001 src: /127.0.0.1:38605 dest: /127.0.0.1:45228
2017-07-05 12:33:26,823 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:38605, dest: /127.0.0.1:45228, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1307375831_1, offset: 0, srvID: b8bfb684-f64a-437b-b2a2-c1139afa7ada, blockid: BP-279280342-172.17.0.8-1499250805704:blk_1073741825_1001, duration: 3406909
2017-07-05 12:33:26,824 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:26,826 INFO  [IPC Server handler 5 on 35772] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:45228 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|RBW]]} size 0
2017-07-05 12:33:26,831 INFO  [IPC Server handler 4 on 35772] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-1307375831_1
2017-07-05 12:33:26,834 INFO  [IPC Server handler 3 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:26,842 INFO  [IPC Server handler 7 on 35772] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-279280342-172.17.0.8-1499250805704 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|RBW]]}
2017-07-05 12:33:26,848 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1307375831_1 at /127.0.0.1:38606 [Receiving block BP-279280342-172.17.0.8-1499250805704:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-279280342-172.17.0.8-1499250805704:blk_1073741826_1002 src: /127.0.0.1:38606 dest: /127.0.0.1:45228
2017-07-05 12:33:26,858 INFO  [IPC Server handler 8 on 35772] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:45228 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2:NORMAL:127.0.0.1:45228|FINALIZED]]} size 0
2017-07-05 12:33:26,859 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:38606, dest: /127.0.0.1:45228, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1307375831_1, offset: 0, srvID: b8bfb684-f64a-437b-b2a2-c1139afa7ada, blockid: BP-279280342-172.17.0.8-1499250805704:blk_1073741826_1002, duration: 2080001
2017-07-05 12:33:26,859 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:26,862 INFO  [IPC Server handler 9 on 35772] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_-1307375831_1
2017-07-05 12:33:26,865 INFO  [IPC Server handler 0 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:26,874 INFO  [IPC Server handler 2 on 35772] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-279280342-172.17.0.8-1499250805704 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|RBW]]}
2017-07-05 12:33:26,878 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1307375831_1 at /127.0.0.1:38607 [Receiving block BP-279280342-172.17.0.8-1499250805704:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-279280342-172.17.0.8-1499250805704:blk_1073741827_1003 src: /127.0.0.1:38607 dest: /127.0.0.1:45228
2017-07-05 12:33:26,886 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:38607, dest: /127.0.0.1:45228, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1307375831_1, offset: 0, srvID: b8bfb684-f64a-437b-b2a2-c1139afa7ada, blockid: BP-279280342-172.17.0.8-1499250805704:blk_1073741827_1003, duration: 2247796
2017-07-05 12:33:26,886 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:26,887 INFO  [IPC Server handler 1 on 35772] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:45228 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|RBW]]} size 0
2017-07-05 12:33:26,890 INFO  [IPC Server handler 6 on 35772] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_-1307375831_1
2017-07-05 12:33:26,895 INFO  [IPC Server handler 5 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:26,899 INFO  [IPC Server handler 4 on 35772] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-279280342-172.17.0.8-1499250805704 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|RBW]]}
2017-07-05 12:33:26,901 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1307375831_1 at /127.0.0.1:38608 [Receiving block BP-279280342-172.17.0.8-1499250805704:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-279280342-172.17.0.8-1499250805704:blk_1073741828_1004 src: /127.0.0.1:38608 dest: /127.0.0.1:45228
2017-07-05 12:33:26,911 INFO  [IPC Server handler 3 on 35772] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:45228 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2:NORMAL:127.0.0.1:45228|FINALIZED]]} size 0
2017-07-05 12:33:26,912 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:38608, dest: /127.0.0.1:45228, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1307375831_1, offset: 0, srvID: b8bfb684-f64a-437b-b2a2-c1139afa7ada, blockid: BP-279280342-172.17.0.8-1499250805704:blk_1073741828_1004, duration: 6439548
2017-07-05 12:33:26,912 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:26,914 INFO  [IPC Server handler 7 on 35772] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_-1307375831_1
2017-07-05 12:33:26,916 INFO  [IPC Server handler 8 on 35772] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 12:33:26,917 INFO  [IPC Server handler 8 on 35772] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 12:33:26,917 INFO  [IPC Server handler 8 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 12:33:26,919 INFO  [IPC Server handler 9 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:26,924 INFO  [IPC Server handler 0 on 35772] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-279280342-172.17.0.8-1499250805704 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2:NORMAL:127.0.0.1:45228|RBW]]}
2017-07-05 12:33:26,929 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1307375831_1 at /127.0.0.1:38609 [Receiving block BP-279280342-172.17.0.8-1499250805704:blk_1073741829_1005]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-279280342-172.17.0.8-1499250805704:blk_1073741829_1005 src: /127.0.0.1:38609 dest: /127.0.0.1:45228
2017-07-05 12:33:26,935 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:38609, dest: /127.0.0.1:45228, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1307375831_1, offset: 0, srvID: b8bfb684-f64a-437b-b2a2-c1139afa7ada, blockid: BP-279280342-172.17.0.8-1499250805704:blk_1073741829_1005, duration: 1376892
2017-07-05 12:33:26,936 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:26,936 INFO  [IPC Server handler 2 on 35772] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:45228 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-b19d0bc9-b182-47c8-b245-570ddb49486c:NORMAL:127.0.0.1:45228|FINALIZED]]} size 0
2017-07-05 12:33:26,939 INFO  [IPC Server handler 1 on 35772] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_-1307375831_1
2017-07-05 12:33:26,942 INFO  [IPC Server handler 6 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:26,946 INFO  [IPC Server handler 5 on 35772] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-279280342-172.17.0.8-1499250805704 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2:NORMAL:127.0.0.1:45228|RBW]]}
2017-07-05 12:33:26,949 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1307375831_1 at /127.0.0.1:38610 [Receiving block BP-279280342-172.17.0.8-1499250805704:blk_1073741830_1006]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-279280342-172.17.0.8-1499250805704:blk_1073741830_1006 src: /127.0.0.1:38610 dest: /127.0.0.1:45228
2017-07-05 12:33:26,958 INFO  [IPC Server handler 4 on 35772] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:45228 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-83951cef-385b-4bc8-a039-e9db1bf2b7c2:NORMAL:127.0.0.1:45228|RBW]]} size 0
2017-07-05 12:33:26,959 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:38610, dest: /127.0.0.1:45228, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1307375831_1, offset: 0, srvID: b8bfb684-f64a-437b-b2a2-c1139afa7ada, blockid: BP-279280342-172.17.0.8-1499250805704:blk_1073741830_1006, duration: 2679203
2017-07-05 12:33:26,959 INFO  [PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-279280342-172.17.0.8-1499250805704:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:26,961 INFO  [IPC Server handler 3 on 35772] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_-1307375831_1
2017-07-05 12:33:26,964 INFO  [IPC Server handler 7 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 12:33:26,973 INFO  [IPC Server handler 8 on 35772] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 12:33:26,981 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 12:33:26,981 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 12:33:26,982 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@aa004a0] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 12:33:26,982 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 12:33:26,997 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:27,098 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 12:33:27,099 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 51391
2017-07-05 12:33:27,099 INFO  [IPC Server listener on 51391] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 51391
2017-07-05 12:33:27,102 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:27,102 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid b8bfb684-f64a-437b-b2a2-c1139afa7ada) service to localhost/127.0.0.1:35772 interrupted
2017-07-05 12:33:27,102 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid b8bfb684-f64a-437b-b2a2-c1139afa7ada) service to localhost/127.0.0.1:35772
2017-07-05 12:33:27,103 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-279280342-172.17.0.8-1499250805704 (Datanode Uuid b8bfb684-f64a-437b-b2a2-c1139afa7ada)
2017-07-05 12:33:27,103 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-279280342-172.17.0.8-1499250805704 from blockPoolScannerMap
2017-07-05 12:33:27,103 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35772] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-279280342-172.17.0.8-1499250805704
2017-07-05 12:33:27,105 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@3ca0ac72] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 12:33:27,105 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 12:33:27,106 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 12:33:27,106 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 12:33:27,106 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 12:33:27,107 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 12:33:27,107 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:27,108 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@3a91d146] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 12:33:27,108 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@d176a31] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 12:33:27,108 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 12:33:27,109 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 33 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 23 SyncTimes(ms): 2 2 
2017-07-05 12:33:27,110 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000033
2017-07-05 12:33:27,111 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000033
2017-07-05 12:33:27,111 INFO  [CacheReplicationMonitor(14954711)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 12:33:27,112 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 35772
2017-07-05 12:33:27,113 INFO  [IPC Server listener on 35772] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 35772
2017-07-05 12:33:27,116 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:27,116 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@659925f4] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 12:33:27,116 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@1bc425e7] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 12:33:27,135 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:27,136 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 12:33:27,161 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:27,262 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 12:33:27,264 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 12:33:27,265 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 12:33:27,299 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 12:33:27,305 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:27,305 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:27,310 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:27,310 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:27,310 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:27,311 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:27
2017-07-05 12:33:27,311 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:27,311 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,311 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:27,312 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:27,322 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:27,323 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:27,323 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:27,323 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:27,324 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:27,324 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:27,324 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:27,324 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:27,325 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:27,325 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:27,325 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:27,325 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:27,326 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:27,326 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:27,327 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:27,327 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,328 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:27,328 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:27,333 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:27,333 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:27,334 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,334 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:27,334 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:27,336 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:27,336 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:27,336 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:27,336 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:27,337 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:27,337 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:27,337 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,337 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:27,338 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:27,338 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:27,339 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:27,339 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:27,340 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:27,370 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 12:33:27,406 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 12:33:27,471 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 12:33:27,472 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 12:33:27,474 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 12:33:27,475 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 12:33:27,475 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 12:33:27,475 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 12:33:27,479 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 12:33:27,479 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 12:33:27,480 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:27,481 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 12:33:27,481 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:27,481 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 12:33:27,482 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:27,483 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 45622
2017-07-05 12:33:27,483 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:27,491 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_45622_hdfs____nq6npb/webapp
2017-07-05 12:33:27,605 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45622
2017-07-05 12:33:27,606 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:27,607 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:27,608 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:27,608 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:27,608 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:27,609 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:27
2017-07-05 12:33:27,609 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:27,609 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,609 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:27,610 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:27,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:27,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:27,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:27,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:27,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:27,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:27,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:27,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:27,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:27,680 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:27,681 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:27,681 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:27,681 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:27,681 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:27,682 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:27,682 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,682 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:27,682 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:27,688 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:27,688 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:27,688 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,689 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:27,689 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:27,691 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:27,691 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:27,691 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:27,691 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:27,691 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:27,692 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:27,692 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:27,692 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:27,692 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:27,693 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:27,694 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:27,694 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:27,704 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:27,711 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:27,713 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 12:33:27,713 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 12:33:27,714 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 12:33:27,715 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 12:33:27,715 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 12:33:27,716 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 12:33:27,716 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 12:33:27,716 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 12:33:27,739 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 12:33:27,739 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 45 msecs
2017-07-05 12:33:27,740 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 12:33:27,740 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:27,741 INFO  [Socket Reader #1 for port 54210] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 54210
2017-07-05 12:33:27,746 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:54210 to access this namenode/service.
2017-07-05 12:33:27,751 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 12:33:27,764 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:27,764 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:27,764 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 12:33:27,764 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 12:33:27,765 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 12:33:27,765 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 12:33:27,789 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 12:33:27,789 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 12:33:27,789 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 12:33:27,789 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 12:33:27,789 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 12:33:27,790 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2017-07-05 12:33:27,791 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:27,795 INFO  [IPC Server listener on 54210] ipc.Server (Server.java:run(674)) - IPC Server listener on 54210: starting
2017-07-05 12:33:27,817 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:54210
2017-07-05 12:33:27,817 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 12:33:27,827 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 12:33:27,827 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 12:33:27,829 INFO  [CacheReplicationMonitor(1199509875)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 12:33:27,829 INFO  [CacheReplicationMonitor(1199509875)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2412096677 milliseconds
2017-07-05 12:33:27,831 INFO  [CacheReplicationMonitor(1199509875)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 12:33:27,848 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 12:33:27,848 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 12:33:27,849 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 12:33:27,850 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:59019
2017-07-05 12:33:27,850 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 12:33:27,850 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 12:33:27,851 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 12:33:27,852 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:27,853 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 12:33:27,853 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:27,854 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:27,854 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 57893
2017-07-05 12:33:27,854 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:27,859 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_57893_datanode____f1d3yv/webapp
2017-07-05 12:33:27,969 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57893
2017-07-05 12:33:27,970 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 12:33:27,971 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 12:33:27,972 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:27,973 INFO  [Socket Reader #1 for port 32778] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 32778
2017-07-05 12:33:27,976 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:32778
2017-07-05 12:33:27,982 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 12:33:27,983 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 12:33:27,985 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:54210 starting to offer service
2017-07-05 12:33:27,985 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:27,986 INFO  [IPC Server listener on 32778] ipc.Server (Server.java:run(674)) - IPC Server listener on 32778: starting
2017-07-05 12:33:28,000 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:28,001 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:28,002 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 12:33:28,012 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:28,013 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,013 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:28,038 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:28,038 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,038 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:28,115 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:28,116 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:28,127 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,127 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:28,127 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-614306034-172.17.0.8-1499250807340 is not formatted.
2017-07-05 12:33:28,127 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:28,127 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-614306034-172.17.0.8-1499250807340 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-614306034-172.17.0.8-1499250807340/current
2017-07-05 12:33:28,138 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:28,139 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-614306034-172.17.0.8-1499250807340 is not formatted.
2017-07-05 12:33:28,139 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:28,139 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-614306034-172.17.0.8-1499250807340 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-614306034-172.17.0.8-1499250807340/current
2017-07-05 12:33:28,154 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:28,154 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:28,186 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1780679504;bpid=BP-614306034-172.17.0.8-1499250807340;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1780679504;c=0;bpid=BP-614306034-172.17.0.8-1499250807340;dnuuid=null
2017-07-05 12:33:28,218 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 7859091e-737a-4454-af38-f282d5e6c710
2017-07-05 12:33:28,218 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:28,219 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:28,222 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 12:33:28,223 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 12:33:28,223 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 12:33:28,223 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 12:33:28,225 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 12:33:28,225 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499257725225 with interval 21600000
2017-07-05 12:33:28,226 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,226 INFO  [Thread-236] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-614306034-172.17.0.8-1499250807340 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:28,228 INFO  [Thread-237] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-614306034-172.17.0.8-1499250807340 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:28,231 INFO  [Thread-236] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-614306034-172.17.0.8-1499250807340 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 4ms
2017-07-05 12:33:28,233 INFO  [Thread-237] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-614306034-172.17.0.8-1499250807340 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 5ms
2017-07-05 12:33:28,233 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-614306034-172.17.0.8-1499250807340: 7ms
2017-07-05 12:33:28,233 INFO  [Thread-240] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-614306034-172.17.0.8-1499250807340 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:28,234 INFO  [Thread-240] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-614306034-172.17.0.8-1499250807340 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 12:33:28,234 INFO  [Thread-241] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-614306034-172.17.0.8-1499250807340 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:28,234 INFO  [Thread-241] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-614306034-172.17.0.8-1499250807340 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 12:33:28,235 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-05 12:33:28,235 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid null) service to localhost/127.0.0.1:54210 beginning handshake with NN
2017-07-05 12:33:28,241 INFO  [IPC Server handler 4 on 54210] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=7859091e-737a-4454-af38-f282d5e6c710, infoPort=57893, ipcPort=32778, storageInfo=lv=-56;cid=testClusterID;nsid=1780679504;c=0) storage 7859091e-737a-4454-af38-f282d5e6c710
2017-07-05 12:33:28,241 INFO  [IPC Server handler 4 on 54210] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:28,241 INFO  [IPC Server handler 4 on 54210] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:59019
2017-07-05 12:33:28,242 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid null) service to localhost/127.0.0.1:54210 successfully registered with NN
2017-07-05 12:33:28,242 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:54210 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 12:33:28,245 INFO  [IPC Server handler 9 on 54210] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:28,245 INFO  [IPC Server handler 9 on 54210] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-79a694fb-8d6e-4c36-bece-3a8fbfa4e7d7 for DN 127.0.0.1:59019
2017-07-05 12:33:28,245 INFO  [IPC Server handler 9 on 54210] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-983935d7-785b-4474-bc80-15875009a74a for DN 127.0.0.1:59019
2017-07-05 12:33:28,246 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid 7859091e-737a-4454-af38-f282d5e6c710) service to localhost/127.0.0.1:54210 trying to claim ACTIVE state with txid=1
2017-07-05 12:33:28,246 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid 7859091e-737a-4454-af38-f282d5e6c710) service to localhost/127.0.0.1:54210
2017-07-05 12:33:28,248 INFO  [IPC Server handler 8 on 54210] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-79a694fb-8d6e-4c36-bece-3a8fbfa4e7d7,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:28,248 INFO  [IPC Server handler 8 on 54210] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-79a694fb-8d6e-4c36-bece-3a8fbfa4e7d7 node DatanodeRegistration(127.0.0.1, datanodeUuid=7859091e-737a-4454-af38-f282d5e6c710, infoPort=57893, ipcPort=32778, storageInfo=lv=-56;cid=testClusterID;nsid=1780679504;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 12:33:28,249 INFO  [IPC Server handler 8 on 54210] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-983935d7-785b-4474-bc80-15875009a74a,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:28,249 INFO  [IPC Server handler 8 on 54210] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-983935d7-785b-4474-bc80-15875009a74a node DatanodeRegistration(127.0.0.1, datanodeUuid=7859091e-737a-4454-af38-f282d5e6c710, infoPort=57893, ipcPort=32778, storageInfo=lv=-56;cid=testClusterID;nsid=1780679504;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 12:33:28,250 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4807d7ef
2017-07-05 12:33:28,250 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,251 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 12:33:28,251 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:28,251 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 12:33:28,251 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 12:33:28,254 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,255 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-614306034-172.17.0.8-1499250807340 to blockPoolScannerMap, new size=1
2017-07-05 12:33:28,321 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:28,326 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:28,331 INFO  [IPC Server handler 3 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 12:33:28,334 INFO  [IPC Server handler 0 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:28,342 INFO  [IPC Server handler 1 on 54210] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-614306034-172.17.0.8-1499250807340 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-983935d7-785b-4474-bc80-15875009a74a:NORMAL:127.0.0.1:59019|RBW]]}
2017-07-05 12:33:28,346 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-529946333_1 at /127.0.0.1:43506 [Receiving block BP-614306034-172.17.0.8-1499250807340:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-614306034-172.17.0.8-1499250807340:blk_1073741825_1001 src: /127.0.0.1:43506 dest: /127.0.0.1:59019
2017-07-05 12:33:28,354 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43506, dest: /127.0.0.1:59019, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529946333_1, offset: 0, srvID: 7859091e-737a-4454-af38-f282d5e6c710, blockid: BP-614306034-172.17.0.8-1499250807340:blk_1073741825_1001, duration: 3257332
2017-07-05 12:33:28,355 INFO  [IPC Server handler 5 on 54210] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59019 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-79a694fb-8d6e-4c36-bece-3a8fbfa4e7d7:NORMAL:127.0.0.1:59019|FINALIZED]]} size 0
2017-07-05 12:33:28,355 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:28,358 INFO  [IPC Server handler 4 on 54210] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_-529946333_1
2017-07-05 12:33:28,361 INFO  [IPC Server handler 9 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:28,367 INFO  [IPC Server handler 8 on 54210] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-614306034-172.17.0.8-1499250807340 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-983935d7-785b-4474-bc80-15875009a74a:NORMAL:127.0.0.1:59019|RBW]]}
2017-07-05 12:33:28,373 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-529946333_1 at /127.0.0.1:43507 [Receiving block BP-614306034-172.17.0.8-1499250807340:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-614306034-172.17.0.8-1499250807340:blk_1073741826_1002 src: /127.0.0.1:43507 dest: /127.0.0.1:59019
2017-07-05 12:33:28,388 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43507, dest: /127.0.0.1:59019, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529946333_1, offset: 0, srvID: 7859091e-737a-4454-af38-f282d5e6c710, blockid: BP-614306034-172.17.0.8-1499250807340:blk_1073741826_1002, duration: 10020016
2017-07-05 12:33:28,389 INFO  [IPC Server handler 6 on 54210] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59019 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-983935d7-785b-4474-bc80-15875009a74a:NORMAL:127.0.0.1:59019|RBW]]} size 0
2017-07-05 12:33:28,390 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:28,394 INFO  [IPC Server handler 7 on 54210] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_-529946333_1
2017-07-05 12:33:28,397 INFO  [IPC Server handler 3 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:28,400 INFO  [IPC Server handler 0 on 54210] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3. BP-614306034-172.17.0.8-1499250807340 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-983935d7-785b-4474-bc80-15875009a74a:NORMAL:127.0.0.1:59019|RBW]]}
2017-07-05 12:33:28,403 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-529946333_1 at /127.0.0.1:43508 [Receiving block BP-614306034-172.17.0.8-1499250807340:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-614306034-172.17.0.8-1499250807340:blk_1073741827_1003 src: /127.0.0.1:43508 dest: /127.0.0.1:59019
2017-07-05 12:33:28,417 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43508, dest: /127.0.0.1:59019, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529946333_1, offset: 0, srvID: 7859091e-737a-4454-af38-f282d5e6c710, blockid: BP-614306034-172.17.0.8-1499250807340:blk_1073741827_1003, duration: 1413294
2017-07-05 12:33:28,418 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:28,418 INFO  [IPC Server handler 1 on 54210] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59019 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-79a694fb-8d6e-4c36-bece-3a8fbfa4e7d7:NORMAL:127.0.0.1:59019|FINALIZED]]} size 0
2017-07-05 12:33:28,421 INFO  [IPC Server handler 2 on 54210] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3 is closed by DFSClient_NONMAPREDUCE_-529946333_1
2017-07-05 12:33:28,424 INFO  [IPC Server handler 5 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:28,428 INFO  [IPC Server handler 4 on 54210] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4. BP-614306034-172.17.0.8-1499250807340 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-79a694fb-8d6e-4c36-bece-3a8fbfa4e7d7:NORMAL:127.0.0.1:59019|RBW]]}
2017-07-05 12:33:28,430 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-529946333_1 at /127.0.0.1:43509 [Receiving block BP-614306034-172.17.0.8-1499250807340:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-614306034-172.17.0.8-1499250807340:blk_1073741828_1004 src: /127.0.0.1:43509 dest: /127.0.0.1:59019
2017-07-05 12:33:28,436 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43509, dest: /127.0.0.1:59019, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-529946333_1, offset: 0, srvID: 7859091e-737a-4454-af38-f282d5e6c710, blockid: BP-614306034-172.17.0.8-1499250807340:blk_1073741828_1004, duration: 3626003
2017-07-05 12:33:28,436 INFO  [PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-614306034-172.17.0.8-1499250807340:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:28,437 INFO  [IPC Server handler 9 on 54210] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59019 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-983935d7-785b-4474-bc80-15875009a74a:NORMAL:127.0.0.1:59019|FINALIZED]]} size 0
2017-07-05 12:33:28,439 INFO  [IPC Server handler 8 on 54210] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4 is closed by DFSClient_NONMAPREDUCE_-529946333_1
2017-07-05 12:33:28,441 INFO  [IPC Server handler 6 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 12:33:28,444 INFO  [IPC Server handler 7 on 54210] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 12:33:28,446 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 12:33:28,446 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 12:33:28,446 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 12:33:28,446 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2764c546] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 12:33:28,451 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:28,552 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 12:33:28,552 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 32778
2017-07-05 12:33:28,554 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:28,554 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid 7859091e-737a-4454-af38-f282d5e6c710) service to localhost/127.0.0.1:54210 interrupted
2017-07-05 12:33:28,554 INFO  [IPC Server listener on 32778] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 32778
2017-07-05 12:33:28,554 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid 7859091e-737a-4454-af38-f282d5e6c710) service to localhost/127.0.0.1:54210
2017-07-05 12:33:28,555 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-614306034-172.17.0.8-1499250807340 (Datanode Uuid 7859091e-737a-4454-af38-f282d5e6c710)
2017-07-05 12:33:28,555 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-614306034-172.17.0.8-1499250807340 from blockPoolScannerMap
2017-07-05 12:33:28,556 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54210] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-614306034-172.17.0.8-1499250807340
2017-07-05 12:33:28,558 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 12:33:28,558 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 12:33:28,558 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@2351b349] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 12:33:28,559 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 12:33:28,559 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 12:33:28,559 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 12:33:28,559 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:28,560 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@5411dd90] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 12:33:28,560 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 12:33:28,560 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@50194e8d] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 12:33:28,561 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 1 2 
2017-07-05 12:33:28,561 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-05 12:33:28,563 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-05 12:33:28,563 INFO  [CacheReplicationMonitor(1199509875)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 12:33:28,564 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 54210
2017-07-05 12:33:28,567 INFO  [IPC Server listener on 54210] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 54210
2017-07-05 12:33:28,567 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:28,568 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@35835e65] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 12:33:28,568 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@4d774249] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 12:33:28,580 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:28,581 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 12:33:28,585 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:28,685 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 12:33:28,687 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 12:33:28,687 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 12:33:28,713 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 12:33:28,718 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:28,719 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:28,720 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:28,720 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:28,720 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:28,721 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:28
2017-07-05 12:33:28,721 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:28,721 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:28,722 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:28,722 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:28,876 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:28,877 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:28,877 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:28,877 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:28,877 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:28,877 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:28,878 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:28,878 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:28,878 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:28,878 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:28,879 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:28,879 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:28,879 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:28,880 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:28,881 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:28,881 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:28,882 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:28,882 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:28,887 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:28,888 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:28,888 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:28,889 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:28,889 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:28,891 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:28,891 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:28,891 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:28,891 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:28,891 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:28,892 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:28,892 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:28,892 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:28,892 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:28,893 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:28,894 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:28,894 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:28,895 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:28,926 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 12:33:28,958 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 12:33:29,009 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 12:33:29,011 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 12:33:29,013 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 12:33:29,017 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 12:33:29,017 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 12:33:29,018 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 12:33:29,021 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 12:33:29,021 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 12:33:29,022 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:29,025 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 12:33:29,025 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:29,026 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 12:33:29,026 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:29,027 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 45783
2017-07-05 12:33:29,027 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:29,040 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_45783_hdfs____fnbzln/webapp
2017-07-05 12:33:29,157 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45783
2017-07-05 12:33:29,159 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:29,160 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:29,161 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:29,161 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:29,162 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:29,162 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:29
2017-07-05 12:33:29,163 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:29,163 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:29,163 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:29,163 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:29,174 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:29,175 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:29,175 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:29,175 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:29,175 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:29,175 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:29,175 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:29,176 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:29,176 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:29,176 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:29,176 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:29,176 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:29,177 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:29,177 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:29,178 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:29,178 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:29,178 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:29,178 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:29,184 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:29,184 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:29,184 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:29,185 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:29,185 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:29,187 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:29,187 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:29,187 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:29,187 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:29,188 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:29,188 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:29,188 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:29,188 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:29,188 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:29,190 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:29,190 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:29,190 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:29,200 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:29,269 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:29,272 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 12:33:29,273 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 12:33:29,273 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 12:33:29,275 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 12:33:29,276 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 12:33:29,276 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 12:33:29,277 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 12:33:29,277 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 12:33:29,359 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 12:33:29,360 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 170 msecs
2017-07-05 12:33:29,360 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 12:33:29,361 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:29,363 INFO  [Socket Reader #1 for port 49856] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 49856
2017-07-05 12:33:29,366 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:49856 to access this namenode/service.
2017-07-05 12:33:29,366 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 12:33:29,389 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:29,389 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:29,389 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 12:33:29,390 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 12:33:29,390 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 12:33:29,391 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 12:33:29,415 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 12:33:29,415 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 12:33:29,415 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 12:33:29,415 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 12:33:29,415 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 12:33:29,415 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
2017-07-05 12:33:29,420 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:29,421 INFO  [IPC Server listener on 49856] ipc.Server (Server.java:run(674)) - IPC Server listener on 49856: starting
2017-07-05 12:33:29,438 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:49856
2017-07-05 12:33:29,438 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 12:33:29,455 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 12:33:29,456 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 12:33:29,457 INFO  [CacheReplicationMonitor(1102144290)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 12:33:29,463 INFO  [CacheReplicationMonitor(1102144290)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2412098311 milliseconds
2017-07-05 12:33:29,466 INFO  [CacheReplicationMonitor(1102144290)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 12:33:29,520 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 12:33:29,521 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 12:33:29,522 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 12:33:29,523 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:49722
2017-07-05 12:33:29,524 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 12:33:29,524 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 12:33:29,526 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 12:33:29,526 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:29,527 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 12:33:29,528 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:29,529 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:29,529 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 45987
2017-07-05 12:33:29,530 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:29,551 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_45987_datanode____nfde0u/webapp
2017-07-05 12:33:29,663 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45987
2017-07-05 12:33:29,663 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 12:33:29,664 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 12:33:29,664 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:29,666 INFO  [Socket Reader #1 for port 35298] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 35298
2017-07-05 12:33:29,678 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:35298
2017-07-05 12:33:29,685 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 12:33:29,685 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 12:33:29,686 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:49856 starting to offer service
2017-07-05 12:33:29,693 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:29,711 INFO  [IPC Server listener on 35298] ipc.Server (Server.java:run(674)) - IPC Server listener on 35298: starting
2017-07-05 12:33:29,767 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 12:33:29,770 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:29,770 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:29,832 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:29,833 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:29,834 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:29,874 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:29,874 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:29,919 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:29,920 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:29,920 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:29,977 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:29,977 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:30,079 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:30,079 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:30,138 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:30,138 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:30,139 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-2070873454-172.17.0.8-1499250808895 is not formatted.
2017-07-05 12:33:30,139 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:30,139 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-2070873454-172.17.0.8-1499250808895 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-2070873454-172.17.0.8-1499250808895/current
2017-07-05 12:33:30,178 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:30,179 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-2070873454-172.17.0.8-1499250808895 is not formatted.
2017-07-05 12:33:30,179 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:30,179 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-2070873454-172.17.0.8-1499250808895 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-2070873454-172.17.0.8-1499250808895/current
2017-07-05 12:33:30,181 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:30,182 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:30,190 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:30,191 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:30,221 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=334339649;bpid=BP-2070873454-172.17.0.8-1499250808895;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=334339649;c=0;bpid=BP-2070873454-172.17.0.8-1499250808895;dnuuid=null
2017-07-05 12:33:30,254 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID d6f6ef4f-1eef-48b1-967e-9597486f7194
2017-07-05 12:33:30,254 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 12:33:30,254 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 12:33:30,255 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 12:33:30,255 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 12:33:30,256 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 12:33:30,257 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499255186257 with interval 21600000
2017-07-05 12:33:30,258 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:30,258 INFO  [Thread-322] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-2070873454-172.17.0.8-1499250808895 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:30,262 INFO  [Thread-323] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-2070873454-172.17.0.8-1499250808895 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:30,268 INFO  [Thread-322] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-2070873454-172.17.0.8-1499250808895 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 7ms
2017-07-05 12:33:30,269 INFO  [Thread-323] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-2070873454-172.17.0.8-1499250808895 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 7ms
2017-07-05 12:33:30,269 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-2070873454-172.17.0.8-1499250808895: 11ms
2017-07-05 12:33:30,270 INFO  [Thread-326] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-2070873454-172.17.0.8-1499250808895 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:30,270 INFO  [Thread-327] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-2070873454-172.17.0.8-1499250808895 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:30,270 INFO  [Thread-326] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-2070873454-172.17.0.8-1499250808895 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 12:33:30,270 INFO  [Thread-327] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-2070873454-172.17.0.8-1499250808895 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 12:33:30,270 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 0ms
2017-07-05 12:33:30,272 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid null) service to localhost/127.0.0.1:49856 beginning handshake with NN
2017-07-05 12:33:30,274 INFO  [IPC Server handler 6 on 49856] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=d6f6ef4f-1eef-48b1-967e-9597486f7194, infoPort=45987, ipcPort=35298, storageInfo=lv=-56;cid=testClusterID;nsid=334339649;c=0) storage d6f6ef4f-1eef-48b1-967e-9597486f7194
2017-07-05 12:33:30,275 INFO  [IPC Server handler 6 on 49856] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:30,275 INFO  [IPC Server handler 6 on 49856] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:49722
2017-07-05 12:33:30,277 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid null) service to localhost/127.0.0.1:49856 successfully registered with NN
2017-07-05 12:33:30,278 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:49856 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 12:33:30,279 INFO  [IPC Server handler 7 on 49856] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:30,280 INFO  [IPC Server handler 7 on 49856] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-ed76298f-7b56-49f5-ba28-dca342d3e366 for DN 127.0.0.1:49722
2017-07-05 12:33:30,280 INFO  [IPC Server handler 7 on 49856] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-a9caa721-cf75-44f6-b30e-04e61fa8c567 for DN 127.0.0.1:49722
2017-07-05 12:33:30,281 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid d6f6ef4f-1eef-48b1-967e-9597486f7194) service to localhost/127.0.0.1:49856 trying to claim ACTIVE state with txid=1
2017-07-05 12:33:30,282 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid d6f6ef4f-1eef-48b1-967e-9597486f7194) service to localhost/127.0.0.1:49856
2017-07-05 12:33:30,285 INFO  [IPC Server handler 9 on 49856] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-ed76298f-7b56-49f5-ba28-dca342d3e366,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:30,286 INFO  [IPC Server handler 9 on 49856] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-ed76298f-7b56-49f5-ba28-dca342d3e366 node DatanodeRegistration(127.0.0.1, datanodeUuid=d6f6ef4f-1eef-48b1-967e-9597486f7194, infoPort=45987, ipcPort=35298, storageInfo=lv=-56;cid=testClusterID;nsid=334339649;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 12:33:30,286 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:30,286 INFO  [IPC Server handler 9 on 49856] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-a9caa721-cf75-44f6-b30e-04e61fa8c567,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:30,286 INFO  [IPC Server handler 9 on 49856] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-a9caa721-cf75-44f6-b30e-04e61fa8c567 node DatanodeRegistration(127.0.0.1, datanodeUuid=d6f6ef4f-1eef-48b1-967e-9597486f7194, infoPort=45987, ipcPort=35298, storageInfo=lv=-56;cid=testClusterID;nsid=334339649;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 12:33:30,289 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 6 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@606a9eeb
2017-07-05 12:33:30,289 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:30,291 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 12:33:30,291 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:30,291 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:30,291 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 12:33:30,298 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 12:33:30,309 INFO  [IPC Server handler 1 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 12:33:30,311 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:30,312 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-2070873454-172.17.0.8-1499250808895 to blockPoolScannerMap, new size=1
2017-07-05 12:33:30,325 INFO  [IPC Server handler 4 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:30,338 INFO  [IPC Server handler 3 on 49856] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-2070873454-172.17.0.8-1499250808895 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ed76298f-7b56-49f5-ba28-dca342d3e366:NORMAL:127.0.0.1:49722|RBW]]}
2017-07-05 12:33:30,344 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1951582011_1 at /127.0.0.1:43281 [Receiving block BP-2070873454-172.17.0.8-1499250808895:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-2070873454-172.17.0.8-1499250808895:blk_1073741825_1001 src: /127.0.0.1:43281 dest: /127.0.0.1:49722
2017-07-05 12:33:30,353 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43281, dest: /127.0.0.1:49722, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951582011_1, offset: 0, srvID: d6f6ef4f-1eef-48b1-967e-9597486f7194, blockid: BP-2070873454-172.17.0.8-1499250808895:blk_1073741825_1001, duration: 1774801
2017-07-05 12:33:30,353 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:30,355 INFO  [IPC Server handler 5 on 49856] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:49722 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ed76298f-7b56-49f5-ba28-dca342d3e366:NORMAL:127.0.0.1:49722|RBW]]} size 0
2017-07-05 12:33:30,356 INFO  [IPC Server handler 6 on 49856] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_1951582011_1
2017-07-05 12:33:30,360 INFO  [IPC Server handler 7 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:30,365 INFO  [IPC Server handler 8 on 49856] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-2070873454-172.17.0.8-1499250808895 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ed76298f-7b56-49f5-ba28-dca342d3e366:NORMAL:127.0.0.1:49722|RBW]]}
2017-07-05 12:33:30,367 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1951582011_1 at /127.0.0.1:43282 [Receiving block BP-2070873454-172.17.0.8-1499250808895:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-2070873454-172.17.0.8-1499250808895:blk_1073741826_1002 src: /127.0.0.1:43282 dest: /127.0.0.1:49722
2017-07-05 12:33:30,376 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43282, dest: /127.0.0.1:49722, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951582011_1, offset: 0, srvID: d6f6ef4f-1eef-48b1-967e-9597486f7194, blockid: BP-2070873454-172.17.0.8-1499250808895:blk_1073741826_1002, duration: 1529878
2017-07-05 12:33:30,376 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:30,377 INFO  [IPC Server handler 9 on 49856] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:49722 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-a9caa721-cf75-44f6-b30e-04e61fa8c567:NORMAL:127.0.0.1:49722|FINALIZED]]} size 0
2017-07-05 12:33:30,379 INFO  [IPC Server handler 0 on 49856] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_1951582011_1
2017-07-05 12:33:30,383 INFO  [IPC Server handler 1 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:30,389 INFO  [IPC Server handler 4 on 49856] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-2070873454-172.17.0.8-1499250808895 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ed76298f-7b56-49f5-ba28-dca342d3e366:NORMAL:127.0.0.1:49722|RBW]]}
2017-07-05 12:33:30,391 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1951582011_1 at /127.0.0.1:43283 [Receiving block BP-2070873454-172.17.0.8-1499250808895:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-2070873454-172.17.0.8-1499250808895:blk_1073741827_1003 src: /127.0.0.1:43283 dest: /127.0.0.1:49722
2017-07-05 12:33:30,405 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43283, dest: /127.0.0.1:49722, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951582011_1, offset: 0, srvID: d6f6ef4f-1eef-48b1-967e-9597486f7194, blockid: BP-2070873454-172.17.0.8-1499250808895:blk_1073741827_1003, duration: 6555084
2017-07-05 12:33:30,405 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:30,406 INFO  [IPC Server handler 3 on 49856] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:49722 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ed76298f-7b56-49f5-ba28-dca342d3e366:NORMAL:127.0.0.1:49722|RBW]]} size 0
2017-07-05 12:33:30,407 INFO  [IPC Server handler 3 on 49856] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_1951582011_1
2017-07-05 12:33:30,410 INFO  [IPC Server handler 5 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:30,413 INFO  [IPC Server handler 6 on 49856] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-2070873454-172.17.0.8-1499250808895 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ed76298f-7b56-49f5-ba28-dca342d3e366:NORMAL:127.0.0.1:49722|RBW]]}
2017-07-05 12:33:30,414 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1951582011_1 at /127.0.0.1:43284 [Receiving block BP-2070873454-172.17.0.8-1499250808895:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-2070873454-172.17.0.8-1499250808895:blk_1073741828_1004 src: /127.0.0.1:43284 dest: /127.0.0.1:49722
2017-07-05 12:33:30,420 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43284, dest: /127.0.0.1:49722, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1951582011_1, offset: 0, srvID: d6f6ef4f-1eef-48b1-967e-9597486f7194, blockid: BP-2070873454-172.17.0.8-1499250808895:blk_1073741828_1004, duration: 1337556
2017-07-05 12:33:30,420 INFO  [PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-2070873454-172.17.0.8-1499250808895:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:30,421 INFO  [IPC Server handler 7 on 49856] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:49722 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-a9caa721-cf75-44f6-b30e-04e61fa8c567:NORMAL:127.0.0.1:49722|FINALIZED]]} size 0
2017-07-05 12:33:30,423 INFO  [IPC Server handler 8 on 49856] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_1951582011_1
2017-07-05 12:33:30,426 INFO  [IPC Server handler 9 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 12:33:30,428 INFO  [IPC Server handler 0 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 12:33:30,442 INFO  [IPC Server handler 1 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 12:33:30,484 INFO  [IPC Server handler 4 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/2.txt	dst=null	perm=null	proto=rpc
2017-07-05 12:33:30,488 INFO  [IPC Server handler 3 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/3.txt	dst=null	perm=null	proto=rpc
2017-07-05 12:33:30,492 INFO  [IPC Server handler 2 on 49856] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/4.txt	dst=null	perm=null	proto=rpc
2017-07-05 12:33:30,495 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 12:33:30,495 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 12:33:30,495 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 12:33:30,495 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6240651f] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 12:33:30,503 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:30,604 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 1
2017-07-05 12:33:30,606 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 1
2017-07-05 12:33:30,610 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 12:33:30,610 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 35298
2017-07-05 12:33:30,612 INFO  [IPC Server listener on 35298] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 35298
2017-07-05 12:33:30,612 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:30,612 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid d6f6ef4f-1eef-48b1-967e-9597486f7194) service to localhost/127.0.0.1:49856 interrupted
2017-07-05 12:33:30,613 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid d6f6ef4f-1eef-48b1-967e-9597486f7194) service to localhost/127.0.0.1:49856
2017-07-05 12:33:30,613 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-2070873454-172.17.0.8-1499250808895 (Datanode Uuid d6f6ef4f-1eef-48b1-967e-9597486f7194)
2017-07-05 12:33:30,613 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-2070873454-172.17.0.8-1499250808895 from blockPoolScannerMap
2017-07-05 12:33:30,613 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49856] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-2070873454-172.17.0.8-1499250808895
2017-07-05 12:33:30,615 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@47d7c058] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 12:33:30,620 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 12:33:30,620 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 12:33:30,620 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 12:33:30,621 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 12:33:30,621 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 12:33:30,621 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:30,621 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 12:33:30,622 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@4e93dcb9] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 12:33:30,622 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 3 1 
2017-07-05 12:33:30,622 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@188b6035] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 12:33:30,623 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-05 12:33:30,623 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-05 12:33:30,624 INFO  [CacheReplicationMonitor(1102144290)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 12:33:30,624 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 49856
2017-07-05 12:33:30,625 INFO  [IPC Server listener on 49856] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 49856
2017-07-05 12:33:30,627 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:30,627 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@4726927c] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 12:33:30,627 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@571a9686] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 12:33:30,639 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:30,640 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 12:33:30,644 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:30,745 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 12:33:30,746 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 12:33:30,746 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 12:33:30,774 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 12:33:30,779 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:30,779 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:30,780 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:30,780 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:30,780 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:30,781 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:30
2017-07-05 12:33:30,781 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:30,781 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:30,781 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:30,782 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:30,862 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:30,863 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:30,864 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:30,864 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:30,864 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:30,864 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:30,864 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:30,865 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:30,865 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:30,865 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:30,866 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:30,866 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:30,871 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:30,871 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:30,871 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:30,872 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:30,872 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:30,873 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:30,873 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:30,874 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:30,874 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:30,874 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:30,874 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:30,874 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:30,874 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:30,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:30,875 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:30,876 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:30,876 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:30,877 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:30,906 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 12:33:30,937 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 12:33:30,996 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 12:33:30,997 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 12:33:30,998 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 12:33:30,999 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 12:33:30,999 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 12:33:31,000 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 12:33:31,003 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 12:33:31,004 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 12:33:31,004 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:31,005 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 12:33:31,005 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:31,006 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 12:33:31,006 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:31,007 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 42939
2017-07-05 12:33:31,007 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:31,016 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_42939_hdfs____.7a0xh/webapp
2017-07-05 12:33:31,132 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42939
2017-07-05 12:33:31,133 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 12:33:31,134 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 12:33:31,135 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 12:33:31,135 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 12:33:31,135 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 12:33:31,136 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 12:33:31
2017-07-05 12:33:31,136 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 12:33:31,136 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:31,136 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 12:33:31,136 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 12:33:31,146 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 12:33:31,147 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 12:33:31,147 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 12:33:31,147 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 12:33:31,147 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 12:33:31,147 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 12:33:31,147 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 12:33:31,148 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 12:33:31,148 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 12:33:31,148 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 12:33:31,148 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 12:33:31,148 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 12:33:31,149 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 12:33:31,149 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 12:33:31,149 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 12:33:31,149 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:31,150 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 12:33:31,150 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 12:33:31,155 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 12:33:31,155 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 12:33:31,155 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:31,156 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 12:33:31,156 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 12:33:31,157 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 12:33:31,158 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 12:33:31,158 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 12:33:31,158 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 12:33:31,158 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 12:33:31,158 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 12:33:31,158 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:31,159 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 12:33:31,159 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 12:33:31,160 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 12:33:31,160 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 12:33:31,160 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 12:33:31,170 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:31,186 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:31,188 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 12:33:31,188 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 12:33:31,189 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 12:33:31,190 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 12:33:31,190 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 12:33:31,191 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 12:33:31,191 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 12:33:31,191 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 12:33:31,218 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 12:33:31,218 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 57 msecs
2017-07-05 12:33:31,219 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 12:33:31,219 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:31,220 INFO  [Socket Reader #1 for port 41808] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 41808
2017-07-05 12:33:31,227 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:41808 to access this namenode/service.
2017-07-05 12:33:31,228 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 12:33:31,240 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:31,240 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 12:33:31,240 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 12:33:31,241 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 12:33:31,241 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 12:33:31,241 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 12:33:31,264 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 12:33:31,265 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:41808
2017-07-05 12:33:31,264 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:31,266 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 12:33:31,266 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 12:33:31,266 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 12:33:31,266 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 12:33:31,266 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 25 msec
2017-07-05 12:33:31,280 INFO  [IPC Server listener on 41808] ipc.Server (Server.java:run(674)) - IPC Server listener on 41808: starting
2017-07-05 12:33:31,281 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 12:33:31,296 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 12:33:31,300 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 12:33:31,303 INFO  [CacheReplicationMonitor(1782375083)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 12:33:31,303 INFO  [CacheReplicationMonitor(1782375083)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2412100150 milliseconds
2017-07-05 12:33:31,305 INFO  [CacheReplicationMonitor(1782375083)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 12:33:31,317 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 12:33:31,318 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 12:33:31,318 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 12:33:31,319 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:36683
2017-07-05 12:33:31,319 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 12:33:31,320 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 12:33:31,321 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 12:33:31,322 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 12:33:31,323 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 12:33:31,323 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 12:33:31,324 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 12:33:31,324 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 52785
2017-07-05 12:33:31,325 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 12:33:31,330 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_52785_datanode____uz884e/webapp
2017-07-05 12:33:31,443 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52785
2017-07-05 12:33:31,444 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 12:33:31,445 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 12:33:31,446 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 12:33:31,446 INFO  [Socket Reader #1 for port 41018] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 41018
2017-07-05 12:33:31,449 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:41018
2017-07-05 12:33:31,451 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 12:33:31,451 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 12:33:31,452 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:41808 starting to offer service
2017-07-05 12:33:31,454 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 12:33:31,454 INFO  [IPC Server listener on 41018] ipc.Server (Server.java:run(674)) - IPC Server listener on 41018: starting
2017-07-05 12:33:31,477 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 12:33:31,479 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:31,480 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:31,489 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:31,490 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,490 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:31,522 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 12:33:31,522 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,522 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 12:33:31,585 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:31,586 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:31,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:31,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1755078580-172.17.0.8-1499250810877 is not formatted.
2017-07-05 12:33:31,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:31,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1755078580-172.17.0.8-1499250810877 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1755078580-172.17.0.8-1499250810877/current
2017-07-05 12:33:31,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 12:33:31,607 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1755078580-172.17.0.8-1499250810877 is not formatted.
2017-07-05 12:33:31,607 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 12:33:31,607 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1755078580-172.17.0.8-1499250810877 directory /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1755078580-172.17.0.8-1499250810877/current
2017-07-05 12:33:31,622 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:31,622 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 12:33:31,654 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1023827043;bpid=BP-1755078580-172.17.0.8-1499250810877;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1023827043;c=0;bpid=BP-1755078580-172.17.0.8-1499250810877;dnuuid=null
2017-07-05 12:33:31,686 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID ebbd9498-6aa6-4932-b9de-07afce3a48d7
2017-07-05 12:33:31,687 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 12:33:31,687 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 12:33:31,687 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 12:33:31,687 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 12:33:31,689 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 12:33:31,689 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 12:33:31,690 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 12:33:31,691 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499261535691 with interval 21600000
2017-07-05 12:33:31,692 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,692 INFO  [Thread-410] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1755078580-172.17.0.8-1499250810877 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:31,693 INFO  [Thread-411] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1755078580-172.17.0.8-1499250810877 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:31,697 INFO  [Thread-410] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1755078580-172.17.0.8-1499250810877 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 4ms
2017-07-05 12:33:31,701 INFO  [Thread-411] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1755078580-172.17.0.8-1499250810877 on /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 9ms
2017-07-05 12:33:31,702 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1755078580-172.17.0.8-1499250810877: 10ms
2017-07-05 12:33:31,702 INFO  [Thread-414] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1755078580-172.17.0.8-1499250810877 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 12:33:31,702 INFO  [Thread-414] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1755078580-172.17.0.8-1499250810877 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 12:33:31,703 INFO  [Thread-415] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1755078580-172.17.0.8-1499250810877 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 12:33:31,703 INFO  [Thread-415] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1755078580-172.17.0.8-1499250810877 on volume /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 12:33:31,703 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-05 12:33:31,704 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid null) service to localhost/127.0.0.1:41808 beginning handshake with NN
2017-07-05 12:33:31,705 INFO  [IPC Server handler 4 on 41808] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=ebbd9498-6aa6-4932-b9de-07afce3a48d7, infoPort=52785, ipcPort=41018, storageInfo=lv=-56;cid=testClusterID;nsid=1023827043;c=0) storage ebbd9498-6aa6-4932-b9de-07afce3a48d7
2017-07-05 12:33:31,706 INFO  [IPC Server handler 4 on 41808] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:31,706 INFO  [IPC Server handler 4 on 41808] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:36683
2017-07-05 12:33:31,707 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid null) service to localhost/127.0.0.1:41808 successfully registered with NN
2017-07-05 12:33:31,707 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:41808 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 12:33:31,709 INFO  [IPC Server handler 2 on 41808] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 12:33:31,709 INFO  [IPC Server handler 2 on 41808] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-aaf4c79b-683f-4bce-ab00-d76ef1152c77 for DN 127.0.0.1:36683
2017-07-05 12:33:31,710 INFO  [IPC Server handler 2 on 41808] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-6ee02599-e6c6-41b9-b5cc-5481dabe9aa7 for DN 127.0.0.1:36683
2017-07-05 12:33:31,710 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid ebbd9498-6aa6-4932-b9de-07afce3a48d7) service to localhost/127.0.0.1:41808 trying to claim ACTIVE state with txid=1
2017-07-05 12:33:31,711 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid ebbd9498-6aa6-4932-b9de-07afce3a48d7) service to localhost/127.0.0.1:41808
2017-07-05 12:33:31,713 INFO  [IPC Server handler 3 on 41808] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-aaf4c79b-683f-4bce-ab00-d76ef1152c77,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:31,713 INFO  [IPC Server handler 3 on 41808] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-aaf4c79b-683f-4bce-ab00-d76ef1152c77 node DatanodeRegistration(127.0.0.1, datanodeUuid=ebbd9498-6aa6-4932-b9de-07afce3a48d7, infoPort=52785, ipcPort=41018, storageInfo=lv=-56;cid=testClusterID;nsid=1023827043;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 12:33:31,713 INFO  [IPC Server handler 3 on 41808] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-6ee02599-e6c6-41b9-b5cc-5481dabe9aa7,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 12:33:31,714 INFO  [IPC Server handler 3 on 41808] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-6ee02599-e6c6-41b9-b5cc-5481dabe9aa7 node DatanodeRegistration(127.0.0.1, datanodeUuid=ebbd9498-6aa6-4932-b9de-07afce3a48d7, infoPort=52785, ipcPort=41018, storageInfo=lv=-56;cid=testClusterID;nsid=1023827043;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2017-07-05 12:33:31,714 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@79820b7d
2017-07-05 12:33:31,715 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,715 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 12:33:31,715 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 12:33:31,716 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 12:33:31,716 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 12:33:31,719 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,720 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1755078580-172.17.0.8-1499250810877 to blockPoolScannerMap, new size=1
2017-07-05 12:33:31,792 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:31,796 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 12:33:31,800 INFO  [IPC Server handler 0 on 41808] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 12:33:31,804 INFO  [IPC Server handler 7 on 41808] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 12:33:31,809 INFO  [IPC Server handler 8 on 41808] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1755078580-172.17.0.8-1499250810877 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-aaf4c79b-683f-4bce-ab00-d76ef1152c77:NORMAL:127.0.0.1:36683|RBW]]}
2017-07-05 12:33:31,815 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1308484881_1 at /127.0.0.1:42102 [Receiving block BP-1755078580-172.17.0.8-1499250810877:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1755078580-172.17.0.8-1499250810877:blk_1073741825_1001 src: /127.0.0.1:42102 dest: /127.0.0.1:36683
2017-07-05 12:33:31,824 INFO  [PacketResponder: BP-1755078580-172.17.0.8-1499250810877:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:42102, dest: /127.0.0.1:36683, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1308484881_1, offset: 0, srvID: ebbd9498-6aa6-4932-b9de-07afce3a48d7, blockid: BP-1755078580-172.17.0.8-1499250810877:blk_1073741825_1001, duration: 4736572
2017-07-05 12:33:31,824 INFO  [PacketResponder: BP-1755078580-172.17.0.8-1499250810877:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1755078580-172.17.0.8-1499250810877:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 12:33:31,826 INFO  [IPC Server handler 5 on 41808] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:36683 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-aaf4c79b-683f-4bce-ab00-d76ef1152c77:NORMAL:127.0.0.1:36683|RBW]]} size 0
2017-07-05 12:33:31,828 INFO  [IPC Server handler 4 on 41808] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-1308484881_1
2017-07-05 12:33:31,830 INFO  [IPC Server handler 2 on 41808] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 12:33:31,831 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 12:33:31,831 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 12:33:31,831 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 12:33:31,831 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@476ee5b3] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 12:33:31,852 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:31,953 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 12:33:31,954 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 41018
2017-07-05 12:33:31,956 INFO  [IPC Server listener on 41018] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 41018
2017-07-05 12:33:31,956 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:31,957 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid ebbd9498-6aa6-4932-b9de-07afce3a48d7) service to localhost/127.0.0.1:41808 interrupted
2017-07-05 12:33:31,957 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid ebbd9498-6aa6-4932-b9de-07afce3a48d7) service to localhost/127.0.0.1:41808
2017-07-05 12:33:31,957 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1755078580-172.17.0.8-1499250810877 (Datanode Uuid ebbd9498-6aa6-4932-b9de-07afce3a48d7)
2017-07-05 12:33:31,957 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1755078580-172.17.0.8-1499250810877 from blockPoolScannerMap
2017-07-05 12:33:31,957 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41808] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1755078580-172.17.0.8-1499250810877
2017-07-05 12:33:31,959 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@2a3b43ca] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 12:33:31,960 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 12:33:31,960 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 12:33:31,960 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 12:33:31,960 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 12:33:31,961 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 12:33:31,961 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:31,962 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 12:33:31,962 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@44b194fe] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 12:33:31,962 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 1 2 
2017-07-05 12:33:31,962 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@49122b8f] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 12:33:31,963 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-05 12:33:31,964 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250252707/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-05 12:33:31,964 INFO  [CacheReplicationMonitor(1782375083)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 12:33:31,966 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 41808
2017-07-05 12:33:31,967 INFO  [IPC Server listener on 41808] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 41808
2017-07-05 12:33:31,967 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 12:33:31,968 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@4207609e] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 12:33:31,968 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@67b100fe] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 12:33:31,981 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 12:33:31,982 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 12:33:32,021 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 12:33:32,122 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 12:33:32,124 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 12:33:32,124 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.33 sec - in org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
Running org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec - in org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Running org.apache.samoa.streams.kafka.KafkaUtilsTest
2017-07-05 12:33:32,702 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-05 12:33:32,702 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:host.name=spirals-librepair
2017-07-05 12:33:32,702 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.version=1.8.0_121
2017-07-05 12:33:32,702 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.vendor=Oracle Corporation
2017-07-05 12:33:32,703 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-05 12:33:32,703 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.class.path=/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/250252707/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-05 12:33:32,703 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-05 12:33:32,703 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.io.tmpdir=/tmp
2017-07-05 12:33:32,703 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.compiler=<NA>
2017-07-05 12:33:32,703 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.name=Linux
2017-07-05 12:33:32,704 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.arch=amd64
2017-07-05 12:33:32,704 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.version=3.16.0-4-amd64
2017-07-05 12:33:32,704 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.name=root
2017-07-05 12:33:32,704 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.home=/root
2017-07-05 12:33:32,704 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.dir=/root/workspace/apache/incubator-samoa/250252707/samoa-api
2017-07-05 12:33:32,725 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-1686023099812863512/version-2 snapdir /tmp/kafka-7115012319458568854/version-2
2017-07-05 12:33:32,732 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 12:33:32,770 INFO  [ZkClient-EventThread-482-127.0.0.1:46388] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 12:33:32,775 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-05 12:33:32,775 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:host.name=spirals-librepair
2017-07-05 12:33:32,776 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.version=1.8.0_121
2017-07-05 12:33:32,776 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.vendor=Oracle Corporation
2017-07-05 12:33:32,776 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-05 12:33:32,776 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.class.path=/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/250252707/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/250252707/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/250252707/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-05 12:33:32,776 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-05 12:33:32,776 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.io.tmpdir=/tmp
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.compiler=<NA>
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.name=Linux
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.arch=amd64
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.version=3.16.0-4-amd64
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.name=root
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.home=/root
2017-07-05 12:33:32,777 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.dir=/root/workspace/apache/incubator-samoa/250252707/samoa-api
2017-07-05 12:33:32,778 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:46388 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@7601bc96
2017-07-05 12:33:32,794 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 12:33:32,803 INFO  [main-SendThread(127.0.0.1:46388)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:46388. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 12:33:32,803 INFO  [main-SendThread(127.0.0.1:46388)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:46388, initiating session
2017-07-05 12:33:32,804 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:56264
2017-07-05 12:33:32,819 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:56264
2017-07-05 12:33:32,824 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 12:33:32,845 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d124fe7440000 with negotiated timeout 10000 for client /127.0.0.1:56264
2017-07-05 12:33:32,846 INFO  [main-SendThread(127.0.0.1:46388)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:46388, sessionid = 0x15d124fe7440000, negotiated timeout = 10000
2017-07-05 12:33:32,849 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 12:33:33,169 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafkaUtils-107392788121151688
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:46388
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 12:33:33,266 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 12:33:33,270 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:46388
2017-07-05 12:33:33,280 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:46388 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@9e54c59
2017-07-05 12:33:33,282 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 12:33:33,285 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 12:33:33,286 INFO  [main-SendThread(127.0.0.1:46388)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:46388. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 12:33:33,286 INFO  [main-SendThread(127.0.0.1:46388)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:46388, initiating session
2017-07-05 12:33:33,287 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:56267
2017-07-05 12:33:33,287 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:56267
2017-07-05 12:33:33,290 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d124fe7440001 with negotiated timeout 6000 for client /127.0.0.1:56267
2017-07-05 12:33:33,290 INFO  [main-SendThread(127.0.0.1:46388)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:46388, sessionid = 0x15d124fe7440001, negotiated timeout = 6000
2017-07-05 12:33:33,290 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 12:33:33,315 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 12:33:33,325 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 12:33:33,335 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 12:33:33,391 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x1b zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 12:33:33,398 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = z9kcvwpKRJijP1_j-pD7KQ
2017-07-05 12:33:33,402 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-107392788121151688/meta.properties
2017-07-05 12:33:33,434 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 12:33:33,437 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 12:33:33,486 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 12:33:33,496 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 12:33:33,542 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 12:33:33,545 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 12:33:33,548 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 12:33:33,553 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 12:33:33,619 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 12:33:33,624 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 12:33:33,656 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:33,658 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:33,709 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 12:33:33,723 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 12:33:33,731 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 12:33:33,732 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 12:33:33,733 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 12:33:33,738 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:setData cxid:0x25 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 12:33:33,742 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 12:33:33,775 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 12:33:33,776 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 12:33:33,777 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 12:33:33,784 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 12:33:33,785 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 12:33:33,788 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 12:33:33,793 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 12:33:33,794 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 12:33:33,799 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 12:33:33,800 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 12:33:33,801 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 12:33:33,838 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 12:33:33,846 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 12:33:33,848 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 12:33:33,849 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 12:33:33,851 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 12:33:33,855 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:delete cxid:0x36 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 12:33:33,858 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 12:33:33,859 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 12:33:33,889 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:33,897 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:33,904 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:33,916 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 12:33:33,926 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 12:33:33,931 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 5 milliseconds.
2017-07-05 12:33:33,988 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 12:33:34,044 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 12:33:34,061 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 12:33:34,062 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 12:33:34,063 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 12:33:34,067 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 12:33:34,070 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 12:33:34,071 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-107392788121151688/meta.properties
2017-07-05 12:33:34,073 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 12:33:34,093 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:34,093 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:34,095 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 12:33:34,140 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 12:33:34,169 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 12:33:34,174 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-r Error:KeeperErrorCode = NoNode for /config/topics/test-r
2017-07-05 12:33:34,176 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:34,177 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 12:33:34,188 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:34,220 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 12:33:34,241 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440000 type:setData cxid:0xb zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 12:33:34,244 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440000 type:create cxid:0xc zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:34,246 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:34,262 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-r)], deleted topics: [Set()], new partition replica assignment [Map([test-r,0] -> List(0))]
2017-07-05 12:33:34,263 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-r,0]
2017-07-05 12:33:34,323 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-r,0]
2017-07-05 12:33:34,324 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-r,0]
2017-07-05 12:33:34,336 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:34,338 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:34,342 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-r,Partition=0,Replica=0]
2017-07-05 12:33:34,408 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-r,0]
2017-07-05 12:33:34,418 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x4c zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions/0
2017-07-05 12:33:34,425 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x4d zxid:0x25 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions
2017-07-05 12:33:34,496 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-r,Partition=0,Replica=0]
2017-07-05 12:33:34,510 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:34,510 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:34,525 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-05 12:33:34,525 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-05 12:33:34,529 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-05 12:33:34,529 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-05 12:33:34,530 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 12:33:34,533 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-05 12:33:34,537 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x56 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 12:33:34,545 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x57 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 12:33:34,593 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 12:33:34,640 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-r-0
2017-07-05 12:33:34,656 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:34,672 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:setData cxid:0x63 zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 12:33:34,674 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x64 zxid:0x2f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:34,679 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 12:33:34,690 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 12:33:34,723 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log test-r-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:34,728 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-r,0] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:34,729 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [test-r,0] on broker 0: No checkpointed highwatermark is found for partition test-r-0
2017-07-05 12:33:34,766 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 12:33:34,773 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:34,774 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 4 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:34,788 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:34,791 INFO  [kafka-request-handler-6] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-05 12:33:34,798 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:34,800 INFO  [kafka-request-handler-6] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:34,802 INFO  [kafka-request-handler-6] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:34,804 INFO  [kafka-request-handler-6] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 12:33:34,814 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 12:33:34,853 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:34,855 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xa4 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 12:33:34,857 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xa5 zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 12:33:34,864 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xa9 zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 12:33:34,869 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xac zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 12:33:34,874 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xaf zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 12:33:34,884 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xb2 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 12:33:34,898 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xb8 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 12:33:34,903 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xbb zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 12:33:34,910 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xbe zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 12:33:34,927 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xc1 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 12:33:34,933 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xc4 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 12:33:34,938 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xc7 zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 12:33:34,945 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xca zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 12:33:34,950 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xcd zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 12:33:34,957 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xd0 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 12:33:34,968 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xd3 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 12:33:34,973 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xd6 zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 12:33:34,980 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xd9 zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 12:33:34,984 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xdc zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 12:33:34,995 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xe0 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 12:33:35,013 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xe5 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 12:33:35,022 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xe8 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 12:33:35,027 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xeb zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 12:33:35,031 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xee zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 12:33:35,036 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xf1 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 12:33:35,041 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xf4 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 12:33:35,045 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xf7 zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 12:33:35,049 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xfa zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 12:33:35,060 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0xfd zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 12:33:35,066 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x100 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 12:33:35,083 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x103 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 12:33:35,089 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x106 zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 12:33:35,097 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x10b zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 12:33:35,109 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x10f zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 12:33:35,115 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x112 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 12:33:35,119 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x115 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 12:33:35,123 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x118 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 12:33:35,127 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x11b zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 12:33:35,138 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x11e zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 12:33:35,143 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x121 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 12:33:35,155 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x124 zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 12:33:35,166 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x127 zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 12:33:35,171 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x12a zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 12:33:35,177 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x12d zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 12:33:35,184 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x130 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 12:33:35,188 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x133 zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 12:33:35,193 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x136 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 12:33:35,200 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x13b zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 12:33:35,204 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x13f zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 12:33:35,208 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x142 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 12:33:35,219 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d124fe7440001 type:create cxid:0x145 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 12:33:35,235 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 12:33:35,296 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 12:33:35,312 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,316 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,316 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 12:33:35,323 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,325 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,325 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 12:33:35,331 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,333 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,334 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 12:33:35,340 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,342 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,342 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 12:33:35,348 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,350 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,350 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 12:33:35,357 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,359 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,359 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 12:33:35,365 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,367 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,368 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 12:33:35,375 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,377 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,377 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 12:33:35,383 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,385 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,389 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 12:33:35,396 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,397 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,398 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 12:33:35,407 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,408 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,409 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 12:33:35,417 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,419 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,419 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 12:33:35,426 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,428 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,428 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 12:33:35,442 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,444 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,444 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 12:33:35,453 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,456 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,456 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 12:33:35,461 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,463 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,464 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 12:33:35,469 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,471 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,471 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 12:33:35,477 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,479 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,479 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 12:33:35,485 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,486 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,487 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 12:33:35,493 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,495 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,496 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 12:33:35,501 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,503 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,503 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 12:33:35,509 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,510 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,511 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 12:33:35,516 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,517 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,518 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 12:33:35,523 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,524 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,525 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 12:33:35,530 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,532 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,532 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 12:33:35,537 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,539 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,539 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 12:33:35,544 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,545 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,546 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 12:33:35,551 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,552 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,553 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 12:33:35,558 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,559 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,560 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 12:33:35,566 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,567 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,568 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 12:33:35,573 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,575 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,575 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 12:33:35,580 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,582 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,582 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 12:33:35,587 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,589 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,589 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 12:33:35,595 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,596 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,596 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 12:33:35,602 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,603 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,604 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 12:33:35,617 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,622 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,623 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 12:33:35,630 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,632 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,633 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 12:33:35,638 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,639 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,640 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 12:33:35,645 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,646 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,647 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 12:33:35,652 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,654 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,654 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 12:33:35,659 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,661 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,661 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 12:33:35,667 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,668 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,669 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 12:33:35,675 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,676 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,677 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 12:33:35,682 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,684 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,684 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 12:33:35,690 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,692 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,692 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 12:33:35,698 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,699 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,700 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 12:33:35,706 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,707 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,708 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 12:33:35,722 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,723 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,724 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 12:33:35,729 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,731 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,731 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 12:33:35,737 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:35,739 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafkaUtils-107392788121151688 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:35,740 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 12:33:35,743 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 12:33:35,841 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:35,842 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 97 milliseconds.
2017-07-05 12:33:35,843 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 12:33:35,845 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 0 milliseconds.
2017-07-05 12:33:35,845 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 12:33:35,845 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 12:33:35,846 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 12:33:35,846 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:35,846 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 12:33:35,847 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-05 12:33:35,848 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 12:33:35,849 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 12:33:35,849 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 12:33:35,850 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 12:33:35,850 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 12:33:35,851 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-05 12:33:35,851 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 12:33:35,852 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 12:33:35,852 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 12:33:35,854 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 0 milliseconds.
2017-07-05 12:33:35,854 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 12:33:35,855 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 12:33:35,855 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 12:33:35,856 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-05 12:33:35,856 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 12:33:35,857 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 1 milliseconds.
2017-07-05 12:33:35,857 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 12:33:35,859 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 0 milliseconds.
2017-07-05 12:33:35,859 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 12:33:35,860 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-05 12:33:35,860 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 12:33:35,861 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 1 milliseconds.
2017-07-05 12:33:35,861 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 12:33:35,862 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 1 milliseconds.
2017-07-05 12:33:35,862 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 12:33:35,863 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 12:33:35,864 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-05 12:33:35,864 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 12:33:35,865 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 1 milliseconds.
2017-07-05 12:33:35,865 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 12:33:35,866 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-05 12:33:35,866 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 12:33:35,867 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 0 milliseconds.
2017-07-05 12:33:35,868 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 12:33:35,869 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-05 12:33:35,869 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 12:33:35,870 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-05 12:33:35,870 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 12:33:35,871 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 12:33:35,871 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 12:33:35,872 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 12:33:35,873 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 12:33:35,874 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 12:33:35,874 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 12:33:35,875 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 12:33:35,875 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 12:33:35,876 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 12:33:35,876 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 12:33:35,877 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 12:33:35,878 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 12:33:35,879 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 0 milliseconds.
2017-07-05 12:33:35,879 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 12:33:35,880 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 12:33:35,880 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 12:33:35,881 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-05 12:33:35,881 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 12:33:35,882 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 12:33:35,882 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 12:33:35,883 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 12:33:35,883 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 12:33:35,885 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 12:33:35,885 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 12:33:35,886 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 12:33:35,886 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 12:33:35,887 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-05 12:33:35,887 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 12:33:35,888 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 12:33:35,888 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 12:33:35,889 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 0 milliseconds.
2017-07-05 12:33:35,890 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 12:33:35,895 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 5 milliseconds.
2017-07-05 12:33:35,895 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 12:33:35,896 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 0 milliseconds.
2017-07-05 12:33:35,897 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 12:33:35,898 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 0 milliseconds.
2017-07-05 12:33:35,898 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 12:33:35,899 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-05 12:33:35,899 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 12:33:35,900 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-05 12:33:35,900 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 12:33:35,901 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 1 milliseconds.
2017-07-05 12:33:35,901 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 12:33:35,903 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 1 milliseconds.
2017-07-05 12:33:35,903 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 12:33:35,904 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-05 12:33:35,904 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 12:33:35,906 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 2 milliseconds.
2017-07-05 12:33:35,906 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 12:33:35,907 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 1 milliseconds.
2017-07-05 12:33:35,908 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 12:33:35,909 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 1 milliseconds.
2017-07-05 12:33:35,909 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 12:33:35,910 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-05 12:33:35,966 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:35,967 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:35,978 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 12:33:35,987 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 12:33:36,002 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 12:33:36,052 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 12:33:36,059 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 12:33:36,088 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-05 12:33:36,089 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 2 is now empty
2017-07-05 12:33:36,770 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:36,771 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:36,774 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:36,774 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:36,787 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:36,788 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 12:33:36,799 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:36,803 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 2
2017-07-05 12:33:36,803 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 3
2017-07-05 12:33:36,805 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 3
2017-07-05 12:33:36,807 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 3
2017-07-05 12:33:36,808 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 12:33:37,771 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:37,772 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:37,777 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:37,777 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:37,778 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = sendM-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 12:33:37,785 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:37,785 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 12:33:37,786 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:37,791 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 3
2017-07-05 12:33:37,804 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 12:33:37,805 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:37,805 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:37,938 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 4
2017-07-05 12:33:37,961 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 4
2017-07-05 12:33:37,967 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 4
2017-07-05 12:33:37,968 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 12:33:38,030 INFO  [main] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-05 12:33:39,576 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = rcv-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 12:33:39,581 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 12:33:39,581 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:39,581 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:39,582 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:39,583 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-4
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:39,586 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:39,586 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:41,042 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 4
2017-07-05 12:33:41,043 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 5 is now empty
2017-07-05 12:33:42,749 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:42,750 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 12:33:42,751 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:42,755 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 5
2017-07-05 12:33:42,755 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 6
2017-07-05 12:33:42,757 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 6
2017-07-05 12:33:42,759 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 6
2017-07-05 12:33:42,759 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-s-0] for group test
2017-07-05 12:33:42,772 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 6
2017-07-05 12:33:42,773 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 7 is now empty
2017-07-05 12:33:42,797 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 12:33:42,798 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 12:33:42,812 INFO  [kafka-request-handler-5] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 12:33:42,828 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 12:33:42,831 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 12:33:42,842 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 12:33:42,843 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 12:33:42,846 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 12:33:42,850 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 12:33:43,437 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 12:33:43,438 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 12:33:43,438 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 12:33:43,447 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 12:33:43,447 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 12:33:43,448 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 12:33:43,450 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 12:33:43,451 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 12:33:43,454 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 12:33:43,454 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:43,469 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:43,469 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:43,470 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:43,668 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:43,669 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:43,691 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 12:33:43,692 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:43,709 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:43,709 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:43,712 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 12:33:43,713 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:43,756 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:43,756 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:43,756 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:43,913 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:43,913 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:43,915 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 12:33:43,916 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 12:33:43,918 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 12:33:43,919 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 12:33:43,919 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 12:33:43,919 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 12:33:44,074 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 12:33:44,080 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 12:33:44,081 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 12:33:44,083 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 12:33:44,085 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 12:33:44,085 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 12:33:44,086 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 12:33:44,087 INFO  [ZkClient-EventThread-486-127.0.0.1:46388] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 12:33:44,088 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d124fe7440001
2017-07-05 12:33:44,090 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d124fe7440001 closed
2017-07-05 12:33:44,090 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 12:33:44,092 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:56267 which had sessionid 0x15d124fe7440001
2017-07-05 12:33:44,093 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 12:33:44,093 INFO  [ZkClient-EventThread-482-127.0.0.1:46388] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 12:33:44,094 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d124fe7440000
2017-07-05 12:33:44,095 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d124fe7440000 closed
2017-07-05 12:33:44,095 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 12:33:44,095 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:56264 which had sessionid 0x15d124fe7440000
2017-07-05 12:33:44,096 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 12:33:44,096 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 12:33:44,096 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 12:33:44,097 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 12:33:44,097 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 12:33:44,097 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 12:33:44,098 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 12:33:44,101 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 12:33:44,101 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 12:33:44,102 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 12:33:44,102 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 12:33:44,102 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 12:33:44,102 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.956 sec - in org.apache.samoa.streams.kafka.KafkaUtilsTest
Running org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
2017-07-05 12:33:44,110 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-4549089357120763883/version-2 snapdir /tmp/kafka-3393470537195242755/version-2
2017-07-05 12:33:44,110 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 12:33:44,114 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:34869 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@4e0cc334
2017-07-05 12:33:44,114 INFO  [ZkClient-EventThread-542-127.0.0.1:34869] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 12:33:44,115 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 12:33:44,115 INFO  [main-SendThread(127.0.0.1:34869)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:34869. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 12:33:44,116 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:37789
2017-07-05 12:33:44,116 INFO  [main-SendThread(127.0.0.1:34869)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:34869, initiating session
2017-07-05 12:33:44,116 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:37789
2017-07-05 12:33:44,117 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 12:33:44,127 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d125013b00000 with negotiated timeout 10000 for client /127.0.0.1:37789
2017-07-05 12:33:44,128 INFO  [main-SendThread(127.0.0.1:34869)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:34869, sessionid = 0x15d125013b00000, negotiated timeout = 10000
2017-07-05 12:33:44,128 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 12:33:44,129 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-591453733017587210
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:34869
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 12:33:44,131 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 12:33:44,131 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:34869
2017-07-05 12:33:44,131 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:34869 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@1e12a5a6
2017-07-05 12:33:44,131 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 12:33:44,132 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 12:33:44,132 INFO  [main-SendThread(127.0.0.1:34869)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:34869. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 12:33:44,133 INFO  [main-SendThread(127.0.0.1:34869)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:34869, initiating session
2017-07-05 12:33:44,133 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:37790
2017-07-05 12:33:44,133 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:37790
2017-07-05 12:33:44,137 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d125013b00001 with negotiated timeout 6000 for client /127.0.0.1:37790
2017-07-05 12:33:44,137 INFO  [main-SendThread(127.0.0.1:34869)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:34869, sessionid = 0x15d125013b00001, negotiated timeout = 6000
2017-07-05 12:33:44,142 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 12:33:44,146 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 12:33:44,151 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 12:33:44,162 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 12:33:44,169 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 12:33:44,172 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = Lhk8jQLmRpO4BQ3HrIwrxQ
2017-07-05 12:33:44,172 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-591453733017587210/meta.properties
2017-07-05 12:33:44,174 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 12:33:44,174 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 12:33:44,175 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 12:33:44,176 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 12:33:44,210 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 12:33:44,211 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 12:33:44,212 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 12:33:44,216 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 12:33:44,222 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 12:33:44,224 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 12:33:44,225 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:44,226 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:44,227 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 12:33:44,228 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 12:33:44,231 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 12:33:44,232 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 12:33:44,232 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 12:33:44,233 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 12:33:44,235 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 12:33:44,242 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 12:33:44,243 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 12:33:44,243 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 12:33:44,243 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 12:33:44,244 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 12:33:44,244 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 12:33:44,245 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 12:33:44,245 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 12:33:44,245 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 12:33:44,245 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 12:33:44,245 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 12:33:44,246 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 12:33:44,246 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 12:33:44,246 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 12:33:44,247 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 12:33:44,247 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 12:33:44,247 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 12:33:44,249 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 12:33:44,249 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 12:33:44,251 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:44,251 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 12:33:44,251 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:44,252 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:44,252 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 12:33:44,252 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 12:33:44,253 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 1 milliseconds.
2017-07-05 12:33:44,256 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 12:33:44,268 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 12:33:44,269 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x40 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 12:33:44,270 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x41 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 12:33:44,272 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 12:33:44,273 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 12:33:44,274 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-591453733017587210/meta.properties
2017-07-05 12:33:44,274 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 12:33:44,279 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 12:33:44,281 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 12:33:44,281 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 12:33:44,282 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 12:33:44,288 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:44,289 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:44,289 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 12:33:44,289 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:setData cxid:0x49 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 12:33:44,291 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x4a zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:44,294 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:44,295 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-05 12:33:44,296 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 55 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:44,299 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-05 12:33:44,300 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-05 12:33:44,302 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-05 12:33:44,302 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-05 12:33:44,303 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 12:33:44,303 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00000 type:setData cxid:0x5 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-05 12:33:44,305 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-05 12:33:44,305 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00000 type:create cxid:0x6 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:44,306 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x52 zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 12:33:44,307 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x53 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 12:33:44,308 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:44,313 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 12:33:44,315 INFO  [kafka-request-handler-3] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-05 12:33:44,318 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:44,319 INFO  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 12:33:44,319 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-5
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:44,319 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,320 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos)], deleted topics: [Set()], new partition replica assignment [Map([samoa_test-oos,0] -> List(0))]
2017-07-05 12:33:44,321 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [samoa_test-oos,0]
2017-07-05 12:33:44,323 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:44,323 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:44,324 WARN  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 12:33:44,324 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,325 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [samoa_test-oos,0]
2017-07-05 12:33:44,325 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [samoa_test-oos,0]
2017-07-05 12:33:44,324 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:44,327 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:44,326 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 12:33:44,325 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 12:33:44,330 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [samoa_test-oos,0]
2017-07-05 12:33:44,330 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x5f zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-05 12:33:44,346 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x61 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-05 12:33:44,346 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:44,370 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 12:33:44,370 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:setData cxid:0x6b zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 12:33:44,371 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x6c zxid:0x2f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:44,372 INFO  [kafka-request-handler-1] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0
2017-07-05 12:33:44,379 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,380 WARN  [kafka-producer-network-thread | test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 1 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:44,380 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,380 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-05 12:33:44,382 INFO  [kafka-request-handler-7] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 12:33:44,384 INFO  [kafka-request-handler-7] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 12:33:44,405 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 12:33:44,406 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:44,408 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:44,411 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:44,416 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 12:33:44,463 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:44,464 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xaa zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 12:33:44,466 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xab zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 12:33:44,471 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xaf zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 12:33:44,474 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xb2 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 12:33:44,478 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xb5 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 12:33:44,481 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xb8 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 12:33:44,485 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xbb zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 12:33:44,494 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xbe zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 12:33:44,499 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xc1 zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 12:33:44,500 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-05 12:33:44,519 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xc4 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 12:33:44,523 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xc7 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 12:33:44,527 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xca zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 12:33:44,532 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xcd zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 12:33:44,536 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xd0 zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 12:33:44,541 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xd3 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 12:33:44,545 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xd6 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 12:33:44,550 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xd9 zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 12:33:44,555 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xde zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 12:33:44,560 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xe2 zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 12:33:44,571 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xe5 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 12:33:44,581 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xe8 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 12:33:44,588 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xeb zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 12:33:44,593 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xee zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 12:33:44,598 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xf1 zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 12:33:44,605 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xf4 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 12:33:44,615 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xf7 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 12:33:44,620 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xfa zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 12:33:44,624 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0xfd zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 12:33:44,628 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x100 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 12:33:44,643 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x103 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 12:33:44,653 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x107 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 12:33:44,657 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x10b zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 12:33:44,661 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x10f zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 12:33:44,665 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x112 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 12:33:44,669 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x115 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 12:33:44,673 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x118 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 12:33:44,676 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x11b zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 12:33:44,680 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x11e zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 12:33:44,683 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x121 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 12:33:44,687 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x124 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 12:33:44,706 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x127 zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 12:33:44,710 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x12a zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 12:33:44,714 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x12d zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 12:33:44,718 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x130 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 12:33:44,721 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x133 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 12:33:44,725 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x136 zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 12:33:44,729 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x139 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 12:33:44,733 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x13c zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 12:33:44,736 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x13f zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 12:33:44,740 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x142 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 12:33:44,751 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125013b00001 type:create cxid:0x145 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 12:33:44,767 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 12:33:44,790 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 12:33:44,795 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,797 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,797 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 12:33:44,803 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,804 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,805 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 12:33:44,811 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,812 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,813 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 12:33:44,818 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,820 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,821 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 12:33:44,827 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,829 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,830 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 12:33:44,835 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,837 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,838 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 12:33:44,843 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,845 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,845 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 12:33:44,852 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,853 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,854 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 12:33:44,862 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,863 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,863 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 12:33:44,867 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,868 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,869 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 12:33:44,873 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,874 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,875 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 12:33:44,878 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,879 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,880 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 12:33:44,884 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,885 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,885 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 12:33:44,889 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,890 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,890 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 12:33:44,896 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,897 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,897 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 12:33:44,901 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,902 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,902 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 12:33:44,907 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,908 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,908 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 12:33:44,912 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,913 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,914 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 12:33:44,918 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,919 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,919 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 12:33:44,923 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,924 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,924 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 12:33:44,929 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,929 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,930 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 12:33:44,934 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,934 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,935 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 12:33:44,939 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,940 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,940 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 12:33:44,945 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,946 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,947 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 12:33:44,951 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,952 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,952 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 12:33:44,956 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,957 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,958 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 12:33:44,965 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,965 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,966 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 12:33:44,970 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,971 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,972 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 12:33:44,976 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,976 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,977 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 12:33:44,981 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,982 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,983 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 12:33:44,987 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,989 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,990 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 12:33:44,994 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:44,995 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:44,995 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 12:33:45,001 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,002 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,003 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 12:33:45,008 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,009 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,010 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 12:33:45,016 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,018 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,019 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 12:33:45,025 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,027 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,028 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 12:33:45,036 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,037 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,038 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 12:33:45,045 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,047 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,047 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 12:33:45,053 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,055 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,056 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 12:33:45,065 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,066 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,066 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 12:33:45,070 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,071 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,071 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 12:33:45,076 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,077 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,077 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 12:33:45,085 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,086 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,086 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 12:33:45,099 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,100 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,100 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 12:33:45,104 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,105 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,106 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 12:33:45,110 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,111 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,111 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 12:33:45,116 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,117 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,117 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 12:33:45,121 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,122 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,122 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 12:33:45,127 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,128 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,128 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 12:33:45,133 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:45,134 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-591453733017587210 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:45,134 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 12:33:45,136 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 12:33:45,137 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-05 12:33:45,137 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 12:33:45,138 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 12:33:45,138 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 12:33:45,139 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 12:33:45,139 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 12:33:45,140 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 0 milliseconds.
2017-07-05 12:33:45,141 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 12:33:45,142 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 12:33:45,142 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 12:33:45,143 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 12:33:45,143 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 12:33:45,144 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-05 12:33:45,144 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 12:33:45,145 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 12:33:45,145 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 12:33:45,147 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-05 12:33:45,148 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 12:33:45,149 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 12:33:45,150 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 12:33:45,151 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-05 12:33:45,151 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 12:33:45,153 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 1 milliseconds.
2017-07-05 12:33:45,153 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 12:33:45,155 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 1 milliseconds.
2017-07-05 12:33:45,155 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 12:33:45,157 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 2 milliseconds.
2017-07-05 12:33:45,158 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 12:33:45,159 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 1 milliseconds.
2017-07-05 12:33:45,159 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 12:33:45,161 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 1 milliseconds.
2017-07-05 12:33:45,161 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 12:33:45,162 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:45,162 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-05 12:33:45,162 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 12:33:45,163 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 1 milliseconds.
2017-07-05 12:33:45,164 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 12:33:45,165 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-05 12:33:45,165 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 12:33:45,166 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-05 12:33:45,167 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 12:33:45,168 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-05 12:33:45,168 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 12:33:45,169 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 12:33:45,169 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:45,169 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-05 12:33:45,170 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 12:33:45,171 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 12:33:45,171 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 12:33:45,172 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 12:33:45,172 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 12:33:45,172 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 12:33:45,173 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 12:33:45,174 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 12:33:45,175 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 12:33:45,175 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 12:33:45,176 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 12:33:45,176 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 12:33:45,177 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 12:33:45,177 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 12:33:45,178 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-05 12:33:45,178 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 12:33:45,180 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 12:33:45,180 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 12:33:45,181 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-05 12:33:45,181 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 12:33:45,182 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 12:33:45,182 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 12:33:45,183 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 12:33:45,183 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 12:33:45,184 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 0 milliseconds.
2017-07-05 12:33:45,185 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 12:33:45,186 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 12:33:45,186 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 12:33:45,188 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-05 12:33:45,189 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 12:33:45,190 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 12:33:45,190 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 12:33:45,191 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-05 12:33:45,191 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 12:33:45,193 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-05 12:33:45,194 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 12:33:45,195 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-05 12:33:45,196 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 12:33:45,197 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 1 milliseconds.
2017-07-05 12:33:45,197 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 12:33:45,198 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-05 12:33:45,198 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 12:33:45,203 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 5 milliseconds.
2017-07-05 12:33:45,203 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 12:33:45,207 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 3 milliseconds.
2017-07-05 12:33:45,207 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 12:33:45,210 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 3 milliseconds.
2017-07-05 12:33:45,210 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 12:33:45,217 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 6 milliseconds.
2017-07-05 12:33:45,217 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 12:33:45,220 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 3 milliseconds.
2017-07-05 12:33:45,220 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 12:33:45,223 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 3 milliseconds.
2017-07-05 12:33:45,223 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 12:33:45,226 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 3 milliseconds.
2017-07-05 12:33:45,226 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 12:33:45,229 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 3 milliseconds.
2017-07-05 12:33:45,288 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:45,289 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:45,290 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 12:33:45,290 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 12:33:45,295 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 12:33:45,298 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 12:33:45,300 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-05 12:33:54,187 INFO  [Thread-434] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-05 12:33:54,189 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 12:33:54,190 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 12:33:54,202 INFO  [kafka-request-handler-0] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 12:33:54,205 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 12:33:54,206 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 12:33:54,208 INFO  [kafka-coordinator-heartbeat-thread | test] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 12:33:54,210 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 12:33:54,210 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 12:33:54,211 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 12:33:54,216 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 12:33:55,175 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 12:33:55,175 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 12:33:55,175 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 12:33:55,181 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 12:33:55,181 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 12:33:55,181 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 12:33:55,181 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 12:33:55,181 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 12:33:55,182 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 12:33:55,182 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:55,289 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:55,289 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:55,290 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:55,318 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:55,318 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:55,327 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 12:33:55,327 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:55,517 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:55,518 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:55,518 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 12:33:55,518 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:55,556 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:55,556 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:55,556 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:33:55,719 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:33:55,719 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:33:55,719 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 12:33:55,719 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 12:33:55,720 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 12:33:55,720 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 12:33:55,720 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 12:33:55,720 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 12:33:55,995 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 12:33:55,995 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 12:33:55,995 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 12:33:55,997 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 12:33:55,997 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 12:33:55,997 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 12:33:55,998 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 12:33:55,998 INFO  [ZkClient-EventThread-545-127.0.0.1:34869] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 12:33:55,999 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d125013b00001
2017-07-05 12:33:56,002 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d125013b00001 closed
2017-07-05 12:33:56,002 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 12:33:56,003 WARN  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:doIO(357)) - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x15d125013b00001, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
	at java.lang.Thread.run(Thread.java:745)
2017-07-05 12:33:56,005 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 12:33:56,007 INFO  [ZkClient-EventThread-542-127.0.0.1:34869] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 12:33:56,008 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:37790 which had sessionid 0x15d125013b00001
2017-07-05 12:33:56,009 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d125013b00000
2017-07-05 12:33:56,010 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:37789 which had sessionid 0x15d125013b00000
2017-07-05 12:33:56,010 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 12:33:56,010 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d125013b00000 closed
2017-07-05 12:33:56,011 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 12:33:56,011 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 12:33:56,012 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 12:33:56,012 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 12:33:56,012 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 12:33:56,013 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 12:33:56,013 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 12:33:56,014 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 12:33:56,015 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 12:33:56,015 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 12:33:56,016 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 12:33:56,016 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 12:33:56,016 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.907 sec - in org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
Running org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
2017-07-05 12:33:56,020 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-8839574161930088077/version-2 snapdir /tmp/kafka-669259640200472297/version-2
2017-07-05 12:33:56,021 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 12:33:56,023 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:40696 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@5e9bbd9d
2017-07-05 12:33:56,023 INFO  [ZkClient-EventThread-594-127.0.0.1:40696] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 12:33:56,023 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 12:33:56,024 INFO  [main-SendThread(127.0.0.1:40696)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:40696. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 12:33:56,024 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:50265
2017-07-05 12:33:56,024 INFO  [main-SendThread(127.0.0.1:40696)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:40696, initiating session
2017-07-05 12:33:56,025 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:50265
2017-07-05 12:33:56,025 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 12:33:56,035 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d125042350000 with negotiated timeout 10000 for client /127.0.0.1:50265
2017-07-05 12:33:56,036 INFO  [main-SendThread(127.0.0.1:40696)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:40696, sessionid = 0x15d125042350000, negotiated timeout = 10000
2017-07-05 12:33:56,036 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 12:33:56,037 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-1046228850044034648
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:40696
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 12:33:56,040 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 12:33:56,040 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:40696
2017-07-05 12:33:56,040 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:40696 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4b87074a
2017-07-05 12:33:56,041 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 12:33:56,041 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 12:33:56,045 INFO  [main-SendThread(127.0.0.1:40696)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:40696. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 12:33:56,045 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:50268
2017-07-05 12:33:56,045 INFO  [main-SendThread(127.0.0.1:40696)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:40696, initiating session
2017-07-05 12:33:56,046 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:50268
2017-07-05 12:33:56,047 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d125042350001 with negotiated timeout 6000 for client /127.0.0.1:50268
2017-07-05 12:33:56,047 INFO  [main-SendThread(127.0.0.1:40696)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:40696, sessionid = 0x15d125042350001, negotiated timeout = 6000
2017-07-05 12:33:56,047 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 12:33:56,050 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 12:33:56,059 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 12:33:56,065 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 12:33:56,071 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 12:33:56,073 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = cfzg3yq0RrqzHU78ny0gyA
2017-07-05 12:33:56,074 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-1046228850044034648/meta.properties
2017-07-05 12:33:56,075 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 12:33:56,076 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 12:33:56,077 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 12:33:56,078 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 12:33:56,113 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 12:33:56,114 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 12:33:56,115 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 12:33:56,117 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 12:33:56,121 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 12:33:56,122 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 12:33:56,123 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:56,124 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:56,125 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 12:33:56,126 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 12:33:56,128 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 12:33:56,129 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 12:33:56,129 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 12:33:56,130 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 12:33:56,132 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 12:33:56,136 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 12:33:56,137 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 12:33:56,137 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 12:33:56,138 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 12:33:56,138 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 12:33:56,138 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 12:33:56,139 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 12:33:56,139 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 12:33:56,140 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 12:33:56,140 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 12:33:56,140 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 12:33:56,141 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 12:33:56,141 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 12:33:56,141 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 12:33:56,141 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 12:33:56,142 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 12:33:56,142 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 12:33:56,143 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 12:33:56,144 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 12:33:56,145 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:56,146 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:56,146 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 12:33:56,147 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 12:33:56,147 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 12:33:56,147 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 12:33:56,148 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-05 12:33:56,149 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 12:33:56,151 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9266 : {samoa_test-oos=INVALID_REPLICATION_FACTOR}
2017-07-05 12:33:56,151 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 57 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-05 12:33:56,154 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 12:33:56,155 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x42 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 12:33:56,155 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 12:33:56,157 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 12:33:56,158 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 12:33:56,158 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 12:33:56,158 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-1046228850044034648/meta.properties
2017-07-05 12:33:56,162 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 12:33:56,164 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 12:33:56,164 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 12:33:56,165 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 12:33:56,166 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:56,166 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:56,166 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 12:33:56,172 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-kdp Error:KeeperErrorCode = NoNode for /config/topics/test-kdp
2017-07-05 12:33:56,173 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:56,175 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:56,179 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-kdp)], deleted topics: [Set()], new partition replica assignment [Map([test-kdp,0] -> List(0))]
2017-07-05 12:33:56,179 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-kdp,0]
2017-07-05 12:33:56,180 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-kdp,0]
2017-07-05 12:33:56,180 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-kdp,0]
2017-07-05 12:33:56,180 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-05 12:33:56,181 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-kdp,0]
2017-07-05 12:33:56,182 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 12:33:56,183 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x4d zxid:0x20 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions/0
2017-07-05 12:33:56,185 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x4e zxid:0x21 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions
2017-07-05 12:33:56,185 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 12:33:56,185 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2017-07-05 12:33:56,185 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:56,185 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:56,187 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:56,188 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-6
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 12:33:56,189 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-05 12:33:56,191 INFO  [kafka-request-handler-3] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-kdp-0
2017-07-05 12:33:56,192 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 12:33:56,192 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 12:33:56,195 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log test-kdp-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,196 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-kdp,0] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,196 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [test-kdp,0] on broker 0: No checkpointed highwatermark is found for partition test-kdp-0
2017-07-05 12:33:56,204 WARN  [Thread-439] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,210 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:setData cxid:0x5a zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 12:33:56,217 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x5b zxid:0x26 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:56,220 INFO  [kafka-request-handler-6] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 12:33:56,230 INFO  [kafka-request-handler-6] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 12:33:56,247 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 12:33:56,248 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:56,249 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:56,250 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:56,252 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 12:33:56,257 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:setData cxid:0x72 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 12:33:56,258 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x74 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:56,260 INFO  [kafka-request-handler-7] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:56,261 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:setData cxid:0x7c zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-05 12:33:56,262 INFO  [kafka-request-handler-7] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-05 12:33:56,263 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 58 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,263 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x7f zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 12:33:56,265 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 12:33:56,266 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic samoa_test-oos with 1 partitions and replication factor 1 is successful
2017-07-05 12:33:56,267 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9267 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,279 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 12:33:56,279 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xa6 zxid:0x31 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 12:33:56,281 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xa7 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 12:33:56,285 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xab zxid:0x36 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 12:33:56,288 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xae zxid:0x39 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 12:33:56,292 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xb1 zxid:0x3c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 12:33:56,295 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xb4 zxid:0x3f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 12:33:56,298 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xb7 zxid:0x42 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 12:33:56,301 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xba zxid:0x45 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 12:33:56,305 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xbd zxid:0x48 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 12:33:56,315 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xc3 zxid:0x4b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 12:33:56,324 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xc6 zxid:0x4e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 12:33:56,327 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xc9 zxid:0x51 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 12:33:56,333 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xcc zxid:0x54 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 12:33:56,336 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xcf zxid:0x57 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 12:33:56,340 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xd2 zxid:0x5a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 12:33:56,343 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xd5 zxid:0x5d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 12:33:56,347 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xd8 zxid:0x60 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 12:33:56,352 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xdb zxid:0x63 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 12:33:56,356 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xde zxid:0x66 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 12:33:56,371 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xe3 zxid:0x69 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 12:33:56,377 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9269 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,380 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xe9 zxid:0x6c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 12:33:56,383 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xed zxid:0x6f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 12:33:56,387 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xf0 zxid:0x72 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 12:33:56,390 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xf3 zxid:0x75 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 12:33:56,394 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xf6 zxid:0x78 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 12:33:56,397 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xf9 zxid:0x7b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 12:33:56,401 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xfc zxid:0x7e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 12:33:56,404 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0xff zxid:0x81 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 12:33:56,408 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x103 zxid:0x84 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 12:33:56,415 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x108 zxid:0x87 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 12:33:56,433 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x10b zxid:0x8a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 12:33:56,438 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x10e zxid:0x8d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 12:33:56,441 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x111 zxid:0x90 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 12:33:56,445 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x114 zxid:0x93 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 12:33:56,449 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x117 zxid:0x96 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 12:33:56,453 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x11a zxid:0x99 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 12:33:56,457 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x11d zxid:0x9c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 12:33:56,461 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x120 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 12:33:56,465 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x123 zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 12:33:56,469 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x126 zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 12:33:56,482 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x12b zxid:0xa8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 12:33:56,490 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9271 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,493 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x131 zxid:0xab txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 12:33:56,498 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x135 zxid:0xae txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 12:33:56,500 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-05 12:33:56,501 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x138 zxid:0xb1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 12:33:56,504 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x13b zxid:0xb4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 12:33:56,507 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x13e zxid:0xb7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 12:33:56,510 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x143 zxid:0xba txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 12:33:56,516 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x147 zxid:0xbd txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 12:33:56,520 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x14a zxid:0xc0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 12:33:56,524 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x14d zxid:0xc3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 12:33:56,527 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x150 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 12:33:56,547 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 12:33:56,554 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos, test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0), [samoa_test-oos,0] -> List(0))]
2017-07-05 12:33:56,554 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0],[samoa_test-oos,0]
2017-07-05 12:33:56,558 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0],[samoa_test-oos,0]
2017-07-05 12:33:56,558 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0],[samoa_test-oos,0]
2017-07-05 12:33:56,559 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 12:33:56,561 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0],[samoa_test-oos,0]
2017-07-05 12:33:56,561 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x15b zxid:0xc9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 12:33:56,566 INFO  [kafka-request-handler-4] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 12:33:56,569 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x15d zxid:0xca txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 12:33:56,574 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x161 zxid:0xce txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-05 12:33:56,575 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d125042350001 type:create cxid:0x162 zxid:0xcf txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-05 12:33:56,577 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,579 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,580 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 12:33:56,580 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 12:33:56,584 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,585 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,585 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 12:33:56,588 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,589 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,590 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 12:33:56,593 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9273 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,596 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,597 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,597 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 12:33:56,601 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,602 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,602 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 12:33:56,605 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,606 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,606 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 12:33:56,610 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,611 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,611 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 12:33:56,616 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,616 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,617 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 12:33:56,620 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,621 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,621 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 12:33:56,625 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,625 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,626 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 12:33:56,629 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,630 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,630 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 12:33:56,635 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,636 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,636 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 12:33:56,639 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,640 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,640 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 12:33:56,644 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,644 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,645 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 12:33:56,648 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,648 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,649 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 12:33:56,652 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,653 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,653 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 12:33:56,656 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,657 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,657 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 12:33:56,661 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,661 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,662 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 12:33:56,665 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,666 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,666 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 12:33:56,669 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,670 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,670 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 12:33:56,673 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,674 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,674 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 12:33:56,678 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,678 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,679 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 12:33:56,683 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,684 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,684 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 12:33:56,687 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,688 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,688 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 12:33:56,692 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,693 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,693 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 12:33:56,696 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,697 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,697 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 12:33:56,699 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9275 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,701 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,702 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,702 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 12:33:56,705 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,706 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,706 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 12:33:56,709 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,710 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,710 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 12:33:56,715 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,716 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,716 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 12:33:56,720 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,721 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,721 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 12:33:56,724 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,725 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,725 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 12:33:56,728 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,729 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,729 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 12:33:56,733 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,733 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,734 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 12:33:56,737 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,738 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,738 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 12:33:56,741 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,742 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,743 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 12:33:56,746 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,747 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,747 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 12:33:56,750 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,751 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,752 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 12:33:56,755 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,755 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,756 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 12:33:56,759 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,760 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,760 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 12:33:56,763 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,764 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,764 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 12:33:56,767 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,768 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,768 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 12:33:56,772 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,772 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,773 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 12:33:56,776 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,776 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,777 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 12:33:56,780 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,780 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,781 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 12:33:56,784 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,785 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,785 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 12:33:56,788 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,789 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,789 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 12:33:56,792 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,793 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,793 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 12:33:56,796 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,797 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,797 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 12:33:56,801 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,804 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9277 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 12:33:56,804 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,804 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 12:33:56,809 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 12:33:56,810 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-05 12:33:56,810 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 12:33:56,811 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 12:33:56,811 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 12:33:56,812 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 12:33:56,812 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 12:33:56,813 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 0 milliseconds.
2017-07-05 12:33:56,814 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 12:33:56,815 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 12:33:56,815 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 12:33:56,816 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 12:33:56,816 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 12:33:56,818 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 2 milliseconds.
2017-07-05 12:33:56,818 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 12:33:56,819 INFO  [kafka-request-handler-5] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0,test-s-0
2017-07-05 12:33:56,819 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 0 milliseconds.
2017-07-05 12:33:56,820 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 12:33:56,821 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 0 milliseconds.
2017-07-05 12:33:56,821 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 12:33:56,821 INFO  [kafka-request-handler-5] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,822 INFO  [kafka-request-handler-5] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,822 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 12:33:56,822 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 12:33:56,822 INFO  [kafka-request-handler-5] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-05 12:33:56,825 INFO  [kafka-request-handler-5] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 12:33:56,825 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 2 milliseconds.
2017-07-05 12:33:56,825 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 12:33:56,825 INFO  [kafka-request-handler-5] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-1046228850044034648 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 12:33:56,826 INFO  [kafka-request-handler-5] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 12:33:56,826 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 1 milliseconds.
2017-07-05 12:33:56,826 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 12:33:56,827 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 0 milliseconds.
2017-07-05 12:33:56,828 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 12:33:56,829 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-05 12:33:56,829 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 12:33:56,830 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 1 milliseconds.
2017-07-05 12:33:56,830 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 12:33:56,831 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 1 milliseconds.
2017-07-05 12:33:56,831 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 12:33:56,832 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-05 12:33:56,832 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 12:33:56,833 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 1 milliseconds.
2017-07-05 12:33:56,833 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 12:33:56,834 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-05 12:33:56,835 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 12:33:56,835 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 0 milliseconds.
2017-07-05 12:33:56,836 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 12:33:56,837 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-05 12:33:56,837 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 12:33:56,838 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-05 12:33:56,838 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 12:33:56,839 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 12:33:56,839 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 12:33:56,840 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 12:33:56,840 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 12:33:56,841 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 12:33:56,841 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 12:33:56,842 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 12:33:56,843 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 12:33:56,844 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 12:33:56,844 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 12:33:56,845 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 12:33:56,845 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 12:33:56,846 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-05 12:33:56,846 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 12:33:56,847 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 12:33:56,847 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 12:33:56,848 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 0 milliseconds.
2017-07-05 12:33:56,849 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 12:33:56,850 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 12:33:56,850 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 12:33:56,851 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 12:33:56,851 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 12:33:56,852 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 12:33:56,852 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 12:33:56,853 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 12:33:56,853 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 12:33:56,854 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 0 milliseconds.
2017-07-05 12:33:56,855 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 12:33:56,856 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 12:33:56,856 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 12:33:56,857 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-05 12:33:56,857 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 12:33:56,858 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-05 12:33:56,858 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 12:33:56,859 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-05 12:33:56,859 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 12:33:56,860 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 1 milliseconds.
2017-07-05 12:33:56,860 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 12:33:56,861 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-05 12:33:56,861 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 12:33:56,862 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-05 12:33:56,862 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 12:33:56,863 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 0 milliseconds.
2017-07-05 12:33:56,864 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 12:33:56,865 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 1 milliseconds.
2017-07-05 12:33:56,865 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 12:33:56,866 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-05 12:33:56,866 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 12:33:56,867 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-05 12:33:56,867 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 12:33:56,868 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 1 milliseconds.
2017-07-05 12:33:56,868 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 12:33:56,870 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 0 milliseconds.
2017-07-05 12:33:56,870 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 12:33:56,871 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-05 12:33:56,906 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:56,917 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 12:33:56,918 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 12:33:56,919 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 12:33:56,920 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 12:33:56,921 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 12:33:56,922 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 12:33:56,924 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 12:33:56,924 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-05 12:34:03,306 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-05 12:34:03,306 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 2 is now empty
2017-07-05 12:34:05,304 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 12:34:05,304 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 12:34:05,311 INFO  [kafka-request-handler-5] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 12:34:05,323 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 12:34:05,324 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 12:34:05,327 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 12:34:05,327 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 12:34:05,328 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 12:34:05,330 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 12:34:05,433 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 12:34:05,433 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 12:34:05,434 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 12:34:06,434 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 12:34:06,434 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 12:34:06,434 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 12:34:06,434 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 12:34:06,435 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 12:34:06,435 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 12:34:06,435 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:34:06,606 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:34:06,606 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:34:06,606 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:34:06,636 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:34:06,636 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:34:06,676 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 12:34:06,677 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:34:06,846 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:34:06,846 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:34:06,847 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 12:34:06,847 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:34:06,966 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:34:06,966 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:34:06,966 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 12:34:07,053 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 12:34:07,053 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 12:34:07,053 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 12:34:07,053 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 12:34:07,053 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 12:34:07,054 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 12:34:07,054 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 12:34:07,054 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 12:34:07,304 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 12:34:07,305 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 12:34:07,305 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 12:34:07,310 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 12:34:07,311 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 12:34:07,311 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 12:34:07,311 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 12:34:07,312 INFO  [ZkClient-EventThread-597-127.0.0.1:40696] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 12:34:07,317 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d125042350001
2017-07-05 12:34:07,322 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:50268 which had sessionid 0x15d125042350001
2017-07-05 12:34:07,322 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d125042350001 closed
2017-07-05 12:34:07,322 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 12:34:07,323 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 12:34:07,323 INFO  [ZkClient-EventThread-594-127.0.0.1:40696] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 12:34:07,324 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d125042350000
2017-07-05 12:34:07,329 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 12:34:07,329 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d125042350000 closed
2017-07-05 12:34:07,329 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 12:34:07,329 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 12:34:07,330 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 12:34:07,330 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 12:34:07,329 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:50265 which had sessionid 0x15d125042350000
2017-07-05 12:34:07,330 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 12:34:07,330 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 12:34:07,331 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 12:34:07,331 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 12:34:07,332 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 12:34:07,332 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 12:34:07,332 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 12:34:07,332 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 12:34:07,333 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.314 sec - in org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
Running org.apache.samoa.core.DoubleVectorTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec - in org.apache.samoa.core.DoubleVectorTest

Results :

Tests run: 22, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-test 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-test ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-test ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250252707/samoa-test/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250252707/samoa-test/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-test ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250252707/samoa-test/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-local 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-local ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-local ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250252707/samoa-local/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18:test (default-test) @ samoa-local ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250252707/samoa-local/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom (3 KB at 115.8 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom (3 KB at 116.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar (67 KB at 1950.7 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.topology.impl.SimpleEngineTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.122 sec - in org.apache.samoa.topology.impl.SimpleEngineTest
Running org.apache.samoa.topology.impl.SimpleStreamTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.163 sec - in org.apache.samoa.topology.impl.SimpleStreamTest
Running org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 sec - in org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Running org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.378 sec <<< FAILURE! - in org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
testStartSendingEvents(org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest)  Time elapsed: 0.322 sec  <<< ERROR!
java.lang.IllegalStateException: Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest$4.<init>(SimpleEntranceProcessingItemTest.java:159)
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest.testStartSendingEvents(SimpleEntranceProcessingItemTest.java:155)

Running org.apache.samoa.topology.impl.SimpleTopologyTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.063 sec - in org.apache.samoa.topology.impl.SimpleTopologyTest
Running org.apache.samoa.topology.impl.SimpleProcessingItemTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.066 sec - in org.apache.samoa.topology.impl.SimpleProcessingItemTest
Running org.apache.samoa.AlgosTest
2017-07-05 12:34:12,458 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test3306648859184457487test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test3306648859184457487test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-05 12:34:12,504 [pool-1-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 12:34:13,397 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:13,403 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 65.645
Kappa Statistic (percent) = 27.592
Kappa Temporal Statistic (percent) = 30.173
2017-07-05 12:34:13,888 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:13,888 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 70.69
Kappa Statistic (percent) = 39.422
Kappa Temporal Statistic (percent) = 40.272
2017-07-05 12:34:14,300 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:14,301 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 73.663
Kappa Statistic (percent) = 45.786
Kappa Temporal Statistic (percent) = 46.436
2017-07-05 12:34:14,515 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:14,516 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 75.545
Kappa Statistic (percent) = 49.77
Kappa Temporal Statistic (percent) = 50.387
2017-07-05 12:34:14,857 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:14,858 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 77.083
Kappa Statistic (percent) = 53.033
Kappa Temporal Statistic (percent) = 53.669
2017-07-05 12:34:15,189 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:15,190 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 78.397
Kappa Statistic (percent) = 55.776
Kappa Temporal Statistic (percent) = 56.372
2017-07-05 12:34:15,395 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:15,396 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 79.531
Kappa Statistic (percent) = 58.117
Kappa Temporal Statistic (percent) = 58.646
2017-07-05 12:34:15,656 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:15,657 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 80.527
Kappa Statistic (percent) = 60.173
Kappa Temporal Statistic (percent) = 60.566
2017-07-05 12:34:16,022 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:16,022 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 81.387
Kappa Statistic (percent) = 61.939
Kappa Temporal Statistic (percent) = 62.332
2017-07-05 12:34:16,271 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:16,272 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 82.144
Kappa Statistic (percent) = 63.505
Kappa Temporal Statistic (percent) = 63.842
2017-07-05 12:34:16,272 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 12:34:16,273 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 12:34:16,274 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,65.645,27.592487187843105,30.172764227642272
40000.0,40000.0,70.69,39.42219140754782,40.27204646186763
60000.0,60000.0,73.66333333333334,45.78556998992169,46.43571404359175
80000.0,80000.0,75.545,49.770097068022935,50.386731925037395
100000.0,100000.0,77.083,53.033355520313066,53.66933527413876
120000.0,120000.0,78.3975,55.77588004396512,56.37180652327577
140000.0,140000.0,79.53071428571428,58.11662806596387,58.64611743654127
160000.0,160000.0,80.526875,60.17259494934151,60.56625026894989
180000.0,180000.0,81.38666666666666,61.93941888230634,62.3317780650964
200000.0,200000.0,82.1435,63.50464751569946,63.842259795484466

2017-07-05 12:34:16,274 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 3 seconds for 200000 instances
2017-07-05 12:34:22,481 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test3306648859184457487test
2017-07-05 12:34:22,495 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test4473235619218526859test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=60.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test4473235619218526859test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)
2017-07-05 12:34:22,502 [pool-4-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 12:34:23,726 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-05 12:34:23,726 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 85.615
Kappa Statistic (percent) = 70.512
Kappa Temporal Statistic (percent) = 70.374
2017-07-05 12:34:24,529 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:24,530 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 89.093
Kappa Statistic (percent) = 77.666
Kappa Temporal Statistic (percent) = 77.603
2017-07-05 12:34:25,440 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:25,441 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 90.785
Kappa Statistic (percent) = 81.183
Kappa Temporal Statistic (percent) = 81.159
2017-07-05 12:34:26,270 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:26,271 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 92.045
Kappa Statistic (percent) = 83.764
Kappa Temporal Statistic (percent) = 83.845
2017-07-05 12:34:27,102 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:27,102 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 92.975
Kappa Statistic (percent) = 85.664
Kappa Temporal Statistic (percent) = 85.706
2017-07-05 12:34:27,935 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:27,936 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 93.617
Kappa Statistic (percent) = 86.967
Kappa Temporal Statistic (percent) = 86.992
2017-07-05 12:34:28,721 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:28,722 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 94.136
Kappa Statistic (percent) = 88.031
Kappa Temporal Statistic (percent) = 88.044
2017-07-05 12:34:29,481 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:29,482 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 94.548
Kappa Statistic (percent) = 88.87
Kappa Temporal Statistic (percent) = 88.863
2017-07-05 12:34:30,183 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:30,184 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 94.885
Kappa Statistic (percent) = 89.557
Kappa Temporal Statistic (percent) = 89.548
2017-07-05 12:34:30,844 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:30,845 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 95.154
Kappa Statistic (percent) = 90.107
Kappa Temporal Statistic (percent) = 90.097
2017-07-05 12:34:30,845 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 12:34:30,846 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 12:34:30,846 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,85.615,70.51223537242369,70.37380290392338
40000.0,40000.0,89.0925,77.6664370007197,77.60266940451744
60000.0,60000.0,90.78500000000001,81.18343000253999,81.15927213248824
80000.0,80000.0,92.045,83.76400898273806,83.84525562268365
100000.0,100000.0,92.975,85.66439213171104,85.70556516430969
120000.0,120000.0,93.61666666666667,86.96695350258192,86.99225648689037
140000.0,140000.0,94.13642857142858,88.03103398939442,88.04381071673052
160000.0,160000.0,94.548125,88.86978609412824,88.8630705394191
180000.0,180000.0,94.88499999999999,89.55689423454625,89.54830799968214
200000.0,200000.0,95.15350000000001,90.10696307836399,90.09655172413792

2017-07-05 12:34:30,846 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 8 seconds for 200000 instances
2017-07-05 12:34:32,498 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test4473235619218526859test
2017-07-05 12:34:32,507 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test1298323941430260550test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=65.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test1298323941430260550test -i 200000 -f 20000 -w 0 -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)
2017-07-05 12:34:32,521 [pool-16-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 12:34:32,754 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:32,754 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 84.275
Kappa Statistic (percent) = 68.502
Kappa Temporal Statistic (percent) = 68.778
2017-07-05 12:34:32,917 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:32,918 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 83.803
Kappa Statistic (percent) = 67.594
Kappa Temporal Statistic (percent) = 67.819
2017-07-05 12:34:33,075 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,075 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 83.59
Kappa Statistic (percent) = 67.214
Kappa Temporal Statistic (percent) = 67.392
2017-07-05 12:34:33,227 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,227 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 83.546
Kappa Statistic (percent) = 67.127
Kappa Temporal Statistic (percent) = 67.269
2017-07-05 12:34:33,377 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,378 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 83.559
Kappa Statistic (percent) = 67.135
Kappa Temporal Statistic (percent) = 67.209
2017-07-05 12:34:33,537 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,537 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 83.531
Kappa Statistic (percent) = 67.072
Kappa Temporal Statistic (percent) = 67.151
2017-07-05 12:34:33,691 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,691 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 83.504
Kappa Statistic (percent) = 67.03
Kappa Temporal Statistic (percent) = 67.054
2017-07-05 12:34:33,840 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,841 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 83.518
Kappa Statistic (percent) = 67.05
Kappa Temporal Statistic (percent) = 67.065
2017-07-05 12:34:33,991 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:33,991 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 83.54
Kappa Statistic (percent) = 67.089
Kappa Temporal Statistic (percent) = 67.126
2017-07-05 12:34:34,140 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 12:34:34,141 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 83.523
Kappa Statistic (percent) = 67.057
Kappa Temporal Statistic (percent) = 67.143
2017-07-05 12:34:34,141 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 12:34:34,141 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 12:34:34,142 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,84.275,68.50233079747552,68.77792117541944
40000.0,40000.0,83.80250000000001,67.59439866955276,67.81900362588786
60000.0,60000.0,83.59,67.21382484118784,67.39195230998509
80000.0,80000.0,83.54625,67.12663795918682,67.26924607121543
100000.0,100000.0,83.55900000000001,67.13483590334214,67.20915853926087
120000.0,120000.0,83.53083333333333,67.07192304919516,67.15145269596435
140000.0,140000.0,83.50357142857143,67.02990305706534,67.05373828442632
160000.0,160000.0,83.518125,67.04964827201962,67.06465673356729
180000.0,180000.0,83.54,67.08882037897833,67.1260235670062
200000.0,200000.0,83.5235,67.05662351796806,67.142614990378

2017-07-05 12:34:34,142 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 1 seconds for 200000 instances
2017-07-05 12:34:42,509 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test1298323941430260550test
2017-07-05 12:34:42,512 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test5574447559376874905test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialCVEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialCVEvaluation -d /tmp/test5574447559376874905test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-05 12:34:42,524 [pool-18-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialCVEvaluation
2017-07-05 12:34:44,755 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:34:44,756 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 20,000
[avg] classified instances = 20,000
[err] classified instances = 0
[avg] classifications correct (percent) = 64.627
[err] classifications correct (percent) = 0.189
[avg] Kappa Statistic (percent) = 25.294
[err] Kappa Statistic (percent) = 0.352
[avg] Kappa Temporal Statistic (percent) = 28.104
[err] Kappa Temporal Statistic (percent) = 0.384
2017-07-05 12:34:47,411 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:34:47,412 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 40,000
[avg] classified instances = 40,000
[err] classified instances = 0
[avg] classifications correct (percent) = 70.032
[err] classifications correct (percent) = 0.203
[avg] Kappa Statistic (percent) = 38.074
[err] Kappa Statistic (percent) = 0.445
[avg] Kappa Temporal Statistic (percent) = 38.931
[err] Kappa Temporal Statistic (percent) = 0.414
2017-07-05 12:34:50,235 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:34:50,236 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 60,000
[avg] classified instances = 60,000
[err] classified instances = 0
[avg] classifications correct (percent) = 73.498
[err] classifications correct (percent) = 0.308
[avg] Kappa Statistic (percent) = 45.523
[err] Kappa Statistic (percent) = 0.671
[avg] Kappa Temporal Statistic (percent) = 46.1
[err] Kappa Temporal Statistic (percent) = 0.627
2017-07-05 12:34:52,743 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:34:52,744 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 80,000
[avg] classified instances = 80,000
[err] classified instances = 0
[avg] classifications correct (percent) = 75.64
[err] classifications correct (percent) = 0.397
[avg] Kappa Statistic (percent) = 50.074
[err] Kappa Statistic (percent) = 0.86
[avg] Kappa Temporal Statistic (percent) = 50.579
[err] Kappa Temporal Statistic (percent) = 0.806
2017-07-05 12:34:55,371 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:34:55,372 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 100,000
[avg] classified instances = 100,000
[err] classified instances = 0
[avg] classifications correct (percent) = 77.273
[err] classifications correct (percent) = 0.442
[avg] Kappa Statistic (percent) = 53.513
[err] Kappa Statistic (percent) = 0.947
[avg] Kappa Temporal Statistic (percent) = 54.053
[err] Kappa Temporal Statistic (percent) = 0.894
2017-07-05 12:34:57,973 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:34:57,974 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 120,000
[avg] classified instances = 120,000
[err] classified instances = 0
[avg] classifications correct (percent) = 78.553
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 56.166
[err] Kappa Statistic (percent) = 0.983
[avg] Kappa Temporal Statistic (percent) = 56.687
[err] Kappa Temporal Statistic (percent) = 0.938
2017-07-05 12:35:01,079 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 12:35:01,080 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 140,000
[avg] classified instances = 140,000
[err] classified instances = 0
[avg] classifications correct (percent) = 79.767
[err] classifications correct (percent) = 0.471
[avg] Kappa Statistic (percent) = 58.667
[err] Kappa Statistic (percent) = 0.991
[avg] Kappa Temporal Statistic (percent) = 59.123
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-05 12:35:03,346 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:35:03,346 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 160,000
[avg] classified instances = 160,000
[err] classified instances = 0
[avg] classifications correct (percent) = 80.8
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 60.79
[err] Kappa Statistic (percent) = 0.973
[avg] Kappa Temporal Statistic (percent) = 61.12
[err] Kappa Temporal Statistic (percent) = 0.939
2017-07-05 12:35:06,046 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 12:35:06,047 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 180,000
[avg] classified instances = 180,000
[err] classified instances = 0
[avg] classifications correct (percent) = 81.614
[err] classifications correct (percent) = 0.462
[avg] Kappa Statistic (percent) = 62.451
[err] Kappa Statistic (percent) = 0.969
[avg] Kappa Temporal Statistic (percent) = 62.792
[err] Kappa Temporal Statistic (percent) = 0.936
2017-07-05 12:35:09,776 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 12:35:09,777 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 200,000
[avg] classified instances = 200,000.9
[err] classified instances = 0.9
[avg] classifications correct (percent) = 82.333
[err] classifications correct (percent) = 0.47
[avg] Kappa Statistic (percent) = 63.93
[err] Kappa Statistic (percent) = 0.984
[avg] Kappa Temporal Statistic (percent) = 64.226
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-05 12:35:09,778 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:215) - last event is received!
2017-07-05 12:35:09,778 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:216) - total count: 200001
2017-07-05 12:35:09,778 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:219) - org.apache.samoa.evaluation.EvaluatorCVProcessorid = 0
evaluation instances,[avg] classified instances,[err] classified instances,[avg] classifications correct (percent),[err] classifications correct (percent),[avg] Kappa Statistic (percent),[err] Kappa Statistic (percent),[avg] Kappa Temporal Statistic (percent),[err] Kappa Temporal Statistic (percent)
20000.0,20000.0,0.0,64.62700000000001,0.18916071943661472,25.293690418855896,0.3519627618187893,28.103658536585368,0.38447300698498993
40000.0,40000.0,0.0,70.032,0.20324410610560478,38.07419026287571,0.44506467062749777,38.931173264048084,0.4141710858537992
60000.0,60000.0,0.0,73.49833333333333,0.3080789741624503,45.52336429068201,0.6710378342453998,46.10013219890851,0.6265800633791055
80000.0,80000.0,0.0,75.63975,0.3973700346584335,50.07382624902415,0.8597101949765279,50.57895671138387,0.8061674935377634
100000.0,100000.0,0.0,77.27289999999999,0.4422633566854323,53.51347314843381,0.9469128869060368,54.05325084910237,0.8941115896115002
120000.0,120000.0,0.0,78.5535,0.4643362247656641,56.1661763049406,0.9830956362259596,56.68686256689892,0.9377688069588297
140000.0,140000.0,0.0,79.76700000000001,0.47139196346560175,58.66745091659613,0.9910982373644812,59.12348297906114,0.9523482240960528
160000.0,160000.0,0.0,80.80025000000002,0.4637947455202826,60.789541927185965,0.9729687801934683,61.11984407234435,0.9392003554346215
180000.0,180000.0,0.0,81.61388888888888,0.46241870972111077,62.451145738939225,0.9693840059052787,62.791612794423514,0.9358071589161842
200000.0,200000.9,0.8999999999996271,82.33328358598864,0.4701473780403362,63.930181311539584,0.9842393397954078,64.2263845297155,0.9520407427484003

2017-07-05 12:35:09,779 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:223) - total evaluation time: 27 seconds for 200001 instances
2017-07-05 12:35:10,520 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test5574447559376874905test
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 58.425 sec - in org.apache.samoa.AlgosTest

Results :


Tests in error: 
  Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope


Tests run: 28, Failures: 0, Errors: 1, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache SAMOA ....................................... SUCCESS [  3.390 s]
[INFO] samoa-instances .................................... SUCCESS [  3.736 s]
[INFO] samoa-api .......................................... SUCCESS [ 52.421 s]
[INFO] samoa-test ......................................... SUCCESS [  1.699 s]
[INFO] samoa-local ........................................ FAILURE [01:01 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:03 min
[INFO] Finished at: 2017-07-05T12:35:10+02:00
[INFO] Final Memory: 40M/1064M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18:test (default-test) on project samoa-local: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/apache/incubator-samoa/250252707/samoa-local/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :samoa-local
