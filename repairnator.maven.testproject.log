[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Apache SAMOA
[INFO] samoa-instances
[INFO] samoa-api
[INFO] samoa-test
[INFO] samoa-local
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache SAMOA 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa ---
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-api
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-instances
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-local
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-storm
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-flink
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-apex
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-samza
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-test
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/samoa-threads
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250359024/bin
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[INFO] Scan 383 files header done in 1.136s.
[INFO] 
 * uptodate header on 373 files.
 * fail header on 10 files.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa ---
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-instances 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-instances ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-instances ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250359024/samoa-instances/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250359024/samoa-instances/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-instances ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250359024/samoa-instances/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom (3 KB at 5.7 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom (3 KB at 82.8 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar (67 KB at 1053.9 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.instances.ArffLoaderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.026 sec - in org.apache.samoa.instances.ArffLoaderTest

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-api 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-api ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250359024/samoa-api/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-api ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
2017-07-05 21:25:15,693 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
2017-07-05 21:25:16,471 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
2017-07-05 21:25:16,666 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:16,703 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:16,765 INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1049)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2017-07-05 21:25:16,766 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:16,766 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:16,769 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:16,772 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:16
2017-07-05 21:25:16,776 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:16,776 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:16,780 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:16,781 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:16,832 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:16,833 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:16,833 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:16,836 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:16,837 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:16,837 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:16,837 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:16,838 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:16,838 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:16,913 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:16,914 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:16,914 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:16,915 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:16,918 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:17,062 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:17,062 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:17,064 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:17,064 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:17,071 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:17,084 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:17,084 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:17,085 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:17,085 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:17,089 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:17,090 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:17,090 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:17,092 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:17,092 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:17,096 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:17,096 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:17,097 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:17,097 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:17,104 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:17,105 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:17,106 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:17,193 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:17,254 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 21:25:17,286 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 21:25:17,475 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 21:25:17,513 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 21:25:17,546 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 21:25:17,647 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 21:25:17,647 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 21:25:17,650 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 21:25:17,699 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 21:25:17,759 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-07-05 21:25:17,767 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 21:25:17,782 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:17,785 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 21:25:17,785 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:17,813 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 21:25:17,816 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:17,875 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 43781
2017-07-05 21:25:17,875 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:17,935 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_43781_hdfs____4qlh7/webapp
2017-07-05 21:25:18,437 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43781
2017-07-05 21:25:18,444 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:18,445 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:18,446 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:18,446 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:18,447 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:18,447 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:18
2017-07-05 21:25:18,448 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:18,448 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:18,448 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:18,448 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:18,459 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:18,460 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:18,460 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:18,460 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:18,460 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:18,461 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:18,461 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:18,461 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:18,461 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:18,462 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:18,462 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:18,462 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:18,462 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:18,463 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:18,463 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:18,463 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:18,464 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:18,464 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:18,470 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:18,470 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:18,470 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:18,471 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:18,471 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:18,473 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:18,473 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:18,473 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:18,473 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:18,474 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:18,474 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:18,474 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:18,474 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:18,475 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:18,477 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:18,477 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:18,478 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:18,491 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:18,506 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:18,510 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 21:25:18,510 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 21:25:18,511 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 21:25:18,536 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 21:25:18,546 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 21:25:18,546 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 21:25:18,557 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 21:25:18,558 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 21:25:18,614 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 21:25:18,614 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 136 msecs
2017-07-05 21:25:19,192 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 21:25:19,201 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:19,224 INFO  [Socket Reader #1 for port 43021] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 43021
2017-07-05 21:25:19,308 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:43021 to access this namenode/service.
2017-07-05 21:25:19,316 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 21:25:19,376 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:19,376 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:19,377 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 21:25:19,378 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 21:25:19,378 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 21:25:19,378 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 21:25:19,416 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 21:25:19,417 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 21:25:19,417 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 21:25:19,417 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 21:25:19,417 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 21:25:19,417 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 39 msec
2017-07-05 21:25:19,445 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:19,449 INFO  [IPC Server listener on 43021] ipc.Server (Server.java:run(674)) - IPC Server listener on 43021: starting
2017-07-05 21:25:19,468 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:43021
2017-07-05 21:25:19,468 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 21:25:19,479 INFO  [CacheReplicationMonitor(1488553735)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 21:25:19,481 INFO  [CacheReplicationMonitor(1488553735)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2444007109 milliseconds
2017-07-05 21:25:19,484 INFO  [CacheReplicationMonitor(1488553735)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2017-07-05 21:25:19,488 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 21:25:19,488 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 21:25:19,628 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 21:25:19,630 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 21:25:19,636 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 21:25:19,643 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:56250
2017-07-05 21:25:19,646 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 21:25:19,646 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 21:25:19,650 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 21:25:19,651 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:19,651 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 21:25:19,651 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:19,656 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:19,656 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 56809
2017-07-05 21:25:19,656 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:19,661 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_56809_datanode____.1exes9/webapp
2017-07-05 21:25:19,778 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56809
2017-07-05 21:25:19,780 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 21:25:19,781 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 21:25:19,804 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:19,808 INFO  [Socket Reader #1 for port 47718] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 47718
2017-07-05 21:25:19,818 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:47718
2017-07-05 21:25:19,829 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 21:25:19,832 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 21:25:19,848 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:43021 starting to offer service
2017-07-05 21:25:19,854 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:19,857 INFO  [IPC Server listener on 47718] ipc.Server (Server.java:run(674)) - IPC Server listener on 47718: starting
2017-07-05 21:25:20,357 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 21:25:20,371 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:20,373 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:20,373 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:20,469 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:20,470 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:20,470 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:20,518 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:20,522 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:20,605 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:20,605 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:20,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1558233062-172.17.0.12-1499282717122 is not formatted.
2017-07-05 21:25:20,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:20,607 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1558233062-172.17.0.12-1499282717122 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1558233062-172.17.0.12-1499282717122/current
2017-07-05 21:25:20,619 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:20,619 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1558233062-172.17.0.12-1499282717122 is not formatted.
2017-07-05 21:25:20,620 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:20,620 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1558233062-172.17.0.12-1499282717122 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1558233062-172.17.0.12-1499282717122/current
2017-07-05 21:25:20,626 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:20,627 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:20,635 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:20,636 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:20,666 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1098153777;bpid=BP-1558233062-172.17.0.12-1499282717122;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1098153777;c=0;bpid=BP-1558233062-172.17.0.12-1499282717122;dnuuid=null
2017-07-05 21:25:20,702 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 237a0cf3-932b-4bbf-9580-1a584f3c17d7
2017-07-05 21:25:20,729 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:20,730 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:20,753 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 21:25:20,753 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 21:25:20,754 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 21:25:20,754 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 21:25:20,780 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 21:25:20,793 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499299924793 with interval 21600000
2017-07-05 21:25:20,796 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:20,797 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1558233062-172.17.0.12-1499282717122 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:20,799 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1558233062-172.17.0.12-1499282717122 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:20,837 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:20,837 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:20,853 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1558233062-172.17.0.12-1499282717122 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 46ms
2017-07-05 21:25:20,854 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1558233062-172.17.0.12-1499282717122 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 54ms
2017-07-05 21:25:20,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1558233062-172.17.0.12-1499282717122: 58ms
2017-07-05 21:25:20,855 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1558233062-172.17.0.12-1499282717122 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:20,855 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1558233062-172.17.0.12-1499282717122 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 21:25:20,869 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1558233062-172.17.0.12-1499282717122 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:20,870 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1558233062-172.17.0.12-1499282717122 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 21:25:20,876 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 22ms
2017-07-05 21:25:20,879 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid null) service to localhost/127.0.0.1:43021 beginning handshake with NN
2017-07-05 21:25:20,891 INFO  [IPC Server handler 3 on 43021] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=237a0cf3-932b-4bbf-9580-1a584f3c17d7, infoPort=56809, ipcPort=47718, storageInfo=lv=-56;cid=testClusterID;nsid=1098153777;c=0) storage 237a0cf3-932b-4bbf-9580-1a584f3c17d7
2017-07-05 21:25:20,896 INFO  [IPC Server handler 3 on 43021] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:20,897 INFO  [IPC Server handler 3 on 43021] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:56250
2017-07-05 21:25:20,916 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid null) service to localhost/127.0.0.1:43021 successfully registered with NN
2017-07-05 21:25:20,916 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:43021 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 21:25:20,935 INFO  [IPC Server handler 5 on 43021] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:20,936 INFO  [IPC Server handler 5 on 43021] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-afc248f6-462e-4daf-9267-c0d8e06ae91c for DN 127.0.0.1:56250
2017-07-05 21:25:20,936 INFO  [IPC Server handler 5 on 43021] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-11b1d55d-2dc0-486e-bdd6-592e4cf33995 for DN 127.0.0.1:56250
2017-07-05 21:25:20,953 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:20,961 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid 237a0cf3-932b-4bbf-9580-1a584f3c17d7) service to localhost/127.0.0.1:43021 trying to claim ACTIVE state with txid=1
2017-07-05 21:25:20,962 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid 237a0cf3-932b-4bbf-9580-1a584f3c17d7) service to localhost/127.0.0.1:43021
2017-07-05 21:25:20,966 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:20,996 INFO  [IPC Server handler 8 on 43021] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-afc248f6-462e-4daf-9267-c0d8e06ae91c,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:20,998 INFO  [IPC Server handler 8 on 43021] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-afc248f6-462e-4daf-9267-c0d8e06ae91c node DatanodeRegistration(127.0.0.1, datanodeUuid=237a0cf3-932b-4bbf-9580-1a584f3c17d7, infoPort=56809, ipcPort=47718, storageInfo=lv=-56;cid=testClusterID;nsid=1098153777;c=0), blocks: 0, hasStaleStorages: true, processing time: 3 msecs
2017-07-05 21:25:21,020 INFO  [IPC Server handler 8 on 43021] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-11b1d55d-2dc0-486e-bdd6-592e4cf33995,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:21,021 INFO  [IPC Server handler 8 on 43021] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-11b1d55d-2dc0-486e-bdd6-592e4cf33995 node DatanodeRegistration(127.0.0.1, datanodeUuid=237a0cf3-932b-4bbf-9580-1a584f3c17d7, infoPort=56809, ipcPort=47718, storageInfo=lv=-56;cid=testClusterID;nsid=1098153777;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2017-07-05 21:25:21,040 INFO  [IPC Server handler 9 on 43021] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 21:25:21,065 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 101 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@77ee9900
2017-07-05 21:25:21,065 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:21,073 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 21:25:21,073 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:21,073 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 21:25:21,073 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 21:25:21,076 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:21,084 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1558233062-172.17.0.12-1499282717122 to blockPoolScannerMap, new size=1
2017-07-05 21:25:21,089 INFO  [IPC Server handler 1 on 43021] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:21,150 INFO  [IPC Server handler 0 on 43021] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1558233062-172.17.0.12-1499282717122 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-afc248f6-462e-4daf-9267-c0d8e06ae91c:NORMAL:127.0.0.1:56250|RBW]]}
2017-07-05 21:25:21,299 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1183817430_1 at /127.0.0.1:34491 [Receiving block BP-1558233062-172.17.0.12-1499282717122:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1558233062-172.17.0.12-1499282717122:blk_1073741825_1001 src: /127.0.0.1:34491 dest: /127.0.0.1:56250
2017-07-05 21:25:21,433 INFO  [PacketResponder: BP-1558233062-172.17.0.12-1499282717122:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:34491, dest: /127.0.0.1:56250, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1183817430_1, offset: 0, srvID: 237a0cf3-932b-4bbf-9580-1a584f3c17d7, blockid: BP-1558233062-172.17.0.12-1499282717122:blk_1073741825_1001, duration: 36947272
2017-07-05 21:25:21,433 INFO  [PacketResponder: BP-1558233062-172.17.0.12-1499282717122:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1558233062-172.17.0.12-1499282717122:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:21,457 INFO  [IPC Server handler 4 on 43021] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-afc248f6-462e-4daf-9267-c0d8e06ae91c:NORMAL:127.0.0.1:56250|RBW]]} has not reached minimal replication 1
2017-07-05 21:25:21,458 INFO  [IPC Server handler 4 on 43021] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:21,458 INFO  [IPC Server handler 4 on 43021] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:21,470 INFO  [IPC Server handler 3 on 43021] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:56250 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-afc248f6-462e-4daf-9267-c0d8e06ae91c:NORMAL:127.0.0.1:56250|RBW]]} size 1
2017-07-05 21:25:21,864 INFO  [IPC Server handler 7 on 43021] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_1183817430_1
2017-07-05 21:25:21,871 INFO  [IPC Server handler 5 on 43021] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 21:25:21,875 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 21:25:21,875 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 21:25:21,876 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37eeec90] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 21:25:21,876 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 21:25:21,892 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:21,994 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 21:25:21,994 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 47718
2017-07-05 21:25:21,998 INFO  [IPC Server listener on 47718] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 47718
2017-07-05 21:25:21,999 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:22,001 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid 237a0cf3-932b-4bbf-9580-1a584f3c17d7) service to localhost/127.0.0.1:43021 interrupted
2017-07-05 21:25:22,002 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid 237a0cf3-932b-4bbf-9580-1a584f3c17d7) service to localhost/127.0.0.1:43021
2017-07-05 21:25:22,003 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1558233062-172.17.0.12-1499282717122 (Datanode Uuid 237a0cf3-932b-4bbf-9580-1a584f3c17d7)
2017-07-05 21:25:22,004 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1558233062-172.17.0.12-1499282717122 from blockPoolScannerMap
2017-07-05 21:25:22,005 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43021] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1558233062-172.17.0.12-1499282717122
2017-07-05 21:25:22,009 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@312a7030] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 21:25:22,012 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 21:25:22,013 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 21:25:22,013 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 21:25:22,013 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 21:25:22,014 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 21:25:22,014 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:22,015 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 21:25:22,015 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 14 2 
2017-07-05 21:25:22,016 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@37f21974] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 21:25:22,019 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@6f80fafe] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 21:25:22,019 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-05 21:25:22,022 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-05 21:25:22,028 INFO  [CacheReplicationMonitor(1488553735)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 21:25:22,032 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 43021
2017-07-05 21:25:22,040 INFO  [IPC Server listener on 43021] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 43021
2017-07-05 21:25:22,045 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:22,045 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@13518f37] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 21:25:22,048 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@165b8a71] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 21:25:22,070 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:22,071 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 21:25:22,090 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:22,191 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 21:25:22,192 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 21:25:22,193 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 21:25:22,232 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 21:25:22,237 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:22,238 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:22,239 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:22,240 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:22,241 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:22,241 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:22
2017-07-05 21:25:22,242 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:22,242 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,243 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:22,243 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:22,315 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:22,316 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:22,316 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:22,316 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:22,316 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:22,316 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:22,317 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:22,317 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:22,317 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:22,317 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:22,317 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:22,317 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:22,318 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:22,318 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:22,319 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:22,319 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,319 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:22,319 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:22,325 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:22,325 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:22,325 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,326 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:22,326 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:22,327 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:22,328 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:22,328 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:22,328 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:22,328 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:22,328 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:22,329 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,329 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:22,329 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:22,330 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:22,330 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:22,330 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:22,332 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:22,361 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 21:25:22,394 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 21:25:22,491 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 21:25:22,492 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 21:25:22,494 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 21:25:22,495 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 21:25:22,495 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 21:25:22,495 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 21:25:22,500 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 21:25:22,502 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 21:25:22,503 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:22,504 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 21:25:22,504 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:22,506 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 21:25:22,506 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:22,507 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 42766
2017-07-05 21:25:22,508 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:22,514 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_42766_hdfs____.2wg5st/webapp
2017-07-05 21:25:22,646 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42766
2017-07-05 21:25:22,653 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:22,654 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:22,655 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:22,655 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:22,656 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:22,656 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:22
2017-07-05 21:25:22,657 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:22,657 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,657 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:22,657 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:22,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:22,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:22,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:22,679 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:22,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:22,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:22,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:22,680 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:22,681 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:22,681 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:22,681 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:22,682 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:22,682 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:22,682 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:22,683 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:22,683 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,684 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:22,684 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:22,700 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:22,701 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:22,701 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,701 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:22,701 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:22,935 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:22,935 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:22,935 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:22,947 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:22,948 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:22,948 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:22,948 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:22,948 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:22,948 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:22,950 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:22,950 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:22,950 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:22,960 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:22,973 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:22,976 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 21:25:22,976 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 21:25:22,977 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 21:25:22,978 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 21:25:22,978 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 21:25:22,979 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 21:25:22,979 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 21:25:22,979 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 21:25:23,014 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 21:25:23,014 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 64 msecs
2017-07-05 21:25:23,014 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 21:25:23,015 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:23,020 INFO  [Socket Reader #1 for port 57323] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 57323
2017-07-05 21:25:23,031 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:57323 to access this namenode/service.
2017-07-05 21:25:23,032 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 21:25:23,095 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:23,095 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:23,095 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 21:25:23,096 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 21:25:23,096 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 21:25:23,096 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 21:25:23,122 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 21:25:23,123 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 21:25:23,123 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 21:25:23,123 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 21:25:23,123 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 21:25:23,123 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 27 msec
2017-07-05 21:25:23,135 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:23,140 INFO  [IPC Server listener on 57323] ipc.Server (Server.java:run(674)) - IPC Server listener on 57323: starting
2017-07-05 21:25:23,153 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:57323
2017-07-05 21:25:23,157 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 21:25:23,176 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 21:25:23,177 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 21:25:23,180 INFO  [CacheReplicationMonitor(572675092)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 21:25:23,180 INFO  [CacheReplicationMonitor(572675092)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2444010808 milliseconds
2017-07-05 21:25:23,182 INFO  [CacheReplicationMonitor(572675092)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 21:25:23,250 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 21:25:23,250 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 21:25:23,250 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 21:25:23,251 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:44315
2017-07-05 21:25:23,252 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 21:25:23,252 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 21:25:23,253 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 21:25:23,254 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:23,254 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 21:25:23,254 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:23,255 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:23,255 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 57827
2017-07-05 21:25:23,256 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:23,260 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_57827_datanode____.gvie4e/webapp
2017-07-05 21:25:23,381 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57827
2017-07-05 21:25:23,392 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 21:25:23,393 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 21:25:23,398 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:23,406 INFO  [Socket Reader #1 for port 33058] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 33058
2017-07-05 21:25:23,426 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:33058
2017-07-05 21:25:23,432 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 21:25:23,433 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 21:25:23,435 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:57323 starting to offer service
2017-07-05 21:25:23,440 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:23,440 INFO  [IPC Server listener on 33058] ipc.Server (Server.java:run(674)) - IPC Server listener on 33058: starting
2017-07-05 21:25:23,476 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 21:25:23,487 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:23,487 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:23,487 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:23,491 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:23,491 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:23,520 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:23,520 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:23,520 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:23,599 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:23,599 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:23,633 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:23,634 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:23,634 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-322829584-172.17.0.12-1499282722332 is not formatted.
2017-07-05 21:25:23,634 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:23,634 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-322829584-172.17.0.12-1499282722332 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-322829584-172.17.0.12-1499282722332/current
2017-07-05 21:25:23,646 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:23,646 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-322829584-172.17.0.12-1499282722332 is not formatted.
2017-07-05 21:25:23,646 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:23,646 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-322829584-172.17.0.12-1499282722332 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-322829584-172.17.0.12-1499282722332/current
2017-07-05 21:25:23,662 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:23,662 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:23,697 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=442742637;bpid=BP-322829584-172.17.0.12-1499282722332;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=442742637;c=0;bpid=BP-322829584-172.17.0.12-1499282722332;dnuuid=null
2017-07-05 21:25:23,708 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:23,712 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:23,729 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID e9aee700-561a-4c08-a64a-226e444fb40d
2017-07-05 21:25:23,731 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 21:25:23,731 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 21:25:23,731 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 21:25:23,731 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 21:25:23,735 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 21:25:23,736 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499283464736 with interval 21600000
2017-07-05 21:25:23,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:23,741 INFO  [Thread-142] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-322829584-172.17.0.12-1499282722332 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:23,741 INFO  [Thread-143] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-322829584-172.17.0.12-1499282722332 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:23,769 INFO  [Thread-143] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-322829584-172.17.0.12-1499282722332 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 28ms
2017-07-05 21:25:23,770 INFO  [Thread-142] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-322829584-172.17.0.12-1499282722332 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 29ms
2017-07-05 21:25:23,770 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-322829584-172.17.0.12-1499282722332: 30ms
2017-07-05 21:25:23,777 INFO  [Thread-146] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-322829584-172.17.0.12-1499282722332 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:23,777 INFO  [Thread-146] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-322829584-172.17.0.12-1499282722332 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 21:25:23,788 INFO  [Thread-147] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-322829584-172.17.0.12-1499282722332 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:23,788 INFO  [Thread-147] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-322829584-172.17.0.12-1499282722332 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 21:25:23,789 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 19ms
2017-07-05 21:25:23,790 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid null) service to localhost/127.0.0.1:57323 beginning handshake with NN
2017-07-05 21:25:23,793 INFO  [IPC Server handler 5 on 57323] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=e9aee700-561a-4c08-a64a-226e444fb40d, infoPort=57827, ipcPort=33058, storageInfo=lv=-56;cid=testClusterID;nsid=442742637;c=0) storage e9aee700-561a-4c08-a64a-226e444fb40d
2017-07-05 21:25:23,793 INFO  [IPC Server handler 5 on 57323] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:23,793 INFO  [IPC Server handler 5 on 57323] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:44315
2017-07-05 21:25:23,800 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid null) service to localhost/127.0.0.1:57323 successfully registered with NN
2017-07-05 21:25:23,801 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:57323 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 21:25:23,809 INFO  [IPC Server handler 6 on 57323] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:23,812 INFO  [IPC Server handler 6 on 57323] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-92688c54-7302-42af-920d-544d3b2fc81a for DN 127.0.0.1:44315
2017-07-05 21:25:23,812 INFO  [IPC Server handler 6 on 57323] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-f6040535-56ab-492a-9fdf-7610aefb96ff for DN 127.0.0.1:44315
2017-07-05 21:25:23,820 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid e9aee700-561a-4c08-a64a-226e444fb40d) service to localhost/127.0.0.1:57323 trying to claim ACTIVE state with txid=1
2017-07-05 21:25:23,832 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid e9aee700-561a-4c08-a64a-226e444fb40d) service to localhost/127.0.0.1:57323
2017-07-05 21:25:23,833 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:23,840 INFO  [IPC Server handler 8 on 57323] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-92688c54-7302-42af-920d-544d3b2fc81a,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:23,841 INFO  [IPC Server handler 8 on 57323] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-92688c54-7302-42af-920d-544d3b2fc81a node DatanodeRegistration(127.0.0.1, datanodeUuid=e9aee700-561a-4c08-a64a-226e444fb40d, infoPort=57827, ipcPort=33058, storageInfo=lv=-56;cid=testClusterID;nsid=442742637;c=0), blocks: 0, hasStaleStorages: true, processing time: 4 msecs
2017-07-05 21:25:23,841 INFO  [IPC Server handler 8 on 57323] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-f6040535-56ab-492a-9fdf-7610aefb96ff,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:23,841 INFO  [IPC Server handler 8 on 57323] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-f6040535-56ab-492a-9fdf-7610aefb96ff node DatanodeRegistration(127.0.0.1, datanodeUuid=e9aee700-561a-4c08-a64a-226e444fb40d, infoPort=57827, ipcPort=33058, storageInfo=lv=-56;cid=testClusterID;nsid=442742637;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 21:25:23,845 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 10 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@364b237e
2017-07-05 21:25:23,845 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:23,848 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 21:25:23,848 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:23,849 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 21:25:23,849 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 21:25:23,852 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:23,854 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:23,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-322829584-172.17.0.12-1499282722332 to blockPoolScannerMap, new size=1
2017-07-05 21:25:23,859 INFO  [IPC Server handler 9 on 57323] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 21:25:23,865 INFO  [IPC Server handler 0 on 57323] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:23,912 INFO  [IPC Server handler 2 on 57323] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-322829584-172.17.0.12-1499282722332 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-92688c54-7302-42af-920d-544d3b2fc81a:NORMAL:127.0.0.1:44315|RBW]]}
2017-07-05 21:25:23,943 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1421205590_1 at /127.0.0.1:57959 [Receiving block BP-322829584-172.17.0.12-1499282722332:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-322829584-172.17.0.12-1499282722332:blk_1073741825_1001 src: /127.0.0.1:57959 dest: /127.0.0.1:44315
2017-07-05 21:25:23,980 INFO  [PacketResponder: BP-322829584-172.17.0.12-1499282722332:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:57959, dest: /127.0.0.1:44315, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1421205590_1, offset: 0, srvID: e9aee700-561a-4c08-a64a-226e444fb40d, blockid: BP-322829584-172.17.0.12-1499282722332:blk_1073741825_1001, duration: 16787450
2017-07-05 21:25:23,994 INFO  [PacketResponder: BP-322829584-172.17.0.12-1499282722332:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-322829584-172.17.0.12-1499282722332:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:24,021 INFO  [IPC Server handler 4 on 57323] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:44315 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-92688c54-7302-42af-920d-544d3b2fc81a:NORMAL:127.0.0.1:44315|RBW]]} size 0
2017-07-05 21:25:24,037 INFO  [IPC Server handler 5 on 57323] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_1421205590_1
2017-07-05 21:25:24,040 INFO  [IPC Server handler 6 on 57323] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 21:25:24,053 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 21:25:24,053 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 21:25:24,053 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 21:25:24,053 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@dffa30b] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 21:25:24,118 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:24,119 WARN  [964981922@qtp-1832657711-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57827] http.HttpServer2 (HttpServer2.java:isRunning(460)) - HttpServer Acceptor: isRunning is false. Rechecking.
2017-07-05 21:25:24,123 WARN  [964981922@qtp-1832657711-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57827] http.HttpServer2 (HttpServer2.java:isRunning(469)) - HttpServer Acceptor: isRunning is false
2017-07-05 21:25:24,124 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 21:25:24,125 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 33058
2017-07-05 21:25:24,126 INFO  [IPC Server listener on 33058] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 33058
2017-07-05 21:25:24,128 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid e9aee700-561a-4c08-a64a-226e444fb40d) service to localhost/127.0.0.1:57323 interrupted
2017-07-05 21:25:24,128 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:24,128 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid e9aee700-561a-4c08-a64a-226e444fb40d) service to localhost/127.0.0.1:57323
2017-07-05 21:25:24,129 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-322829584-172.17.0.12-1499282722332 (Datanode Uuid e9aee700-561a-4c08-a64a-226e444fb40d)
2017-07-05 21:25:24,129 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-322829584-172.17.0.12-1499282722332 from blockPoolScannerMap
2017-07-05 21:25:24,129 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:57323] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-322829584-172.17.0.12-1499282722332
2017-07-05 21:25:24,130 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@117073e1] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 21:25:24,131 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 21:25:24,131 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 21:25:24,132 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 21:25:24,132 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 21:25:24,132 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 21:25:24,133 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:24,133 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@5965844d] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 21:25:24,134 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 21:25:24,134 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@6d4a65c6] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 21:25:24,135 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 2 3 
2017-07-05 21:25:24,136 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-05 21:25:24,138 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-05 21:25:24,139 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 57323
2017-07-05 21:25:24,139 INFO  [CacheReplicationMonitor(572675092)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 21:25:24,148 INFO  [IPC Server listener on 57323] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 57323
2017-07-05 21:25:24,148 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:24,148 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@2787de58] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 21:25:24,149 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@6ceb7b5e] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 21:25:24,162 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:24,162 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 21:25:24,221 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:24,232 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 21:25:24,234 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 21:25:24,235 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 21:25:24,275 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 21:25:24,279 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:24,280 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:24,281 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:24,281 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:24,281 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:24,282 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:24
2017-07-05 21:25:24,282 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:24,282 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,283 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:24,283 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:24,294 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:24,294 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:24,294 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:24,294 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:24,294 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:24,295 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:24,295 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:24,295 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:24,295 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:24,295 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:24,295 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:24,296 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:24,296 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:24,296 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:24,297 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:24,297 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,298 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:24,298 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:24,303 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:24,304 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:24,304 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,304 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:24,304 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:24,306 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:24,306 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:24,306 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:24,306 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:24,307 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:24,307 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:24,307 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,307 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:24,307 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:24,308 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:24,308 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:24,309 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:24,310 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:24,337 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 21:25:24,369 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 21:25:24,443 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 21:25:24,445 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 21:25:24,446 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 21:25:24,452 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 21:25:24,452 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 21:25:24,453 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 21:25:24,457 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 21:25:24,458 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 21:25:24,458 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:24,459 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 21:25:24,459 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:24,460 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 21:25:24,460 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:24,461 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 54518
2017-07-05 21:25:24,462 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:24,476 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_54518_hdfs____cf266f/webapp
2017-07-05 21:25:24,603 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54518
2017-07-05 21:25:24,604 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:24,605 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:24,609 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:24,609 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:24,610 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:24,611 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:24
2017-07-05 21:25:24,611 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:24,611 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,612 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:24,612 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:24,721 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:24,722 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:24,722 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:24,723 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:24,723 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:24,724 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:24,725 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:24,726 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:24,726 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:24,727 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:24,727 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:24,727 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:24,740 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:24,741 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:24,741 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:24,742 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,743 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:24,750 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:24,756 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:24,760 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:24,761 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,762 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:24,762 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:24,764 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:24,772 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:24,772 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:24,773 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:24,774 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:24,774 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:24,774 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:24,775 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:24,775 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:24,777 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:24,778 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:24,778 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:24,791 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:24,806 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:24,809 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 21:25:24,810 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 21:25:24,811 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 21:25:24,817 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 21:25:24,818 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 21:25:24,819 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 21:25:24,819 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 21:25:24,820 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 21:25:24,849 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 21:25:24,850 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 71 msecs
2017-07-05 21:25:24,851 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 21:25:24,852 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:24,855 INFO  [Socket Reader #1 for port 36572] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 36572
2017-07-05 21:25:24,873 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:36572 to access this namenode/service.
2017-07-05 21:25:24,880 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 21:25:24,924 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:24,925 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:24,925 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 21:25:24,928 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 21:25:24,928 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 21:25:24,928 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 21:25:25,012 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 21:25:25,013 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 21:25:25,013 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 21:25:25,013 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 21:25:25,013 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 21:25:25,013 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 84 msec
2017-07-05 21:25:25,023 INFO  [IPC Server listener on 36572] ipc.Server (Server.java:run(674)) - IPC Server listener on 36572: starting
2017-07-05 21:25:25,024 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:25,029 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:36572
2017-07-05 21:25:25,030 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 21:25:25,049 INFO  [CacheReplicationMonitor(1523751019)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 21:25:25,049 INFO  [CacheReplicationMonitor(1523751019)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2444012677 milliseconds
2017-07-05 21:25:25,054 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 21:25:25,054 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 21:25:25,058 INFO  [CacheReplicationMonitor(1523751019)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 9 millisecond(s).
2017-07-05 21:25:25,105 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 21:25:25,105 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 21:25:25,106 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 21:25:25,113 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:43513
2017-07-05 21:25:25,114 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 21:25:25,114 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 21:25:25,115 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 21:25:25,115 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:25,116 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 21:25:25,116 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:25,117 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:25,133 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 37027
2017-07-05 21:25:25,133 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:25,138 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_37027_datanode____n111iw/webapp
2017-07-05 21:25:25,253 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37027
2017-07-05 21:25:25,264 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 21:25:25,264 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 21:25:25,265 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:25,288 INFO  [Socket Reader #1 for port 49061] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 49061
2017-07-05 21:25:25,302 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:49061
2017-07-05 21:25:25,304 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 21:25:25,305 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 21:25:25,308 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36572 starting to offer service
2017-07-05 21:25:25,336 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:25,348 INFO  [IPC Server listener on 49061] ipc.Server (Server.java:run(674)) - IPC Server listener on 49061: starting
2017-07-05 21:25:25,460 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:25,460 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:25,461 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 21:25:25,471 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:25,472 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:25,472 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:25,501 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:25,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:25,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:25,576 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:25,579 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:25,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:25,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:25,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1864658624-172.17.0.12-1499282724310 is not formatted.
2017-07-05 21:25:25,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:25,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1864658624-172.17.0.12-1499282724310 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1864658624-172.17.0.12-1499282724310/current
2017-07-05 21:25:25,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:25,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1864658624-172.17.0.12-1499282724310 is not formatted.
2017-07-05 21:25:25,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:25,606 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1864658624-172.17.0.12-1499282724310 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1864658624-172.17.0.12-1499282724310/current
2017-07-05 21:25:25,626 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:25,626 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:25,657 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1892588353;bpid=BP-1864658624-172.17.0.12-1499282724310;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1892588353;c=0;bpid=BP-1864658624-172.17.0.12-1499282724310;dnuuid=null
2017-07-05 21:25:25,681 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:25,681 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:25,689 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb
2017-07-05 21:25:25,690 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 21:25:25,690 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 21:25:25,691 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 21:25:25,691 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 21:25:25,696 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 21:25:25,698 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499293117698 with interval 21600000
2017-07-05 21:25:25,699 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:25,701 INFO  [Thread-216] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1864658624-172.17.0.12-1499282724310 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:25,701 INFO  [Thread-217] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1864658624-172.17.0.12-1499282724310 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:25,739 INFO  [Thread-217] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1864658624-172.17.0.12-1499282724310 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 38ms
2017-07-05 21:25:25,747 INFO  [Thread-216] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1864658624-172.17.0.12-1499282724310 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 45ms
2017-07-05 21:25:25,747 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1864658624-172.17.0.12-1499282724310: 48ms
2017-07-05 21:25:25,750 INFO  [Thread-220] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1864658624-172.17.0.12-1499282724310 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:25,750 INFO  [Thread-220] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1864658624-172.17.0.12-1499282724310 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 21:25:25,757 INFO  [Thread-221] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1864658624-172.17.0.12-1499282724310 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:25,758 INFO  [Thread-221] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1864658624-172.17.0.12-1499282724310 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 21:25:25,758 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 11ms
2017-07-05 21:25:25,759 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid null) service to localhost/127.0.0.1:36572 beginning handshake with NN
2017-07-05 21:25:25,762 INFO  [IPC Server handler 4 on 36572] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, infoPort=37027, ipcPort=49061, storageInfo=lv=-56;cid=testClusterID;nsid=1892588353;c=0) storage a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb
2017-07-05 21:25:25,762 INFO  [IPC Server handler 4 on 36572] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:25,762 INFO  [IPC Server handler 4 on 36572] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:43513
2017-07-05 21:25:25,766 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid null) service to localhost/127.0.0.1:36572 successfully registered with NN
2017-07-05 21:25:25,766 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:36572 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 21:25:25,769 INFO  [IPC Server handler 5 on 36572] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:25,769 INFO  [IPC Server handler 5 on 36572] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-74dd804b-b37c-4222-8415-6b95e4961840 for DN 127.0.0.1:43513
2017-07-05 21:25:25,769 INFO  [IPC Server handler 5 on 36572] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-f87f44eb-ae39-455f-9041-d3a65261a2cf for DN 127.0.0.1:43513
2017-07-05 21:25:25,770 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb) service to localhost/127.0.0.1:36572 trying to claim ACTIVE state with txid=1
2017-07-05 21:25:25,770 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb) service to localhost/127.0.0.1:36572
2017-07-05 21:25:25,772 INFO  [IPC Server handler 6 on 36572] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-74dd804b-b37c-4222-8415-6b95e4961840,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:25,773 INFO  [IPC Server handler 6 on 36572] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-74dd804b-b37c-4222-8415-6b95e4961840 node DatanodeRegistration(127.0.0.1, datanodeUuid=a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, infoPort=37027, ipcPort=49061, storageInfo=lv=-56;cid=testClusterID;nsid=1892588353;c=0), blocks: 0, hasStaleStorages: true, processing time: 0 msecs
2017-07-05 21:25:25,773 INFO  [IPC Server handler 6 on 36572] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-f87f44eb-ae39-455f-9041-d3a65261a2cf,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:25,773 INFO  [IPC Server handler 6 on 36572] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-f87f44eb-ae39-455f-9041-d3a65261a2cf node DatanodeRegistration(127.0.0.1, datanodeUuid=a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, infoPort=37027, ipcPort=49061, storageInfo=lv=-56;cid=testClusterID;nsid=1892588353;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 21:25:25,776 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@6437de7a
2017-07-05 21:25:25,776 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:25,777 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 21:25:25,777 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:25,777 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 21:25:25,777 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 21:25:25,780 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:25,781 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1864658624-172.17.0.12-1499282724310 to blockPoolScannerMap, new size=1
2017-07-05 21:25:25,784 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:25,788 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:25,793 INFO  [IPC Server handler 9 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 21:25:25,796 INFO  [IPC Server handler 0 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:25,801 INFO  [IPC Server handler 1 on 36572] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1864658624-172.17.0.12-1499282724310 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f87f44eb-ae39-455f-9041-d3a65261a2cf:NORMAL:127.0.0.1:43513|RBW]]}
2017-07-05 21:25:25,808 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-271650168_1 at /127.0.0.1:43462 [Receiving block BP-1864658624-172.17.0.12-1499282724310:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1864658624-172.17.0.12-1499282724310:blk_1073741825_1001 src: /127.0.0.1:43462 dest: /127.0.0.1:43513
2017-07-05 21:25:25,819 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43462, dest: /127.0.0.1:43513, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-271650168_1, offset: 0, srvID: a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, blockid: BP-1864658624-172.17.0.12-1499282724310:blk_1073741825_1001, duration: 5185594
2017-07-05 21:25:25,820 INFO  [IPC Server handler 3 on 36572] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:43513 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74dd804b-b37c-4222-8415-6b95e4961840:NORMAL:127.0.0.1:43513|FINALIZED]]} size 0
2017-07-05 21:25:25,821 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:25,828 INFO  [IPC Server handler 4 on 36572] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-271650168_1
2017-07-05 21:25:25,832 INFO  [IPC Server handler 5 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:25,836 INFO  [IPC Server handler 6 on 36572] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-1864658624-172.17.0.12-1499282724310 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f87f44eb-ae39-455f-9041-d3a65261a2cf:NORMAL:127.0.0.1:43513|RBW]]}
2017-07-05 21:25:25,845 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-271650168_1 at /127.0.0.1:43471 [Receiving block BP-1864658624-172.17.0.12-1499282724310:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1864658624-172.17.0.12-1499282724310:blk_1073741826_1002 src: /127.0.0.1:43471 dest: /127.0.0.1:43513
2017-07-05 21:25:25,856 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43471, dest: /127.0.0.1:43513, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-271650168_1, offset: 0, srvID: a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, blockid: BP-1864658624-172.17.0.12-1499282724310:blk_1073741826_1002, duration: 7439858
2017-07-05 21:25:25,856 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:25,858 INFO  [IPC Server handler 7 on 36572] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:43513 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f87f44eb-ae39-455f-9041-d3a65261a2cf:NORMAL:127.0.0.1:43513|RBW]]} size 0
2017-07-05 21:25:25,860 INFO  [IPC Server handler 8 on 36572] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_-271650168_1
2017-07-05 21:25:25,863 INFO  [IPC Server handler 9 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:25,866 INFO  [IPC Server handler 0 on 36572] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-1864658624-172.17.0.12-1499282724310 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f87f44eb-ae39-455f-9041-d3a65261a2cf:NORMAL:127.0.0.1:43513|RBW]]}
2017-07-05 21:25:25,870 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-271650168_1 at /127.0.0.1:43476 [Receiving block BP-1864658624-172.17.0.12-1499282724310:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1864658624-172.17.0.12-1499282724310:blk_1073741827_1003 src: /127.0.0.1:43476 dest: /127.0.0.1:43513
2017-07-05 21:25:25,879 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43476, dest: /127.0.0.1:43513, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-271650168_1, offset: 0, srvID: a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, blockid: BP-1864658624-172.17.0.12-1499282724310:blk_1073741827_1003, duration: 1602349
2017-07-05 21:25:25,880 INFO  [IPC Server handler 1 on 36572] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:43513 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74dd804b-b37c-4222-8415-6b95e4961840:NORMAL:127.0.0.1:43513|FINALIZED]]} size 0
2017-07-05 21:25:25,880 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:25,882 INFO  [IPC Server handler 2 on 36572] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_-271650168_1
2017-07-05 21:25:25,885 INFO  [IPC Server handler 3 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:25,887 INFO  [IPC Server handler 4 on 36572] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-1864658624-172.17.0.12-1499282724310 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74dd804b-b37c-4222-8415-6b95e4961840:NORMAL:127.0.0.1:43513|RBW]]}
2017-07-05 21:25:25,897 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-271650168_1 at /127.0.0.1:43483 [Receiving block BP-1864658624-172.17.0.12-1499282724310:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1864658624-172.17.0.12-1499282724310:blk_1073741828_1004 src: /127.0.0.1:43483 dest: /127.0.0.1:43513
2017-07-05 21:25:25,901 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43483, dest: /127.0.0.1:43513, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-271650168_1, offset: 0, srvID: a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, blockid: BP-1864658624-172.17.0.12-1499282724310:blk_1073741828_1004, duration: 1444461
2017-07-05 21:25:25,901 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:25,904 INFO  [IPC Server handler 5 on 36572] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:43513 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f87f44eb-ae39-455f-9041-d3a65261a2cf:NORMAL:127.0.0.1:43513|FINALIZED]]} size 0
2017-07-05 21:25:25,906 INFO  [IPC Server handler 6 on 36572] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_-271650168_1
2017-07-05 21:25:25,908 INFO  [IPC Server handler 7 on 36572] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:25,908 INFO  [IPC Server handler 7 on 36572] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:25,908 INFO  [IPC Server handler 7 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 21:25:25,911 INFO  [IPC Server handler 8 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:25,915 INFO  [IPC Server handler 9 on 36572] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-1864658624-172.17.0.12-1499282724310 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74dd804b-b37c-4222-8415-6b95e4961840:NORMAL:127.0.0.1:43513|RBW]]}
2017-07-05 21:25:25,923 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-271650168_1 at /127.0.0.1:43488 [Receiving block BP-1864658624-172.17.0.12-1499282724310:blk_1073741829_1005]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1864658624-172.17.0.12-1499282724310:blk_1073741829_1005 src: /127.0.0.1:43488 dest: /127.0.0.1:43513
2017-07-05 21:25:25,939 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43488, dest: /127.0.0.1:43513, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-271650168_1, offset: 0, srvID: a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, blockid: BP-1864658624-172.17.0.12-1499282724310:blk_1073741829_1005, duration: 5371127
2017-07-05 21:25:25,940 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:25,941 INFO  [IPC Server handler 0 on 36572] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:43513 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74dd804b-b37c-4222-8415-6b95e4961840:NORMAL:127.0.0.1:43513|RBW]]} size 0
2017-07-05 21:25:25,942 INFO  [IPC Server handler 1 on 36572] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_-271650168_1
2017-07-05 21:25:25,945 INFO  [IPC Server handler 2 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:25,948 INFO  [IPC Server handler 3 on 36572] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-1864658624-172.17.0.12-1499282724310 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74dd804b-b37c-4222-8415-6b95e4961840:NORMAL:127.0.0.1:43513|RBW]]}
2017-07-05 21:25:25,950 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-271650168_1 at /127.0.0.1:43497 [Receiving block BP-1864658624-172.17.0.12-1499282724310:blk_1073741830_1006]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1864658624-172.17.0.12-1499282724310:blk_1073741830_1006 src: /127.0.0.1:43497 dest: /127.0.0.1:43513
2017-07-05 21:25:25,954 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:43497, dest: /127.0.0.1:43513, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-271650168_1, offset: 0, srvID: a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb, blockid: BP-1864658624-172.17.0.12-1499282724310:blk_1073741830_1006, duration: 2251495
2017-07-05 21:25:25,954 INFO  [PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1864658624-172.17.0.12-1499282724310:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:25,955 INFO  [IPC Server handler 4 on 36572] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:43513 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f87f44eb-ae39-455f-9041-d3a65261a2cf:NORMAL:127.0.0.1:43513|FINALIZED]]} size 0
2017-07-05 21:25:25,957 INFO  [IPC Server handler 5 on 36572] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_-271650168_1
2017-07-05 21:25:25,959 INFO  [IPC Server handler 6 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 21:25:25,968 INFO  [IPC Server handler 7 on 36572] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 21:25:25,973 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 21:25:25,973 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 21:25:25,974 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 21:25:25,974 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4d774249] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 21:25:25,989 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:26,089 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 21:25:26,090 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 49061
2017-07-05 21:25:26,091 INFO  [IPC Server listener on 49061] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 49061
2017-07-05 21:25:26,092 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:26,092 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb) service to localhost/127.0.0.1:36572 interrupted
2017-07-05 21:25:26,092 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb) service to localhost/127.0.0.1:36572
2017-07-05 21:25:26,092 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1864658624-172.17.0.12-1499282724310 (Datanode Uuid a46f4d4a-3b4a-4805-a9b1-bfefee82dfcb)
2017-07-05 21:25:26,093 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1864658624-172.17.0.12-1499282724310 from blockPoolScannerMap
2017-07-05 21:25:26,093 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36572] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1864658624-172.17.0.12-1499282724310
2017-07-05 21:25:26,094 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@7a1e3586] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 21:25:26,095 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 21:25:26,095 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 21:25:26,095 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 21:25:26,095 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 21:25:26,096 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 21:25:26,096 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:26,096 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 21:25:26,096 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2b61a019] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 21:25:26,097 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 33 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 23 SyncTimes(ms): 2 2 
2017-07-05 21:25:26,097 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000033
2017-07-05 21:25:26,098 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@11a00961] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 21:25:26,098 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000033
2017-07-05 21:25:26,098 INFO  [CacheReplicationMonitor(1523751019)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 21:25:26,103 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 36572
2017-07-05 21:25:26,110 INFO  [IPC Server listener on 36572] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 36572
2017-07-05 21:25:26,110 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:26,116 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@3bead518] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 21:25:26,116 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@5c82cd4f] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 21:25:26,133 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:26,133 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 21:25:26,138 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:26,239 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 21:25:26,240 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 21:25:26,240 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 21:25:26,271 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 21:25:26,275 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:26,276 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:26,277 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:26,277 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:26,277 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:26,278 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:26
2017-07-05 21:25:26,278 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:26,278 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,278 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:26,278 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:26,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:26,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:26,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:26,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:26,392 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:26,392 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:26,392 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:26,392 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:26,392 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:26,392 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:26,393 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:26,393 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:26,393 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:26,395 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:26,395 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:26,395 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,397 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:26,397 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:26,403 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:26,404 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:26,404 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,404 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:26,404 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:26,406 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:26,406 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:26,406 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:26,407 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:26,407 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:26,407 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:26,407 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,408 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:26,408 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:26,409 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:26,409 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:26,409 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:26,410 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:26,442 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 21:25:26,477 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 21:25:26,537 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 21:25:26,539 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 21:25:26,541 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 21:25:26,549 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 21:25:26,549 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 21:25:26,550 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 21:25:26,553 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 21:25:26,557 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 21:25:26,558 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:26,559 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 21:25:26,560 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:26,560 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 21:25:26,561 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:26,562 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 44925
2017-07-05 21:25:26,563 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:26,603 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_44925_hdfs____.f6cp7e/webapp
2017-07-05 21:25:26,834 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44925
2017-07-05 21:25:26,835 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:26,836 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:26,840 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:26,840 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:26,841 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:26,841 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:26
2017-07-05 21:25:26,842 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:26,842 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,843 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:26,843 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:26,855 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:26,856 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:26,857 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:26,857 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:26,857 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:26,858 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:26,858 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:26,859 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:26,859 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:26,860 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:26,860 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:26,861 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:26,861 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:26,862 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:26,863 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:26,863 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,864 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:26,864 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:26,871 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:26,871 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:26,872 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,873 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:26,873 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:26,877 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:26,878 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:26,878 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:26,879 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:26,879 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:26,879 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:26,880 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:26,880 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:26,880 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:26,882 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:26,883 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:26,883 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:26,894 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:26,911 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:26,914 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 21:25:26,914 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 21:25:26,915 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 21:25:26,917 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 21:25:26,917 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 21:25:26,917 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 21:25:26,918 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 21:25:26,918 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 21:25:26,950 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 21:25:26,950 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 67 msecs
2017-07-05 21:25:26,950 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 21:25:26,951 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:26,952 INFO  [Socket Reader #1 for port 36370] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 36370
2017-07-05 21:25:26,955 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:36370 to access this namenode/service.
2017-07-05 21:25:26,957 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 21:25:26,971 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:26,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:26,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 21:25:26,972 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 21:25:26,973 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 21:25:26,973 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 21:25:26,992 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 21:25:26,993 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 21:25:26,993 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 21:25:26,994 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 21:25:26,994 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 21:25:26,994 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 21 msec
2017-07-05 21:25:27,001 INFO  [IPC Server listener on 36370] ipc.Server (Server.java:run(674)) - IPC Server listener on 36370: starting
2017-07-05 21:25:27,009 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:36370
2017-07-05 21:25:27,009 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 21:25:27,010 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:27,019 INFO  [CacheReplicationMonitor(1114470633)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 21:25:27,019 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 21:25:27,019 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 21:25:27,019 INFO  [CacheReplicationMonitor(1114470633)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2444014647 milliseconds
2017-07-05 21:25:27,027 INFO  [CacheReplicationMonitor(1114470633)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 8 millisecond(s).
2017-07-05 21:25:27,052 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 21:25:27,053 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 21:25:27,053 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 21:25:27,055 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:54501
2017-07-05 21:25:27,055 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 21:25:27,055 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 21:25:27,056 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 21:25:27,057 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:27,057 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 21:25:27,058 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:27,058 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:27,059 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 33008
2017-07-05 21:25:27,059 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:27,066 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_33008_datanode____.2cxttt/webapp
2017-07-05 21:25:27,178 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33008
2017-07-05 21:25:27,179 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 21:25:27,180 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 21:25:27,181 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:27,182 INFO  [Socket Reader #1 for port 38702] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 38702
2017-07-05 21:25:27,185 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:38702
2017-07-05 21:25:27,197 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 21:25:27,198 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 21:25:27,200 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36370 starting to offer service
2017-07-05 21:25:27,206 INFO  [IPC Server listener on 38702] ipc.Server (Server.java:run(674)) - IPC Server listener on 38702: starting
2017-07-05 21:25:27,208 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:27,234 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:27,234 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:27,236 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 21:25:27,247 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:27,247 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:27,247 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:27,277 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:27,278 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:27,278 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:27,339 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:27,340 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:27,363 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:27,364 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:27,364 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-105515515-172.17.0.12-1499282726410 is not formatted.
2017-07-05 21:25:27,364 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:27,364 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-105515515-172.17.0.12-1499282726410 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-105515515-172.17.0.12-1499282726410/current
2017-07-05 21:25:27,374 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:27,374 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-105515515-172.17.0.12-1499282726410 is not formatted.
2017-07-05 21:25:27,374 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:27,375 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-105515515-172.17.0.12-1499282726410 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-105515515-172.17.0.12-1499282726410/current
2017-07-05 21:25:27,390 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:27,390 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:27,421 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1020527039;bpid=BP-105515515-172.17.0.12-1499282726410;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1020527039;c=0;bpid=BP-105515515-172.17.0.12-1499282726410;dnuuid=null
2017-07-05 21:25:27,441 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:27,442 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:27,453 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID f64e295b-a71e-4821-b2fc-b0184e5a6a4a
2017-07-05 21:25:27,454 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 21:25:27,455 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 21:25:27,455 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 21:25:27,455 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 21:25:27,457 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 21:25:27,457 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499282932457 with interval 21600000
2017-07-05 21:25:27,458 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:27,459 INFO  [Thread-310] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-105515515-172.17.0.12-1499282726410 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:27,459 INFO  [Thread-311] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-105515515-172.17.0.12-1499282726410 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:27,481 INFO  [Thread-311] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-105515515-172.17.0.12-1499282726410 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 22ms
2017-07-05 21:25:27,483 INFO  [Thread-310] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-105515515-172.17.0.12-1499282726410 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 23ms
2017-07-05 21:25:27,483 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-105515515-172.17.0.12-1499282726410: 24ms
2017-07-05 21:25:27,484 INFO  [Thread-314] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-105515515-172.17.0.12-1499282726410 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:27,484 INFO  [Thread-314] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-105515515-172.17.0.12-1499282726410 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 21:25:27,485 INFO  [Thread-315] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-105515515-172.17.0.12-1499282726410 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:27,485 INFO  [Thread-315] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-105515515-172.17.0.12-1499282726410 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 1ms
2017-07-05 21:25:27,485 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-05 21:25:27,486 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid null) service to localhost/127.0.0.1:36370 beginning handshake with NN
2017-07-05 21:25:27,488 INFO  [IPC Server handler 5 on 36370] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=f64e295b-a71e-4821-b2fc-b0184e5a6a4a, infoPort=33008, ipcPort=38702, storageInfo=lv=-56;cid=testClusterID;nsid=1020527039;c=0) storage f64e295b-a71e-4821-b2fc-b0184e5a6a4a
2017-07-05 21:25:27,488 INFO  [IPC Server handler 5 on 36370] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:27,488 INFO  [IPC Server handler 5 on 36370] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:54501
2017-07-05 21:25:27,490 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid null) service to localhost/127.0.0.1:36370 successfully registered with NN
2017-07-05 21:25:27,490 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:36370 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 21:25:27,492 INFO  [IPC Server handler 4 on 36370] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:27,492 INFO  [IPC Server handler 4 on 36370] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-23fc1d0c-de3c-420c-b29d-5a9271b95483 for DN 127.0.0.1:54501
2017-07-05 21:25:27,492 INFO  [IPC Server handler 4 on 36370] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-30bbe505-411a-44d6-833e-3f457de97ce3 for DN 127.0.0.1:54501
2017-07-05 21:25:27,495 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid f64e295b-a71e-4821-b2fc-b0184e5a6a4a) service to localhost/127.0.0.1:36370 trying to claim ACTIVE state with txid=1
2017-07-05 21:25:27,495 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid f64e295b-a71e-4821-b2fc-b0184e5a6a4a) service to localhost/127.0.0.1:36370
2017-07-05 21:25:27,497 INFO  [IPC Server handler 7 on 36370] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-30bbe505-411a-44d6-833e-3f457de97ce3,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:27,497 INFO  [IPC Server handler 7 on 36370] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-30bbe505-411a-44d6-833e-3f457de97ce3 node DatanodeRegistration(127.0.0.1, datanodeUuid=f64e295b-a71e-4821-b2fc-b0184e5a6a4a, infoPort=33008, ipcPort=38702, storageInfo=lv=-56;cid=testClusterID;nsid=1020527039;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 21:25:27,497 INFO  [IPC Server handler 7 on 36370] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-23fc1d0c-de3c-420c-b29d-5a9271b95483,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:27,497 INFO  [IPC Server handler 7 on 36370] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-23fc1d0c-de3c-420c-b29d-5a9271b95483 node DatanodeRegistration(127.0.0.1, datanodeUuid=f64e295b-a71e-4821-b2fc-b0184e5a6a4a, infoPort=33008, ipcPort=38702, storageInfo=lv=-56;cid=testClusterID;nsid=1020527039;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 21:25:27,498 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@688d01dd
2017-07-05 21:25:27,501 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:27,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 21:25:27,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:27,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 21:25:27,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 21:25:27,514 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:27,515 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-105515515-172.17.0.12-1499282726410 to blockPoolScannerMap, new size=1
2017-07-05 21:25:27,545 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:27,557 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:27,561 INFO  [IPC Server handler 1 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 21:25:27,564 INFO  [IPC Server handler 0 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:27,574 INFO  [IPC Server handler 3 on 36370] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-105515515-172.17.0.12-1499282726410 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]}
2017-07-05 21:25:27,585 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1601162738_1 at /127.0.0.1:36091 [Receiving block BP-105515515-172.17.0.12-1499282726410:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-105515515-172.17.0.12-1499282726410:blk_1073741825_1001 src: /127.0.0.1:36091 dest: /127.0.0.1:54501
2017-07-05 21:25:27,619 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:36091, dest: /127.0.0.1:54501, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1601162738_1, offset: 0, srvID: f64e295b-a71e-4821-b2fc-b0184e5a6a4a, blockid: BP-105515515-172.17.0.12-1499282726410:blk_1073741825_1001, duration: 12080799
2017-07-05 21:25:27,619 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:27,622 INFO  [IPC Server handler 6 on 36370] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:54501 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} size 0
2017-07-05 21:25:27,625 INFO  [IPC Server handler 5 on 36370] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_1601162738_1
2017-07-05 21:25:27,630 INFO  [IPC Server handler 4 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:27,639 INFO  [IPC Server handler 7 on 36370] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-105515515-172.17.0.12-1499282726410 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]}
2017-07-05 21:25:27,645 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1601162738_1 at /127.0.0.1:36106 [Receiving block BP-105515515-172.17.0.12-1499282726410:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-105515515-172.17.0.12-1499282726410:blk_1073741826_1002 src: /127.0.0.1:36106 dest: /127.0.0.1:54501
2017-07-05 21:25:27,667 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:36106, dest: /127.0.0.1:54501, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1601162738_1, offset: 0, srvID: f64e295b-a71e-4821-b2fc-b0184e5a6a4a, blockid: BP-105515515-172.17.0.12-1499282726410:blk_1073741826_1002, duration: 7981293
2017-07-05 21:25:27,668 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:27,671 INFO  [IPC Server handler 9 on 36370] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741826_1002{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} has not reached minimal replication 1
2017-07-05 21:25:27,672 INFO  [IPC Server handler 9 on 36370] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:27,672 INFO  [IPC Server handler 9 on 36370] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:27,672 INFO  [IPC Server handler 8 on 36370] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:54501 is added to blk_1073741826_1002{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} size 1
2017-07-05 21:25:28,076 INFO  [IPC Server handler 1 on 36370] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_1601162738_1
2017-07-05 21:25:28,080 INFO  [IPC Server handler 0 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:28,083 INFO  [IPC Server handler 3 on 36370] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3. BP-105515515-172.17.0.12-1499282726410 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]}
2017-07-05 21:25:28,091 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1601162738_1 at /127.0.0.1:36196 [Receiving block BP-105515515-172.17.0.12-1499282726410:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-105515515-172.17.0.12-1499282726410:blk_1073741827_1003 src: /127.0.0.1:36196 dest: /127.0.0.1:54501
2017-07-05 21:25:28,114 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:36196, dest: /127.0.0.1:54501, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1601162738_1, offset: 0, srvID: f64e295b-a71e-4821-b2fc-b0184e5a6a4a, blockid: BP-105515515-172.17.0.12-1499282726410:blk_1073741827_1003, duration: 12111731
2017-07-05 21:25:28,115 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:28,117 INFO  [IPC Server handler 6 on 36370] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741827_1003{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} has not reached minimal replication 1
2017-07-05 21:25:28,117 INFO  [IPC Server handler 6 on 36370] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:28,117 INFO  [IPC Server handler 6 on 36370] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:28,118 INFO  [IPC Server handler 2 on 36370] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:54501 is added to blk_1073741827_1003{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} size 1
2017-07-05 21:25:28,520 INFO  [IPC Server handler 5 on 36370] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3 is closed by DFSClient_NONMAPREDUCE_1601162738_1
2017-07-05 21:25:28,522 INFO  [IPC Server handler 4 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:28,525 INFO  [IPC Server handler 7 on 36370] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4. BP-105515515-172.17.0.12-1499282726410 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]}
2017-07-05 21:25:28,528 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1601162738_1 at /127.0.0.1:36470 [Receiving block BP-105515515-172.17.0.12-1499282726410:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-105515515-172.17.0.12-1499282726410:blk_1073741828_1004 src: /127.0.0.1:36470 dest: /127.0.0.1:54501
2017-07-05 21:25:28,544 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:36470, dest: /127.0.0.1:54501, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1601162738_1, offset: 0, srvID: f64e295b-a71e-4821-b2fc-b0184e5a6a4a, blockid: BP-105515515-172.17.0.12-1499282726410:blk_1073741828_1004, duration: 6591533
2017-07-05 21:25:28,544 INFO  [PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-105515515-172.17.0.12-1499282726410:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:28,549 INFO  [IPC Server handler 8 on 36370] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741828_1004{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} has not reached minimal replication 1
2017-07-05 21:25:28,551 INFO  [IPC Server handler 8 on 36370] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:28,551 INFO  [IPC Server handler 9 on 36370] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:54501 is added to blk_1073741828_1004{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-23fc1d0c-de3c-420c-b29d-5a9271b95483:NORMAL:127.0.0.1:54501|RBW]]} size 1
2017-07-05 21:25:28,551 INFO  [IPC Server handler 8 on 36370] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 21:25:28,955 INFO  [IPC Server handler 1 on 36370] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4 is closed by DFSClient_NONMAPREDUCE_1601162738_1
2017-07-05 21:25:28,957 INFO  [IPC Server handler 0 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 21:25:28,959 INFO  [IPC Server handler 3 on 36370] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 21:25:28,961 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 21:25:28,961 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 21:25:28,962 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 21:25:28,962 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5496c165] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 21:25:28,977 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:28,981 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 21:25:28,981 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 38702
2017-07-05 21:25:28,982 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:28,982 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid f64e295b-a71e-4821-b2fc-b0184e5a6a4a) service to localhost/127.0.0.1:36370 interrupted
2017-07-05 21:25:28,982 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid f64e295b-a71e-4821-b2fc-b0184e5a6a4a) service to localhost/127.0.0.1:36370
2017-07-05 21:25:28,982 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-105515515-172.17.0.12-1499282726410 (Datanode Uuid f64e295b-a71e-4821-b2fc-b0184e5a6a4a)
2017-07-05 21:25:28,982 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-105515515-172.17.0.12-1499282726410 from blockPoolScannerMap
2017-07-05 21:25:28,983 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36370] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-105515515-172.17.0.12-1499282726410
2017-07-05 21:25:28,983 INFO  [IPC Server listener on 38702] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 38702
2017-07-05 21:25:28,987 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@3fdda6f9] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 21:25:28,992 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 21:25:28,993 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 21:25:28,994 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 21:25:28,994 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 21:25:28,995 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 21:25:28,995 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:28,996 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@3fc05ea2] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 21:25:28,997 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@7c891ba7] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 21:25:28,997 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 21:25:28,997 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 19 SyncTimes(ms): 2 4 
2017-07-05 21:25:28,999 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-05 21:25:29,000 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-05 21:25:29,002 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 36370
2017-07-05 21:25:29,002 INFO  [CacheReplicationMonitor(1114470633)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 21:25:29,005 INFO  [IPC Server listener on 36370] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 36370
2017-07-05 21:25:29,005 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:29,005 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@5b275174] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 21:25:29,006 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@6c575325] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 21:25:29,020 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:29,020 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 21:25:29,025 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:29,128 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 21:25:29,130 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 21:25:29,130 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 21:25:29,174 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 21:25:29,185 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:29,186 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:29,187 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:29,187 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:29,188 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:29,370 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:29
2017-07-05 21:25:29,371 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:29,371 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,375 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:29,375 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:29,390 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:29,391 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:29,392 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:29,404 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:29,404 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:29,404 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:29,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:29,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:29,405 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:29,405 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,406 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:29,406 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:29,424 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:29,424 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:29,424 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,425 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:29,425 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:29,427 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:29,427 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:29,427 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:29,428 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:29,428 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:29,429 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:29,430 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,431 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:29,432 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:29,433 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:29,433 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:29,434 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:29,439 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:29,472 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 21:25:29,504 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 21:25:29,601 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 21:25:29,602 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 21:25:29,604 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 21:25:29,608 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 21:25:29,608 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 21:25:29,609 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 21:25:29,627 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 21:25:29,628 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 21:25:29,628 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:29,629 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 21:25:29,632 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:29,633 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 21:25:29,633 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:29,634 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 53057
2017-07-05 21:25:29,634 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:29,638 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_53057_hdfs____xpe1pa/webapp
2017-07-05 21:25:29,755 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:53057
2017-07-05 21:25:29,756 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 21:25:29,791 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 21:25:29,793 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 21:25:29,793 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 21:25:29,793 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 21:25:29,794 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 21:25:29
2017-07-05 21:25:29,795 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 21:25:29,795 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,800 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 21:25:29,800 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 21:25:29,812 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 21:25:29,812 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 21:25:29,812 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 21:25:29,813 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 21:25:29,813 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 21:25:29,813 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 21:25:29,813 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 21:25:29,813 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 21:25:29,813 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 21:25:29,813 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 21:25:29,814 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 21:25:29,814 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 21:25:29,814 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 21:25:29,814 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 21:25:29,815 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 21:25:29,815 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,815 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 21:25:29,815 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 21:25:29,821 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 21:25:29,821 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 21:25:29,821 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,822 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 21:25:29,822 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 21:25:29,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 21:25:29,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 21:25:29,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 21:25:29,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 21:25:29,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 21:25:29,825 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 21:25:29,825 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:29,825 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 21:25:29,825 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 21:25:29,827 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 21:25:29,827 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 21:25:29,827 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 21:25:29,838 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:29,853 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:29,857 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 21:25:29,857 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 21:25:29,857 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 21:25:29,858 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 21:25:29,864 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 21:25:29,865 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 21:25:29,865 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 21:25:29,866 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 21:25:29,894 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 21:25:29,894 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 67 msecs
2017-07-05 21:25:29,894 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 21:25:29,895 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:29,898 INFO  [Socket Reader #1 for port 51206] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 51206
2017-07-05 21:25:29,901 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:51206 to access this namenode/service.
2017-07-05 21:25:29,902 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 21:25:29,929 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:29,929 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 21:25:29,929 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 21:25:29,929 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 21:25:29,930 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 21:25:29,930 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 21:25:29,950 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:29,981 INFO  [IPC Server listener on 51206] ipc.Server (Server.java:run(674)) - IPC Server listener on 51206: starting
2017-07-05 21:25:29,986 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:51206
2017-07-05 21:25:29,995 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 21:25:29,996 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 21:25:29,996 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 21:25:29,996 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 21:25:29,996 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 21:25:29,996 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 66 msec
2017-07-05 21:25:29,996 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 21:25:30,015 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 21:25:30,015 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 21:25:30,016 INFO  [CacheReplicationMonitor(1751124986)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 21:25:30,016 INFO  [CacheReplicationMonitor(1751124986)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2444017644 milliseconds
2017-07-05 21:25:30,023 INFO  [CacheReplicationMonitor(1751124986)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2017-07-05 21:25:30,086 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 21:25:30,086 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 21:25:30,086 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 21:25:30,087 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:60289
2017-07-05 21:25:30,088 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 21:25:30,088 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 21:25:30,089 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 21:25:30,089 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 21:25:30,090 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 21:25:30,090 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 21:25:30,092 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 21:25:30,093 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 54704
2017-07-05 21:25:30,093 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 21:25:30,097 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_54704_datanode____esus83/webapp
2017-07-05 21:25:30,220 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54704
2017-07-05 21:25:30,223 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 21:25:30,224 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 21:25:30,224 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 21:25:30,226 INFO  [Socket Reader #1 for port 56785] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 56785
2017-07-05 21:25:30,235 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:56785
2017-07-05 21:25:30,237 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 21:25:30,238 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 21:25:30,252 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:51206 starting to offer service
2017-07-05 21:25:30,264 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 21:25:30,265 INFO  [IPC Server listener on 56785] ipc.Server (Server.java:run(674)) - IPC Server listener on 56785: starting
2017-07-05 21:25:30,296 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:30,296 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:30,308 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 21:25:30,320 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:30,320 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:30,320 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:30,349 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 259@spirals-librepair
2017-07-05 21:25:30,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:30,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 21:25:30,408 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:30,420 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:30,450 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:30,451 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:30,451 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1453955996-172.17.0.12-1499282729439 is not formatted.
2017-07-05 21:25:30,451 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:30,451 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1453955996-172.17.0.12-1499282729439 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1453955996-172.17.0.12-1499282729439/current
2017-07-05 21:25:30,462 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 21:25:30,462 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1453955996-172.17.0.12-1499282729439 is not formatted.
2017-07-05 21:25:30,462 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 21:25:30,462 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1453955996-172.17.0.12-1499282729439 directory /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1453955996-172.17.0.12-1499282729439/current
2017-07-05 21:25:30,478 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:30,478 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 21:25:30,509 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1740051667;bpid=BP-1453955996-172.17.0.12-1499282729439;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1740051667;c=0;bpid=BP-1453955996-172.17.0.12-1499282729439;dnuuid=null
2017-07-05 21:25:30,522 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:30,522 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:30,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID ccd47fd8-b02c-4853-b3f8-d71ba546583c
2017-07-05 21:25:30,594 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 21:25:30,594 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 21:25:30,595 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 21:25:30,595 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 21:25:30,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 21:25:30,596 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499292940596 with interval 21600000
2017-07-05 21:25:30,597 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:30,597 INFO  [Thread-396] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1453955996-172.17.0.12-1499282729439 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:30,598 INFO  [Thread-397] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1453955996-172.17.0.12-1499282729439 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:30,613 INFO  [Thread-397] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1453955996-172.17.0.12-1499282729439 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 14ms
2017-07-05 21:25:30,616 INFO  [Thread-396] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1453955996-172.17.0.12-1499282729439 on /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 19ms
2017-07-05 21:25:30,616 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1453955996-172.17.0.12-1499282729439: 20ms
2017-07-05 21:25:30,623 INFO  [Thread-401] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1453955996-172.17.0.12-1499282729439 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 21:25:30,624 INFO  [Thread-401] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1453955996-172.17.0.12-1499282729439 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/current: 1ms
2017-07-05 21:25:30,623 INFO  [Thread-400] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1453955996-172.17.0.12-1499282729439 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 21:25:30,633 INFO  [Thread-400] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1453955996-172.17.0.12-1499282729439 on volume /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 21:25:30,633 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 21:25:30,634 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 21:25:30,634 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 18ms
2017-07-05 21:25:30,636 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid null) service to localhost/127.0.0.1:51206 beginning handshake with NN
2017-07-05 21:25:30,638 INFO  [IPC Server handler 6 on 51206] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=ccd47fd8-b02c-4853-b3f8-d71ba546583c, infoPort=54704, ipcPort=56785, storageInfo=lv=-56;cid=testClusterID;nsid=1740051667;c=0) storage ccd47fd8-b02c-4853-b3f8-d71ba546583c
2017-07-05 21:25:30,638 INFO  [IPC Server handler 6 on 51206] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:30,638 INFO  [IPC Server handler 6 on 51206] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:60289
2017-07-05 21:25:30,641 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid null) service to localhost/127.0.0.1:51206 successfully registered with NN
2017-07-05 21:25:30,641 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:51206 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 21:25:30,649 INFO  [IPC Server handler 7 on 51206] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 21:25:30,649 INFO  [IPC Server handler 7 on 51206] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-df136706-1f95-4b6a-923b-250d20255bb0 for DN 127.0.0.1:60289
2017-07-05 21:25:30,649 INFO  [IPC Server handler 7 on 51206] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0 for DN 127.0.0.1:60289
2017-07-05 21:25:30,656 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid ccd47fd8-b02c-4853-b3f8-d71ba546583c) service to localhost/127.0.0.1:51206 trying to claim ACTIVE state with txid=1
2017-07-05 21:25:30,656 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid ccd47fd8-b02c-4853-b3f8-d71ba546583c) service to localhost/127.0.0.1:51206
2017-07-05 21:25:30,658 INFO  [IPC Server handler 9 on 51206] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-df136706-1f95-4b6a-923b-250d20255bb0,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:30,658 INFO  [IPC Server handler 9 on 51206] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-df136706-1f95-4b6a-923b-250d20255bb0 node DatanodeRegistration(127.0.0.1, datanodeUuid=ccd47fd8-b02c-4853-b3f8-d71ba546583c, infoPort=54704, ipcPort=56785, storageInfo=lv=-56;cid=testClusterID;nsid=1740051667;c=0), blocks: 0, hasStaleStorages: true, processing time: 0 msecs
2017-07-05 21:25:30,658 INFO  [IPC Server handler 9 on 51206] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 21:25:30,659 INFO  [IPC Server handler 9 on 51206] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0 node DatanodeRegistration(127.0.0.1, datanodeUuid=ccd47fd8-b02c-4853-b3f8-d71ba546583c, infoPort=54704, ipcPort=56785, storageInfo=lv=-56;cid=testClusterID;nsid=1740051667;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 21:25:30,659 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@6c08933c
2017-07-05 21:25:30,660 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:30,664 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 21:25:30,664 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 21:25:30,664 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 21:25:30,664 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 21:25:30,668 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:30,669 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1453955996-172.17.0.12-1499282729439 to blockPoolScannerMap, new size=1
2017-07-05 21:25:30,736 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:30,742 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 21:25:30,747 INFO  [IPC Server handler 0 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 21:25:30,750 INFO  [IPC Server handler 1 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:30,764 INFO  [IPC Server handler 2 on 51206] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1453955996-172.17.0.12-1499282729439 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0:NORMAL:127.0.0.1:60289|RBW]]}
2017-07-05 21:25:30,776 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-2114735926_1 at /127.0.0.1:45829 [Receiving block BP-1453955996-172.17.0.12-1499282729439:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1453955996-172.17.0.12-1499282729439:blk_1073741825_1001 src: /127.0.0.1:45829 dest: /127.0.0.1:60289
2017-07-05 21:25:30,804 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45829, dest: /127.0.0.1:60289, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2114735926_1, offset: 0, srvID: ccd47fd8-b02c-4853-b3f8-d71ba546583c, blockid: BP-1453955996-172.17.0.12-1499282729439:blk_1073741825_1001, duration: 19739150
2017-07-05 21:25:30,804 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:30,808 INFO  [IPC Server handler 5 on 51206] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60289 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-df136706-1f95-4b6a-923b-250d20255bb0:NORMAL:127.0.0.1:60289|FINALIZED]]} size 0
2017-07-05 21:25:30,811 INFO  [IPC Server handler 6 on 51206] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-2114735926_1
2017-07-05 21:25:30,814 INFO  [IPC Server handler 7 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:30,820 INFO  [IPC Server handler 9 on 51206] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-1453955996-172.17.0.12-1499282729439 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0:NORMAL:127.0.0.1:60289|RBW]]}
2017-07-05 21:25:30,833 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-2114735926_1 at /127.0.0.1:45841 [Receiving block BP-1453955996-172.17.0.12-1499282729439:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1453955996-172.17.0.12-1499282729439:blk_1073741826_1002 src: /127.0.0.1:45841 dest: /127.0.0.1:60289
2017-07-05 21:25:30,868 INFO  [IPC Server handler 8 on 51206] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60289 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0:NORMAL:127.0.0.1:60289|RBW]]} size 0
2017-07-05 21:25:30,869 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45841, dest: /127.0.0.1:60289, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2114735926_1, offset: 0, srvID: ccd47fd8-b02c-4853-b3f8-d71ba546583c, blockid: BP-1453955996-172.17.0.12-1499282729439:blk_1073741826_1002, duration: 17699858
2017-07-05 21:25:30,869 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:30,873 INFO  [IPC Server handler 3 on 51206] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_-2114735926_1
2017-07-05 21:25:30,875 INFO  [IPC Server handler 0 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:30,881 INFO  [IPC Server handler 1 on 51206] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-1453955996-172.17.0.12-1499282729439 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-df136706-1f95-4b6a-923b-250d20255bb0:NORMAL:127.0.0.1:60289|RBW]]}
2017-07-05 21:25:30,888 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-2114735926_1 at /127.0.0.1:45854 [Receiving block BP-1453955996-172.17.0.12-1499282729439:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1453955996-172.17.0.12-1499282729439:blk_1073741827_1003 src: /127.0.0.1:45854 dest: /127.0.0.1:60289
2017-07-05 21:25:30,918 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45854, dest: /127.0.0.1:60289, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2114735926_1, offset: 0, srvID: ccd47fd8-b02c-4853-b3f8-d71ba546583c, blockid: BP-1453955996-172.17.0.12-1499282729439:blk_1073741827_1003, duration: 16696144
2017-07-05 21:25:30,918 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:30,919 INFO  [IPC Server handler 2 on 51206] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60289 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-df136706-1f95-4b6a-923b-250d20255bb0:NORMAL:127.0.0.1:60289|RBW]]} size 0
2017-07-05 21:25:30,920 INFO  [IPC Server handler 4 on 51206] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_-2114735926_1
2017-07-05 21:25:30,930 INFO  [IPC Server handler 5 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 21:25:30,942 INFO  [IPC Server handler 6 on 51206] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-1453955996-172.17.0.12-1499282729439 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0:NORMAL:127.0.0.1:60289|RBW]]}
2017-07-05 21:25:30,944 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-2114735926_1 at /127.0.0.1:45867 [Receiving block BP-1453955996-172.17.0.12-1499282729439:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1453955996-172.17.0.12-1499282729439:blk_1073741828_1004 src: /127.0.0.1:45867 dest: /127.0.0.1:60289
2017-07-05 21:25:30,982 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45867, dest: /127.0.0.1:60289, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2114735926_1, offset: 0, srvID: ccd47fd8-b02c-4853-b3f8-d71ba546583c, blockid: BP-1453955996-172.17.0.12-1499282729439:blk_1073741828_1004, duration: 35601160
2017-07-05 21:25:30,982 INFO  [PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1453955996-172.17.0.12-1499282729439:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 21:25:30,984 INFO  [IPC Server handler 7 on 51206] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60289 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9fa8f0d1-4095-4f71-ae2f-51f70877eca0:NORMAL:127.0.0.1:60289|RBW]]} size 0
2017-07-05 21:25:30,988 INFO  [IPC Server handler 9 on 51206] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_-2114735926_1
2017-07-05 21:25:30,990 INFO  [IPC Server handler 8 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 21:25:30,992 INFO  [IPC Server handler 3 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 21:25:31,014 INFO  [IPC Server handler 0 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 21:25:31,075 INFO  [IPC Server handler 1 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/2.txt	dst=null	perm=null	proto=rpc
2017-07-05 21:25:31,089 INFO  [IPC Server handler 2 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/3.txt	dst=null	perm=null	proto=rpc
2017-07-05 21:25:31,096 INFO  [IPC Server handler 4 on 51206] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/4.txt	dst=null	perm=null	proto=rpc
2017-07-05 21:25:31,102 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 21:25:31,102 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 21:25:31,102 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 21:25:31,104 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@9aa2002] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 21:25:31,130 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:31,231 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 1
2017-07-05 21:25:31,233 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 21:25:31,233 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 56785
2017-07-05 21:25:31,234 INFO  [IPC Server listener on 56785] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 56785
2017-07-05 21:25:31,234 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:31,234 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid ccd47fd8-b02c-4853-b3f8-d71ba546583c) service to localhost/127.0.0.1:51206 interrupted
2017-07-05 21:25:31,237 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid ccd47fd8-b02c-4853-b3f8-d71ba546583c) service to localhost/127.0.0.1:51206
2017-07-05 21:25:31,237 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1453955996-172.17.0.12-1499282729439 (Datanode Uuid ccd47fd8-b02c-4853-b3f8-d71ba546583c)
2017-07-05 21:25:31,237 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1453955996-172.17.0.12-1499282729439 from blockPoolScannerMap
2017-07-05 21:25:31,237 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:51206] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1453955996-172.17.0.12-1499282729439
2017-07-05 21:25:31,240 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@7fc41bfc] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 21:25:31,240 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 21:25:31,243 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 21:25:31,243 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 21:25:31,243 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 21:25:31,243 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 21:25:31,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:31,244 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 21:25:31,244 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2123064f] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 21:25:31,244 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@4f6b687e] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 21:25:31,244 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 1 2 
2017-07-05 21:25:31,245 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-05 21:25:31,246 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250359024/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-05 21:25:31,246 INFO  [CacheReplicationMonitor(1751124986)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 21:25:31,248 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 51206
2017-07-05 21:25:31,249 INFO  [IPC Server listener on 51206] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 51206
2017-07-05 21:25:31,249 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 21:25:31,249 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@71d9cb05] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 21:25:31,250 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@2016f509] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 21:25:31,263 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 21:25:31,264 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 21:25:31,268 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 21:25:31,368 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 21:25:31,370 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 21:25:31,370 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.373 sec - in org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
Running org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec - in org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Running org.apache.samoa.streams.kafka.KafkaUtilsTest
2017-07-05 21:25:31,912 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-05 21:25:31,913 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:host.name=spirals-librepair
2017-07-05 21:25:31,913 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.version=1.8.0_121
2017-07-05 21:25:31,913 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.vendor=Oracle Corporation
2017-07-05 21:25:31,913 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-05 21:25:31,913 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.class.path=/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/250359024/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-05 21:25:31,914 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-05 21:25:31,914 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.io.tmpdir=/tmp
2017-07-05 21:25:31,914 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.compiler=<NA>
2017-07-05 21:25:31,914 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.name=Linux
2017-07-05 21:25:31,915 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.arch=amd64
2017-07-05 21:25:31,915 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.version=3.16.0-4-amd64
2017-07-05 21:25:31,915 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.name=root
2017-07-05 21:25:31,915 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.home=/root
2017-07-05 21:25:31,915 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.dir=/root/workspace/apache/incubator-samoa/250359024/samoa-api
2017-07-05 21:25:31,946 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-8037865090985312194/version-2 snapdir /tmp/kafka-1989043596548451588/version-2
2017-07-05 21:25:31,955 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 21:25:32,001 INFO  [ZkClient-EventThread-482-127.0.0.1:38293] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 21:25:32,007 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-05 21:25:32,008 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:host.name=spirals-librepair
2017-07-05 21:25:32,008 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.version=1.8.0_121
2017-07-05 21:25:32,008 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.vendor=Oracle Corporation
2017-07-05 21:25:32,009 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-05 21:25:32,009 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.class.path=/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/250359024/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/250359024/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/250359024/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-05 21:25:32,010 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-05 21:25:32,011 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.io.tmpdir=/tmp
2017-07-05 21:25:32,011 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.compiler=<NA>
2017-07-05 21:25:32,011 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.name=Linux
2017-07-05 21:25:32,012 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.arch=amd64
2017-07-05 21:25:32,012 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.version=3.16.0-4-amd64
2017-07-05 21:25:32,012 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.name=root
2017-07-05 21:25:32,013 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.home=/root
2017-07-05 21:25:32,013 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.dir=/root/workspace/apache/incubator-samoa/250359024/samoa-api
2017-07-05 21:25:32,015 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:38293 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@6192a5d5
2017-07-05 21:25:32,032 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 21:25:32,087 INFO  [main-SendThread(127.0.0.1:38293)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:38293. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 21:25:32,088 INFO  [main-SendThread(127.0.0.1:38293)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:38293, initiating session
2017-07-05 21:25:32,088 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:47393
2017-07-05 21:25:32,099 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:47393
2017-07-05 21:25:32,103 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 21:25:32,170 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d1436f3bf0000 with negotiated timeout 10000 for client /127.0.0.1:47393
2017-07-05 21:25:32,171 INFO  [main-SendThread(127.0.0.1:38293)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:38293, sessionid = 0x15d1436f3bf0000, negotiated timeout = 10000
2017-07-05 21:25:32,173 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 21:25:32,497 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafkaUtils-2676398940459256636
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:38293
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 21:25:32,591 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 21:25:32,594 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:38293
2017-07-05 21:25:32,596 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:38293 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4a2e7bcb
2017-07-05 21:25:32,597 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 21:25:32,597 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 21:25:32,598 INFO  [main-SendThread(127.0.0.1:38293)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:38293. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 21:25:32,598 INFO  [main-SendThread(127.0.0.1:38293)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:38293, initiating session
2017-07-05 21:25:32,600 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:47495
2017-07-05 21:25:32,600 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:47495
2017-07-05 21:25:32,630 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d1436f3bf0001 with negotiated timeout 6000 for client /127.0.0.1:47495
2017-07-05 21:25:32,631 INFO  [main-SendThread(127.0.0.1:38293)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:38293, sessionid = 0x15d1436f3bf0001, negotiated timeout = 6000
2017-07-05 21:25:32,631 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 21:25:32,689 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 21:25:32,741 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 21:25:32,760 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 21:25:32,823 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x1b zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 21:25:32,833 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = 6yxRql0IS0iEUwOXCu5iIw
2017-07-05 21:25:32,838 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-2676398940459256636/meta.properties
2017-07-05 21:25:32,871 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 21:25:32,875 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 21:25:32,927 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 21:25:32,939 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 21:25:32,987 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 21:25:32,991 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 21:25:33,001 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 21:25:33,025 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 21:25:33,079 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 21:25:33,084 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 21:25:33,116 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:33,118 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:33,167 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 21:25:33,180 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 21:25:33,203 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 21:25:33,204 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 21:25:33,205 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 21:25:33,209 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:setData cxid:0x25 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 21:25:33,238 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 21:25:33,266 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 21:25:33,267 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 21:25:33,269 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 21:25:33,276 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 21:25:33,277 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 21:25:33,279 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 21:25:33,285 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 21:25:33,286 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 21:25:33,293 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 21:25:33,294 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 21:25:33,295 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 21:25:33,338 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 21:25:33,342 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 21:25:33,344 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 21:25:33,345 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 21:25:33,347 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 21:25:33,351 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:delete cxid:0x36 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 21:25:33,355 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 21:25:33,357 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 21:25:33,378 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:33,389 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:33,390 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:33,424 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 21:25:33,426 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 21:25:33,429 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 3 milliseconds.
2017-07-05 21:25:33,458 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 21:25:33,494 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 21:25:33,496 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 21:25:33,500 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 21:25:33,506 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 21:25:33,508 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 21:25:33,509 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-2676398940459256636/meta.properties
2017-07-05 21:25:33,536 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 21:25:33,540 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 21:25:33,549 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:33,549 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:33,550 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 21:25:33,585 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 21:25:33,619 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 21:25:33,621 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 21:25:33,634 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 21:25:33,640 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-r Error:KeeperErrorCode = NoNode for /config/topics/test-r
2017-07-05 21:25:33,665 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:25:33,673 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:25:33,693 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0000 type:setData cxid:0xb zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 21:25:33,702 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-r)], deleted topics: [Set()], new partition replica assignment [Map([test-r,0] -> List(0))]
2017-07-05 21:25:33,703 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-r,0]
2017-07-05 21:25:33,720 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0000 type:create cxid:0xc zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:25:33,730 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-r,0]
2017-07-05 21:25:33,734 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-r,0]
2017-07-05 21:25:33,740 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-r,Partition=0,Replica=0]
2017-07-05 21:25:33,747 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:25:33,749 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-r,0]
2017-07-05 21:25:33,755 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x4c zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions/0
2017-07-05 21:25:33,769 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:33,770 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:33,814 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:33,814 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:33,821 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x4d zxid:0x25 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions
2017-07-05 21:25:33,840 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-r,Partition=0,Replica=0]
2017-07-05 21:25:33,855 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-05 21:25:33,855 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-05 21:25:33,857 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-05 21:25:33,857 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-05 21:25:33,857 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 21:25:33,858 INFO  [kafka-request-handler-2] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-r-0
2017-07-05 21:25:33,861 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-05 21:25:33,862 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-05 21:25:33,863 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x5a zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 21:25:33,882 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x5d zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 21:25:33,904 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:setData cxid:0x61 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 21:25:33,910 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x63 zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:25:33,960 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log test-r-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:33,964 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-r,0] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:33,969 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [test-r,0] on broker 0: No checkpointed highwatermark is found for partition test-r-0
2017-07-05 21:25:33,979 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 21:25:33,980 INFO  [kafka-request-handler-3] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 21:25:33,983 INFO  [kafka-request-handler-3] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 21:25:34,039 INFO  [kafka-request-handler-7] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-05 21:25:34,055 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:34,055 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 21:25:34,058 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:34,061 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:34,062 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 4 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-05 21:25:34,063 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:34,070 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 21:25:34,093 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:34,106 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 21:25:34,139 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:34,140 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xa4 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 21:25:34,144 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xa5 zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 21:25:34,152 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xa9 zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 21:25:34,157 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xac zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 21:25:34,176 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xb1 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 21:25:34,190 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xb5 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 21:25:34,208 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xb8 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 21:25:34,214 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xbb zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 21:25:34,220 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xbe zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 21:25:34,407 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xc3 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 21:25:34,412 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xc6 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 21:25:34,419 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xca zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 21:25:34,438 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xd0 zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 21:25:34,445 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xd3 zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 21:25:34,450 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xd6 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 21:25:34,455 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xd9 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 21:25:34,459 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xdc zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 21:25:34,463 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xdf zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 21:25:34,468 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xe2 zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 21:25:34,507 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xe5 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 21:25:34,517 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xe8 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 21:25:34,522 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xeb zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 21:25:34,527 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xf0 zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 21:25:34,531 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xf3 zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 21:25:34,542 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xf7 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 21:25:34,600 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xfa zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 21:25:34,604 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0xfd zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 21:25:34,609 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x100 zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 21:25:34,612 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x103 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 21:25:34,617 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x106 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 21:25:34,688 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x10b zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 21:25:34,698 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x10f zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 21:25:34,706 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x112 zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 21:25:34,710 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x115 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 21:25:34,715 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x118 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 21:25:34,719 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x11b zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 21:25:34,724 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x11e zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 21:25:34,728 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x122 zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 21:25:34,749 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x127 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 21:25:34,755 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x12a zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 21:25:34,767 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x12d zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 21:25:34,792 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x130 zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 21:25:34,797 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x133 zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 21:25:34,802 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x136 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 21:25:34,806 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x139 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 21:25:34,821 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x13c zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 21:25:34,826 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x13f zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 21:25:34,831 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x143 zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 21:25:34,840 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x148 zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 21:25:34,845 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x14b zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 21:25:34,883 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1436f3bf0001 type:create cxid:0x14e zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 21:25:34,917 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 21:25:34,978 INFO  [kafka-request-handler-7] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 21:25:34,985 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:34,986 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:34,987 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 21:25:34,993 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:34,995 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:34,995 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 21:25:35,003 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,005 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,005 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 21:25:35,014 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,016 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,016 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 21:25:35,022 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,025 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,025 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 21:25:35,032 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,035 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,036 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 21:25:35,043 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,045 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,046 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 21:25:35,051 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,053 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,053 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 21:25:35,058 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,060 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,061 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 21:25:35,066 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,067 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,068 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 21:25:35,074 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,076 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,076 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 21:25:35,082 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,084 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,085 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 21:25:35,090 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,092 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,093 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 21:25:35,098 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,100 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,100 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 21:25:35,106 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,108 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,108 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 21:25:35,113 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,115 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,115 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 21:25:35,120 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,122 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,122 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 21:25:35,127 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,129 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,129 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 21:25:35,134 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,136 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,137 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 21:25:35,146 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,147 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,148 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 21:25:35,153 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,155 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,156 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 21:25:35,164 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,166 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,166 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 21:25:35,172 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,174 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,174 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 21:25:35,181 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,182 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,183 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 21:25:35,200 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,201 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,202 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 21:25:35,209 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,211 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,212 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 21:25:35,219 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,221 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,222 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 21:25:35,227 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,229 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,229 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 21:25:35,234 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,236 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,236 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 21:25:35,243 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,244 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,245 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 21:25:35,254 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,256 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,257 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 21:25:35,262 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,263 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,264 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 21:25:35,269 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,270 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,270 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 21:25:35,277 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,278 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,279 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 21:25:35,284 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,286 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,287 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 21:25:35,292 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,294 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,294 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 21:25:35,299 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,301 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,301 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 21:25:35,314 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,315 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,316 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 21:25:35,321 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,323 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,323 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 21:25:35,337 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,338 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,339 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 21:25:35,360 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,362 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,362 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 21:25:35,371 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,378 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,379 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 21:25:35,384 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,386 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,386 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 21:25:35,391 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,392 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,393 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 21:25:35,397 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,399 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,399 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 21:25:35,411 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,412 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,413 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 21:25:35,420 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,422 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,422 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 21:25:35,428 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,429 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,429 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 21:25:35,434 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,436 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,436 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 21:25:35,447 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:35,449 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafkaUtils-2676398940459256636 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:35,449 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 21:25:35,476 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 21:25:35,500 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 22 milliseconds.
2017-07-05 21:25:35,501 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 21:25:35,503 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 21:25:35,503 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 21:25:35,504 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 21:25:35,504 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 21:25:35,506 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-05 21:25:35,506 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 21:25:35,507 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 21:25:35,507 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 21:25:35,508 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 21:25:35,509 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 21:25:35,510 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-05 21:25:35,510 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 21:25:35,511 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 21:25:35,511 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 21:25:35,512 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-05 21:25:35,513 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 21:25:35,514 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 21:25:35,514 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 21:25:35,515 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-05 21:25:35,516 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 21:25:35,517 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 1 milliseconds.
2017-07-05 21:25:35,517 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 21:25:35,518 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 1 milliseconds.
2017-07-05 21:25:35,518 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 21:25:35,519 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-05 21:25:35,519 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 21:25:35,520 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 0 milliseconds.
2017-07-05 21:25:35,521 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 21:25:35,522 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 1 milliseconds.
2017-07-05 21:25:35,522 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 21:25:35,523 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-05 21:25:35,523 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 21:25:35,524 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 1 milliseconds.
2017-07-05 21:25:35,525 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 21:25:35,526 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-05 21:25:35,526 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 21:25:35,527 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-05 21:25:35,527 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 21:25:35,529 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 0 milliseconds.
2017-07-05 21:25:35,529 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 21:25:35,530 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-05 21:25:35,530 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 21:25:35,531 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 21:25:35,531 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 21:25:35,532 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 21:25:35,532 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 21:25:35,533 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 21:25:35,534 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 21:25:35,535 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 21:25:35,535 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 21:25:35,536 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 21:25:35,536 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 21:25:35,537 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 21:25:35,538 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 21:25:35,539 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-05 21:25:35,539 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 21:25:35,540 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 21:25:35,540 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 21:25:35,541 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-05 21:25:35,541 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 21:25:35,542 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 21:25:35,543 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 21:25:35,544 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 21:25:35,544 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 21:25:35,545 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 21:25:35,545 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 21:25:35,546 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 21:25:35,547 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 21:25:35,548 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-05 21:25:35,548 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 21:25:35,549 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 21:25:35,549 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 21:25:35,550 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 0 milliseconds.
2017-07-05 21:25:35,551 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 21:25:35,551 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:25:35,552 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-05 21:25:35,552 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 21:25:35,553 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-05 21:25:35,553 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 21:25:35,554 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 1 milliseconds.
2017-07-05 21:25:35,554 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 21:25:35,556 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 2 milliseconds.
2017-07-05 21:25:35,556 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 21:25:35,557 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-05 21:25:35,557 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 21:25:35,558 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 1 milliseconds.
2017-07-05 21:25:35,558 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 21:25:35,559 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 0 milliseconds.
2017-07-05 21:25:35,560 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 21:25:35,561 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-05 21:25:35,561 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 21:25:35,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-05 21:25:35,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 21:25:35,563 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 1 milliseconds.
2017-07-05 21:25:35,564 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 21:25:35,565 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 1 milliseconds.
2017-07-05 21:25:35,565 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 21:25:35,566 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-05 21:25:35,576 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 21:25:35,576 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:25:35,601 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 21:25:35,610 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 21:25:35,643 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 21:25:35,706 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 21:25:35,710 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 21:25:35,735 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-05 21:25:35,736 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 2 is now empty
2017-07-05 21:25:36,259 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:36,260 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:36,266 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:36,269 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:36,277 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:25:36,278 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 21:25:36,280 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:25:36,285 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 2
2017-07-05 21:25:36,286 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 3
2017-07-05 21:25:36,287 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 3
2017-07-05 21:25:36,289 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 3
2017-07-05 21:25:36,296 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 21:25:37,274 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = sendM-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 21:25:37,297 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:37,311 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:37,316 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 21:25:37,316 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:37,316 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:37,327 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:37,327 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:37,351 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:25:37,364 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 21:25:37,365 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:25:37,369 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 3
2017-07-05 21:25:37,604 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 4
2017-07-05 21:25:37,625 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 4
2017-07-05 21:25:37,627 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 4
2017-07-05 21:25:37,628 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 21:25:37,796 INFO  [main] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-05 21:25:39,346 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = rcv-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 21:25:39,356 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 21:25:39,357 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:39,357 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:39,358 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:39,362 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-4
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:39,369 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:39,370 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:40,798 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 4
2017-07-05 21:25:40,799 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 5 is now empty
2017-07-05 21:25:42,762 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:25:42,764 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 21:25:42,764 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:25:42,777 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 5
2017-07-05 21:25:42,777 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 6
2017-07-05 21:25:42,781 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 6
2017-07-05 21:25:42,786 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 6
2017-07-05 21:25:42,787 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-s-0] for group test
2017-07-05 21:25:42,808 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 6
2017-07-05 21:25:42,808 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 7 is now empty
2017-07-05 21:25:42,851 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 21:25:42,853 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 21:25:42,875 INFO  [kafka-request-handler-4] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 21:25:42,900 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 21:25:42,904 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 21:25:42,920 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 21:25:42,922 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 21:25:42,927 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 21:25:42,935 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 21:25:43,880 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 21:25:43,880 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 21:25:43,881 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 21:25:44,880 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 21:25:44,880 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 21:25:44,882 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 21:25:44,884 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 21:25:44,885 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 21:25:44,887 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 21:25:44,888 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:25:44,904 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:25:44,904 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:25:44,905 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:25:44,941 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:25:44,941 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:25:45,063 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 21:25:45,064 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:25:45,208 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:25:45,208 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:25:45,210 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 21:25:45,211 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:25:45,373 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:25:45,373 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:25:45,373 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:25:45,408 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:25:45,408 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:25:45,410 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 21:25:45,411 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 21:25:45,412 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 21:25:45,413 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 21:25:45,414 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 21:25:45,415 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 21:25:45,713 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 21:25:45,718 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 21:25:45,720 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 21:25:45,723 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 21:25:45,724 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 21:25:45,724 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 21:25:45,726 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 21:25:45,727 INFO  [ZkClient-EventThread-486-127.0.0.1:38293] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 21:25:45,728 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d1436f3bf0001
2017-07-05 21:25:45,734 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d1436f3bf0001 closed
2017-07-05 21:25:45,736 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 21:25:45,736 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:47495 which had sessionid 0x15d1436f3bf0001
2017-07-05 21:25:45,738 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 21:25:45,738 INFO  [ZkClient-EventThread-482-127.0.0.1:38293] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 21:25:45,739 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d1436f3bf0000
2017-07-05 21:25:45,745 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d1436f3bf0000 closed
2017-07-05 21:25:45,745 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:47393 which had sessionid 0x15d1436f3bf0000
2017-07-05 21:25:45,745 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 21:25:45,746 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 21:25:45,746 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 21:25:45,746 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 21:25:45,747 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 21:25:45,747 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 21:25:45,747 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 21:25:45,748 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 21:25:45,751 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 21:25:45,751 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 21:25:45,752 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 21:25:45,752 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 21:25:45,752 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 21:25:45,753 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.358 sec - in org.apache.samoa.streams.kafka.KafkaUtilsTest
Running org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
2017-07-05 21:25:45,762 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-3761924488323559485/version-2 snapdir /tmp/kafka-9020494251567905656/version-2
2017-07-05 21:25:45,762 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 21:25:45,767 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:42277 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@72b0a004
2017-07-05 21:25:45,767 INFO  [ZkClient-EventThread-542-127.0.0.1:42277] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 21:25:45,768 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 21:25:45,772 INFO  [main-SendThread(127.0.0.1:42277)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:42277. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 21:25:45,772 INFO  [main-SendThread(127.0.0.1:42277)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:42277, initiating session
2017-07-05 21:25:45,772 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:41317
2017-07-05 21:25:45,773 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:41317
2017-07-05 21:25:45,773 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 21:25:45,783 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d143729a50000 with negotiated timeout 10000 for client /127.0.0.1:41317
2017-07-05 21:25:45,783 INFO  [main-SendThread(127.0.0.1:42277)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:42277, sessionid = 0x15d143729a50000, negotiated timeout = 10000
2017-07-05 21:25:45,784 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 21:25:45,785 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-5066882802958725320
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:42277
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 21:25:45,791 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 21:25:45,792 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:42277
2017-07-05 21:25:45,792 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:42277 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4cdba2ed
2017-07-05 21:25:45,792 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 21:25:45,793 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 21:25:45,794 INFO  [main-SendThread(127.0.0.1:42277)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:42277. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 21:25:45,794 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:41322
2017-07-05 21:25:45,795 INFO  [main-SendThread(127.0.0.1:42277)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:42277, initiating session
2017-07-05 21:25:45,795 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:41322
2017-07-05 21:25:45,797 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d143729a50001 with negotiated timeout 6000 for client /127.0.0.1:41322
2017-07-05 21:25:45,797 INFO  [main-SendThread(127.0.0.1:42277)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:42277, sessionid = 0x15d143729a50001, negotiated timeout = 6000
2017-07-05 21:25:45,797 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 21:25:45,801 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 21:25:45,806 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 21:25:45,813 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 21:25:45,820 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 21:25:45,823 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = cuLV9VRbRZycmhTsaE5-rA
2017-07-05 21:25:45,824 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-5066882802958725320/meta.properties
2017-07-05 21:25:45,826 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 21:25:45,827 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 21:25:45,828 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 21:25:45,829 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 21:25:45,850 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 21:25:45,851 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 21:25:45,852 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 21:25:45,852 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 21:25:45,859 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 21:25:45,862 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 21:25:45,864 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:45,865 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:45,866 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 21:25:45,868 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 21:25:45,870 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 21:25:45,870 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 21:25:45,870 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 21:25:45,882 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 21:25:45,923 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 21:25:45,930 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 21:25:45,930 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 21:25:45,930 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 21:25:45,931 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 21:25:45,932 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 21:25:45,932 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 21:25:45,933 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 21:25:45,934 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 21:25:45,934 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 21:25:45,934 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 21:25:45,935 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 21:25:45,935 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 21:25:45,936 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 21:25:45,936 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 21:25:45,936 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 21:25:45,937 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 21:25:45,937 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 21:25:45,939 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 21:25:45,943 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 21:25:45,945 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:45,948 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:45,948 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 21:25:45,949 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:25:45,950 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 21:25:45,951 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 21:25:45,951 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-05 21:25:45,955 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 21:25:45,956 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 55 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-05 21:25:45,962 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 21:25:45,963 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 21:25:45,964 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 21:25:46,000 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-05 21:25:46,055 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 21:25:46,055 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 21:25:46,056 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-5066882802958725320/meta.properties
2017-07-05 21:25:46,056 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 21:25:46,062 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 21:25:46,067 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 21:25:46,068 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:setData cxid:0x4a zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 21:25:46,069 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x4b zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:25:46,071 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 21:25:46,071 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 21:25:46,071 INFO  [kafka-request-handler-0] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:25:46,175 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:46,176 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:46,176 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 21:25:46,226 INFO  [kafka-request-handler-0] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-05 21:25:46,227 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 56 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-05 21:25:46,230 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-05 21:25:46,230 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-05 21:25:46,232 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-05 21:25:46,232 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-05 21:25:46,233 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 21:25:46,234 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-05 21:25:46,235 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x53 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 21:25:46,250 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x54 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 21:25:46,252 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50000 type:setData cxid:0x5 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-05 21:25:46,253 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50000 type:create cxid:0x6 zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:25:46,256 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:25:46,256 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 21:25:46,257 INFO  [kafka-request-handler-2] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-05 21:25:46,261 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:46,262 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos)], deleted topics: [Set()], new partition replica assignment [Map([samoa_test-oos,0] -> List(0))]
2017-07-05 21:25:46,262 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [samoa_test-oos,0]
2017-07-05 21:25:46,262 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:46,262 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 21:25:46,263 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [samoa_test-oos,0]
2017-07-05 21:25:46,263 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [samoa_test-oos,0]
2017-07-05 21:25:46,264 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:46,265 INFO  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 21:25:46,265 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 21:25:46,265 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-5
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:25:46,267 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [samoa_test-oos,0]
2017-07-05 21:25:46,268 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x5e zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-05 21:25:46,271 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:46,271 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:46,278 WARN  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 21:25:46,278 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:25:46,278 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:25:46,286 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x61 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-05 21:25:46,328 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:25:46,337 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 21:25:46,347 INFO  [kafka-request-handler-1] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0
2017-07-05 21:25:46,354 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:46,355 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:46,356 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-05 21:25:46,362 WARN  [kafka-producer-network-thread | test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 1 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:25:46,380 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:setData cxid:0x6e zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 21:25:46,392 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x6f zxid:0x2f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:25:46,399 INFO  [kafka-request-handler-6] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 21:25:46,401 INFO  [kafka-request-handler-6] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 21:25:46,422 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 21:25:46,423 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:46,441 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:46,442 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:46,448 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 21:25:46,477 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:25:46,477 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xab zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 21:25:46,481 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xac zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 21:25:46,499 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xb0 zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 21:25:46,504 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xb3 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 21:25:46,508 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xb6 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 21:25:46,513 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xb9 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 21:25:46,518 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xbc zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 21:25:46,524 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xbf zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 21:25:46,530 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xc2 zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 21:25:46,597 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xc7 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 21:25:46,604 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xcb zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 21:25:46,668 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xd1 zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 21:25:46,674 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xd4 zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 21:25:46,678 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xd7 zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 21:25:46,683 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xda zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 21:25:46,687 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xdd zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 21:25:46,692 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xe0 zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 21:25:46,695 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xe3 zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 21:25:46,699 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xe6 zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 21:25:46,769 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xeb zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 21:25:46,807 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xef zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 21:25:46,812 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xf2 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 21:25:46,816 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xf5 zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 21:25:46,829 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xf8 zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 21:25:46,870 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0xfe zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 21:25:46,876 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x101 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 21:25:46,884 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x104 zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 21:25:46,888 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x107 zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 21:25:46,893 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x10a zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 21:25:46,954 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x10e zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 21:25:47,023 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x113 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 21:25:47,029 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x116 zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 21:25:47,034 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x119 zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 21:25:47,098 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x11e zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 21:25:47,103 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x122 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 21:25:47,109 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x125 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 21:25:47,124 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x128 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 21:25:47,250 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x12e zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 21:25:47,347 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x134 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 21:25:47,350 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x137 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 21:25:47,385 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x13c zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 21:25:47,401 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x140 zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 21:25:47,434 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x143 zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 21:25:47,438 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x146 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 21:25:47,443 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x149 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 21:25:47,449 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x14c zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 21:25:47,575 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x153 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 21:25:47,579 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x157 zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 21:25:47,585 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x15b zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 21:25:47,590 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x15e zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 21:25:47,650 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143729a50001 type:create cxid:0x161 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 21:25:47,698 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 21:25:47,728 INFO  [kafka-request-handler-1] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 21:25:47,735 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,736 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,737 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 21:25:47,743 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,744 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,745 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 21:25:47,750 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,751 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,752 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 21:25:47,758 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,759 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,760 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 21:25:47,766 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,767 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,768 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 21:25:47,773 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,774 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,775 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 21:25:47,785 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,786 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,787 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 21:25:47,792 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,794 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,795 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 21:25:47,800 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,802 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,803 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 21:25:47,808 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,809 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,810 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 21:25:47,815 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,816 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,817 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 21:25:47,822 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,824 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,824 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 21:25:47,830 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,831 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,832 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 21:25:47,837 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,840 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,840 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 21:25:47,845 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,847 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,848 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 21:25:47,852 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,853 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,854 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 21:25:47,859 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,861 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,862 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 21:25:47,868 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,869 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,870 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 21:25:47,875 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,877 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,877 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 21:25:47,883 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,884 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,885 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 21:25:47,890 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,892 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,893 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 21:25:47,898 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,899 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,900 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 21:25:47,905 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,906 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,907 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 21:25:47,912 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,913 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,914 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 21:25:47,919 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,920 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,921 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 21:25:47,926 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,927 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,928 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 21:25:47,933 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,934 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,934 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 21:25:47,938 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,941 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,942 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 21:25:47,948 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,949 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,950 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 21:25:47,956 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,957 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,958 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 21:25:47,963 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,964 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,965 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 21:25:47,969 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,970 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,971 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 21:25:47,977 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,978 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,978 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 21:25:47,988 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,990 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,990 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 21:25:47,996 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:47,997 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:47,997 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 21:25:48,001 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,003 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,003 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 21:25:48,009 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,010 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,010 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 21:25:48,016 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,017 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,017 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 21:25:48,021 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,022 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,022 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 21:25:48,027 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,028 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,029 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 21:25:48,033 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,034 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,035 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 21:25:48,040 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,041 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,042 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 21:25:48,046 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,048 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,048 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 21:25:48,054 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,055 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,055 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 21:25:48,062 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,063 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,064 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 21:25:48,068 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,069 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,070 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 21:25:48,074 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,075 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,075 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 21:25:48,079 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,080 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,081 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 21:25:48,085 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,086 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,087 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 21:25:48,092 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:25:48,093 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-5066882802958725320 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:25:48,093 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 21:25:48,100 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 21:25:48,101 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-05 21:25:48,102 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 21:25:48,103 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 21:25:48,103 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 21:25:48,104 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 21:25:48,104 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 21:25:48,105 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-05 21:25:48,105 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 21:25:48,107 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 2 milliseconds.
2017-07-05 21:25:48,107 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 21:25:48,108 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 21:25:48,108 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 21:25:48,111 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 3 milliseconds.
2017-07-05 21:25:48,111 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 21:25:48,112 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 21:25:48,112 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 21:25:48,115 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 2 milliseconds.
2017-07-05 21:25:48,115 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 21:25:48,118 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 3 milliseconds.
2017-07-05 21:25:48,118 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 21:25:48,121 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 3 milliseconds.
2017-07-05 21:25:48,122 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 21:25:48,125 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 3 milliseconds.
2017-07-05 21:25:48,126 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 21:25:48,129 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 3 milliseconds.
2017-07-05 21:25:48,130 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 21:25:48,134 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 4 milliseconds.
2017-07-05 21:25:48,135 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 21:25:48,139 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 4 milliseconds.
2017-07-05 21:25:48,139 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 21:25:48,143 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 3 milliseconds.
2017-07-05 21:25:48,143 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 21:25:48,148 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 3 milliseconds.
2017-07-05 21:25:48,148 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 21:25:48,150 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 2 milliseconds.
2017-07-05 21:25:48,150 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 21:25:48,151 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-05 21:25:48,151 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 21:25:48,236 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 85 milliseconds.
2017-07-05 21:25:48,237 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 21:25:48,238 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-05 21:25:48,238 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 21:25:48,239 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:25:48,240 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-05 21:25:48,240 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 21:25:48,241 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 21:25:48,242 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 21:25:48,242 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 21:25:48,242 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:25:48,243 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 21:25:48,243 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 21:25:48,245 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 21:25:48,246 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 3 milliseconds.
2017-07-05 21:25:48,246 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 21:25:48,247 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 21:25:48,248 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 21:25:48,249 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 21:25:48,249 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 21:25:48,250 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 21:25:48,250 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 21:25:48,251 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-05 21:25:48,251 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 21:25:48,252 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 21:25:48,253 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 21:25:48,254 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-05 21:25:48,254 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 21:25:48,255 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 21:25:48,255 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 21:25:48,256 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 21:25:48,257 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 21:25:48,258 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 21:25:48,258 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 21:25:48,259 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 21:25:48,259 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 21:25:48,260 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-05 21:25:48,261 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 21:25:48,262 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 21:25:48,262 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 21:25:48,263 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-05 21:25:48,263 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 21:25:48,265 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-05 21:25:48,266 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 21:25:48,267 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-05 21:25:48,267 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 21:25:48,272 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 4 milliseconds.
2017-07-05 21:25:48,273 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 21:25:48,274 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-05 21:25:48,274 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 21:25:48,275 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-05 21:25:48,275 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 21:25:48,277 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 2 milliseconds.
2017-07-05 21:25:48,277 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 21:25:48,279 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 2 milliseconds.
2017-07-05 21:25:48,279 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 21:25:48,280 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-05 21:25:48,280 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 21:25:48,281 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-05 21:25:48,282 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 21:25:48,283 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 1 milliseconds.
2017-07-05 21:25:48,283 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 21:25:48,284 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 1 milliseconds.
2017-07-05 21:25:48,284 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 21:25:48,285 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-05 21:25:48,348 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:25:48,348 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:25:48,349 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 21:25:48,349 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 21:25:48,351 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 21:25:48,354 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 21:25:48,355 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-05 21:25:59,602 INFO  [Thread-434] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-05 21:25:59,607 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 21:25:59,607 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 21:25:59,616 INFO  [kafka-request-handler-7] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 21:25:59,618 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 21:25:59,619 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 21:25:59,621 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 21:25:59,623 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 21:25:59,623 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 21:25:59,625 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 21:25:59,640 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 21:25:59,836 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 21:25:59,836 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 21:25:59,837 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 21:25:59,839 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 21:25:59,839 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 21:25:59,840 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 21:25:59,840 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 21:25:59,841 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 21:25:59,841 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 21:25:59,841 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:25:59,972 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:25:59,973 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:25:59,973 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:00,109 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:00,109 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:00,119 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 21:26:00,119 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:00,167 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:00,167 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:00,168 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 21:26:00,168 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:00,249 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:00,249 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:00,249 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:00,271 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:00,271 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:00,272 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 21:26:00,272 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 21:26:00,272 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 21:26:00,272 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 21:26:00,272 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 21:26:00,273 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 21:26:00,621 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 21:26:00,622 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 21:26:00,622 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 21:26:00,624 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 21:26:00,624 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 21:26:00,624 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 21:26:00,625 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 21:26:00,625 INFO  [ZkClient-EventThread-545-127.0.0.1:42277] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 21:26:00,626 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d143729a50001
2017-07-05 21:26:00,630 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:41322 which had sessionid 0x15d143729a50001
2017-07-05 21:26:00,630 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d143729a50001 closed
2017-07-05 21:26:00,630 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 21:26:00,631 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 21:26:00,631 INFO  [ZkClient-EventThread-542-127.0.0.1:42277] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 21:26:00,632 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d143729a50000
2017-07-05 21:26:00,633 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:41317 which had sessionid 0x15d143729a50000
2017-07-05 21:26:00,634 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d143729a50000 closed
2017-07-05 21:26:00,634 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 21:26:00,634 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 21:26:00,635 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 21:26:00,635 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 21:26:00,635 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 21:26:00,635 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 21:26:00,636 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 21:26:00,636 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 21:26:00,638 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 21:26:00,638 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 21:26:00,638 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 21:26:00,638 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 21:26:00,639 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 21:26:00,639 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.88 sec - in org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
Running org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
2017-07-05 21:26:00,645 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-6369834831764841308/version-2 snapdir /tmp/kafka-6234699572127094323/version-2
2017-07-05 21:26:00,646 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 21:26:00,656 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:50345 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@1d15c0a1
2017-07-05 21:26:00,656 INFO  [ZkClient-EventThread-593-127.0.0.1:50345] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 21:26:00,657 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 21:26:00,658 INFO  [main-SendThread(127.0.0.1:50345)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:50345. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 21:26:00,658 INFO  [main-SendThread(127.0.0.1:50345)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:50345, initiating session
2017-07-05 21:26:00,658 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:52649
2017-07-05 21:26:00,658 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:52649
2017-07-05 21:26:00,659 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 21:26:00,671 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d143763c70000 with negotiated timeout 10000 for client /127.0.0.1:52649
2017-07-05 21:26:00,672 INFO  [main-SendThread(127.0.0.1:50345)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:50345, sessionid = 0x15d143763c70000, negotiated timeout = 10000
2017-07-05 21:26:00,673 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 21:26:00,674 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-8188562053668700243
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:50345
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 21:26:00,676 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 21:26:00,679 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:50345
2017-07-05 21:26:00,680 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:50345 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@15639d09
2017-07-05 21:26:00,680 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 21:26:00,681 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 21:26:00,690 INFO  [main-SendThread(127.0.0.1:50345)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:50345. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 21:26:00,692 INFO  [main-SendThread(127.0.0.1:50345)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:50345, initiating session
2017-07-05 21:26:00,692 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:52659
2017-07-05 21:26:00,692 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:52659
2017-07-05 21:26:00,693 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d143763c70001 with negotiated timeout 6000 for client /127.0.0.1:52659
2017-07-05 21:26:00,694 INFO  [main-SendThread(127.0.0.1:50345)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:50345, sessionid = 0x15d143763c70001, negotiated timeout = 6000
2017-07-05 21:26:00,695 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 21:26:00,699 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 21:26:00,704 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 21:26:00,713 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 21:26:00,719 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 21:26:00,723 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = mOUoL-rJRzaID7Dav9c0bw
2017-07-05 21:26:00,723 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-8188562053668700243/meta.properties
2017-07-05 21:26:00,726 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 21:26:00,727 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 21:26:00,728 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 21:26:00,728 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 21:26:00,755 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 21:26:00,757 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 21:26:00,759 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 21:26:00,761 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 21:26:00,765 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 21:26:00,766 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 21:26:00,768 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:26:00,768 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:26:00,769 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 21:26:00,770 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 21:26:00,772 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 21:26:00,772 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 21:26:00,773 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 21:26:00,775 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 21:26:00,777 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 21:26:00,782 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 21:26:00,782 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 21:26:00,782 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 21:26:00,783 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 21:26:00,783 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 21:26:00,783 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 21:26:00,784 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 21:26:00,784 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 21:26:00,797 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 21:26:00,797 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 21:26:00,797 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 21:26:00,799 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 21:26:00,799 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 21:26:00,800 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 21:26:00,801 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 21:26:00,801 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 21:26:00,806 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 21:26:00,807 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 21:26:00,808 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 21:26:00,809 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 21:26:00,811 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:26:00,818 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:26:00,819 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 21:26:00,825 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 21:26:00,828 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 21:26:00,829 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-05 21:26:00,834 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7154 : {samoa_test-oos=INVALID_REPLICATION_FACTOR}
2017-07-05 21:26:00,834 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 58 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-05 21:26:00,841 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 21:26:00,846 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 21:26:00,847 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x42 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 21:26:00,847 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 21:26:00,849 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 21:26:00,850 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 21:26:00,850 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-8188562053668700243/meta.properties
2017-07-05 21:26:00,851 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 21:26:00,856 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 21:26:00,862 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 21:26:00,863 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 21:26:00,863 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:26:00,864 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:26:00,864 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 21:26:00,865 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 21:26:00,884 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-kdp Error:KeeperErrorCode = NoNode for /config/topics/test-kdp
2017-07-05 21:26:00,886 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:26:00,890 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:26:00,893 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 21:26:00,896 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 21:26:00,896 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2017-07-05 21:26:00,896 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:26:00,896 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:26:00,900 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:26:00,903 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-6
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 21:26:00,907 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-kdp)], deleted topics: [Set()], new partition replica assignment [Map([test-kdp,0] -> List(0))]
2017-07-05 21:26:00,907 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-kdp,0]
2017-07-05 21:26:00,917 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-kdp,0]
2017-07-05 21:26:00,917 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-kdp,0]
2017-07-05 21:26:00,918 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-05 21:26:00,919 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-kdp,0]
2017-07-05 21:26:00,920 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x4f zxid:0x20 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions/0
2017-07-05 21:26:00,921 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x50 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions
2017-07-05 21:26:00,924 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 21:26:00,924 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 21:26:00,927 WARN  [kafka-producer-network-thread | test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 1 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:00,932 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-05 21:26:00,934 INFO  [kafka-request-handler-5] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-kdp-0
2017-07-05 21:26:00,938 INFO  [kafka-request-handler-5] log.Log (Logging.scala:info(70)) - Completed load of log test-kdp-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:00,940 INFO  [kafka-request-handler-5] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-kdp,0] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:00,941 INFO  [kafka-request-handler-5] cluster.Partition (Logging.scala:info(70)) - Partition [test-kdp,0] on broker 0: No checkpointed highwatermark is found for partition test-kdp-0
2017-07-05 21:26:00,950 WARN  [Thread-439] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:00,951 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:setData cxid:0x61 zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-05 21:26:00,966 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x64 zxid:0x26 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:26:00,966 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:setData cxid:0x66 zxid:0x27 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 21:26:00,967 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x67 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:26:00,968 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:setData cxid:0x68 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 21:26:00,970 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x6b zxid:0x2c txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 21:26:00,971 INFO  [kafka-request-handler-7] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:26:00,972 INFO  [kafka-request-handler-7] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-05 21:26:00,972 INFO  [kafka-request-handler-6] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 21:26:00,973 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 59 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:00,982 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 21:26:00,989 INFO  [kafka-request-handler-6] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic samoa_test-oos with 1 partitions and replication factor 1 is successful
2017-07-05 21:26:00,990 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 21:26:00,991 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7155 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,002 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-05 21:26:01,021 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos, __consumer_offsets, test-s)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [test-s,0] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [samoa_test-oos,0] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 21:26:01,022 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,3],[__consumer_offsets,21],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[test-s,0],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[samoa_test-oos,0],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:26:01,033 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,3],[__consumer_offsets,21],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[test-s,0],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[samoa_test-oos,0],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:26:01,038 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,3],[__consumer_offsets,21],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[test-s,0],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[samoa_test-oos,0],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:26:01,040 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=test-s,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 21:26:01,078 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,3],[__consumer_offsets,21],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[test-s,0],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[samoa_test-oos,0],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 21:26:01,078 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xb2 zxid:0x31 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 21:26:01,080 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xb3 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 21:26:01,084 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xb7 zxid:0x36 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 21:26:01,087 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xba zxid:0x39 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 21:26:01,090 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xbd zxid:0x3c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 21:26:01,097 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xc3 zxid:0x3f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 21:26:01,098 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7157 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,103 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xc9 zxid:0x42 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 21:26:01,106 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xcc zxid:0x45 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 21:26:01,109 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xcf zxid:0x48 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 21:26:01,122 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xd2 zxid:0x4b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 21:26:01,131 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xd5 zxid:0x4e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 21:26:01,135 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xd8 zxid:0x51 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 21:26:01,138 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xdb zxid:0x54 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 21:26:01,142 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xde zxid:0x57 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 21:26:01,145 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xe1 zxid:0x5a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 21:26:01,148 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xe4 zxid:0x5d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 21:26:01,151 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xe7 zxid:0x60 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 21:26:01,155 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xea zxid:0x63 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 21:26:01,160 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xef zxid:0x66 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 21:26:01,170 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xf3 zxid:0x69 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 21:26:01,179 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xf6 zxid:0x6c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 21:26:01,182 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xf9 zxid:0x6f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 21:26:01,186 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xfc zxid:0x72 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 21:26:01,189 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0xff zxid:0x75 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 21:26:01,192 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x102 zxid:0x78 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 21:26:01,195 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x105 zxid:0x7b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 21:26:01,199 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x109 zxid:0x7e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 21:26:01,202 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x10d zxid:0x81 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 21:26:01,205 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7159 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,205 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x111 zxid:0x84 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 21:26:01,206 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x113 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 21:26:01,222 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x119 zxid:0x89 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 21:26:01,238 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x11c zxid:0x8c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 21:26:01,241 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x11f zxid:0x8f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 21:26:01,244 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x122 zxid:0x92 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 21:26:01,249 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x125 zxid:0x95 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 21:26:01,282 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x12b zxid:0x98 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 21:26:01,290 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x12e zxid:0x9b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 21:26:01,298 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x131 zxid:0x9e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 21:26:01,302 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x134 zxid:0xa1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 21:26:01,306 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x137 zxid:0xa4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 21:26:01,320 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x13d zxid:0xa7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 21:26:01,323 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7161 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,332 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x142 zxid:0xaa txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 21:26:01,353 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x146 zxid:0xad txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 21:26:01,359 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x149 zxid:0xb0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 21:26:01,371 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x14f zxid:0xb3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 21:26:01,376 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x152 zxid:0xb6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 21:26:01,390 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x155 zxid:0xb9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 21:26:01,395 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x158 zxid:0xbc txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 21:26:01,400 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x15b zxid:0xbf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 21:26:01,404 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x15e zxid:0xc2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 21:26:01,408 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x161 zxid:0xc5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-05 21:26:01,409 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x162 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-05 21:26:01,424 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x166 zxid:0xca txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 21:26:01,435 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7163 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,439 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x16e zxid:0xcd txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 21:26:01,452 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d143763c70001 type:create cxid:0x172 zxid:0xd0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 21:26:01,472 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=test-s,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 21:26:01,497 INFO  [kafka-request-handler-2] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,test-s-0,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,samoa_test-oos-0,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 21:26:01,502 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,503 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,503 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 21:26:01,510 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,511 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,511 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 21:26:01,515 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,525 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,525 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 21:26:01,529 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,530 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,530 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 21:26:01,540 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,540 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,541 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 21:26:01,547 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7165 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,550 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,556 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,556 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 21:26:01,560 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,561 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,561 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 21:26:01,575 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,575 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,576 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 21:26:01,583 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,583 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,584 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 21:26:01,589 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,589 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,590 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 21:26:01,606 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,607 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,607 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 21:26:01,611 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,625 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,625 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 21:26:01,629 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,629 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,630 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 21:26:01,635 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,636 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,636 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 21:26:01,639 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,640 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,641 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 21:26:01,647 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,653 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,653 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 21:26:01,657 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7167 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,660 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,662 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,663 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 21:26:01,667 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,677 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,677 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 21:26:01,687 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,690 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,690 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 21:26:01,698 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,700 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,700 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 21:26:01,714 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,715 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,715 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 21:26:01,722 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,723 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,723 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 21:26:01,727 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,728 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,732 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 21:26:01,739 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,739 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,740 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 21:26:01,747 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,748 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,748 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 21:26:01,755 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,755 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,756 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 21:26:01,764 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,765 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,765 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 21:26:01,777 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7169 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:01,787 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,788 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,788 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 21:26:01,809 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,811 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,812 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 21:26:01,817 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,820 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,821 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 21:26:01,827 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,831 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,833 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 21:26:01,843 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,844 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,845 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-05 21:26:01,853 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:01,854 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:01,856 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 21:26:02,405 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,406 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,406 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 21:26:02,439 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,440 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7171 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:02,441 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,442 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 21:26:02,449 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,450 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,451 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 21:26:02,460 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,461 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,468 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 21:26:02,474 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,475 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,475 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 21:26:02,479 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,479 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,480 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 21:26:02,484 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,485 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,485 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 21:26:02,489 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,490 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,490 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 21:26:02,503 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,504 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,504 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 21:26:02,510 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,511 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,516 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 21:26:02,523 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,524 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,524 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 21:26:02,530 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,531 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,531 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 21:26:02,539 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,540 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,540 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 21:26:02,552 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,553 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,553 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 21:26:02,557 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 7173 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 21:26:02,566 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,567 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,567 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 21:26:02,574 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,574 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,575 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 21:26:02,578 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,579 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,579 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 21:26:02,592 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,595 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,595 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 21:26:02,600 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 21:26:02,601 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-8188562053668700243 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 21:26:02,602 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 21:26:02,604 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 21:26:02,605 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-05 21:26:02,606 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 21:26:02,607 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 21:26:02,608 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 21:26:02,609 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 21:26:02,610 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 21:26:02,611 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-05 21:26:02,612 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 21:26:02,614 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 21:26:02,614 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 21:26:02,616 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 21:26:02,616 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 21:26:02,617 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-05 21:26:02,619 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 21:26:02,620 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 21:26:02,621 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 21:26:02,622 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-05 21:26:02,622 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 21:26:02,624 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 21:26:02,624 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 21:26:02,628 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 4 milliseconds.
2017-07-05 21:26:02,628 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 21:26:02,629 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 1 milliseconds.
2017-07-05 21:26:02,629 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 21:26:02,634 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 5 milliseconds.
2017-07-05 21:26:02,634 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 21:26:02,635 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-05 21:26:02,636 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 21:26:02,637 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 1 milliseconds.
2017-07-05 21:26:02,637 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 21:26:02,637 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:26:02,646 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 21:26:02,646 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:26:02,652 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 14 milliseconds.
2017-07-05 21:26:02,652 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 21:26:02,653 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-05 21:26:02,653 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 21:26:02,654 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 0 milliseconds.
2017-07-05 21:26:02,655 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 21:26:02,659 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:26:02,660 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 21:26:02,661 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-05 21:26:02,662 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 21:26:02,663 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-05 21:26:02,663 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 21:26:02,664 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-05 21:26:02,664 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 21:26:02,669 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 21:26:02,672 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 8 milliseconds.
2017-07-05 21:26:02,674 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 21:26:02,675 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 21:26:02,675 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 21:26:02,676 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 21:26:02,676 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 21:26:02,682 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 21:26:02,683 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 21:26:02,684 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 21:26:02,685 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 21:26:02,686 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 21:26:02,686 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 21:26:02,688 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 2 milliseconds.
2017-07-05 21:26:02,688 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 21:26:02,690 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 2 milliseconds.
2017-07-05 21:26:02,690 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 21:26:02,692 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 21:26:02,692 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 21:26:02,695 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 3 milliseconds.
2017-07-05 21:26:02,696 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 21:26:02,698 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 2 milliseconds.
2017-07-05 21:26:02,699 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 21:26:02,700 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 21:26:02,700 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 21:26:02,701 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 21:26:02,701 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 21:26:02,702 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 21:26:02,702 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 21:26:02,703 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-05 21:26:02,704 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 21:26:02,705 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 21:26:02,705 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 21:26:02,706 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-05 21:26:02,706 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 21:26:02,707 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-05 21:26:02,707 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 21:26:02,708 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-05 21:26:02,708 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 21:26:02,709 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 1 milliseconds.
2017-07-05 21:26:02,709 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 21:26:02,710 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 0 milliseconds.
2017-07-05 21:26:02,711 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 21:26:02,712 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-05 21:26:02,712 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 21:26:02,714 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 2 milliseconds.
2017-07-05 21:26:02,715 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 21:26:02,716 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 1 milliseconds.
2017-07-05 21:26:02,717 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 21:26:02,718 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-05 21:26:02,719 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 21:26:02,719 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 0 milliseconds.
2017-07-05 21:26:02,720 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 21:26:02,722 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 2 milliseconds.
2017-07-05 21:26:02,722 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 21:26:02,724 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 2 milliseconds.
2017-07-05 21:26:02,724 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 21:26:02,726 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 2 milliseconds.
2017-07-05 21:26:02,760 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 21:26:02,760 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 21:26:02,761 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 21:26:02,761 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 21:26:02,764 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 21:26:02,773 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 21:26:02,773 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-05 21:26:10,830 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-05 21:26:10,830 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 2 is now empty
2017-07-05 21:26:12,828 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 21:26:12,828 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 21:26:12,835 INFO  [kafka-request-handler-1] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 21:26:12,840 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 21:26:12,840 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 21:26:12,845 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 21:26:12,845 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 21:26:12,846 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 21:26:12,847 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 21:26:13,747 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 21:26:13,747 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 21:26:13,748 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 21:26:14,739 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 21:26:14,739 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 21:26:14,739 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 21:26:14,740 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 21:26:14,740 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 21:26:14,740 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 21:26:14,740 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:14,931 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:14,931 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:14,932 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:15,020 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:15,021 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:15,121 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 21:26:15,121 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:15,220 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:15,221 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:15,221 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 21:26:15,222 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:15,382 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:15,382 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:15,383 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 21:26:15,424 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 21:26:15,424 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 21:26:15,424 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 21:26:15,424 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 21:26:15,424 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 21:26:15,425 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 21:26:15,425 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 21:26:15,425 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 21:26:15,967 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 21:26:15,968 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 21:26:15,968 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 21:26:15,970 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 21:26:15,970 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 21:26:15,970 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 21:26:15,971 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 21:26:15,971 INFO  [ZkClient-EventThread-596-127.0.0.1:50345] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 21:26:15,972 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d143763c70001
2017-07-05 21:26:15,981 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d143763c70001 closed
2017-07-05 21:26:15,982 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:52659 which had sessionid 0x15d143763c70001
2017-07-05 21:26:15,983 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 21:26:15,984 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 21:26:15,984 INFO  [ZkClient-EventThread-593-127.0.0.1:50345] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 21:26:15,985 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d143763c70000
2017-07-05 21:26:15,986 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 21:26:15,986 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d143763c70000 closed
2017-07-05 21:26:15,986 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 21:26:15,986 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:52649 which had sessionid 0x15d143763c70000
2017-07-05 21:26:15,987 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 21:26:15,988 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 21:26:15,988 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 21:26:15,988 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 21:26:15,989 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 21:26:15,989 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 21:26:15,989 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 21:26:15,990 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 21:26:15,990 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 21:26:15,990 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 21:26:15,990 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 21:26:15,991 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.349 sec - in org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
Running org.apache.samoa.core.DoubleVectorTest
2017-07-05 21:26:16,000 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec - in org.apache.samoa.core.DoubleVectorTest

Results :

Tests run: 22, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-test 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-test ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-test ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250359024/samoa-test/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250359024/samoa-test/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-test ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250359024/samoa-test/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-local 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-local ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-local ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250359024/samoa-local/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18:test (default-test) @ samoa-local ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250359024/samoa-local/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom (3 KB at 115.8 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom (3 KB at 116.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar (67 KB at 1842.3 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.topology.impl.SimpleEngineTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.24 sec - in org.apache.samoa.topology.impl.SimpleEngineTest
Running org.apache.samoa.topology.impl.SimpleStreamTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.17 sec - in org.apache.samoa.topology.impl.SimpleStreamTest
Running org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.047 sec - in org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Running org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.378 sec <<< FAILURE! - in org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
testStartSendingEvents(org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest)  Time elapsed: 0.32 sec  <<< ERROR!
java.lang.IllegalStateException: Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest$4.<init>(SimpleEntranceProcessingItemTest.java:159)
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest.testStartSendingEvents(SimpleEntranceProcessingItemTest.java:155)

Running org.apache.samoa.topology.impl.SimpleTopologyTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.126 sec - in org.apache.samoa.topology.impl.SimpleTopologyTest
Running org.apache.samoa.topology.impl.SimpleProcessingItemTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.064 sec - in org.apache.samoa.topology.impl.SimpleProcessingItemTest
Running org.apache.samoa.AlgosTest
2017-07-05 21:26:21,490 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test4135167932581714780test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test4135167932581714780test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-05 21:26:21,536 [pool-1-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 21:26:22,728 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-05 21:26:22,738 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 65.645
Kappa Statistic (percent) = 27.592
Kappa Temporal Statistic (percent) = 30.173
2017-07-05 21:26:23,220 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:23,220 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 70.69
Kappa Statistic (percent) = 39.422
Kappa Temporal Statistic (percent) = 40.272
2017-07-05 21:26:23,548 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:23,549 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 73.663
Kappa Statistic (percent) = 45.786
Kappa Temporal Statistic (percent) = 46.436
2017-07-05 21:26:23,747 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:23,747 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 75.545
Kappa Statistic (percent) = 49.77
Kappa Temporal Statistic (percent) = 50.387
2017-07-05 21:26:24,070 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:24,071 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 77.083
Kappa Statistic (percent) = 53.033
Kappa Temporal Statistic (percent) = 53.669
2017-07-05 21:26:24,384 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:24,385 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 78.397
Kappa Statistic (percent) = 55.776
Kappa Temporal Statistic (percent) = 56.372
2017-07-05 21:26:24,586 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:24,587 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 79.531
Kappa Statistic (percent) = 58.117
Kappa Temporal Statistic (percent) = 58.646
2017-07-05 21:26:24,836 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:24,836 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 80.527
Kappa Statistic (percent) = 60.173
Kappa Temporal Statistic (percent) = 60.566
2017-07-05 21:26:25,196 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:25,197 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 81.387
Kappa Statistic (percent) = 61.939
Kappa Temporal Statistic (percent) = 62.332
2017-07-05 21:26:25,465 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:25,465 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 82.144
Kappa Statistic (percent) = 63.505
Kappa Temporal Statistic (percent) = 63.842
2017-07-05 21:26:25,466 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 21:26:25,466 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 21:26:25,467 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,65.645,27.592487187843105,30.172764227642272
40000.0,40000.0,70.69,39.42219140754782,40.27204646186763
60000.0,60000.0,73.66333333333334,45.78556998992169,46.43571404359175
80000.0,80000.0,75.545,49.770097068022935,50.386731925037395
100000.0,100000.0,77.083,53.033355520313066,53.66933527413876
120000.0,120000.0,78.3975,55.77588004396512,56.37180652327577
140000.0,140000.0,79.53071428571428,58.11662806596387,58.64611743654127
160000.0,160000.0,80.526875,60.17259494934151,60.56625026894989
180000.0,180000.0,81.38666666666666,61.93941888230634,62.3317780650964
200000.0,200000.0,82.1435,63.50464751569946,63.842259795484466

2017-07-05 21:26:25,467 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 3 seconds for 200000 instances
2017-07-05 21:26:31,519 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test4135167932581714780test
2017-07-05 21:26:31,543 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test5777503575017433327test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=60.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test5777503575017433327test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)
2017-07-05 21:26:31,557 [pool-4-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 21:26:33,181 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-05 21:26:33,182 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 85.07
Kappa Statistic (percent) = 69.417
Kappa Temporal Statistic (percent) = 69.251
2017-07-05 21:26:33,953 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:33,954 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 88.648
Kappa Statistic (percent) = 76.744
Kappa Temporal Statistic (percent) = 76.689
2017-07-05 21:26:34,845 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:34,845 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 90.497
Kappa Statistic (percent) = 80.591
Kappa Temporal Statistic (percent) = 80.57
2017-07-05 21:26:35,618 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:35,619 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 91.769
Kappa Statistic (percent) = 83.198
Kappa Temporal Statistic (percent) = 83.284
2017-07-05 21:26:36,325 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:36,325 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 92.723
Kappa Statistic (percent) = 85.148
Kappa Temporal Statistic (percent) = 85.193
2017-07-05 21:26:37,103 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:37,104 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 93.433
Kappa Statistic (percent) = 86.593
Kappa Temporal Statistic (percent) = 86.619
2017-07-05 21:26:37,898 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:37,899 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 94.006
Kappa Statistic (percent) = 87.765
Kappa Temporal Statistic (percent) = 87.777
2017-07-05 21:26:38,735 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:38,735 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 94.466
Kappa Statistic (percent) = 88.702
Kappa Temporal Statistic (percent) = 88.695
2017-07-05 21:26:39,520 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:39,521 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 94.829
Kappa Statistic (percent) = 89.444
Kappa Temporal Statistic (percent) = 89.435
2017-07-05 21:26:40,299 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:40,300 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 95.125
Kappa Statistic (percent) = 90.05
Kappa Temporal Statistic (percent) = 90.038
2017-07-05 21:26:40,302 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 21:26:40,302 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 21:26:40,303 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,85.07000000000001,69.41665276367094,69.25136443208733
40000.0,40000.0,88.64750000000001,76.74394784416755,76.68891170431212
60000.0,60000.0,90.49666666666667,80.59059840788376,80.56975396987664
80000.0,80000.0,91.76875,83.19754375800399,83.28425648575926
100000.0,100000.0,92.723,85.14783110089526,85.19279682571981
120000.0,120000.0,93.43333333333334,86.59269924629635,86.61866594212744
140000.0,140000.0,94.00571428571428,87.76487469612945,87.77727610364263
160000.0,160000.0,94.465625,88.70239125036883,88.69454197255027
180000.0,180000.0,94.82944444444445,89.44442627515313,89.43478902498553
200000.0,200000.0,95.125,90.04969296410039,90.03831417624522

2017-07-05 21:26:40,305 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 8 seconds for 200000 instances
2017-07-05 21:26:41,544 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test5777503575017433327test
2017-07-05 21:26:41,548 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test2478042638449975466test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=65.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test2478042638449975466test -i 200000 -f 20000 -w 0 -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)
2017-07-05 21:26:41,567 [pool-16-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 21:26:41,811 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:41,811 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 84.275
Kappa Statistic (percent) = 68.502
Kappa Temporal Statistic (percent) = 68.778
2017-07-05 21:26:41,977 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:41,978 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 83.803
Kappa Statistic (percent) = 67.594
Kappa Temporal Statistic (percent) = 67.819
2017-07-05 21:26:42,164 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:42,165 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 83.59
Kappa Statistic (percent) = 67.214
Kappa Temporal Statistic (percent) = 67.392
2017-07-05 21:26:42,326 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:42,327 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 83.546
Kappa Statistic (percent) = 67.127
Kappa Temporal Statistic (percent) = 67.269
2017-07-05 21:26:42,488 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:42,489 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 83.559
Kappa Statistic (percent) = 67.135
Kappa Temporal Statistic (percent) = 67.209
2017-07-05 21:26:42,653 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:42,654 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 83.531
Kappa Statistic (percent) = 67.072
Kappa Temporal Statistic (percent) = 67.151
2017-07-05 21:26:42,824 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:42,824 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 83.504
Kappa Statistic (percent) = 67.03
Kappa Temporal Statistic (percent) = 67.054
2017-07-05 21:26:42,986 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:42,986 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 83.518
Kappa Statistic (percent) = 67.05
Kappa Temporal Statistic (percent) = 67.065
2017-07-05 21:26:43,146 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:43,147 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 83.54
Kappa Statistic (percent) = 67.089
Kappa Temporal Statistic (percent) = 67.126
2017-07-05 21:26:43,308 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 21:26:43,309 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 83.523
Kappa Statistic (percent) = 67.057
Kappa Temporal Statistic (percent) = 67.143
2017-07-05 21:26:43,309 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 21:26:43,309 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 21:26:43,310 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,84.275,68.50233079747552,68.77792117541944
40000.0,40000.0,83.80250000000001,67.59439866955276,67.81900362588786
60000.0,60000.0,83.59,67.21382484118784,67.39195230998509
80000.0,80000.0,83.54625,67.12663795918682,67.26924607121543
100000.0,100000.0,83.55900000000001,67.13483590334214,67.20915853926087
120000.0,120000.0,83.53083333333333,67.07192304919516,67.15145269596435
140000.0,140000.0,83.50357142857143,67.02990305706534,67.05373828442632
160000.0,160000.0,83.518125,67.04964827201962,67.06465673356729
180000.0,180000.0,83.54,67.08882037897833,67.1260235670062
200000.0,200000.0,83.5235,67.05662351796806,67.142614990378

2017-07-05 21:26:43,310 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 1 seconds for 200000 instances
2017-07-05 21:26:51,550 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test2478042638449975466test
2017-07-05 21:26:51,552 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test3777355101289303536test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialCVEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialCVEvaluation -d /tmp/test3777355101289303536test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-05 21:26:51,561 [pool-18-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialCVEvaluation
2017-07-05 21:26:54,064 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:26:54,065 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 20,000
[avg] classified instances = 20,000
[err] classified instances = 0
[avg] classifications correct (percent) = 64.627
[err] classifications correct (percent) = 0.189
[avg] Kappa Statistic (percent) = 25.294
[err] Kappa Statistic (percent) = 0.352
[avg] Kappa Temporal Statistic (percent) = 28.104
[err] Kappa Temporal Statistic (percent) = 0.384
2017-07-05 21:26:56,824 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:26:56,827 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 40,000
[avg] classified instances = 40,000
[err] classified instances = 0
[avg] classifications correct (percent) = 70.032
[err] classifications correct (percent) = 0.203
[avg] Kappa Statistic (percent) = 38.074
[err] Kappa Statistic (percent) = 0.445
[avg] Kappa Temporal Statistic (percent) = 38.931
[err] Kappa Temporal Statistic (percent) = 0.414
2017-07-05 21:26:59,753 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:26:59,755 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 60,000
[avg] classified instances = 60,000
[err] classified instances = 0
[avg] classifications correct (percent) = 73.498
[err] classifications correct (percent) = 0.308
[avg] Kappa Statistic (percent) = 45.523
[err] Kappa Statistic (percent) = 0.671
[avg] Kappa Temporal Statistic (percent) = 46.1
[err] Kappa Temporal Statistic (percent) = 0.627
2017-07-05 21:27:02,366 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:27:02,367 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 80,000
[avg] classified instances = 80,000
[err] classified instances = 0
[avg] classifications correct (percent) = 75.64
[err] classifications correct (percent) = 0.397
[avg] Kappa Statistic (percent) = 50.074
[err] Kappa Statistic (percent) = 0.86
[avg] Kappa Temporal Statistic (percent) = 50.579
[err] Kappa Temporal Statistic (percent) = 0.806
2017-07-05 21:27:05,002 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:27:05,003 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 100,000
[avg] classified instances = 100,000
[err] classified instances = 0
[avg] classifications correct (percent) = 77.273
[err] classifications correct (percent) = 0.442
[avg] Kappa Statistic (percent) = 53.513
[err] Kappa Statistic (percent) = 0.947
[avg] Kappa Temporal Statistic (percent) = 54.053
[err] Kappa Temporal Statistic (percent) = 0.894
2017-07-05 21:27:07,802 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:27:07,803 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 120,000
[avg] classified instances = 120,000
[err] classified instances = 0
[avg] classifications correct (percent) = 78.553
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 56.166
[err] Kappa Statistic (percent) = 0.983
[avg] Kappa Temporal Statistic (percent) = 56.687
[err] Kappa Temporal Statistic (percent) = 0.938
2017-07-05 21:27:10,942 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 21:27:10,943 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 140,000
[avg] classified instances = 140,000
[err] classified instances = 0
[avg] classifications correct (percent) = 79.767
[err] classifications correct (percent) = 0.471
[avg] Kappa Statistic (percent) = 58.667
[err] Kappa Statistic (percent) = 0.991
[avg] Kappa Temporal Statistic (percent) = 59.123
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-05 21:27:13,263 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:27:13,264 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 160,000
[avg] classified instances = 160,000
[err] classified instances = 0
[avg] classifications correct (percent) = 80.8
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 60.79
[err] Kappa Statistic (percent) = 0.973
[avg] Kappa Temporal Statistic (percent) = 61.12
[err] Kappa Temporal Statistic (percent) = 0.939
2017-07-05 21:27:15,885 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 21:27:15,886 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 180,000
[avg] classified instances = 180,000
[err] classified instances = 0
[avg] classifications correct (percent) = 81.614
[err] classifications correct (percent) = 0.462
[avg] Kappa Statistic (percent) = 62.451
[err] Kappa Statistic (percent) = 0.969
[avg] Kappa Temporal Statistic (percent) = 62.792
[err] Kappa Temporal Statistic (percent) = 0.936
2017-07-05 21:27:19,533 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 21:27:19,534 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 200,000
[avg] classified instances = 200,000.9
[err] classified instances = 0.9
[avg] classifications correct (percent) = 82.333
[err] classifications correct (percent) = 0.47
[avg] Kappa Statistic (percent) = 63.93
[err] Kappa Statistic (percent) = 0.984
[avg] Kappa Temporal Statistic (percent) = 64.226
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-05 21:27:19,535 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:215) - last event is received!
2017-07-05 21:27:19,535 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:216) - total count: 200001
2017-07-05 21:27:19,535 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:219) - org.apache.samoa.evaluation.EvaluatorCVProcessorid = 0
evaluation instances,[avg] classified instances,[err] classified instances,[avg] classifications correct (percent),[err] classifications correct (percent),[avg] Kappa Statistic (percent),[err] Kappa Statistic (percent),[avg] Kappa Temporal Statistic (percent),[err] Kappa Temporal Statistic (percent)
20000.0,20000.0,0.0,64.62700000000001,0.18916071943661472,25.293690418855896,0.3519627618187893,28.103658536585368,0.38447300698498993
40000.0,40000.0,0.0,70.032,0.20324410610560478,38.07419026287571,0.44506467062749777,38.931173264048084,0.4141710858537992
60000.0,60000.0,0.0,73.49833333333333,0.3080789741624503,45.52336429068201,0.6710378342453998,46.10013219890851,0.6265800633791055
80000.0,80000.0,0.0,75.63975,0.3973700346584335,50.07382624902415,0.8597101949765279,50.57895671138387,0.8061674935377634
100000.0,100000.0,0.0,77.27289999999999,0.4422633566854323,53.51347314843381,0.9469128869060368,54.05325084910237,0.8941115896115002
120000.0,120000.0,0.0,78.5535,0.4643362247656641,56.1661763049406,0.9830956362259596,56.68686256689892,0.9377688069588297
140000.0,140000.0,0.0,79.76700000000001,0.47139196346560175,58.66745091659613,0.9910982373644812,59.12348297906114,0.9523482240960528
160000.0,160000.0,0.0,80.80025000000002,0.4637947455202826,60.789541927185965,0.9729687801934683,61.11984407234435,0.9392003554346215
180000.0,180000.0,0.0,81.61388888888888,0.46241870972111077,62.451145738939225,0.9693840059052787,62.791612794423514,0.9358071589161842
200000.0,200000.9,0.8999999999996271,82.33328358598864,0.4701473780403362,63.930181311539584,0.9842393397954078,64.2263845297155,0.9520407427484003

2017-07-05 21:27:19,536 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:223) - total evaluation time: 27 seconds for 200001 instances
2017-07-05 21:27:19,574 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test3777355101289303536test
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 58.472 sec - in org.apache.samoa.AlgosTest

Results :


Tests in error: 
  Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope


Tests run: 28, Failures: 0, Errors: 1, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache SAMOA ....................................... SUCCESS [  3.537 s]
[INFO] samoa-instances .................................... SUCCESS [  3.549 s]
[INFO] samoa-api .......................................... SUCCESS [01:06 min]
[INFO] samoa-test ......................................... SUCCESS [  1.709 s]
[INFO] samoa-local ........................................ FAILURE [01:01 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:17 min
[INFO] Finished at: 2017-07-05T21:27:19+02:00
[INFO] Final Memory: 37M/985M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18:test (default-test) on project samoa-local: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/apache/incubator-samoa/250359024/samoa-local/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :samoa-local
