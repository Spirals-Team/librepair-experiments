[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-common:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354187058/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-client:jar:0.4.2-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hbase:hbase-client:jar -> version (?) vs 1.2.3 @ com.uber.hoodie:hoodie-client:[unknown-version], /root/workspace/uber/hudi/354187058/hoodie-client/pom.xml, line 193, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354187058/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-cli:jar:0.4.2-SNAPSHOT
[WARNING] 'dependencies.dependency.systemPath' for dnl.utils:textutils:jar should not point at files within the project directory, ${basedir}/lib/dnl/utils/textutils/0.3.3/textutils-0.3.3.jar will be unresolvable by dependent projects @ com.uber.hoodie:hoodie-cli:[unknown-version], /root/workspace/uber/hudi/354187058/hoodie-cli/pom.xml, line 162, column 19
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354187058/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hadoop-mr:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354187058/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hive:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.2-SNAPSHOT, /root/workspace/uber/hudi/354187058/pom.xml, line 154, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-utilities:jar:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie-utilities:[unknown-version], /root/workspace/uber/hudi/354187058/hoodie-utilities/pom.xml, line 35, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie:pom:0.4.2-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ line 154, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Hoodie
[INFO] hoodie-common
[INFO] hoodie-hadoop-mr
[INFO] hoodie-client
[INFO] hoodie-spark
[INFO] hoodie-hive
[INFO] hoodie-utilities
[INFO] hoodie-cli
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hoodie 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-common 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-common ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/354187058/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/354187058/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- avro-maven-plugin:1.7.6:schema (default) @ hoodie-common ---
[INFO] Importing File: /root/workspace/uber/hudi/354187058/hoodie-common/src/main/avro/HoodieCommitMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354187058/hoodie-common/src/main/avro/HoodieSavePointMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354187058/hoodie-common/src/main/avro/HoodieCompactionMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354187058/hoodie-common/src/main/avro/HoodieCleanMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/354187058/hoodie-common/src/main/avro/HoodieRollbackMetadata.avsc
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/354187058/hoodie-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-common ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 85 source files to /root/workspace/uber/hudi/354187058/hoodie-common/target/classes
[WARNING] /root/workspace/uber/hudi/354187058/hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieCommitMetadata.java: Some input files use or override a deprecated API.
[WARNING] /root/workspace/uber/hudi/354187058/hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieCommitMetadata.java: Recompile with -Xlint:deprecation for details.
[WARNING] /root/workspace/uber/hudi/354187058/hoodie-common/target/generated-sources/src/main/java/com/uber/hoodie/avro/model/HoodieSavepointPartitionMetadata.java: Some input files use unchecked or unsafe operations.
[WARNING] /root/workspace/uber/hudi/354187058/hoodie-common/target/generated-sources/src/main/java/com/uber/hoodie/avro/model/HoodieSavepointPartitionMetadata.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-common ---
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom (4 KB at 7.0 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom (3 KB at 139.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar (74 KB at 1673.9 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.common.util.collection.TestExternalSpillableMap
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.056 sec - in com.uber.hoodie.common.util.collection.TestExternalSpillableMap
Running com.uber.hoodie.common.util.collection.TestDiskBasedMap
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.217 sec - in com.uber.hoodie.common.util.collection.TestDiskBasedMap
Running com.uber.hoodie.common.util.TestParquetUtils
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.51 sec - in com.uber.hoodie.common.util.TestParquetUtils
Running com.uber.hoodie.common.util.TestFSUtils
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.029 sec - in com.uber.hoodie.common.util.TestFSUtils
Running com.uber.hoodie.common.util.TestNumericUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.uber.hoodie.common.util.TestNumericUtils
Running com.uber.hoodie.common.TestBloomFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec - in com.uber.hoodie.common.TestBloomFilter
Running com.uber.hoodie.common.table.log.HoodieLogFormatTest
844  [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - Cleaning HDFS cluster data at: /tmp/1521184704527-0/dfs and starting fresh.
850  [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS force binding to ip: 127.0.0.1
Formatting using clusterid: testClusterID
6269 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
9020 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS Minicluster service started.
9055 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper force binding to: 127.0.0.1
9106 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper Minicluster service started on client port: 2828
9161 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
9919 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
9932 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
10017 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
10017 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
10021 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
10022 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5789635313641499616
10043 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5789635313641499616 as 1
10043 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5789635313641499616/.test-fileid1_100.log.1
10048 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5789635313641499616/.test-fileid1_100.log.1} does not exist. Create a new file
10145 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
10153 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
10159 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
10254 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1}
10257 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10260 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1}
10260 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10263 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1}
10263 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10265 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1}
10265 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10265 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10265 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10265 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10265 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
10268 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1}
10268 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit5789635313641499616/.test-fileid1_100.log.1
10268 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
10268 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
10268 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
10269 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
10269 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
10269 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
10291 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
10722 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
10731 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
10741 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
10741 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
10742 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
10742 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5541699417430500284
10745 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5541699417430500284 as 1
10745 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5541699417430500284/.test-fileid1_100.log.1
10747 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5541699417430500284/.test-fileid1_100.log.1} does not exist. Create a new file
11206 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
11638 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
11647 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
11655 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
11655 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
11656 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
11656 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4747008176823625737
11659 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4747008176823625737 as 1
11660 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4747008176823625737/.test-fileid1_100.log.1
11662 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4747008176823625737/.test-fileid1_100.log.1} does not exist. Create a new file
12106 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
12136 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
12144 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
12150 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
12150 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
12150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
12150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1568080365417245383
12152 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1568080365417245383 as 1
12152 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1568080365417245383/.test-fileid1_100.log.1
12154 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1568080365417245383/.test-fileid1_100.log.1} does not exist. Create a new file
12176 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
12176 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1568080365417245383
12180 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1568080365417245383 as 1
12180 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1568080365417245383/.test-fileid1_100.log.1
12182 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1568080365417245383/.test-fileid1_100.log.1} exists. Appending to existing file
12188 [IPC Server handler 5 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit1568080365417245383/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
12189 [IPC Server handler 5 on 34676] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit1568080365417245383/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
12196 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit1568080365417245383/.test-fileid1_100.log.1
12202 [IPC Server handler 3 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1568080365417245383/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1009 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c:NORMAL:127.0.0.1:50010|RBW]]}
13210 [IPC Server handler 6 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1568080365417245383/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1010 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c:NORMAL:127.0.0.1:50010|RBW]]}
14214 [IPC Server handler 7 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1568080365417245383/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1011 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c:NORMAL:127.0.0.1:50010|RBW]]}
14992 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57506 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741832_1008]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57506 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:57506]. 57188 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
14992 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741832_1008] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741832_1008
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
15217 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit1568080365417245383/.test-fileid1_100.log.1
16280 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16712 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16720 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16727 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16727 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16728 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16728 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9126147993927624356
16730 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9126147993927624356 as 1
16730 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9126147993927624356/.test-fileid1_100.log.1
16732 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9126147993927624356/.test-fileid1_100.log.1} does not exist. Create a new file
16798 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16798 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9126147993927624356
16800 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9126147993927624356 as 1
16801 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9126147993927624356/.test-fileid1_100.log.1
16802 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9126147993927624356/.test-fileid1_100.log.1} exists. Appending to existing file
17261 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
17269 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
17277 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
17286 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1}
17286 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1
17289 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1} has a corrupted block at 9289
18053 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1} starts at 9853
18054 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1}
18054 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1
18055 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1}
18055 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1
18055 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1
18055 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1
18055 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
18056 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1}
18056 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit9126147993927624356/.test-fileid1_100.log.1
18056 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
18079 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18079 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
18079 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
18079 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
18079 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71815
18103 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18526 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18534 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18538 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18538 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18571 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18571 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit4002018561082700851/append_test
18572 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit4002018561082700851/append_test as 1
18572 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit4002018561082700851/append_test/.commits.archive_.archive.1
18572 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit4002018561082700851/append_test/.commits.archive_.archive.1} does not exist. Create a new file
18586 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18586 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit4002018561082700851/append_test
18587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit4002018561082700851/append_test as 1
18587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit4002018561082700851/append_test/.commits.archive_.archive.1
18587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit4002018561082700851/append_test/.commits.archive_.archive.1} exists. Appending to existing file
18587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
18607 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19031 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19037 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19042 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19042 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19042 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19042 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1893996769740249039
19044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1893996769740249039 as 1
19044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1893996769740249039/.test-fileid1_100.log.1
19046 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1893996769740249039/.test-fileid1_100.log.1} does not exist. Create a new file
19487 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19493 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19498 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19504 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1}
19504 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1
19506 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1}
19506 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1
19507 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1}
19507 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1
19507 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1
19507 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1
19507 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
19509 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1}
19509 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1893996769740249039/.test-fileid1_100.log.1
19509 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
19534 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
19535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
19535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8360
19535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
19535 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71812
19567 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19589 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19594 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19598 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19598 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19598 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19598 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit79160241750345153
19599 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit79160241750345153 as 1
19599 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit79160241750345153/.test-fileid1_100.log.1
19600 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit79160241750345153/.test-fileid1_100.log.1} does not exist. Create a new file
19622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit79160241750345153
19625 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit79160241750345153 as 1
19625 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit79160241750345153/.test-fileid1_100.log.1
19627 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit79160241750345153/.test-fileid1_100.log.1} exists. Appending to existing file
19662 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19662 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit79160241750345153
19664 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit79160241750345153 as 1
19664 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit79160241750345153/.test-fileid1_100.log.1
19666 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit79160241750345153/.test-fileid1_100.log.1} exists. Appending to existing file
20106 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
20532 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
20539 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
20544 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
20544 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
20545 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20545 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9033519656019784858
20546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9033519656019784858 as 1
20546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9033519656019784858/.test-fileid1_100.log.1
20548 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9033519656019784858/.test-fileid1_100.log.1} does not exist. Create a new file
20974 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20974 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9033519656019784858
20977 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9033519656019784858 as 1
20978 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9033519656019784858/.test-fileid1_100.log.1
20979 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9033519656019784858/.test-fileid1_100.log.1} exists. Appending to existing file
21424 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21424 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9033519656019784858
21426 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9033519656019784858 as 1
21426 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9033519656019784858/.test-fileid1_100.log.1
21428 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9033519656019784858/.test-fileid1_100.log.1} exists. Appending to existing file
21885 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22303 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22309 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22313 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22313 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22313 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22313 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2219954880640798143
22315 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2219954880640798143 as 1
22315 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2219954880640798143/.test-fileid1_100.log.1
22316 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} does not exist. Create a new file
23178 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} has a corrupted block at 2163
23214 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} starts at 2195
23647 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23647 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2219954880640798143
23650 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2219954880640798143 as 1
23650 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2219954880640798143/.test-fileid1_100.log.1
23652 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} exists. Appending to existing file
24099 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} has a corrupted block at 2163
24141 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} starts at 2195
24142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} has a corrupted block at 2209
24191 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit2219954880640798143/.test-fileid1_100.log.1} starts at 2246
24202 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
24622 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24628 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24633 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24633 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
24634 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
24634 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7206825232697661585
24635 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7206825232697661585 as 1
24636 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7206825232697661585/.test-fileid1_100.log.1
24637 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7206825232697661585/.test-fileid1_100.log.1} does not exist. Create a new file
24665 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24670 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24677 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1}
24691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1}
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1
24693 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1}
24693 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1
24693 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1
24693 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1
24694 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
24694 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1}
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit7206825232697661585/.test-fileid1_100.log.1
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
24695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
24703 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
25127 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25133 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25138 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25138 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
25139 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25139 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3097383911605586786
25140 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3097383911605586786 as 1
25140 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3097383911605586786/.test-fileid1_100.log.1
25142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3097383911605586786/.test-fileid1_100.log.1} does not exist. Create a new file
25164 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25169 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25173 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25186 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit3097383911605586786/.test-fileid1_100.log.1}
25186 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit3097383911605586786/.test-fileid1_100.log.1
25187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit3097383911605586786/.test-fileid1_100.log.1}
25187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit3097383911605586786/.test-fileid1_100.log.1
25187 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:34676/tmp/junit3097383911605586786/.test-fileid1_100.log.1
25187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
25187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
25199 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
25199 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
25199 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
25199 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 89
25199 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 33824
25215 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
25637 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25643 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25647 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25647 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
25647 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25648 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7746452160426249552
25649 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7746452160426249552 as 1
25650 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7746452160426249552/.test-fileid1_100.log.1
25651 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7746452160426249552/.test-fileid1_100.log.1} does not exist. Create a new file
26075 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26075 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7746452160426249552
26078 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7746452160426249552 as 1
26078 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7746452160426249552/.test-fileid1_100.log.1
26079 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7746452160426249552/.test-fileid1_100.log.1} exists. Appending to existing file
26519 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26519 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7746452160426249552
26521 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7746452160426249552 as 1
26521 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7746452160426249552/.test-fileid1_100.log.1
26522 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7746452160426249552/.test-fileid1_100.log.1} exists. Appending to existing file
26975 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27394 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27400 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27410 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27410 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27411 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27411 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6119670485474465655
27414 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6119670485474465655 as 1
27414 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6119670485474465655/.test-fileid1_100.log.1
27416 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6119670485474465655/.test-fileid1_100.log.1} does not exist. Create a new file
28704 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
28705 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6119670485474465655
28707 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6119670485474465655 as 1
28707 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6119670485474465655/.test-fileid1_100.log.1
28709 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6119670485474465655/.test-fileid1_100.log.1} exists. Appending to existing file
29573 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
29573 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6119670485474465655
29575 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6119670485474465655 as 1
29576 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6119670485474465655/.test-fileid1_100.log.1
29577 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6119670485474465655/.test-fileid1_100.log.1} exists. Appending to existing file
29606 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29611 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29614 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29626 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29626 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29627 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29627 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29628 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29628 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29628 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1} has a corrupted block at 10486
29648 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1} starts at 10502
29648 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29648 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29650 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1} has a corrupted block at 10516
29670 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1} starts at 10532
29670 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29670 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29672 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1} has a corrupted block at 11139
29689 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1} starts at 11155
29689 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29689 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1}
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit6119670485474465655/.test-fileid1_100.log.1
29690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
29691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
29691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
29691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
29691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
29698 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
30117 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
30122 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
30126 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
30126 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
30127 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
30127 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1576681723755439889
30128 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1576681723755439889 as 1
30128 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1576681723755439889/.test-fileid1_100.log.1
30129 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1576681723755439889/.test-fileid1_100.log.1} does not exist. Create a new file
30549 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
30550 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1576681723755439889
30551 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1576681723755439889 as 1
30552 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1576681723755439889/.test-fileid1_100.log.1
30553 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1576681723755439889/.test-fileid1_100.log.1} exists. Appending to existing file
30584 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4298 has reached threshold 2148. Rolling over to the next version
30993 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1576681723755439889/.test-fileid1_100.log.2} does not exist. Create a new file
31021 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
31441 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
31447 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
31451 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
31451 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
31451 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
31451 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6315885535683781101
31453 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6315885535683781101 as 1
31454 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6315885535683781101/.test-fileid1_100.log.1
31455 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.1} does not exist. Create a new file
31475 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
31882 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.2} does not exist. Create a new file
31898 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
32306 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.3} does not exist. Create a new file
32323 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
32733 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.4} does not exist. Create a new file
32740 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
32745 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
32749 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
32754 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.3}
32754 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit6315885535683781101/.test-fileid1_100.log.3
32759 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.2}
32759 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit6315885535683781101/.test-fileid1_100.log.2
32764 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit6315885535683781101/.test-fileid1_100.log.1}
32764 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit6315885535683781101/.test-fileid1_100.log.1
32764 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
32798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
32798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
32798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
32798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 289
32798 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 109795
32833 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
33250 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
33255 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
33260 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
33260 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
33261 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
33261 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4185166128599066685
33262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4185166128599066685 as 1
33262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4185166128599066685/.test-fileid1_100.log.1
33263 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4185166128599066685/.test-fileid1_100.log.1} does not exist. Create a new file
33298 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
33298 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4185166128599066685
33299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4185166128599066685 as 1
33299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4185166128599066685/.test-fileid1_100.log.1
33301 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4185166128599066685/.test-fileid1_100.log.1} exists. Appending to existing file
33328 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
33328 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4185166128599066685
33330 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4185166128599066685 as 1
33330 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4185166128599066685/.test-fileid1_100.log.1
33331 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4185166128599066685/.test-fileid1_100.log.1} exists. Appending to existing file
33332 [IPC Server handler 6 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit4185166128599066685/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
33333 [IPC Server handler 6 on 34676] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit4185166128599066685/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
33334 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit4185166128599066685/.test-fileid1_100.log.1
33335 [IPC Server handler 8 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit4185166128599066685/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1058 for block blk_1073741859_1057{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-6d5f17a6-ef18-49d9-8755-16fea7b40621:NORMAL:127.0.0.1:50010|RBW]]}
34338 [IPC Server handler 0 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit4185166128599066685/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1059 for block blk_1073741859_1057{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-6d5f17a6-ef18-49d9-8755-16fea7b40621:NORMAL:127.0.0.1:50010|RBW]]}
35340 [IPC Server handler 1 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit4185166128599066685/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1060 for block blk_1073741859_1057{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-6d5f17a6-ef18-49d9-8755-16fea7b40621:NORMAL:127.0.0.1:50010|RBW]]}
35969 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57728 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741859_1056]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57728 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:57728]. 57360 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
35969 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741859_1057] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741859_1057
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
36343 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit4185166128599066685/.test-fileid1_100.log.1
36801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
37219 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
37223 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
37227 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
37227 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
37228 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
37228 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5700856232706381881
37229 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5700856232706381881 as 1
37230 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5700856232706381881/.test-fileid1_100.log.1
37231 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5700856232706381881/.test-fileid1_100.log.1} does not exist. Create a new file
37248 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
37656 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5700856232706381881/.test-fileid1_100.log.2} does not exist. Create a new file
37673 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
38080 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5700856232706381881/.test-fileid1_100.log.3} does not exist. Create a new file
38086 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
38090 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
38092 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
38097 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5700856232706381881/.test-fileid1_100.log.1}
38097 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit5700856232706381881/.test-fileid1_100.log.1
38101 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit5700856232706381881/.test-fileid1_100.log.2}
38101 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit5700856232706381881/.test-fileid1_100.log.2
38102 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
38122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
38122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
38122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
38122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
38122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71808
38147 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
38567 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
38569 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
38572 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
38572 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
38572 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
38573 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1035378754465236070
38574 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1035378754465236070 as 1
38574 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1035378754465236070/.test-fileid1_100.log.1
38574 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1035378754465236070/.test-fileid1_100.log.1} does not exist. Create a new file
38589 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
38592 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
38595 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
38601 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1}
38601 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1}
38602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38603 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1}
38603 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1}
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1}
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit1035378754465236070/.test-fileid1_100.log.1
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
38604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
38605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
38605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
38608 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
39019 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
39023 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
39027 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
39027 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
39028 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
39028 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit536852329361279727
39029 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit536852329361279727 as 1
39030 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit536852329361279727/.test-fileid1_100.log.1
39031 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit536852329361279727/.test-fileid1_100.log.1} does not exist. Create a new file
39060 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
39065 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
39070 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
39081 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1}
39081 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1
39082 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1}
39082 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1
39083 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1}
39083 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1
39083 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
39112 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
39112 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 2
39112 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 1504
39112 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 148
39112 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71796
39132 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
39136 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
39140 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
39150 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1}
39150 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1
39151 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1}
39151 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1
39152 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit536852329361279727/.test-fileid1_100.log.1}
39152 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
39172 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
39172 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
39172 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
39172 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
39172 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71796
39195 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
39612 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
39617 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
39622 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
39622 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
39622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
39622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2833954657485849751
39623 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2833954657485849751 as 1
39623 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2833954657485849751/.test-fileid1_100.log.1
39625 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2833954657485849751/.test-fileid1_100.log.1} does not exist. Create a new file
40478 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
40478 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2833954657485849751
40480 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2833954657485849751 as 1
40480 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2833954657485849751/.test-fileid1_100.log.1
40482 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2833954657485849751/.test-fileid1_100.log.1} exists. Appending to existing file
40930 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
41345 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
41349 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
41353 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
41353 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
41353 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
41353 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2236154519724254682
41354 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2236154519724254682 as 1
41354 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2236154519724254682/.test-fileid1_100.log.1
41355 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2236154519724254682/.test-fileid1_100.log.1} does not exist. Create a new file
41374 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
41374 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2236154519724254682
41375 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2236154519724254682 as 1
41375 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2236154519724254682/.test-fileid1_100.log.1
41376 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2236154519724254682/.test-fileid1_100.log.1} exists. Appending to existing file
41403 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
41403 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2236154519724254682
41405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2236154519724254682 as 1
41405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2236154519724254682/.test-fileid1_100.log.1
41405 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2236154519724254682/.test-fileid1_100.log.1} exists. Appending to existing file
41857 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
42273 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
42277 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
42281 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
42281 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
42282 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
42282 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit422142183076406409
42283 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit422142183076406409 as 1
42283 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit422142183076406409/.test-fileid1_100.log.1
42284 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit422142183076406409/.test-fileid1_100.log.1} does not exist. Create a new file
42293 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
42709 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
42713 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
42717 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
42717 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
42717 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
42717 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit354219283582595790
42719 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit354219283582595790 as 1
42719 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit354219283582595790/.test-fileid1_100.log.1
42720 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit354219283582595790/.test-fileid1_100.log.1} does not exist. Create a new file
42745 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
42749 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
42752 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
42763 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1}
42763 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42764 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1}
42764 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42765 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1}
42765 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42766 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1}
42766 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42766 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42766 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42766 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42766 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
42767 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1}
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit354219283582595790/.test-fileid1_100.log.1
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
42768 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
42774 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
43191 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
43199 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
43203 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
43204 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
43204 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
43204 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6654959091316512578
43205 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6654959091316512578 as 1
43205 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6654959091316512578/.test-fileid1_100.log.1
43207 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6654959091316512578/.test-fileid1_100.log.1} does not exist. Create a new file
43639 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
44058 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
44062 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
44066 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
44067 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
44067 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
44067 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6113475008540629642
44068 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6113475008540629642 as 1
44068 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6113475008540629642/.test-fileid1_100.log.1
44069 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6113475008540629642/.test-fileid1_100.log.1} does not exist. Create a new file
44494 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
44912 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
44917 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
44923 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
44924 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
44924 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
44924 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5276175155921675005
44927 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5276175155921675005 as 1
44927 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5276175155921675005/.test-fileid1_100.log.1
44928 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5276175155921675005/.test-fileid1_100.log.1} does not exist. Create a new file
44945 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
44945 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5276175155921675005
44947 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5276175155921675005 as 1
44947 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5276175155921675005/.test-fileid1_100.log.1
44948 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5276175155921675005/.test-fileid1_100.log.1} exists. Appending to existing file
44949 [IPC Server handler 5 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit5276175155921675005/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
44949 [IPC Server handler 5 on 34676] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit5276175155921675005/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
44951 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit5276175155921675005/.test-fileid1_100.log.1
44952 [IPC Server handler 7 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit5276175155921675005/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1086 for block blk_1073741879_1085{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-6d5f17a6-ef18-49d9-8755-16fea7b40621:NORMAL:127.0.0.1:50010|RBW]]}
44968 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57848 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741879_1085]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57848 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:57848]. 59976 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
44968 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741879_1085] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741879_1085
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
45954 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit5276175155921675005/.test-fileid1_100.log.1
46399 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
46820 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
46825 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
46829 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
46829 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
46830 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
46830 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6947695384614731200
46831 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6947695384614731200 as 1
46831 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6947695384614731200/.test-fileid1_100.log.1
46832 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6947695384614731200/.test-fileid1_100.log.1} does not exist. Create a new file
47684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
47685 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6947695384614731200
47686 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6947695384614731200 as 1
47687 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6947695384614731200/.test-fileid1_100.log.1
47688 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6947695384614731200/.test-fileid1_100.log.1} exists. Appending to existing file
48129 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
48134 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
48137 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
48142 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1}
48142 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1
48143 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1} has a corrupted block at 9289
48671 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1} starts at 9853
48671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1}
48671 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1
48672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1}
48672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1
48672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1
48672 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1
48672 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
48673 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1}
48673 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit6947695384614731200/.test-fileid1_100.log.1
48673 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
48687 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
48687 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
48687 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
48687 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
48687 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71823
48705 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
49120 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
49124 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
49128 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
49128 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
49165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
49165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit9176797536685379545/append_test
49165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit9176797536685379545/append_test as 1
49165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit9176797536685379545/append_test/.commits.archive_.archive.1
49165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit9176797536685379545/append_test/.commits.archive_.archive.1} does not exist. Create a new file
49182 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
49182 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit9176797536685379545/append_test
49183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit9176797536685379545/append_test as 1
49183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit9176797536685379545/append_test/.commits.archive_.archive.1
49183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit9176797536685379545/append_test/.commits.archive_.archive.1} exists. Appending to existing file
49183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
49205 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
49621 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
49625 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
49629 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
49629 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
49629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
49629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4664569691939999828
49631 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4664569691939999828 as 1
49631 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4664569691939999828/.test-fileid1_100.log.1
49632 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4664569691939999828/.test-fileid1_100.log.1} does not exist. Create a new file
50069 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
50073 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
50076 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
50081 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1}
50082 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1
50083 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1}
50083 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1
50096 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1}
50096 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1
50096 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1
50096 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
50098 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1}
50098 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit4664569691939999828/.test-fileid1_100.log.1
50098 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
50106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
50106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
50106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
50106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
50106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71798
50128 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
50144 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
50147 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
50150 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
50150 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
50150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
50150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5524773615787778623
50151 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5524773615787778623 as 1
50151 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5524773615787778623/.test-fileid1_100.log.1
50152 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5524773615787778623/.test-fileid1_100.log.1} does not exist. Create a new file
50569 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
50569 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5524773615787778623
50570 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5524773615787778623 as 1
50571 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5524773615787778623/.test-fileid1_100.log.1
50572 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5524773615787778623/.test-fileid1_100.log.1} exists. Appending to existing file
51008 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
51008 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5524773615787778623
51010 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5524773615787778623 as 1
51010 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5524773615787778623/.test-fileid1_100.log.1
51011 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5524773615787778623/.test-fileid1_100.log.1} exists. Appending to existing file
51450 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
51867 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
51871 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
51874 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
51874 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
51874 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
51874 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3511783502455367648
51875 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3511783502455367648 as 1
51876 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3511783502455367648/.test-fileid1_100.log.1
51877 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3511783502455367648/.test-fileid1_100.log.1} does not exist. Create a new file
52297 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
52297 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3511783502455367648
52299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3511783502455367648 as 1
52299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3511783502455367648/.test-fileid1_100.log.1
52300 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3511783502455367648/.test-fileid1_100.log.1} exists. Appending to existing file
52738 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
52738 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3511783502455367648
52739 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3511783502455367648 as 1
52739 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3511783502455367648/.test-fileid1_100.log.1
52740 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3511783502455367648/.test-fileid1_100.log.1} exists. Appending to existing file
53190 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
53208 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
53213 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
53217 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
53217 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
53217 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
53217 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5844720790831490010
53218 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5844720790831490010 as 1
53218 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5844720790831490010/.test-fileid1_100.log.1
53219 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} does not exist. Create a new file
54072 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} has a corrupted block at 2163
54108 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} starts at 2195
54539 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
54539 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5844720790831490010
54541 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5844720790831490010 as 1
54541 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5844720790831490010/.test-fileid1_100.log.1
54542 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} exists. Appending to existing file
54983 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} has a corrupted block at 2163
55022 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} starts at 2195
55023 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} has a corrupted block at 2209
55060 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit5844720790831490010/.test-fileid1_100.log.1} starts at 2246
55070 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
55491 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
55494 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
55499 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
55499 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
55500 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
55500 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8219631207736058300
55502 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8219631207736058300 as 1
55503 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8219631207736058300/.test-fileid1_100.log.1
55504 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8219631207736058300/.test-fileid1_100.log.1} does not exist. Create a new file
55524 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
55528 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
55533 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
55544 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1}
55544 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1
55545 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1}
55545 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1
55546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1}
55546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1
55546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1
55546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1
55546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1}
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit8219631207736058300/.test-fileid1_100.log.1
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
55547 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
55552 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
55968 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
55972 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
55975 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
55975 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
55976 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
55976 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8745328986257886137
55977 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8745328986257886137 as 1
55978 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8745328986257886137/.test-fileid1_100.log.1
55978 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8745328986257886137/.test-fileid1_100.log.1} does not exist. Create a new file
56112 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
56115 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
56119 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
56132 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8745328986257886137/.test-fileid1_100.log.1}
56132 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit8745328986257886137/.test-fileid1_100.log.1
56133 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8745328986257886137/.test-fileid1_100.log.1}
56133 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit8745328986257886137/.test-fileid1_100.log.1
56133 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:34676/tmp/junit8745328986257886137/.test-fileid1_100.log.1
56133 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
56133 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
56147 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
56147 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
56147 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8360
56147 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 89
56147 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 33808
56163 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
56577 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
56581 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
56584 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
56584 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
56584 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
56584 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1493839036655838238
56585 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1493839036655838238 as 1
56586 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1493839036655838238/.test-fileid1_100.log.1
56586 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1493839036655838238/.test-fileid1_100.log.1} does not exist. Create a new file
57004 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
57004 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1493839036655838238
57005 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1493839036655838238 as 1
57006 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1493839036655838238/.test-fileid1_100.log.1
57007 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1493839036655838238/.test-fileid1_100.log.1} exists. Appending to existing file
57436 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
57436 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1493839036655838238
57438 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1493839036655838238 as 1
57438 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1493839036655838238/.test-fileid1_100.log.1
57439 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1493839036655838238/.test-fileid1_100.log.1} exists. Appending to existing file
57483 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
57898 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
57901 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
57906 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
57906 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
57907 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
57907 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8501671490544215302
57908 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8501671490544215302 as 1
57908 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8501671490544215302/.test-fileid1_100.log.1
57909 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8501671490544215302/.test-fileid1_100.log.1} does not exist. Create a new file
59182 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
59182 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8501671490544215302
59183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8501671490544215302 as 1
59184 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8501671490544215302/.test-fileid1_100.log.1
59184 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8501671490544215302/.test-fileid1_100.log.1} exists. Appending to existing file
59629 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
59630 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8501671490544215302
59631 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8501671490544215302 as 1
59631 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8501671490544215302/.test-fileid1_100.log.1
59632 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8501671490544215302/.test-fileid1_100.log.1} exists. Appending to existing file
59653 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
59656 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
59659 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
59667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59668 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59668 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59669 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1} has a corrupted block at 10486
59686 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1} starts at 10502
59686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59687 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1} has a corrupted block at 10516
59702 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1} starts at 10532
59703 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59703 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59704 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59704 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59704 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1} has a corrupted block at 11139
59721 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1} starts at 11155
59721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59722 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1}
59722 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59722 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59722 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit8501671490544215302/.test-fileid1_100.log.1
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
59723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
59728 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
60151 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
60159 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
60163 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
60163 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
60164 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
60164 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8649879156008049355
60165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8649879156008049355 as 1
60165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8649879156008049355/.test-fileid1_100.log.1
60166 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8649879156008049355/.test-fileid1_100.log.1} does not exist. Create a new file
60587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
60587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8649879156008049355
60590 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8649879156008049355 as 1
60590 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8649879156008049355/.test-fileid1_100.log.1
60591 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8649879156008049355/.test-fileid1_100.log.1} exists. Appending to existing file
60619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4298 has reached threshold 2148. Rolling over to the next version
61025 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8649879156008049355/.test-fileid1_100.log.2} does not exist. Create a new file
61035 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
61451 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
61456 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
61459 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
61459 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
61459 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
61459 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4212676140583720116
61460 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4212676140583720116 as 1
61460 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4212676140583720116/.test-fileid1_100.log.1
61461 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.1} does not exist. Create a new file
61477 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
61884 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.2} does not exist. Create a new file
61899 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
61905 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.3} does not exist. Create a new file
61920 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
62327 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.4} does not exist. Create a new file
62331 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
62335 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
62338 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
62343 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.3}
62343 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4212676140583720116/.test-fileid1_100.log.3
62347 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.1}
62347 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4212676140583720116/.test-fileid1_100.log.1
62351 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4212676140583720116/.test-fileid1_100.log.2}
62351 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4212676140583720116/.test-fileid1_100.log.2
62351 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
62381 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
62381 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
62381 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8360
62381 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 289
62381 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 109778
62414 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
62830 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
62834 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
62837 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
62837 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
62837 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
62837 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8440880998414656010
62838 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8440880998414656010 as 1
62839 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8440880998414656010/.test-fileid1_100.log.1
62840 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8440880998414656010/.test-fileid1_100.log.1} does not exist. Create a new file
63267 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
63267 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8440880998414656010
63269 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8440880998414656010 as 1
63269 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8440880998414656010/.test-fileid1_100.log.1
63270 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8440880998414656010/.test-fileid1_100.log.1} exists. Appending to existing file
63301 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
63301 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8440880998414656010
63303 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8440880998414656010 as 1
63303 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8440880998414656010/.test-fileid1_100.log.1
63304 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8440880998414656010/.test-fileid1_100.log.1} exists. Appending to existing file
63305 [IPC Server handler 8 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit8440880998414656010/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
63305 [IPC Server handler 8 on 34676] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit8440880998414656010/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_-1099761205_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
63307 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit8440880998414656010/.test-fileid1_100.log.1
63308 [IPC Server handler 2 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8440880998414656010/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1133 for block blk_1073741906_1132{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c:NORMAL:127.0.0.1:50010|RBW]]}
64310 [IPC Server handler 1 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8440880998414656010/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1134 for block blk_1073741906_1132{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c:NORMAL:127.0.0.1:50010|RBW]]}
65312 [IPC Server handler 5 on 34676] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8440880998414656010/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1135 for block blk_1073741906_1132{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c:NORMAL:127.0.0.1:50010|RBW]]}
65969 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741906_1132] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741906_1132
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
65969 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:58076 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741906_1131]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:58076 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:58076]. 57333 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
66317 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit8440880998414656010/.test-fileid1_100.log.1
66774 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
67190 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
67193 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
67197 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
67197 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
67198 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
67198 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2860129140720190146
67199 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2860129140720190146 as 1
67199 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2860129140720190146/.test-fileid1_100.log.1
67200 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2860129140720190146/.test-fileid1_100.log.1} does not exist. Create a new file
67218 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
67625 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2860129140720190146/.test-fileid1_100.log.2} does not exist. Create a new file
67642 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
68050 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2860129140720190146/.test-fileid1_100.log.3} does not exist. Create a new file
68055 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
68059 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
68062 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
68067 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit2860129140720190146/.test-fileid1_100.log.1}
68067 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit2860129140720190146/.test-fileid1_100.log.1
68070 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit2860129140720190146/.test-fileid1_100.log.2}
68071 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit2860129140720190146/.test-fileid1_100.log.2
68072 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
68091 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
68091 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
68091 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
68091 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
68091 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71804
68115 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
68531 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
68534 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
68538 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
68538 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
68538 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
68538 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit421617315743956442
68539 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit421617315743956442 as 1
68539 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit421617315743956442/.test-fileid1_100.log.1
68540 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit421617315743956442/.test-fileid1_100.log.1} does not exist. Create a new file
68556 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
68559 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
68562 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
68570 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1}
68570 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68571 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1}
68571 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68571 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1}
68572 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68572 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1}
68572 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1}
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:34676/tmp/junit421617315743956442/.test-fileid1_100.log.1
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
68573 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
68579 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
68994 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
68998 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
69001 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
69001 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
69002 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
69002 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1074512521624278756
69003 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1074512521624278756 as 1
69003 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1074512521624278756/.test-fileid1_100.log.1
69004 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1074512521624278756/.test-fileid1_100.log.1} does not exist. Create a new file
69025 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
69029 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
69033 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
69042 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1}
69042 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1
69043 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1}
69043 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1
69052 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1}
69052 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1
69060 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
69064 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
69064 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 7
69064 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 5264
69064 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 143
69064 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71805
69076 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
69080 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
69082 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
69092 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1}
69092 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1
69093 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1}
69093 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1
69100 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:34676/tmp/junit1074512521624278756/.test-fileid1_100.log.1}
69100 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
69106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
69106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
69106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
69106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
69106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71805
69121 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
69536 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
69541 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
69544 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
69544 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
69544 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
69544 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit942557113917586507
69546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit942557113917586507 as 1
69546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit942557113917586507/.test-fileid1_100.log.1
69546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit942557113917586507/.test-fileid1_100.log.1} does not exist. Create a new file
70395 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
70395 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit942557113917586507
70397 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit942557113917586507 as 1
70397 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit942557113917586507/.test-fileid1_100.log.1
70398 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit942557113917586507/.test-fileid1_100.log.1} exists. Appending to existing file
70842 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
71258 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
71262 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
71265 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
71266 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
71266 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
71266 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit391964400085810735
71267 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit391964400085810735 as 1
71267 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit391964400085810735/.test-fileid1_100.log.1
71268 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit391964400085810735/.test-fileid1_100.log.1} does not exist. Create a new file
71685 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
71685 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit391964400085810735
71686 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit391964400085810735 as 1
71686 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit391964400085810735/.test-fileid1_100.log.1
71687 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit391964400085810735/.test-fileid1_100.log.1} exists. Appending to existing file
72119 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
72119 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit391964400085810735
72120 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit391964400085810735 as 1
72121 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit391964400085810735/.test-fileid1_100.log.1
72122 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit391964400085810735/.test-fileid1_100.log.1} exists. Appending to existing file
72572 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
72988 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
72992 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
72995 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
72995 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
72995 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
72995 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4336552183653291662
72997 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4336552183653291662 as 1
72997 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4336552183653291662/.test-fileid1_100.log.1
72998 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4336552183653291662/.test-fileid1_100.log.1} does not exist. Create a new file
73002 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
73003 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741892_1109] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741892_1109
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73003 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741898_1122] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741898_1122
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73003 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741911_1141] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741911_1141
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73003 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741845_1034] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741845_1034
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73003 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741864_1066] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741864_1066
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73005 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741894_1111] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741894_1111
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73005 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741866_1068] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741866_1068
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73005 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741851_1047] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741851_1047
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73005 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741873_1079] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741873_1079
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73005 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741826_1002] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741826_1002
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73004 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741913_1143] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741913_1143
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73005 [ResponseProcessor for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741847_1036] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-1048382472-172.17.0.2-1521184705148:blk_1073741847_1036
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
73149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57982 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741894_1111]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57982 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 42966 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73152 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57486 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741826_1002]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57486 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 56994 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73151 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57818 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741873_1079]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57818 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59599 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73150 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:58134 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741913_1143]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:58134 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55932 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73159 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:58120 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741911_1141]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:58120 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55411 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73159 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57764 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741864_1066]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57764 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55445 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73159 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:58010 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741898_1121]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:58010 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 46507 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73159 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57638 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741847_1036]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57638 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 42018 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73158 [DataNode: [[[DISK]file:/tmp/1521184704527-0/dfs/data/data1/, [DISK]file:/tmp/1521184704527-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:34676] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1048382472-172.17.0.2-1521184705148 (Datanode Uuid 76c6bc67-e301-4ad0-95e5-4b2add2aacf5) service to localhost/127.0.0.1:34676 interrupted
73158 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57680 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741851_1046]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57680 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 46457 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73156 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57776 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741866_1068]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57776 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55988 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73156 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57618 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741845_1034]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57618 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 41519 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73153 [DataXceiver for client DFSClient_NONMAPREDUCE_-1099761205_1 at /127.0.0.1:57974 [Receiving block BP-1048382472-172.17.0.2-1521184705148:blk_1073741892_1109]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:57974 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 42378 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
73167 [DataNode: [[[DISK]file:/tmp/1521184704527-0/dfs/data/data1/, [DISK]file:/tmp/1521184704527-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:34676] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1048382472-172.17.0.2-1521184705148 (Datanode Uuid 76c6bc67-e301-4ad0-95e5-4b2add2aacf5) service to localhost/127.0.0.1:34676
73196 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@6e7d2f3a] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 46, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 72.764 sec - in com.uber.hoodie.common.table.log.HoodieLogFormatTest
Running com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
73588 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7391982430420072007 as hoodie dataset /tmp/junit7391982430420072007
73620 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7391982430420072007
73620 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7391982430420072007/.hoodie/hoodie.properties
73621 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7391982430420072007
73621 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7391982430420072007
73628 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73635 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7391982430420072007
73635 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7391982430420072007/.hoodie/hoodie.properties
73635 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7391982430420072007
73635 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit7391982430420072007
73636 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
73657 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit4461469647489058989 as hoodie dataset /tmp/junit4461469647489058989
73686 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4461469647489058989
73686 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4461469647489058989/.hoodie/hoodie.properties
73687 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4461469647489058989
73687 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4461469647489058989
73687 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73691 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4461469647489058989
73691 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4461469647489058989/.hoodie/hoodie.properties
73691 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4461469647489058989
73691 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit4461469647489058989
73691 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73692 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
73704 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit4461469647489058989/.hoodie/1.inflight
73704 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
73704 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4461469647489058989
73704 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4461469647489058989/.hoodie/hoodie.properties
73704 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4461469647489058989
73705 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit4461469647489058989
73705 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
73706 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4461469647489058989
73706 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4461469647489058989/.hoodie/hoodie.properties
73707 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4461469647489058989
73707 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit4461469647489058989
73707 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
73708 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>2__commit]
73719 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit4461469647489058989/.hoodie/2.inflight
73719 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>2__commit]
73719 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4461469647489058989
73719 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4461469647489058989/.hoodie/hoodie.properties
73719 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4461469647489058989
73719 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit4461469647489058989
73720 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit]]
73723 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8988855698833110057 as hoodie dataset /tmp/junit8988855698833110057
73750 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8988855698833110057
73750 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8988855698833110057/.hoodie/hoodie.properties
73750 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8988855698833110057
73751 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8988855698833110057
73751 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73754 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8988855698833110057
73754 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8988855698833110057/.hoodie/hoodie.properties
73755 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8988855698833110057
73755 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit8988855698833110057
73755 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
73768 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6345734691823154001 as hoodie dataset /tmp/junit6345734691823154001
73796 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6345734691823154001
73796 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6345734691823154001/.hoodie/hoodie.properties
73797 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6345734691823154001
73797 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6345734691823154001
73797 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73800 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6345734691823154001
73800 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6345734691823154001/.hoodie/hoodie.properties
73801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6345734691823154001
73801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit6345734691823154001
73801 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
73807 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3433307639026338398 as hoodie dataset /tmp/junit3433307639026338398
73837 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3433307639026338398
73837 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3433307639026338398/.hoodie/hoodie.properties
73838 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3433307639026338398
73838 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3433307639026338398
73838 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73843 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3433307639026338398
73844 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3433307639026338398/.hoodie/hoodie.properties
73844 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3433307639026338398
73844 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit3433307639026338398
73845 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
73848 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3819092907620677123 as hoodie dataset /tmp/junit3819092907620677123
73877 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3819092907620677123
73877 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3819092907620677123/.hoodie/hoodie.properties
73877 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3819092907620677123
73877 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3819092907620677123
73877 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73881 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3819092907620677123
73881 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3819092907620677123/.hoodie/hoodie.properties
73881 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3819092907620677123
73881 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit3819092907620677123
73882 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.29 sec - in com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
Running com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
73889 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6423318674364994535 as hoodie dataset /tmp/junit6423318674364994535
73915 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6423318674364994535
73915 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6423318674364994535/.hoodie/hoodie.properties
73915 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6423318674364994535
73915 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6423318674364994535
73916 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
73916 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
73923 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit6423318674364994535/.hoodie/1.inflight
73923 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
73923 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>3__commit]
73931 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit6423318674364994535/.hoodie/3.inflight
73931 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>3__commit]
73931 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>5__commit]
73938 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit6423318674364994535/.hoodie/5.inflight
73938 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>5__commit]
73939 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>8__commit]
73946 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit6423318674364994535/.hoodie/8.inflight
73946 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>8__commit]
73946 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>9__commit]
73953 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit6423318674364994535/.hoodie/9.inflight
73954 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [3__commit], [5__commit], [8__commit], [==>9__commit]]
73958 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7165497234668376153 as hoodie dataset /tmp/junit7165497234668376153
73987 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7165497234668376153
73987 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7165497234668376153/.hoodie/hoodie.properties
73988 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7165497234668376153
73988 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7165497234668376153
73997 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit9042750869855482034 as hoodie dataset /tmp/junit9042750869855482034
74031 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit9042750869855482034
74032 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit9042750869855482034/.hoodie/hoodie.properties
74032 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit9042750869855482034
74032 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit9042750869855482034
74032 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.145 sec - in com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
Running com.uber.hoodie.common.table.HoodieTableMetaClientTest
74036 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8692621690262260007 as hoodie dataset /tmp/junit8692621690262260007
74066 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8692621690262260007
74066 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8692621690262260007/.hoodie/hoodie.properties
74066 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8692621690262260007
74066 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8692621690262260007
74205 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
74205 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>1__commit]
74217 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit8692621690262260007/.hoodie/1.inflight
74217 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
74229 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
74229 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
74233 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit4538011458596106833 as hoodie dataset /tmp/junit4538011458596106833
74261 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4538011458596106833
74261 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4538011458596106833/.hoodie/hoodie.properties
74262 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4538011458596106833
74262 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4538011458596106833
74262 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
74262 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>1__commit]
74273 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit4538011458596106833/.hoodie/1.inflight
74273 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
74283 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
74284 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
74286 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3813494347965156290 as hoodie dataset /tmp/junit3813494347965156290
74322 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3813494347965156290
74322 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3813494347965156290/.hoodie/hoodie.properties
74323 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3813494347965156290
74323 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3813494347965156290
74588 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8422395927189073749 as hoodie dataset /tmp/junit8422395927189073749
74616 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8422395927189073749
74617 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8422395927189073749/.hoodie/hoodie.properties
74617 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8422395927189073749
74617 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8422395927189073749
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.582 sec - in com.uber.hoodie.common.table.HoodieTableMetaClientTest
Running com.uber.hoodie.common.model.TestHoodieWriteStat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.common.model.TestHoodieWriteStat
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 44,000
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 22,424B for [_hoodie_record_key] BINARY: 1,000 values, 40,007B raw, 22,322B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:18:24 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 1000
74756 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16450
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-6d5f17a6-ef18-49d9-8755-16fea7b40621,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74757 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16420
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-6d5f17a6-ef18-49d9-8755-16fea7b40621,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74758 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16453
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74759 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16391
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74760 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16423
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74761 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16524
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-6d5f17a6-ef18-49d9-8755-16fea7b40621,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74762 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16429
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-6d5f17a6-ef18-49d9-8755-16fea7b40621,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74766 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16462
java.io.EOFException: End of File Exception between local host is: "spirals-vortex.lille.inria.fr/172.17.0.2"; destination host is: "localhost":34676; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy30.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1080)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:975)
74768 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16494
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74769 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16527
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74770 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16465
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74771 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16497
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74772 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16503
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-fd62bdf5-e8dc-42a0-869a-d2e30812ed7c,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
74775 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16536
java.net.ConnectException: Call From spirals-vortex.lille.inria.fr/172.17.0.2 to localhost:34676 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy30.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 19 more

Results :

Tests run: 82, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:report (post-unit-test) @ hoodie-common ---
[INFO] Loading execution data file /root/workspace/uber/hudi/354187058/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] Analyzed bundle 'hoodie-common' with 116 classes
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-hadoop-mr 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/354187058/hoodie-hadoop-mr/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-hadoop-mr ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
169  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
345  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1179970138240822188 as hoodie dataset /tmp/junit1179970138240822188
346  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
370  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1179970138240822188
371  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
372  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1179970138240822188/.hoodie/hoodie.properties
377  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit1179970138240822188
377  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit1179970138240822188
379  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3104366248869385151 as hoodie dataset /tmp/junit3104366248869385151
409  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
416  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3104366248869385151
417  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
418  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3104366248869385151/.hoodie/hoodie.properties
418  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3104366248869385151
418  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3104366248869385151
1257 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
1258 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit3104366248869385151/2016/05/01
1271 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit3104366248869385151/2016/05/01 as 1
1272 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3104366248869385151/2016/05/01/.fileid0_100.log.1
1273 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3104366248869385151/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
1984 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favoriteIntNumber,favoriteNumber,favoriteFloatNumber,favoriteDoubleNumber,tags,testNestedRecord,stringArray
1993 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit3104366248869385151/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit3104366248869385151/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favoriteIntNumber, favoriteNumber, favoriteFloatNumber, favoriteDoubleNumber, tags, testNestedRecord, stringArray]
1995 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
1995 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3104366248869385151
1995 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
1996 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3104366248869385151/.hoodie/hoodie.properties
1997 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3104366248869385151
2000 [main] INFO  com.uber.hoodie.common.util.collection.DiskBasedMap  - Spilling to file location /tmp/dec306a8-e873-47e5-a319-78cccb2c43bc
2004 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit3104366248869385151/2016/05/01/.fileid0_100.log.1}
2007 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit3104366248869385151/2016/05/01/.fileid0_100.log.1
2007 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
2073 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 1640
2106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
2106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 1
2106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 1640
2106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 49
2106 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 21060
2210 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2213 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7970826760486539040 as hoodie dataset /tmp/junit7970826760486539040
2213 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2222 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7970826760486539040
2223 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2223 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7970826760486539040/.hoodie/hoodie.properties
2224 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit7970826760486539040
2224 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit7970826760486539040
2226 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2237878619214022505 as hoodie dataset /tmp/junit2237878619214022505
2251 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2259 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2237878619214022505
2259 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2259 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2237878619214022505/.hoodie/hoodie.properties
2260 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2237878619214022505
2260 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2237878619214022505
2414 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
2414 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit2237878619214022505/2016/05/01
2415 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit2237878619214022505/2016/05/01 as 1
2415 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2237878619214022505/2016/05/01/.fileid0_100.log.1
2415 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2237878619214022505/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
2553 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favorite_number,favorite_color,favorite_movie
2555 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit2237878619214022505/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit2237878619214022505/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favorite_number, favorite_color, favorite_movie]
2555 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2555 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2237878619214022505
2555 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2555 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2237878619214022505/.hoodie/hoodie.properties
2556 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2237878619214022505
2557 [main] INFO  com.uber.hoodie.common.util.collection.DiskBasedMap  - Spilling to file location /tmp/d7fc8d60-de5c-409c-ae70-501fba3e4a1a
2557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit2237878619214022505/2016/05/01/.fileid0_100.log.1}
2557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit2237878619214022505/2016/05/01/.fileid0_100.log.1
2557 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
2565 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 728
2580 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
2580 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 1
2580 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 728
2580 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 99
2580 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 29036
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.129 sec - in com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
Running com.uber.hoodie.hadoop.AnnotationTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.uber.hoodie.hadoop.AnnotationTest
Running com.uber.hoodie.hadoop.HoodieInputFormatTest
2630 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3735157249476351720 as hoodie dataset /tmp/junit3735157249476351720
2644 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2650 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3735157249476351720
2651 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2651 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3735157249476351720/.hoodie/hoodie.properties
2651 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3735157249476351720
2651 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3735157249476351720
2751 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3735157249476351720
2751 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3735157249476351720
2752 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2752 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3735157249476351720/.hoodie/hoodie.properties
2753 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3735157249476351720
2753 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
2759 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
2787 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
2812 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
2813 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
2813 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
2820 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid0_1_200.parquet
2820 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid0_1_200.parquet
2820 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid2_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid2_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid1_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid1_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid4_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid4_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid3_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid3_1_200.parquet
2821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
2898 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3735157249476351720
2898 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3735157249476351720
2899 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
2899 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3735157249476351720/.hoodie/hoodie.properties
2900 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3735157249476351720
2900 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
2901 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
2903 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
2906 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
2907 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 3
2907 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid0_1_400.parquet
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid0_1_400.parquet
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid2_1_400.parquet
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid2_1_400.parquet
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid1_1_400.parquet
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid1_1_400.parquet
2909 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid4_1_200.parquet
2910 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid4_1_200.parquet
2910 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid3_1_300.parquet
2910 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid3_1_300.parquet
2910 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
3003 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3735157249476351720
3003 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3735157249476351720
3004 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3004 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3735157249476351720/.hoodie/hoodie.properties
3005 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3735157249476351720
3005 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3005 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
3007 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
3009 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
3010 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 2147483647
3010 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
3011 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid0_1_600.parquet
3011 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid0_1_600.parquet
3011 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid2_1_400.parquet
3011 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid2_1_400.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid1_1_500.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid1_1_500.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid4_1_200.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid4_1_200.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit3735157249476351720/2016/05/01/fileid3_1_300.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3735157249476351720/2016/05/01/fileid3_1_300.parquet
3012 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
3036 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2055710151372704109 as hoodie dataset /tmp/junit2055710151372704109
3053 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3059 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2055710151372704109
3059 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3060 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2055710151372704109/.hoodie/hoodie.properties
3060 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2055710151372704109
3060 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2055710151372704109
3093 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit2055710151372704109
3093 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2055710151372704109
3093 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3094 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2055710151372704109/.hoodie/hoodie.properties
3094 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2055710151372704109
3094 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3095 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3095 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3097 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3098 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid0_1_100.parquet
3098 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid2_1_100.parquet
3098 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid1_1_100.parquet
3098 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid4_1_100.parquet
3098 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid3_1_100.parquet
3099 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid6_1_100.parquet
3099 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid5_1_100.parquet
3099 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid8_1_100.parquet
3099 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid7_1_100.parquet
3099 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid9_1_100.parquet
3132 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit2055710151372704109
3132 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2055710151372704109
3132 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3133 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2055710151372704109/.hoodie/hoodie.properties
3133 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2055710151372704109
3133 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3134 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3134 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3135 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3135 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid0_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid2_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid1_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid4_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid3_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid6_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid5_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid8_1_100.parquet
3136 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid7_1_100.parquet
3137 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2055710151372704109/2016/05/01/fileid9_1_100.parquet
3155 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7049423086840738516 as hoodie dataset /tmp/junit7049423086840738516
3170 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3176 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7049423086840738516
3176 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3176 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7049423086840738516/.hoodie/hoodie.properties
3177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7049423086840738516
3177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7049423086840738516
3211 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit7049423086840738516
3211 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit7049423086840738516
3211 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3211 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit7049423086840738516/.hoodie/hoodie.properties
3212 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit7049423086840738516
3212 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3212 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
3213 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3214 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
3214 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
3215 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
3215 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 0
3234 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6296860636934334070 as hoodie dataset /tmp/junit6296860636934334070
3249 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3256 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6296860636934334070
3256 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3257 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6296860636934334070/.hoodie/hoodie.properties
3258 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6296860636934334070
3258 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6296860636934334070
3298 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit6296860636934334070
3298 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6296860636934334070
3298 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3299 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6296860636934334070/.hoodie/hoodie.properties
3299 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6296860636934334070
3299 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3300 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3300 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3301 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3301 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid0_1_100.parquet
3301 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid2_1_100.parquet
3301 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid1_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid4_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid3_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid6_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid5_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid8_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid7_1_100.parquet
3302 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid9_1_100.parquet
3371 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit6296860636934334070
3371 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6296860636934334070
3372 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3372 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6296860636934334070/.hoodie/hoodie.properties
3372 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6296860636934334070
3372 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3373 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3373 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid0_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid2_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid1_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid4_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid3_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid6_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid5_1_100.parquet
3375 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid8_1_100.parquet
3376 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid7_1_100.parquet
3376 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid9_1_100.parquet
3425 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit6296860636934334070
3426 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6296860636934334070
3426 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3426 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6296860636934334070/.hoodie/hoodie.properties
3426 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6296860636934334070
3426 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
3427 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
3427 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit]]
3428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
3428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid0_1_200.parquet
3428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid2_1_200.parquet
3428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid1_1_200.parquet
3428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid4_1_100.parquet
3428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid3_1_200.parquet
3429 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid6_1_100.parquet
3429 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid5_1_200.parquet
3429 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid8_1_100.parquet
3429 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid7_1_100.parquet
3429 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit6296860636934334070/2016/05/01/fileid9_1_100.parquet
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.818 sec - in com.uber.hoodie.hadoop.HoodieInputFormatTest
Running com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
3432 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3298139889801830973 as hoodie dataset /tmp/junit3298139889801830973
3446 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3453 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3298139889801830973
3454 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3454 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3298139889801830973/.hoodie/hoodie.properties
3454 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3298139889801830973
3454 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3298139889801830973
3481 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3298139889801830973
3481 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@b2c9a9c]
3482 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3298139889801830973/.hoodie/hoodie.properties
3482 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3298139889801830973
3483 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[001__commit], [002__commit], [==>003__commit]]
3484 [main] INFO  com.uber.hoodie.hadoop.HoodieROTablePathFilter  - Based on hoodie metadata from base path: file:/tmp/junit3298139889801830973, caching 3 files under file:/tmp/junit3298139889801830973/2017/01/01
3561 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/
3562 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@12968227]
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.129 sec - in com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
Mar 16, 2018 8:19:41 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 88,387
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteIntNumber] INT32: 100 values, 407B raw, 407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteNumber] INT64: 100 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteFloatNumber] FLOAT: 100 values, 407B raw, 407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteDoubleNumber] DOUBLE: 100 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 108B for [tags, map, key] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 2 entries, 24B raw, 2B comp}
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,156B for [tags, map, value, item1] BINARY: 200 values, 2,117B raw, 2,117B comp, 1 pages, encodings: [RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,963B for [tags, map, value, item2] BINARY: 200 values, 2,917B raw, 2,917B comp, 1 pages, encodings: [RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 48B for [testNestedRecord, isAdmin] BOOLEAN: 100 values, 20B raw, 20B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,647B for [testNestedRecord, userId] BINARY: 100 values, 1,597B raw, 1,597B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [stringArray, array] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 2 entries, 40B raw, 2B comp}
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8,589
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_number] INT64: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_color] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:43 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_movie] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 16, 2018 8:19:42 AM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 16, 2018 8:19:42 AM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 16, 2018 8:19:42 AM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:42 AM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 21 ms. row count = 100
Mar 16, 2018 8:19:43 AM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 16, 2018 8:19:43 AM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 16, 2018 8:19:43 AM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:43 AM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 100

Results :

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-client 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-client ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/354187058/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/354187058/hoodie-client/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-client ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.io.TestHoodieCommitArchiveLog
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.468 sec - in com.uber.hoodie.io.TestHoodieCommitArchiveLog
Running com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec - in com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Running com.uber.hoodie.io.TestHoodieCompactor
4957 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 100
6548 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=29, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=38, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=33, numUpdates=0}}}
6566 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
6567 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
6567 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 29, totalInsertBuckets => 1, recordsPerBucket => 500000
6569 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
6569 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
6569 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 38, totalInsertBuckets => 1, recordsPerBucket => 500000
6569 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
6570 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
6570 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 33, totalInsertBuckets => 1, recordsPerBucket => 500000
6570 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
6570 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
6643 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 100
6643 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 100
6890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
6890 [pool-31-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
6940 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
6940 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
7443 [pool-31-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
7499 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
7499 [pool-32-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
7550 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
7550 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
7591 [pool-32-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
7665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
7665 [pool-33-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
7701 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
7701 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,553
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 137B for [_hoodie_commit_seqno] BINARY: 29 values, 345B raw, 96B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 767B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 668B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 758B for [_row_key] BINARY: 29 values, 1,160B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,254
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_hoodie_commit_seqno] BINARY: 38 values, 462B raw, 112B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 961B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 862B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 953B for [_row_key] BINARY: 38 values, 1,520B raw, 854B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,315
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 150B for [_hoodie_commit_seqno] BINARY: 33 values, 403B raw, 106B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 752B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 842B for [_row_key] BINARY: 33 values, 1,320B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, e7730 [pool-33-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
7786 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
7786 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
7870 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
7870 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
8207 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
8220 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 100 as complete
8220 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 100
8334 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 101
8947 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
8948 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
9234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit3162375088201141401/2015/03/16/0a7fe7ae-5d79-4c4e-8776-61ac6a912fba_1_100.parquet
9316 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 38 row keys from /tmp/junit3162375088201141401/2015/03/16/0a7fe7ae-5d79-4c4e-8776-61ac6a912fba_1_100.parquet
9317 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit3162375088201141401/2015/03/16/0a7fe7ae-5d79-4c4e-8776-61ac6a912fba_1_100.parquet => [002b5b19-f1cb-43c8-993d-adb8ee8a2426, 00a4aa7e-f832-4f48-ad2d-d752ceeea684, 00b38f91-27b0-41c5-8601-27ae682158ed, 054ab82a-13ee-4516-b3f0-6394b24c1661, 07a46b51-6315-466f-846b-ca90c9a0fff8, 215cb3a3-539b-46f6-aeeb-ef661029306a, 30c9bec6-7f2c-4eb0-b848-19a3f1a3d0dc, 3173c8bc-d219-45f8-89f6-d55ce2af7de2, 340e1422-7bc5-4045-ad30-d8906683dbc1, 38d9ff32-fce7-4559-9a01-6349ca0cf696, 405ab0a3-6c6d-428e-9bc9-3d13dae5ae25, 534b4a14-bf47-490c-a1ae-d25e6e2cf066, 5ee42cd1-3d2a-4dda-bc70-a2c0c7fb1fd8, 674fd52f-c8f4-46d4-8967-75f9f68cf9f8, 6c158b19-9778-45eb-a5e4-95e469a07a4a, 6ee20de0-c1b7-4c8d-9148-8f43881b4f6c, 7c005120-d034-4035-bc38-5d6358d36d61, 86b89950-8272-4fca-a083-0ee90b9f2d56, 888d7145-f39d-4caf-808e-f497f6c5770e, 8924ee35-6d94-4f5a-bd47-7a106648c4f1, 89f53d4e-a303-4877-979c-f961fda011ad, 8b2766cc-7e64-4ed7-8793-318b5f4ce89e, 938e5169-df43-4e7c-a46a-1e74f717f57c, 949bed6c-e395-4f20-beb4-e63fed84fbf9, a1b67254-a47e-42b2-856c-d6ee6b6f20ae, a27c4596-dd0a-4a13-9d2b-10409607fa65, a3b20694-e1c4-4f7a-a235-e2eb173f3a3c, a44409bb-5702-4850-9e5a-49b43f59d767, a821279a-1c1a-4395-9c27-dde447be722c, ab9e5650-cd51-4210-8968-57f1eb06246c, ac9f4a6c-addb-44c8-97bc-b756dfeb3361, bbf1c61f-401a-4aac-868a-4ea0ae76f0ad, be024383-d3d8-498c-86ce-ac48928fbbae, d27f16ab-e5a8-4e2b-8bc3-17ea42765a7c, d5509134-6256-4d8f-aad9-452987d64eaf, d6569ade-6697-44a5-9b2b-aaadccf099eb, eea4d6fa-e6be-4dde-8e40-1bfb09ddcfb3, f020169b-c296-4297-9d49-ce260f2ff51b]
9348 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit3162375088201141401/2016/03/15/8c444e80-d765-4158-9ae9-ceda78e10751_0_100.parquet
9363 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 29 row keys from /tmp/junit3162375088201141401/2016/03/15/8c444e80-d765-4158-9ae9-ceda78e10751_0_100.parquet
9363 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit3162375088201141401/2016/03/15/8c444e80-d765-4158-9ae9-ceda78e10751_0_100.parquet => [04c40dfc-c0d7-4822-b95c-159df25529dd, 10af0ebf-4b54-437d-958c-ad4241866c61, 18b5a8b8-9b98-475c-ba0e-c38deba1196a, 1c22791d-e0d3-48bd-a03a-a5cae1b5eb91, 1e06c221-dfe3-45f7-8e44-af063daa2bc2, 2fa97432-36c9-46de-ba72-b1eeba3c35e2, 3388d1bf-fb24-4799-aac1-14db35a4d784, 3b594db2-9cba-45f9-927d-75da972d3df5, 4efcd696-57d5-4b04-a33c-1baa48ac3390, 6aa4501e-2ecb-41ca-b229-d2646253a02e, 6c105c27-46c4-4ba4-bee4-4dbfb74c2e02, 6e6c4131-4ad7-408f-9efb-3a1be6bea87a, 7e8560db-84c2-4eb4-9552-02606c73be6c, 81baaeab-ffb1-4ffe-8691-bfd324e37073, 81ecbe52-1625-400d-9f13-dd323e5834c2, 87bf9768-c838-41fc-b853-ed00d9b35e56, 901f813a-f65e-4708-9b25-2d24ac84d21f, 9e46f01a-aecb-4ef2-aa41-c827d1a815f8, a6f9b8e2-842e-4eea-928f-e458dc2c5d69, ae765be4-d94b-43ec-ae97-b44c794aa527, c1f53ff5-bc52-4e48-b684-e95c6da5d045, cb44a89a-8960-42e4-9c64-ea9ed80ef34f, d418438d-335b-4e4e-b1c3-c7589ea55b8c, d6667507-5891-4635-a45c-198eb0b1dd96, dbb241f0-8040-4020-8889-db0630e1dc96, dff6170c-baf0-46a1-8c14-4f60a1a70d6c, e1c66e81-97ce-4fbc-a794-17f27704c88d, f82f8849-d305-4b94-b058-872d03ecee32, fc2cebc5-51ad-4d19-ab99-01eb6a2e87dc]
9380 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit3162375088201141401/2015/03/17/b344a333-0587-4b55-ab6a-6740690c3810_2_100.parquet
9394 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 33 row keys from /tmp/junit3162375088201141401/2015/03/17/b344a333-0587-4b55-ab6a-6740690c3810_2_100.parquet
9394 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit3162375088201141401/2015/03/17/b344a333-0587-4b55-ab6a-6740690c3810_2_100.parquet => [02bb6603-f933-4030-8d5d-1d5c4eee1780, 065b873f-5d37-42a8-a531-909a0e5c853a, 093327cf-a2ae-4bc5-bc70-12da63079764, 1e97cb88-dff8-4b71-ac42-c32f1b6e7447, 24a495eb-a95f-4a3e-a73a-bf3a0a766f82, 29c41924-36a8-4270-ab85-20fb654d9f1c, 2b236e49-059a-4ac5-ba57-22d14916a5c7, 32711053-4bd9-4979-ac7b-dbc3d426895b, 3697f5cb-00db-4e81-8e28-11ffb509ab81, 3bedfd3d-0f6c-41ae-bdd2-40b52fa27c9b, 4351d564-afd0-457c-b314-b76eb8b3cfe9, 4592271f-29c8-4a7a-929b-1075cd142bb6, 58b5a14d-662e-462a-b9e1-7f3945cd17ac, 678cb486-f79b-42b3-9618-9400039353d6, 714cc617-cfe9-40a4-969f-6639129b4265, 75aa72c1-911f-4064-a812-bf3f82d16f68, 78fde6f6-fe5d-4fab-82c6-7933290583c3, b5f1f8ac-7367-404e-bacb-114deb02e7a1, b7afa20a-2a0c-4ecd-a0eb-84fb05ff60e3, bbfd9d24-0f3e-4796-b50f-ba811fa70f12, bced6930-a429-42ed-b629-f4df09fbb941, bea96307-fb3b-4dbc-a471-757debec46bf, bf1b6cad-1886-4ce7-b1a1-58038e643dd0, c1a85a4d-bb18-4c35-9da4-3b55e28ad4a6, c5a67c1a-55ea-43b2-ab5c-f6a6eeb148b8, d0830af9-daf0-460d-8c81-3418b0832b39, e00c3be9-5921-4875-a228-9aacc796a166, e12b128c-5c6c-4afd-8c38-6e8e353631c6, e4c9532c-019b-427a-85b0-e4096a5a06fc, eb06a0dc-c427-45d0-9b00-6919662df642, f1246d68-2678-410a-8b2a-d7891d859957, f7bb1e46-0ab3-49c7-89a6-2ffdd7a1e418, ffe63931-0e59-4264-961c-aa90e9f59e92]
ncodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:54 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 17 ms. row count = 38
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 38
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 195B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 128B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 961B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 862B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 953B for [_row_key] BINARY: 38 values, 1,520B raw, 854B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:56 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 187B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 120B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 752B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 842B for [_row_key] BINARY: 33 values, 1,320B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 113B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 767B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 668B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 758B for [_row_key] BINARY: 29 values, 1,160B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 38
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 195B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 128B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 961B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 862B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 953B for [_row_key] BINARY: 38 values, 1,520B raw, 854B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 187B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 120B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 752B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 842B for [_row_key] BINARY: 33 values, 1,320B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 113B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 767B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 668B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 758B for [_row_key] BINARY: 29 values, 1,160B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 38
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 195B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 128B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 961B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 862B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 953B for [_row_key] BINARY: 38 values, 1,520B raw, 854B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 187B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 120B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 752B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.par10819 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316081957
10923 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=33, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=30, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=37, numUpdates=0}}}
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 33, totalInsertBuckets => 1, recordsPerBucket => 500000
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 30, totalInsertBuckets => 1, recordsPerBucket => 500000
10935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
10936 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
10936 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
10936 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
10936 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
10963 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316081957
10963 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316081957
11206 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
11207 [pool-73-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
11233 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
11233 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
quet.hadoop.ColumnChunkPageWriteStore: written 842B for [_row_key] BINARY: 33 values, 1,320B raw, 743B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 179B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 112B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 767B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 668B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 758B for [_row_key] BINARY: 29 values, 1,160B raw, 659B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,754
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 186B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 119B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 856B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 757B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 849B for [_row_key] BINARY: 33 values, 1,320B raw, 750B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PAC11260 [pool-73-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
11293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
11293 [pool-74-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
11316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
11316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
11347 [pool-74-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
11373 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
11373 [pool-75-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
11398 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
11398 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
11430 [pool-75-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
11460 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
11460 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
11529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
11529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
11953 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
11961 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316081957 as complete
11962 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316081957
12349 [main] WARN  com.uber.hoodie.io.compact.HoodieRealtimeTableCompactor  - After filtering, Nothing to compact for /tmp/junit9012101177055384855
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.585 sec - in com.uber.hoodie.io.TestHoodieCompactor
Running com.uber.hoodie.func.TestUpdateMapFunction
12468 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
12469 [pool-76-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
12471 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
12471 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
KED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,154
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 183B for [_hoodie_commit_seqno] BINARY: 30 values, 726B raw, 116B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 791B for [_hoodie_record_key] BINARY: 30 values, 1,206B raw, 692B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 30 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 783B for [_row_key] BINARY: 30 values, 1,200B raw, 684B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 30 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 30 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [begin_lat] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [begin_lon] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [end_lat] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [end_lon] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [fare] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,554
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 195B for [_hoodie_commit_seqno] BINARY: 37 values, 894B raw, 128B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 940B for [_hoodie_record_key] BINARY: 37 values, 1,486B raw, 841B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 37 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 932B for [_row_key] BINARY: 37 values, 1,480B raw, 833B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 24B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 25B raw, 1B comp}
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [fare] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 518
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [_hoodie_commit_seqno] BINARY: 3 values, 45B raw, 43B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [_hoodie_record_key] BINARY: 3 values, 126B raw, 79B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 174B for [_row_key] BINARY: 3 values, 120B raw, 75B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 12536 [pool-76-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
49caa024-8358-486f-9868-ffbb46965751
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.174 sec <<< FAILURE! - in com.uber.hoodie.func.TestUpdateMapFunction
testSchemaEvolutionOnUpdate(com.uber.hoodie.func.TestUpdateMapFunction)  Time elapsed: 0.174 sec  <<< ERROR!
java.lang.NullPointerException
	at com.uber.hoodie.func.TestUpdateMapFunction.testSchemaEvolutionOnUpdate(TestUpdateMapFunction.java:106)

Running com.uber.hoodie.func.TestBufferedIterator
12593 [pool-77-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
12605 [pool-77-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
com.uber.hoodie.exception.HoodieException: operation has failed
	at com.uber.hoodie.func.BufferedIterator.throwExceptionIfFailed(BufferedIterator.java:195)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:163)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$2(TestBufferedIterator.java:155)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Failing it :)
	at com.uber.hoodie.func.TestBufferedIterator.testException(TestBufferedIterator.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
12830 [pool-77-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
12834 [pool-77-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
java.lang.RuntimeException: failing record reading
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$3(TestBufferedIterator.java:185)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17856 [pool-78-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
17877 [pool-78-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at com.uber.hoodie.func.BufferedIterator.insertRecord(BufferedIterator.java:126)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testMemoryLimitForBuffering$1(TestBufferedIterator.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17899 [pool-79-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
17989 [pool-79-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.412 sec - in com.uber.hoodie.func.TestBufferedIterator
Running com.uber.hoodie.config.HoodieWriteConfigTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.uber.hoodie.config.HoodieWriteConfigTest
Running com.uber.hoodie.table.TestCopyOnWriteTable
18165 [dispatcher-event-loop-13] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (152 KB). The maximum recommended task size is 100 KB.
18226 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
18228 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
18228 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
18228 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
18228 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]
18229 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]}, 
UpdateLocations mapped to buckets =>{file1=0}
18591 [dispatcher-event-loop-21] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 2 contains a task of very large size (752 KB). The maximum recommended task size is 100 KB.
18686 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
18687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
18687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
18687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 2200, totalInsertBuckets => 2, recordsPerBucket => 1000
18687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]
18687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]}, 
UpdateLocations mapped to buckets =>{file1=0}
19001 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
19001 [pool-110-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 3 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 56B raw, 2B comp}
Mar 16, 2018 8:19:59 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 66B for [number] INT32: 3 values, 18B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,240 > 65,536: flushing 280 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,325
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 723B for [_hoodie_commit_seqno] BINARY: 280 values, 6,727B raw, 655B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,324B for [_hoodie_record_key] BINARY: 280 values, 11,207B raw, 6,223B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,315B for [_row_key] BINARY: 280 values, 11,200B raw, 6,214B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 280 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 504B for [number] INT32: 280 values, 1,127B raw, 468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,304 > 65,536: flushing 280 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,389
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 720B for [_hoodie_commit_seqno] BINARY: 280 values, 6,791B raw, 651B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,342B for [_hoodie_record_key] BINARY: 280 values, 11,207B raw, 6,241B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,332B for [_row_key] BINARY: 280 values, 11,200B raw, 6,231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 280 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 505B for [number] INT32: 280 values, 1,127B raw, 469B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 727B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 657B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,314B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,307B for [_row_key] BINARY: 279 values, 11,160B raw, 6,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 503B for [number] INT32: 279 values, 1,123B raw, 467B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 727B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 657B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,322B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,221B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 19749 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
19749 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,313B for [_row_key] BINARY: 279 values, 11,160B raw, 6,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 504B for [number] INT32: 279 values, 1,123B raw, 468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 719B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 649B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,297B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,196B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,288B for [_row_key] BINARY: 279 values, 11,160B raw, 6,187B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 503B for [number] INT32: 279 values, 1,123B raw, 467B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 730B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 660B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,298B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,197B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,289B for [_row_key] BINARY: 279 values, 11,160B raw, 6,188B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 502B for [number] INT32: 279 values, 1,123B raw, 466B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 724B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 654B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,311B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,210B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,303B for [_row_key] BINARY: 279 values, 11,160B raw, 6,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 490B for [number] INT32: 279 values, 1,123B raw, 454B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,470
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 45 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 214B for [_hoodie_commit_seqno] BINARY: 45 values, 1,131B raw, 145B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,112B for [_hoodie_record_key] BINARY: 45 values, 1,806B raw, 1,013B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 45 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 en19766 [pool-110-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
ff56b96e-0f77-4ca5-a579-cfaac30f60b7_0_20180316082005.parquet-478559
b66cf947-b3c9-40a7-a7f3-ee02594792b2_0_20180316082005.parquet-478441
35438b43-f1c1-40f8-ab19-0cdf12bf1669_0_20180316082005.parquet-451782
19840 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
19841 [pool-126-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
19845 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
19845 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
19921 [pool-126-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
19923 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
19923 [pool-127-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
19927 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
19927 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
tries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 45 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,103B for [_row_key] BINARY: 45 values, 1,800B raw, 1,004B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 45 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 135B for [number] INT32: 45 values, 186B raw, 100B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,535
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 143B for [_hoodie_commit_seqno] BINARY: 10 values, 256B raw, 74B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 357B for [_hoodie_record_key] BINARY: 10 values, 406B raw, 258B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 350B for [_row_key] BINARY: 10 values, 400B raw, 251B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 10 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [number] INT32: 10 values, 46B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 830
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 5 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 131B for [_hoodie_commit_seqno] BINARY: 5 values, 131B raw, 63B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 255B for [_hoodie_record_key] BINARY: 5 values, 206B raw, 156B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 5 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 5 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 248B for [_row_key] BINARY: 5 values, 200B raw, 149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 5 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 71B for [number] INT32: 5 values, 26B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO20016 [pool-127-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
20227 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
20227 [pool-158-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
20231 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
20231 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
20308 [pool-158-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
20309 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
20310 [pool-159-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
20314 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
20314 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 37B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [time] BINARY: 1 values, 28B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,535
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 143B for [_hoodie_commit_seqno] BINARY: 10 values, 256B raw, 74B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 355B for [_hoodie_record_key] BINARY: 10 values, 406B raw, 256B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 347B for [_row_key] BINARY: 10 values, 400B raw, 248B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 10 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [number] INT32: 10 values, 46B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,535
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 143B for [_hoodie_commit_seqno] BINARY: 10 values, 256B raw, 74B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 359B for [_hoodie_record_key] BINARY: 10 values, 406B raw, 260B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 351B for [_row_key] BINARY: 10 values, 400B raw, 252B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 10 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 28B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [number] INT32: 10 values, 46B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PA20363 [pool-159-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
20521 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
20522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
20522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
20522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]
20522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]}, 
UpdateLocations mapped to buckets =>{file1=0}
20631 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
20631 [pool-190-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
20634 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
20634 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
20678 [pool-190-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
20767 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
20767 [pool-206-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
20769 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
20769 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
20802 [pool-206-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.864 sec <<< FAILURE! - in com.uber.hoodie.table.TestCopyOnWriteTable
testUpdateRecords(com.uber.hoodie.table.TestCopyOnWriteTable)  Time elapsed: 1.17 sec  <<< ERROR!
java.util.NoSuchElementException: spark.executor.memory
	at com.uber.hoodie.table.TestCopyOnWriteTable.testUpdateRecords(TestCopyOnWriteTable.java:197)

Running com.uber.hoodie.table.TestMergeOnReadTable
Formatting using clusterid: testClusterID
26419 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
26616 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
27781 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
29938 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
30594 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
30595 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
31386 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=73, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=63, numUpdates=0}}}
31840 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
31840 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
31840 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
31841 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
31861 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
31861 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
32150 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
32150 [pool-232-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
32205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
32205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
CKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 576
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [_hoodie_commit_seqno] BINARY: 3 values, 81B raw, 56B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [_hoodie_record_key] BINARY: 3 values, 126B raw, 79B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 174B for [_row_key] BINARY: 3 values, 120B raw, 75B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 3 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 56B raw, 2B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 66B for [number] INT32: 3 values, 18B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 576
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [_hoodie_commit_seqno] BINARY: 3 values, 81B raw, 56B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [_hoodie_record_key] BINARY: 3 values, 126B raw, 79B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 174B for [_row_key] BINARY: 3 values, 120B raw, 75B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 3 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 56B raw, 2B comp}
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 66B for [number] INT32: 3 values, 18B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3 records.
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:07 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 3
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,980
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 223B for [_hoodie_commit_seqno] BINARY: 73 values, 1,029B raw, 175B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,739B for [_hoodie_record_key] BINARY: 73 values, 2,927B raw, 1,639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 73 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,730B for [_row_key] BINARY: 73 values, 2,920B raw, 1,630B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 73 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 73 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for33096 [pool-232-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
33120 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
33121 [pool-233-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
33163 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
33163 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
33647 [pool-233-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
33678 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
33678 [pool-234-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
33717 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
33717 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
34188 [pool-234-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
34250 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
34250 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
34284 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
34285 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
34575 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
34599 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
34599 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
34800 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
35366 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 81
35366 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
35655 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit8385931549291184186/2015/03/16/af89a756-09fc-483c-8b55-387b558d8bcc_1_001.parquet
35686 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit8385931549291184186/2015/03/16/af89a756-09fc-483c-8b55-387b558d8bcc_1_001.parquet
35687 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit8385931549291184186/2015/03/16/af89a756-09fc-483c-8b55-387b558d8bcc_1_001.parquet => [0722b54a-ce7d-4bab-9115-ba64c5f48e6f, 111748b0-a4dc-4364-87cf-5e51aa63cb44, 1e9f0706-da34-4544-9cae-167b5e97736b, 3e39707e-42bb-4b70-8ce7-f64a08c95d51, 4129effc-f9c7-44f4-91b0-3eb30c4496ec, 4210c91a-6954-45f2-9692-c87515e7affc, 50de663a-8339-4723-8241-2a992041d06d, 588a81fd-d70c-4080-bf69-93744a2d8c06, 82c6b987-8b35-4315-b47e-0cd457429a1b, 835d8c75-eb61-4114-bf72-0a8bcf02f545, 8db175d9-b0bf-4029-ae78-553ed04a4858, 9c1d13d4-cf46-4db7-9324-bf7b821fa3e7, 9eaeb696-054c-4d91-9c20-6cd8d49aca0d, a54a56e7-07ba-4871-be28-4087cca70e4c, b1aaa7b1-8bd9-4f13-b1a3-25d5e8c55c40, be893434-7640-48f1-8076-1e53e357c22b, c4566558-4ca1-425a-ae99-68e3cc706dda, c4a666f9-426f-4521-a3dc-287b583a2d40, dda0372a-0788-4bc5-859b-83968db8c094, f6917694-58a3-43f3-bcab-7747dfca5ed4]
35721 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 21 for /tmp/junit8385931549291184186/2015/03/17/c69b3c20-c3d6-4b90-a50f-6ff5746641b0_2_001.parquet
35754 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit8385931549291184186/2015/03/17/c69b3c20-c3d6-4b90-a50f-6ff5746641b0_2_001.parquet
35754 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 21 results, for file /tmp/junit8385931549291184186/2015/03/17/c69b3c20-c3d6-4b90-a50f-6ff5746641b0_2_001.parquet => [06ad44d8-0361-4602-a3d1-37196dfcc0fe, 07f5eb80-4713-44b5-a328-9deecbfeb0bd, 07f9dbcb-d533-4f68-b211-20d76ef9cfd8, 2ca06f97-2811-4e0c-a1b9-eaf89a651cfc, 3936d90c-372e-4126-a425-b3cd3c1eaa80, 3b5279b7-593f-4729-bff2-70bc0c9814b5, 3fb95913-2dd4-4a1b-9791-4ef06e751339, 568f0e80-17cd-4cac-bc0c-9fdb7811d19e, 5851733d-347e-4f79-9fd8-0df04b5a7137, 69e8b168-a3d5-4ca8-9b10-c55ee54c695a, 6b375cf3-325e-4a11-a807-b23e399f502a, 76734aae-fbe9-4583-b293-836b0c561689, 7abbf1b7-d4d9-4945-bb79-f42d16af32b2, 7e7a98be-c408-4b60-9e0c-10c8778763de, 898cd2aa-98ec-45af-aa41-ea58a00a770d, 8d0b9e26-baed-451a-913a-c17abe2d4232, 9301095d-b29c-400a-924a-047e82544865, a32977ba-788f-467a-a2e3-8750ade20840, b3d8f790-f970-4a0f-934d-c8c8287249cd, b5bc2761-6e6b-4459-b08d-ad9e009c4526, b66a696d-446f-4469-a4a0-9f30c73e21c0]
35799 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit8385931549291184186/2015/03/17/c69b3c20-c3d6-4b90-a50f-6ff5746641b0_2_001.parquet
 [end_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:19 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [fare] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,270
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 64 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_commit_seqno] BINARY: 64 values, 903B raw, 161B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,537B for [_hoodie_record_key] BINARY: 64 values, 2,567B raw, 1,437B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 64 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 64 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 64 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,527B for [_row_key] BINARY: 64 values, 2,560B raw, 1,427B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 64 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 64 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [begin_lat] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [begin_lon] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [end_lat] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [end_lon] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:20 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [fare] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 157B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,513B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,507B for [_row_key] BINARY: 63 values, 2,520B raw, 1,408B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 13B raw, 1B comp}
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:21 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 64 records.
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 64
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 63
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.h35833 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit8385931549291184186/2015/03/17/c69b3c20-c3d6-4b90-a50f-6ff5746641b0_2_001.parquet
35834 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit8385931549291184186/2015/03/17/c69b3c20-c3d6-4b90-a50f-6ff5746641b0_2_001.parquet => [b91dbc32-a707-43e7-ada2-d358d493da3e, bac7ac52-3e3a-46a6-97bd-372713fde887, c0a77427-7c02-450b-9f17-aa02cda6ae76, c980a106-9f82-4efb-a664-2a28b11595e6, cc01785d-07e6-40de-8345-2b2db088276d, d83e8cb9-fc89-4369-9555-e913b8766bd8, dc0e0f84-8155-4939-9a8c-dad4ad49038e, ede96453-bb29-49ca-b8d6-2b0725937081, f1a61637-9bc1-4c9c-972d-3f908ffea9fe, fc035be5-0c38-476c-82ae-202da1c9eb5a]
35866 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 30 for /tmp/junit8385931549291184186/2016/03/15/d2e0826c-18e2-4241-af98-9bbb52517944_0_001.parquet
35892 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 73 row keys from /tmp/junit8385931549291184186/2016/03/15/d2e0826c-18e2-4241-af98-9bbb52517944_0_001.parquet
35892 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 30 results, for file /tmp/junit8385931549291184186/2016/03/15/d2e0826c-18e2-4241-af98-9bbb52517944_0_001.parquet => [0f291a53-96fd-4a3b-9db6-47af0da289b7, 2b10d4a8-830b-4076-a027-4c429fe31506, 3e35a9b1-1c8e-4c1a-813e-8c1b97a7f07a, 459f8b49-263d-422f-a0e5-599144c6a8bb, 48d68fbe-ccbf-4830-9aed-786418cd6899, 4c93f6d7-8a72-4586-b55e-05317011beaa, 522d097d-9b5f-46dc-98dc-6ed506ee5acc, 5494f0d1-f5d1-4c7f-8ba1-1d1230b37691, 5873444f-ed84-45db-b6c8-019c9d5c8af1, 719bd9a7-bcba-4e12-9263-54a5a5805f85, 774df747-d01f-4301-8422-84a947c1e216, 787d3184-13b1-47d1-b608-d6684deb7a42, 7d0a7a85-5204-4129-afdd-91783cdbf9eb, 7e627c06-6f29-4b12-b87c-62838a17ed3b, 85b9c35c-b1b5-4791-9428-9bf7f3b3848b, 8e77127b-1f52-46dd-82ab-1f009d807b45, 9b66fde4-fe8b-4092-a207-47bb32e56970, 9e8c0125-6000-4748-8c9c-f9e6e778c26a, a6739a39-bf1a-4fe7-aa97-353596ac3927, b275ed75-6520-4090-82ab-fe6661ced0c2, b6a150fb-2370-4bf1-a732-a9616b115ec2, cd9e57c2-7457-4973-99cb-432150ea7861, d8988ad9-2e45-40a9-be6f-0d90e53bfb1f, ebfc6c22-1b7a-40b5-9654-2894c4898855, ecb244b0-35e0-44de-9d85-5f974dcb827f, f12a013c-cbb9-4d1c-a9b8-7da596f7ee90, f3b485d5-9c95-42f4-8690-9f9afb0b0eec, f7cfaff6-c989-4a37-88e6-8a17635bd53f, f83bdfcb-2df7-4167-a5a7-0d11b277e050, ff9bfe4e-b78c-4fc4-a2d3-f7dfff58ee08]
35979 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=81}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=20}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=31}}}
36402 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
36402 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=c69b3c20-c3d6-4b90-a50f-6ff5746641b0}, 1=BucketInfo {bucketType=UPDATE, fileLoc=af89a756-09fc-483c-8b55-387b558d8bcc}, 2=BucketInfo {bucketType=UPDATE, fileLoc=d2e0826c-18e2-4241-af98-9bbb52517944}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{c69b3c20-c3d6-4b90-a50f-6ff5746641b0=0, af89a756-09fc-483c-8b55-387b558d8bcc=1, d2e0826c-18e2-4241-af98-9bbb52517944=2}
36429 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
36429 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
36681 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file c69b3c20-c3d6-4b90-a50f-6ff5746641b0
36789 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file af89a756-09fc-483c-8b55-387b558d8bcc
36860 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file d2e0826c-18e2-4241-af98-9bbb52517944
37375 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
37375 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
37409 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
37409 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
37663 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
38090 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
38091 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
38260 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082025
38290 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Checking if compaction needs to be run on /tmp/junit8385931549291184186
38292 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Compacting merge on read table /tmp/junit8385931549291184186
38495 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082025
38495 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082025
adoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 63
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 73 records.
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:22 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 73
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 63
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,531
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_commit_time] BINARY: 63 values, 16B raw, 36B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 25B raw, 2B comp}
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 258B for [_hoodie_commit_seqno] BINARY: 63 values, 1,229B raw, 200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,513B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 172B for [_hoodie_file_name] BINARY: 63 values, 16B raw, 36B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 119B raw, 2B comp}
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,507B for [_row_key] BINARY: 63 values, 2,520B raw, 1,408B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [rider] BINARY: 63 values, 10B raw, 30B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 26B raw, 2B comp}
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 75B for [driver] BINARY: 63 values, 10B raw, 30B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 28B raw, 2B comp}
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:25 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 73 records.
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 73
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 14,420
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_time] BINARY: 73 values, 19B raw, 40B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 25B raw, 2B comp}
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 284B for [_hoodie_commit_seqno] BINARY: 73 values, 1,359B raw, 225B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,739B for [_hoodie_record_key] BINARY: 73 values, 2,927B raw, 1,639B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 177B for [_hoodie_file_name] BINARY: 73 values, 19B raw, 40B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 119B raw, 2B comp}
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 73 values, 3B raw, 23B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 8B raw, 1B comp}
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,730B for [_row_key] BINARY: 73 values, 2,920B raw, 1,630B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 77B for [rider] BINARY: 73 values, 12B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 26B raw, 2B comp}
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B for [driver] BINARY: 73 values, 12B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 28B raw, 2B comp}
Mar 16, 2018 8:20:26 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lat] D40669 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
40669 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
40694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
40694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
40976 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
41398 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082025 as complete
41398 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082025
47991 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
48665 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
48665 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
48942 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=63, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=73, numUpdates=0}}}
48957 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
48957 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
48957 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
48958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
48972 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
48973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
49182 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
49183 [pool-265-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
49222 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
49222 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
50077 [pool-265-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
50104 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
50105 [pool-266-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
50146 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
50146 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
50986 [pool-266-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
51014 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
51014 [pool-267-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
51058 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
51058 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
51920 [pool-267-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
52367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
52367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
52385 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
52385 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
52665 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
53085 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
53085 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
53209 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
53753 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
53753 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
54050 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 73 for /tmp/junit311318339661483548/2015/03/17/24c3553d-ba86-49b6-a7d4-9539f3272806_2_001.parquet
54089 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 73 row keys from /tmp/junit311318339661483548/2015/03/17/24c3553d-ba86-49b6-a7d4-9539f3272806_2_001.parquet
54089 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 73 results, for file /tmp/junit311318339661483548/2015/03/17/24c3553d-ba86-49b6-a7d4-9539f3272806_2_001.parquet => [016a8f04-0741-4282-b8f3-e2fb7e067cf2, 0502f3d7-21c0-457a-9346-fce2ceca5d2c, 0788c69c-297f-4a03-8001-0a4923815bec, 0885954b-be49-4036-8848-d62f85044f60, 0c85073d-8b72-4f3a-b85e-1f24d56e66b5, 10b56d47-1603-4b98-acfa-7dec2059d7c7, 1d28e83f-5eb0-4b6c-8c10-edf4b31b1e01, 221675ce-10f0-400b-83cb-163ac2c076b1, 2286652d-a6a4-48b7-ae4e-e3b55e80024c, 24e0b19f-809d-4d10-9429-954013b1a089, 272f617b-b40f-4e26-acea-963dac5e40ee, 27e6bba7-2fee-4f5f-91f9-1094f21430cf, 2c96fed4-90f7-4070-83e9-b1eb65196291, 303fee8b-5908-43f6-bc1f-d3d583042653, 3296c4cb-8c44-481a-b538-e45133828f22, 3416cdf0-6943-44b6-a31e-9261ab498f2b, 34bbaa00-88ab-4ca8-b83f-307ec1340317, 364ea7aa-8d08-4a99-b7a1-b91dd56efa8d, 3f687ee2-9e04-4725-8066-71624784ea80, 3fa93b8c-dcfd-47b8-ab57-e18ca35b4677, 412e6cd2-ba90-477a-a8a0-bf818da84348, 42644892-a291-4e56-8a24-07675007b9b2, 4334bede-422e-4ae3-9837-0809f26ce2bb, 47729a7e-8df0-4dd6-9a9c-5765ffb93e32, 4c89a602-2360-46a7-8f0f-ec6ac920a29c, 4cc94d82-4cf5-4eef-8ecb-fad7cd187131, 538ec7eb-9bbf-4248-a875-df1f98cfaa64, 53db7d85-afb4-4890-905d-355d9ef4442e, 545dd120-9683-4007-a2ca-759771f7db5b, 5818b599-a5ed-4532-b77e-4f6f79b1261a, 58b1dcb3-0f13-4333-83e2-9622be86663d, 6178993d-fc7e-4273-ac5e-028ebd02709e, 6c9fb6da-2105-455a-a31c-3ba8d9012dbb, 6e602da4-42c6-4455-be83-884bfd0424ce, 6e65752e-5b33-470a-ad60-eea8defb2c3a, 77490d17-e036-4a9d-b877-7c1c38e0da1f, 7c568ffb-6f6d-495b-b047-97ff50c5dd8d, 84bd6211-2a91-4f74-8f43-1ef3f1129776, 84be9335-244c-472a-8838-aa87091c938f, 85e5d33f-2dd2-42ea-81af-53b4036a2a1c, 888fdf0e-4895-4738-ae83-ec432caf9a09, 8d9c311a-f81d-479a-9503-89302233dd9c, 8eb20ea5-39e3-44c1-adb3-abcd0721b00d, 9320b091-f3b1-45ae-b058-1bd237f1927f, 9385c8fd-0774-42c9-8bdf-2393481571f0, 95ddd70d-a7b7-4beb-95ad-aa6b7db69e80, 98c8abae-06cf-41d3-9684-3c7d4e8db27c, 9a57d876-442a-4a38-9405-564060ca9b78, 9bca6797-6a3b-42ca-a29e-fc99879f7ef2, 9d34818b-798d-4072-bdfc-1a504eeefd50, a05a914f-aebb-432d-be41-b34b0dcbd8a9, a2342cea-c2ad-4a15-8b34-e69c769918ff, b1745e92-dc70-48bb-8ef5-3a23d63b8c8f, b1c106b4-5510-41f3-a258-2a6d5ac3171d, b694a003-8236-493e-8b41-de5f3674c5ff, b8ddbd75-6373-4067-b62d-c01695bc8f4f, bbc50751-b090-4951-8b42-4cf3006ff7e6, bca43442-6f0f-4e19-81c3-7986ce314c64, c54874c3-df17-4cfd-bdee-19463f15930e, c57847cc-7799-4056-b715-890719f11eb5, c9b79a09-8130-48c7-b12b-6f85186de3bd, cf13a1aa-304a-4fd7-9387-38a1ba652c9b, d219080a-b83e-4d03-af74-2ca176462d68, d30d1259-0ab8-401d-a0ce-65813090c7df, d32641fc-460b-4854-9b2e-554fb5bd1006, d550060b-6e28-4404-87ba-66c9bf0eefe3, deca6559-2dcd-474b-a159-0a69b555c9d8, e0312066-7a30-4404-bf4a-b9987aaddefd, f057f8a7-6f32-4a12-acfd-7fec092e9019, f2e46f57-5dec-4e1a-8a8f-e1e759d32f52, f3049255-e13e-4d83-b692-b2a73f66aa70, fab09a81-9969-439d-a119-35d9e8d2b9e8, fbcdca59-703e-4d2b-a8cb-9839c9a0606b]
54113 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit311318339661483548/2015/03/16/302158be-7a12-40f1-b054-6bafe6df60a1_1_001.parquet
54137 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit311318339661483548/2015/03/16/302158be-7a12-40f1-b054-6bafe6df60a1_1_001.parquet
54137 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit311318339661483548/2015/03/16/302158be-7a12-40f1-b054-6bafe6df60a1_1_001.parquet => [02e09020-fa9f-4c20-9ec7-c04a0f664ec7, 05b3ec9b-4c81-4315-8e86-1ca195f399d1, 07c283b7-918e-40ac-95a2-fec2bd840f19, 0f4911a6-2492-4f19-b255-41997a8dc5f3, 171fc3fa-b917-46b9-b935-469528dba4aa, 186d6e19-0a43-430f-961a-5a567ffa815a, 18ecc310-e1ab-4ab6-a9de-bc4b5497e181, 19970eb0-8170-423d-b7e1-c35a20b44c72, 1bbce48f-a03b-425e-b056-7e44bfa6fb23, 1f48260d-0f01-493d-97d6-32c170ed4a5a, 20bb2fd3-1a8d-4f4d-a5dc-eff99602d46d, 213073b4-037e-4856-87b8-8e7e364f3d81, 22f02682-6b0e-40cd-976c-06264181d13c, 233b30e8-d101-409e-9b7d-fa8c87e4171c, 34792276-1fab-4e3d-89b0-c21b749ad351, 375a73ed-d021-41c1-a267-8e4e1c1a8ec5, 37f6d201-1355-427f-bd33-4e1166202de0, 39ba3bf4-2a44-4caa-8430-2f23b9688cd9, 39f64dd9-75af-4b0d-955f-916eed07c453, 430d84d6-cd09-45e0-b7e3-edaa7d5cd18e, 4fb6779e-22aa-4db4-8324-80f8a84b1780, 5b29b3bf-0965-492a-afe2-3cf91d4b67e7, 5b629d4e-1d08-4b95-b51b-003275574bb5, 62970866-c63c-4895-b942-7ea9d8fb02f6, 65a66172-d892-411a-ac93-b804ec6c0ba8, 6864fe0f-ff12-42e6-bbb8-0543d1b2524a, 6917d295-d866-411d-96b5-bbb61cb43e23, 6a5c8205-2faa-4ea3-bbce-dbefe24cb298, 7764bbe4-e222-4042-b784-ec3ca6e3bd81, 7c3c517b-8cc2-4db0-904c-59afe23ae559, 80f5c675-e1e7-499b-a9dd-06a1fe8feff4, 8109e72c-b652-4b8e-b637-5880603fcd50, 8601596a-6cc4-4653-a158-8d6ce36dbfd6]
54182 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 30 for /tmp/junit311318339661483548/2015/03/16/302158be-7a12-40f1-b054-6bafe6df60a1_1_001.parquet
54204 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit311318339661483548/2015/03/16/302158be-7a12-40f1-b054-6bafe6df60a1_1_001.parquet
54205 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 30 results, for file /tmp/junit311318339661483548/2015/03/16/302158be-7a12-40f1-b054-6bafe6df60a1_1_001.parquet => [8b4db5f0-c240-46c3-b85c-2c93d354125b, 8b4dbf79-1fa8-4947-ba3e-954f9dc8dbf1, 907bb5b3-c7be-467a-8c96-8961c2cab5bc, 9c015eb5-4dae-4449-8511-5aa205e58114, 9f76b182-7199-4f2d-a1a2-b5ccaa757883, a372573c-1bec-48fe-a1bb-876195516362, aaac3354-0257-42ec-96cb-bd461158262a, ab45af74-a337-4bf0-a504-59db231f311c, ae87798e-ae0a-4064-af3c-4cac5e13ad0e, b05dc288-270a-4563-a3df-bcf276d2057e, bf47b769-ab9d-4be1-bb87-22a3523445f0, c71a1ea1-fe3b-441b-a2b0-5cc94cf069d5, c71c1f09-4fb6-482c-91a9-457f703a9f7e, c7a5864a-93c8-485d-ad01-b7b5de1205cd, c82d51d9-391a-45df-8fab-e54a00386bc5, d1a15397-c6e4-4089-8025-11a86be75f51, d3d077b6-52e7-4345-b66b-705a81dfbc94, d99f0f76-e688-4e26-bede-24ba9fd77050, da9be3f9-45f5-45a4-af17-06934c1e32fc, dc7a6f9b-753e-472d-b16c-24de054d5031, e6c14ad8-efc8-4821-ad65-cc5de5399d12, e7add7eb-aee5-46be-8d47-b7eb1ba0a987, e81bbb20-78aa-464f-bd42-192f741c0f24, ed24ef97-3589-45d2-bfc0-c3fee1a64c1f, ed9b76fe-9298-4e6a-b2cd-fd6bed45f442, efb33e49-13bd-430f-9562-7e6f8d239172, f24d7e1c-b3e9-4789-9675-135d23d2dac3, f41d438a-9bfa-42d0-be15-e2061ea0d9e5, f634dcb6-92be-492e-82b1-cfb63d8dc94a, f9702921-9ae6-4683-94bb-c37990ab35f2]
54231 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 64 for /tmp/junit311318339661483548/2016/03/15/344c7175-a60e-4c06-87c5-ee6caced1e51_0_001.parquet
54256 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit311318339661483548/2016/03/15/344c7175-a60e-4c06-87c5-ee6caced1e51_0_001.parquet
54256 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 64 results, for file /tmp/junit311318339661483548/2016/03/15/344c7175-a60e-4c06-87c5-ee6caced1e51_0_001.parquet => [058cfad4-3668-454b-a7f5-a1670d49eab7, 0e7df637-9895-427d-b28c-477e751e8b2b, 0f901b29-e583-4ff2-99bd-2ec560f2e23f, 0f998cba-5b2e-4b0e-ac29-d3bc69561b82, 104bca74-5575-4576-9ead-42bb74cf67a5, 137a1859-7d2e-45d0-a3d8-493dd957d98a, 1d9f6ef8-74fb-4d6e-8f96-31ece32f9ec9, 1ecf82d7-ca0b-49e3-80c6-42c59fead642, 206ea95c-6526-409c-bb25-0959f4696f10, 2142932b-b17c-4f99-9266-84d05b0ad3fc, 228416ed-36c2-4611-9c52-642cdb5d40a4, 25050f04-d6c3-4b9d-9df1-ebb84951c8f2, 31449d7f-ef5b-42b8-817d-5e087b09fde6, 388d8dac-5250-4c58-8067-c34d01b3f3fe, 404866a8-6f8f-44ef-a271-89f883737577, 40ec2eb4-4e16-4978-b6d7-2b88d30dc2c6, 46d4c3ba-8e98-46da-8af4-43e6c2c56405, 49c4d04a-dff8-40f9-b321-9a8c8fa1f322, 4b951013-52c1-4377-8c76-79b5dafa8a0c, 4bf9517f-7b35-43c9-b1b4-860991963cd3, 4d239e10-b6ee-4f55-a2d0-4e454771d4ae, 4e078cb2-8022-4067-baf8-5dd9492c0c1f, 4ec884aa-93cb-4d0f-929e-d99f10817a3d, 50085863-6fb1-4718-a314-d3a7e0c967bb, 56df7272-37a0-454f-b454-405b628aad9c, 57cb9235-4119-4241-b122-91642972bcdf, 588fb15c-ad19-4446-b4bb-d7f370a42001, 58b0fbfb-dcac-4096-8282-18004cee7739, 5ce433b7-c224-4daf-8457-c926a681fd79, 6567d9de-9f31-4353-9fed-38300ceb194e, 66071072-2150-479c-b54a-9d72cde948bc, 6745ec8a-3ca8-4946-918d-c53cf6cd5f92, 69e2019c-097d-4cd4-83a2-955e9ea556d0, 6a29e7bd-2760-48c8-b903-40f54b4c26d5, 716bb6e9-b288-49f8-99c4-59b5066a4dcd, 74030d4a-776b-42f0-8f49-9b1cba4d3efb, 7494a937-2203-4413-8111-e62d23703598, 7ba1d14a-7e41-423d-8027-c6e127025774, 7ef6a3a8-3f5e-4570-8d51-ca33181711fa, 8a53a0c4-353c-442c-bdc1-a462231ce6b0, 8e463a4c-47fc-4e0f-a60a-33ab63a3824e, 8f1e504d-ccab-4af9-b243-0c3fab5fec07, 8f5d66d7-fadc-4291-944c-8674a04e1fdf, 91d90186-a346-435c-b6cd-c18216ee18f9, 9260a409-609d-4cf2-b8c6-a35f87ecbb10, 97cd3daa-ddb8-4a23-974e-d8e5a5c39d3d, 9888b642-d62f-494e-a9ba-38ae6dce8748, b45492ae-7844-4188-a6c3-1595b79452c6, be877498-cbac-4505-b2b4-4e8776f194b5, bfbd88c8-e358-4b6a-9169-1851cb8e3ccf, c4a12a3a-8135-4873-9970-d577ce099450, c824ec12-67ea-4c56-973a-11eef100c2dd, cb427b1e-7603-4188-afec-7c3b9f9fa4db, cb91276c-483a-4469-ad16-8d329e3b48b0, d236e6dc-9b29-4d82-b64e-73d7dc051949, d9c4d264-69bf-4e7a-ac80-83c62a895d67, e451e723-7c19-47b7-90de-3e983ff54e21, e5532064-608c-459c-a5c8-964af888e5ff, eb6fbc6c-cd31-4f59-8397-c69ffa571370, ebaf7511-fe6d-4b23-b8a9-ded0b1ed372a, ed0db896-93ac-4466-a4c0-c21690855593, f76cb7b1-4bda-4c9f-8c8e-df8fb173ccbe, faa48f85-31af-4d5c-a101-b6b3920dbedd, ff9e7b27-06cf-47ed-9103-2514ecd5cba1]
54373 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=64}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=63}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=73}}}
54805 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
54805 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=24c3553d-ba86-49b6-a7d4-9539f3272806}, 1=BucketInfo {bucketType=UPDATE, fileLoc=344c7175-a60e-4c06-87c5-ee6caced1e51}, 2=BucketInfo {bucketType=UPDATE, fileLoc=302158be-7a12-40f1-b054-6bafe6df60a1}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{24c3553d-ba86-49b6-a7d4-9539f3272806=0, 344c7175-a60e-4c06-87c5-ee6caced1e51=1, 302158be-7a12-40f1-b054-6bafe6df60a1=2}
54822 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
54822 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
55034 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
55037 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_0 failed due to an exception
55038 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_0 could not be removed as it was not found on disk or in memory
55042 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 50.0 (TID 66)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
55065 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

55067 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 50.0 failed 1 times; aborting job
55070 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :1
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
55071 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_1 failed due to an exception
55071 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_1 could not be removed as it was not found on disk or in memory
55072 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 50.0 (TID 67)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
55074 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 1.0 in stage 50.0 (TID 67, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

55599 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
55973 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
55973 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
56278 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=9, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=8, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=3, numUpdates=0}}}
56694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
56694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
56694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 9, totalInsertBuckets => 1, recordsPerBucket => 500000
56694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
56694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
56695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 8, totalInsertBuckets => 1, recordsPerBucket => 500000
56695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
56695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
56695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 3, totalInsertBuckets => 1, recordsPerBucket => 500000
56695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
56695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
56716 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
56716 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
56926 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
56926 [pool-291-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
56934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
56934 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
57802 [pool-291-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
57823 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
57823 [pool-292-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
57828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
57828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
58697 [pool-292-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
58732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
58732 [pool-293-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
58735 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
58735 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
59214 [pool-293-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
59655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
59655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
59676 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
59676 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
60017 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
60433 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
60433 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
60611 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
61139 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
61139 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
61378 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet
61409 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 3 row keys from /tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet
61409 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet => [45ac1011-2d26-4835-8cb4-0ad2b78e791f, a83425e9-fea6-4434-bf34-487783b057d7, d0ac1f6a-eabb-4f1a-8953-1b0d5ef4ba95]
61439 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
61471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
61471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet => [346213d7-0e10-4095-b268-ec1667ddf920, 44c5c7a7-7868-4cfc-ad2c-003ba8244722, 5a95472a-97ed-4c56-b6a1-fcf7dc11bce4, 6ccb2c05-228c-48cd-95dc-ecafc3d4eda2, 95a007e5-0851-484c-bbc9-f6a6d55a31b2, 9f962b0f-290f-47b0-bc25-a507386c6258, ac7541e1-f49c-40ed-b5f3-0066c9a6e539]
61522 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 2 for /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
61545 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
61545 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet => [bb6e5a6f-a7e9-429a-af93-0683eaf1cc70, cd544c9c-63ca-4406-8bfe-d58fd0fd17ac]
61571 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 8 for /tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet
61593 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet
61593 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 8 results, for file /tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet => [19b08ac9-2896-47fa-a992-d39642fada79, 2b7b061f-2032-4346-ab26-b5fb0b0d1e0b, 2f42eff6-f113-4b4e-abd8-321d4481da74, 377b3ea6-9049-420a-a14b-99fb605ed76c, 4a74ec0f-94b7-4667-ab76-bad15054b4f8, 68a62200-a67a-4942-b995-e83a1f66563e, b38beff3-4c62-4b84-9835-2a7ce69ccd86, b729f5c8-2956-4dac-88ca-32b7f0bc67e0]
61699 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=9}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=8}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=3}}}
62119 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
62119 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=623a3c0d-c7fb-4051-ad14-5e3067ce879f}, 1=BucketInfo {bucketType=UPDATE, fileLoc=a582e15b-9dda-4089-8525-39292810d658}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4e921783-1296-4aad-bef1-fcfc38a9a51a}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{623a3c0d-c7fb-4051-ad14-5e3067ce879f=0, a582e15b-9dda-4089-8525-39292810d658=1, 4e921783-1296-4aad-bef1-fcfc38a9a51a=2}
62139 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
62139 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
62347 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 623a3c0d-c7fb-4051-ad14-5e3067ce879f
62810 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file a582e15b-9dda-4089-8525-39292810d658
63267 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 4e921783-1296-4aad-bef1-fcfc38a9a51a
64158 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
64158 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
64178 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
64178 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
64445 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
64861 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
64862 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
65009 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
65430 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
65430 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
65758 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet
65779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 3 row keys from /tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet
65779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet => [45ac1011-2d26-4835-8cb4-0ad2b78e791f, a83425e9-fea6-4434-bf34-487783b057d7, d0ac1f6a-eabb-4f1a-8953-1b0d5ef4ba95]
65799 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
65820 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
65820 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet => [346213d7-0e10-4095-b268-ec1667ddf920, 44c5c7a7-7868-4cfc-ad2c-003ba8244722, 5a95472a-97ed-4c56-b6a1-fcf7dc11bce4, 6ccb2c05-228c-48cd-95dc-ecafc3d4eda2, 95a007e5-0851-484c-bbc9-f6a6d55a31b2, 9f962b0f-290f-47b0-bc25-a507386c6258, ac7541e1-f49c-40ed-b5f3-0066c9a6e539]
65857 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 2 for /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
65876 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet
65876 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet => [bb6e5a6f-a7e9-429a-af93-0683eaf1cc70, cd544c9c-63ca-4406-8bfe-d58fd0fd17ac]
65897 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 8 for /tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet
65918 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet
65918 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 8 results, for file /tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet => [19b08ac9-2896-47fa-a992-d39642fada79, 2b7b061f-2032-4346-ab26-b5fb0b0d1e0b, 2f42eff6-f113-4b4e-abd8-321d4481da74, 377b3ea6-9049-420a-a14b-99fb605ed76c, 4a74ec0f-94b7-4667-ab76-bad15054b4f8, 68a62200-a67a-4942-b995-e83a1f66563e, b38beff3-4c62-4b84-9835-2a7ce69ccd86, b729f5c8-2956-4dac-88ca-32b7f0bc67e0]
65980 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=9}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=8}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=3}}}
66395 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
66395 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=623a3c0d-c7fb-4051-ad14-5e3067ce879f}, 1=BucketInfo {bucketType=UPDATE, fileLoc=a582e15b-9dda-4089-8525-39292810d658}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4e921783-1296-4aad-bef1-fcfc38a9a51a}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{623a3c0d-c7fb-4051-ad14-5e3067ce879f=0, a582e15b-9dda-4089-8525-39292810d658=1, 4e921783-1296-4aad-bef1-fcfc38a9a51a=2}
66407 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
66407 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
66602 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 623a3c0d-c7fb-4051-ad14-5e3067ce879f
67112 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file a582e15b-9dda-4089-8525-39292810d658
67575 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 4e921783-1296-4aad-bef1-fcfc38a9a51a
68458 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
68458 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
68475 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
68475 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
68721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
69137 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
69137 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
69423 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit1794041572377723184
69430 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
69436 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
69438 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
69451 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit1794041572377723184
69468 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
69468 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
70041 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
70290 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
70313 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit1794041572377723184/2016/03/15/.623a3c0d-c7fb-4051-ad14-5e3067ce879f_001.log.1] for base split hdfs://localhost:36206/tmp/junit1794041572377723184/2016/03/15/623a3c0d-c7fb-4051-ad14-5e3067ce879f_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
70353 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit1794041572377723184
70360 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
70361 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
70363 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
70372 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit1794041572377723184
70380 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
70380 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
70391 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
70407 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
70418 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit1794041572377723184/2015/03/16/.a582e15b-9dda-4089-8525-39292810d658_001.log.1] for base split hdfs://localhost:36206/tmp/junit1794041572377723184/2015/03/16/a582e15b-9dda-4089-8525-39292810d658_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
70445 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit1794041572377723184
70451 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
70452 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
70454 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
70459 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit1794041572377723184
70466 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
70466 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
70477 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
70493 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
70503 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit1794041572377723184/2015/03/17/.4e921783-1296-4aad-bef1-fcfc38a9a51a_001.log.1] for base split hdfs://localhost:36206/tmp/junit1794041572377723184/2015/03/17/4e921783-1296-4aad-bef1-fcfc38a9a51a_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
71032 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
71507 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
71507 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
71781 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=5, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=6, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=9, numUpdates=0}}}
72198 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 5, totalInsertBuckets => 1, recordsPerBucket => 500000
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 6, totalInsertBuckets => 1, recordsPerBucket => 500000
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 9, totalInsertBuckets => 1, recordsPerBucket => 500000
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
72199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
72220 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
72220 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
72435 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
72436 [pool-325-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
72439 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
72439 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
72916 [pool-325-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
72943 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
72943 [pool-326-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
72949 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
72949 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
73819 [pool-326-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
73847 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
73847 [pool-327-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
73854 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
73854 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
74708 [pool-327-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
75149 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
75149 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
75164 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
75164 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
75447 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
75867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
75867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
76037 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
76576 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 37
76576 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
76832 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit6471937896888016085/2015/03/17/b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38_2_001.parquet
76867 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit6471937896888016085/2015/03/17/b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38_2_001.parquet
76867 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit6471937896888016085/2015/03/17/b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38_2_001.parquet => [09875160-9804-493b-bbc3-41af7882cabe, 26fd1bc4-40f6-44ce-afb5-dfc247d1f276, 553a4cc4-82a9-4c00-bb78-c6c24bdd07b1, 598280f4-5d7a-4679-b56a-490a1ac0ed43, 854e6b19-ed3f-4621-bbc8-8cb922c7d8a5, a0a06fe0-439e-4fe1-befb-5fce5440fff1, a1cbb2d4-3416-4a56-a5c2-bfd64c2e700e, ba43a8a9-c71a-4db4-a594-4e45a5d436b3, e9ec6f3d-746b-494f-80ec-d0460666c185]
76899 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6471937896888016085/2015/03/16/d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2_1_001.parquet
76925 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 6 row keys from /tmp/junit6471937896888016085/2015/03/16/d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2_1_001.parquet
76925 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6471937896888016085/2015/03/16/d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2_1_001.parquet => [0179aa32-76fc-42ea-a80c-6d8c0c685a13]
76970 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit6471937896888016085/2015/03/16/d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2_1_001.parquet
76993 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 6 row keys from /tmp/junit6471937896888016085/2015/03/16/d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2_1_001.parquet
76993 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit6471937896888016085/2015/03/16/d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2_1_001.parquet => [6e6a58b6-5589-4ce6-beff-de212142d7e2, 6f908525-ce06-48e8-be4c-9d84fcd1816d, 7f51e309-ab75-4f71-bf30-6fd05b507fa3, ab13d5df-9559-4919-b457-68780686a247, b73d8467-fc7f-4206-942f-8e8b5bebded0]
77031 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit6471937896888016085/2016/03/15/e8362569-a30e-4175-9591-89df360db5ab_0_001.parquet
77062 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit6471937896888016085/2016/03/15/e8362569-a30e-4175-9591-89df360db5ab_0_001.parquet
77062 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit6471937896888016085/2016/03/15/e8362569-a30e-4175-9591-89df360db5ab_0_001.parquet => [0148f969-ebcf-4ae2-8361-9a8b40495974, 3f827f12-3a83-4e43-8221-47c4b1a67505, 774becd6-ba52-42b1-ace7-28841dad45f5, ae675e94-cedc-4723-a93f-c4b9a303b36f, c822c722-1833-4993-8bc2-bbb98fb1ff7e]
77144 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=7, numUpdates=5}, 2015/03/16=WorkloadStat {numInserts=4, numUpdates=6}, 2015/03/17=WorkloadStat {numInserts=9, numUpdates=9}}}
77563 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
77570 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=e8362569-a30e-4175-9591-89df360db5ab}, sizeBytes=435597}]
77571 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 7 inserts to existing update bucket 2
77571 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=2, weight=1.0}]
77573 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2}, sizeBytes=435692}]
77573 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 4 inserts to existing update bucket 0
77573 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=0, weight=1.0}]
77575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38}, sizeBytes=435949}]
77575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 9 inserts to existing update bucket 1
77575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=1, weight=1.0}]
77575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38}, 2=BucketInfo {bucketType=UPDATE, fileLoc=e8362569-a30e-4175-9591-89df360db5ab}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=2, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=1, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2=0, b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38=1, e8362569-a30e-4175-9591-89df360db5ab=2}
77600 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
77600 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
77834 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2
77835 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file d73ff0c2-fc9c-4ee0-8b88-e1cb747431c2
77836 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:158)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
77837 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_0 failed due to an exception
77837 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_0 could not be removed as it was not found on disk or in memory
77838 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 50.0 (TID 66)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:158)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
77841 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:158)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

77842 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 50.0 failed 1 times; aborting job
77852 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38
77853 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file b4ab06ae-b9f8-46ac-a26a-06dd9cc6bd38
77855 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :1
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:158)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
77856 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_1 failed due to an exception
77856 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_1 could not be removed as it was not found on disk or in memory
77856 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 50.0 (TID 67)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:158)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
77859 [dispatcher-event-loop-14] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 36496, None),rdd_82_1,StorageLevel(1 replicas),0,0))
78391 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082105
78869 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
78869 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
79244 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=56, numUpdates=0}}}
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
79258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
79277 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
79504 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
79504 [pool-351-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
79553 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
79553 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
80398 [pool-351-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
80428 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
80428 [pool-352-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
80472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
80472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
81308 [pool-352-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
81342 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
81342 [pool-353-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
81378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
81378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
82221 [pool-353-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
82775 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
83281 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
83281 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
83556 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=74, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=67, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=59, numUpdates=0}}}
83971 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
83971 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
83971 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 74, totalInsertBuckets => 1, recordsPerBucket => 500000
83971 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
83971 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
83971 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 67, totalInsertBuckets => 1, recordsPerBucket => 500000
83972 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
83972 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
83972 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 59, totalInsertBuckets => 1, recordsPerBucket => 500000
83972 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
83972 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
83992 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
83992 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
84218 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
84218 [pool-369-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
84262 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
84262 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
85081 [pool-369-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
85109 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
85109 [pool-370-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
85147 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
85147 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
85984 [pool-370-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
86009 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
86009 [pool-371-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
86041 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
86041 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
86882 [pool-371-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
87326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
87326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
87342 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
87342 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
87591 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
88004 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
88004 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
88148 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
88619 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
88619 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
88886 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 67 for /tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet
88908 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet
88908 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 67 results, for file /tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet => [0473da13-dcae-49b1-9237-7e9f7e4d7a94, 05628705-5e8a-4332-af9a-2641ffa753c8, 06345a9b-e54b-47fc-bee0-cfb77ea6dac5, 07765706-aeb2-4814-9bcc-4fd08f9cdc67, 0795fd5b-fc74-48d1-a137-ceb4712ffcb5, 0bbc1684-3bd6-4541-afc2-a5201834d833, 0cfb3619-4d26-4317-a12d-1430cad01276, 0d340692-b4ac-40e4-89b8-9c42175f7fc4, 0d6cf430-20cd-46c3-a452-99f18158365c, 116ae877-7082-462e-a69b-25209f2bcad1, 12487234-528a-4196-b2c5-24e89676f984, 14af3403-4cfa-4c08-b2a0-1e5fed5c68cd, 15393dae-80ac-4670-bf52-ed9f85d4eb97, 28b32d9a-7d3d-4e14-8b9f-87787b82c815, 2e5e3cc4-8553-420a-8b61-f78266c2041e, 351c1bbc-6f6b-407e-8d94-d0898f9abf37, 36ee00fe-762f-41cb-96ab-07a60a28c035, 3ae9fc28-70a3-4b7a-9b72-7c12347c197f, 3c96e589-4a6d-439f-94b3-8d0a2c5a2c1d, 41a08e63-98f8-4958-974a-d3e4b37a8c47, 4b9e1e44-53f1-4e14-825c-4479474b91bd, 4fb2335e-7b59-4ab9-943c-e23c1933f768, 50424bce-c3a0-4426-9c3a-319eef57a48c, 51b79e23-cc48-46e2-a279-d4c8c05eb4ce, 5ca62389-47df-4dd1-9a4c-15501cb1504e, 61130294-091a-4740-849f-ec76fcda0a98, 65a29c06-9763-4770-b7d4-5b3ae4290097, 65f8dc37-3c68-4173-a13f-2a7f15f76b83, 67ca23b2-3673-4d21-bbe3-9ccdda9c757e, 6a5ff7a6-3b27-4945-9a82-deffdf2134b9, 6bb228db-c833-490e-96ab-ab05ab4a1467, 6e6ea0eb-b646-4184-9cc7-209d679934a0, 6eb55ce2-0d51-4ae5-975a-06c186e232f7, 6eefff9a-f46d-426e-92a8-1e542b73b95d, 7775919d-232f-4ae5-a1c2-f4039a4c0624, 7857e4d6-4efc-4bd6-89f7-4aac374edee3, 79e4e4dd-fa43-4e2d-aa13-e9022cbbba96, 7dd349de-e0e0-462c-a2de-5aed0f3b57ac, 875c8936-7869-4c68-b60c-f8365e0bcce3, 8ad432e8-ab06-467e-a556-e0cfee0b5f31, 93a7ae05-52a5-4996-8573-1647de699edc, 941bb319-0986-4c2d-a911-f523868e8246, 95baf6be-b282-4f1b-b271-af33167f9abf, 974230c1-2687-4e60-a17e-22ccc9087720, 9e499e44-b14b-43e1-8458-5adad1b0ea33, a63254be-d1fa-4e82-836e-fd9b1606ef5c, a6dafa78-ab6e-454d-9471-2f32fdcdf11e, aa316d24-daf6-4a94-9b34-fdfa83ac9038, ac3d4c61-dbea-4fc5-85c5-6b51dd57a263, b599ab52-9b0e-47c1-b97c-b13a6a952711, bbb2b5ed-a35f-4928-8c04-b1f8dec736fa, bd409c90-3c50-4aae-b05c-cd86884e360a, c2eb606f-c6e3-4d16-88d4-a5a8b4207d72, c9bad025-03aa-4156-a423-2e894fd8ca91, cfc5da27-db3b-47ec-a3c4-b099e54d9777, d09209f9-8bcc-47aa-a5ee-b2e2b40d362d, d84b1261-14da-4433-916f-495b82b455e0, db564a71-edbe-495e-a780-62655a973e06, dbfa5f5d-0b34-4dec-9b35-47e7b3b77070, e1206dfa-4041-42a2-9dc0-1f34519254cc, e3a6eb24-bf63-47b1-8dc3-6fe508084341, e615964b-7396-4b47-9baa-683c339192c4, e72ff402-1865-48e6-b2c3-a4cdf42c73d4, ed32e634-57ef-41ed-b6d8-b067ac9b6d93, f1ec229a-074f-4ed3-b711-7cee5a84bcd2, f5938896-72c6-4799-8ff8-871cd4c7acd1, f9fb304b-78f0-4fb1-b6d1-ab716274d737]
88930 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
88950 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
88950 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet => [020bab65-68ce-45ed-9e5b-328c2d448f69, 05fdf485-b40c-42bd-8bab-1a4c7e3d9bde, 0f8df5ba-f730-4689-9331-e8b1e0ae5dfd, 118a23d6-6f5f-4513-a773-c6b1d6908804, 188c3854-9757-4400-b513-781a0866b3ce, 1a223158-108f-4f59-9145-c17be4bdb2d6, 2233ebbd-4e89-487d-a54a-25bf158ed0b8, 240fed97-2e5f-4f06-adeb-d593432f3601, 242e3a12-3bc4-46d6-af1b-998cfa99cba3, 282d3e7e-37a2-40f2-ae2d-a28c54ac80eb, 289d71ee-17aa-43f3-b0d8-43d47dca6fc1, 2a1c341f-817f-46bc-b439-1c5b395dfd40, 2b5bd0db-0a47-4724-a61b-c88dbcca3aa1, 3097407f-b1e0-49aa-87b2-fd1b98f8b1ed, 38e59bba-dec1-4cfa-8be3-8b97596f6b3b, 3a146110-c27d-4bf5-a84c-8287423bcf7d, 42bac81d-2584-453d-aa3d-662098cf7da5, 42decf37-6b79-4918-b31f-330182ee520e, 43152d96-dce7-4e36-bc22-6784cd7c8a39, 4a4a7097-6abb-41e2-bcbb-b67f73d2d572, 4e0949c4-8942-4d1c-8fd6-7a807cb06ae2, 53fef97e-3b5f-44a3-a762-9340ea810aea, 5986816e-b38f-400e-b1e4-081113d206c5, 5bcabba6-df96-4bc2-bf62-f2adf0ec583c, 5dc7b64a-532a-4aa3-90bc-ceda9cdb54bc, 5f4f2705-0f24-4a99-8c48-310e2a246ed8, 60503125-6896-45d8-89d3-d580dde9cad7, 667cbf0c-4cdd-4510-8e74-d1076b1ddf0f, 6e74792f-e353-42ca-9b4f-a15ecc47b66b, 73ea6868-a850-418b-99f0-9a9fff18cf56, 7505d837-805d-4f08-b6c4-ab36978f7c17]
88991 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 43 for /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
89011 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
89011 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 43 results, for file /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet => [76de663c-84bf-40aa-b5f4-60c461e29f61, 771cdb0b-4d15-4d3f-b619-f6f3c2c19984, 7753f327-6d7f-482b-bcae-2fa8c137c795, 785956e2-bb97-4c1d-9674-3ab90ca4703d, 7b7a92ab-e816-44d1-a70e-cc894690ca49, 7db276a8-af23-4968-ba98-a7141aa41210, 8628456d-2d42-43fe-a95a-2a8ee4b1fb0b, 8c2bafd2-d038-4da3-b116-aa08dd476e73, 92e71cf2-0204-4f4a-a0f4-16ce42c7bc51, 93146826-ddd3-4806-a637-b025de40d866, 94b63031-0890-445f-bba9-8411e385de17, 96552636-5ba0-449b-8007-ce037adfab60, 97611209-5b83-4aa3-8740-98cc2862a707, 98a2d751-f692-4bbb-a95a-f92faf3317b9, 99116604-118f-4580-9d4a-27be86c52917, 9980e750-879d-4aee-97c4-7b4916328408, a03e29e3-a453-47b9-a049-d18195561d81, a475ab49-1acd-49de-b974-4d1da54a750f, a55898fd-57d1-4a3c-918f-7445dcfe5f90, a6a922f7-6db6-4ec0-a5e7-488328e29981, b26bcdbe-85ee-492a-a9ce-a2f934b8b7d7, b4023ce4-e37a-42e5-aec5-dd8daf96692a, b6d42555-2398-490c-ac31-989b70ff3d91, ba0df039-ed40-40a1-b4a8-06e524668213, bcf33752-c5a1-4f17-9106-4285ef11a4a8, c06cca4e-f866-4f2d-aec7-df494f77ef7a, c3ee63c6-d2fe-4dfa-a0c3-2c6b53a05c2e, d65fdfad-a596-401f-a29d-d51dcc38cd16, d7159de3-e490-4f86-a404-c8596d92539e, dafec464-81b8-4f14-a17f-896e2ea744a8, dbe58092-d52c-4b9f-8fc0-28d1b665db37, e55ddb62-d4e4-42ee-814c-b468c326525a, e7713b5f-727b-4f15-aea8-34c0f5647df3, e8242168-d497-4428-9a6a-13a63041cf2d, ea46ba0a-5c49-4a19-a0d7-4213037a3b65, eb57f1bb-0484-49c0-a5fd-c7684838c1e0, eef98f3d-91b7-4413-8e2b-3c58b6d18a49, f28ce53b-c76c-407b-bd2b-2a0106dd9dd4, f6ace061-a1fa-4962-98dc-bfa99e8b8fd6, fb396e44-49b7-414a-835e-a635ef265561, fc4c25b2-4b6e-48ea-9edd-a0527ffcb717, fde54998-1e74-4d70-b5e8-12e9dde704aa, ff4364a3-b3a8-4075-9403-f14f403f35a3]
89033 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 59 for /tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet
89051 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 59 row keys from /tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet
89051 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 59 results, for file /tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet => [0f6f84b5-28e4-43d1-881c-5156d57084cb, 13b287e7-f4d6-49bd-ba46-dde30557263d, 154060bf-7286-4b8b-9b07-f098ad12597e, 2557f6ae-14cb-4920-9063-1b85170c9e84, 265dd1cd-d023-4e0d-ae22-36073a79fb60, 2a0e666d-a106-43b8-9c27-87b2736c082f, 2e596a56-6994-407b-bdb6-cc866a7fd3da, 30a90a54-066e-4457-b1f2-f4059b1232c5, 34ce6c31-60fc-4465-af5b-08e0110afe69, 34cf877a-290c-4609-9efb-6c5a6d427933, 374a4a5d-8e3d-471e-8885-9f8f84c40404, 3a2e9e73-ed73-4a92-895a-4246e9eb4bcf, 43036e6c-1cf0-49f2-ab1e-efc597284c4e, 432ce885-16aa-4ece-bc5e-2d6097437257, 5162df55-8972-43df-9858-8c501ab42cc0, 55022736-79c9-448e-8574-42413d4d697e, 5cc3f556-3b88-427a-a3a3-905b403ada97, 5de61521-9061-449a-a508-7ea5dd824ac1, 657cc6b0-c82e-4437-af43-4180422762e3, 668a6b2f-8210-4445-9417-dc51de9ce9be, 673c4d15-7f2c-412b-881c-c8ebceae59e5, 6f867c18-2ad7-40e1-9b96-e9d27a476197, 6ff890d5-b4e3-48a8-981f-2239caf3b7ee, 7aa9b363-7631-4687-9fbb-9a9426ee7417, 7bca19ee-2231-40f5-bc0d-5aaeac042f8a, 7f2196bf-7e0d-451f-b79e-bf692ac1fa96, 80e102c0-7629-401e-8ecf-86528a1ff88c, 868bce5f-8bb2-4cae-bfb2-3faa25f8e83a, 899b9beb-9ed7-4c1d-a74b-f70be6d922a0, 89aea5ab-2256-431f-9859-c3b93c98d621, 89ec1c76-cbbb-47a7-8782-0b1a950068ce, 8f50252f-9830-48aa-9fb4-d40836295f3c, 90d1cc11-e651-4e53-b989-cffac00c9e25, 96437a24-fb6c-46b2-869b-cb26d22fcf87, a1f2d2c7-7414-4323-be4f-8b99ca2a737e, aa2faf94-11af-4e91-aeaa-28f04eb32e68, aae4b85b-c84d-4fe2-ad41-405217a65f32, ab945de0-daa9-4855-979a-78d5ca2e1aaa, adcffc70-fd5d-4ec2-bda9-286775279dc4, ae434a72-2ba6-4734-8826-64f3c6f37c62, b4ad49df-d48f-4368-88bc-378447172f53, b5f6edab-d65b-4c50-9684-2701bab3b481, bb80cc7b-4e86-4aff-b375-f8e7b29d5a75, c0014b32-05f7-4c5a-b0a5-3f038d685d1f, c0f77cef-b6cb-476c-baf3-5eb39d075b06, c143a77d-3efe-4fe4-8af6-c830a763713b, c3385363-3026-4a7e-9233-06136f8dbc05, c6d50df7-bf30-4e10-ae91-b6cce6d70195, c75b8c9f-4aae-4a07-b2da-f3f9efb0ebf8, d5d497f0-54bb-43d6-b45c-0ddca5c95d88, d77feb12-45c1-4e0d-8aee-28ddb725ee7a, da03277a-78af-4a66-9a5b-ed2ea13a1302, e3385f2c-4e64-47f1-9ce8-6328fa9533f3, e5d10bd5-b0a6-4acc-8f53-c6ed5aad7ce8, ee2bc68f-a736-4e14-b38f-10134a62f7d2, eeeec35f-bc14-4a72-a4ec-5777eeb69125, effecfd8-1777-4b73-9402-8a0178556269, f2ea87dd-8da8-4835-8f40-756165b200c0, fe4fc2a8-8f48-411d-9605-4a3a8ddda682]
89149 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=74}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=67}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=59}}}
89571 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
89571 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=65019474-2421-4585-a97b-1b57550f80fb}, 1=BucketInfo {bucketType=UPDATE, fileLoc=4b0ae810-1c96-4af3-988c-d1d4a86e0d7f}, 2=BucketInfo {bucketType=UPDATE, fileLoc=96edef2a-9048-4878-8371-270fe9e339ba}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{65019474-2421-4585-a97b-1b57550f80fb=0, 4b0ae810-1c96-4af3-988c-d1d4a86e0d7f=1, 96edef2a-9048-4878-8371-270fe9e339ba=2}
89594 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
89594 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
89781 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 65019474-2421-4585-a97b-1b57550f80fb
90274 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 4b0ae810-1c96-4af3-988c-d1d4a86e0d7f
90766 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 96edef2a-9048-4878-8371-270fe9e339ba
91667 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
91667 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
91681 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
91681 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
91918 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
92333 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
92333 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
92568 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
92575 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
92575 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
92577 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
92582 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
92591 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
92591 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
92602 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
92618 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
92630 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/.65019474-2421-4585-a97b-1b57550f80fb_001.log.1] for base split hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
92684 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
92693 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
92699 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
92702 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
92707 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
92716 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
92716 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
92728 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
92745 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
92753 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/.4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_001.log.1] for base split hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
92795 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
92801 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
92802 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
92804 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
92808 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
92815 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
92815 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
92824 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
92837 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
92846 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/.96edef2a-9048-4878-8371-270fe9e339ba_001.log.1] for base split hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
92885 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
92978 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
93427 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
93868 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
93918 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
94336 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
94336 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
94426 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
94436 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
94436 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
94438 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
94442 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
94453 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
94453 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
94463 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
94480 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
94488 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/.65019474-2421-4585-a97b-1b57550f80fb_001.log.1] for base split hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
94505 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
94512 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
94513 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
94514 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
94519 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
94529 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
94529 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
94538 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
94553 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
94562 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/.4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_001.log.1] for base split hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
94582 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
94589 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
94590 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
94592 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
94596 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:36206/tmp/junit3076354357367460905
94604 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
94604 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
94612 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
94628 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
94636 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/.96edef2a-9048-4878-8371-270fe9e339ba_001.log.1] for base split hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
94646 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
94738 [dispatcher-event-loop-22] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 61 contains a task of very large size (118 KB). The maximum recommended task size is 100 KB.
95151 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 180
95151 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
95407 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 62 for /tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet
95428 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet
95428 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 62 results, for file /tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_001.parquet => [0473da13-dcae-49b1-9237-7e9f7e4d7a94, 05628705-5e8a-4332-af9a-2641ffa753c8, 06345a9b-e54b-47fc-bee0-cfb77ea6dac5, 07765706-aeb2-4814-9bcc-4fd08f9cdc67, 0795fd5b-fc74-48d1-a137-ceb4712ffcb5, 0bbc1684-3bd6-4541-afc2-a5201834d833, 0cfb3619-4d26-4317-a12d-1430cad01276, 0d340692-b4ac-40e4-89b8-9c42175f7fc4, 116ae877-7082-462e-a69b-25209f2bcad1, 12487234-528a-4196-b2c5-24e89676f984, 14af3403-4cfa-4c08-b2a0-1e5fed5c68cd, 15393dae-80ac-4670-bf52-ed9f85d4eb97, 28b32d9a-7d3d-4e14-8b9f-87787b82c815, 2e5e3cc4-8553-420a-8b61-f78266c2041e, 351c1bbc-6f6b-407e-8d94-d0898f9abf37, 36ee00fe-762f-41cb-96ab-07a60a28c035, 3c96e589-4a6d-439f-94b3-8d0a2c5a2c1d, 41a08e63-98f8-4958-974a-d3e4b37a8c47, 4b9e1e44-53f1-4e14-825c-4479474b91bd, 4fb2335e-7b59-4ab9-943c-e23c1933f768, 50424bce-c3a0-4426-9c3a-319eef57a48c, 51b79e23-cc48-46e2-a279-d4c8c05eb4ce, 5ca62389-47df-4dd1-9a4c-15501cb1504e, 61130294-091a-4740-849f-ec76fcda0a98, 65a29c06-9763-4770-b7d4-5b3ae4290097, 65f8dc37-3c68-4173-a13f-2a7f15f76b83, 67ca23b2-3673-4d21-bbe3-9ccdda9c757e, 6a5ff7a6-3b27-4945-9a82-deffdf2134b9, 6bb228db-c833-490e-96ab-ab05ab4a1467, 6e6ea0eb-b646-4184-9cc7-209d679934a0, 6eb55ce2-0d51-4ae5-975a-06c186e232f7, 6eefff9a-f46d-426e-92a8-1e542b73b95d, 7775919d-232f-4ae5-a1c2-f4039a4c0624, 7857e4d6-4efc-4bd6-89f7-4aac374edee3, 79e4e4dd-fa43-4e2d-aa13-e9022cbbba96, 7dd349de-e0e0-462c-a2de-5aed0f3b57ac, 875c8936-7869-4c68-b60c-f8365e0bcce3, 8ad432e8-ab06-467e-a556-e0cfee0b5f31, 93a7ae05-52a5-4996-8573-1647de699edc, 941bb319-0986-4c2d-a911-f523868e8246, 95baf6be-b282-4f1b-b271-af33167f9abf, 974230c1-2687-4e60-a17e-22ccc9087720, 9e499e44-b14b-43e1-8458-5adad1b0ea33, a63254be-d1fa-4e82-836e-fd9b1606ef5c, a6dafa78-ab6e-454d-9471-2f32fdcdf11e, aa316d24-daf6-4a94-9b34-fdfa83ac9038, b599ab52-9b0e-47c1-b97c-b13a6a952711, bbb2b5ed-a35f-4928-8c04-b1f8dec736fa, bd409c90-3c50-4aae-b05c-cd86884e360a, c2eb606f-c6e3-4d16-88d4-a5a8b4207d72, c9bad025-03aa-4156-a423-2e894fd8ca91, cfc5da27-db3b-47ec-a3c4-b099e54d9777, d09209f9-8bcc-47aa-a5ee-b2e2b40d362d, d84b1261-14da-4433-916f-495b82b455e0, dbfa5f5d-0b34-4dec-9b35-47e7b3b77070, e1206dfa-4041-42a2-9dc0-1f34519254cc, e3a6eb24-bf63-47b1-8dc3-6fe508084341, e615964b-7396-4b47-9baa-683c339192c4, ed32e634-57ef-41ed-b6d8-b067ac9b6d93, f1ec229a-074f-4ed3-b711-7cee5a84bcd2, f5938896-72c6-4799-8ff8-871cd4c7acd1, f9fb304b-78f0-4fb1-b6d1-ab716274d737]
95450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
95471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
95471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet => [020bab65-68ce-45ed-9e5b-328c2d448f69, 05fdf485-b40c-42bd-8bab-1a4c7e3d9bde, 0f8df5ba-f730-4689-9331-e8b1e0ae5dfd, 118a23d6-6f5f-4513-a773-c6b1d6908804, 188c3854-9757-4400-b513-781a0866b3ce, 1a223158-108f-4f59-9145-c17be4bdb2d6, 2233ebbd-4e89-487d-a54a-25bf158ed0b8, 240fed97-2e5f-4f06-adeb-d593432f3601, 282d3e7e-37a2-40f2-ae2d-a28c54ac80eb, 289d71ee-17aa-43f3-b0d8-43d47dca6fc1, 2a1c341f-817f-46bc-b439-1c5b395dfd40, 2b5bd0db-0a47-4724-a61b-c88dbcca3aa1, 3097407f-b1e0-49aa-87b2-fd1b98f8b1ed, 38e59bba-dec1-4cfa-8be3-8b97596f6b3b, 3a146110-c27d-4bf5-a84c-8287423bcf7d, 42bac81d-2584-453d-aa3d-662098cf7da5, 42decf37-6b79-4918-b31f-330182ee520e, 4a4a7097-6abb-41e2-bcbb-b67f73d2d572, 4e0949c4-8942-4d1c-8fd6-7a807cb06ae2, 53fef97e-3b5f-44a3-a762-9340ea810aea, 5986816e-b38f-400e-b1e4-081113d206c5, 5dc7b64a-532a-4aa3-90bc-ceda9cdb54bc, 5f4f2705-0f24-4a99-8c48-310e2a246ed8, 60503125-6896-45d8-89d3-d580dde9cad7, 667cbf0c-4cdd-4510-8e74-d1076b1ddf0f, 6e74792f-e353-42ca-9b4f-a15ecc47b66b, 73ea6868-a850-418b-99f0-9a9fff18cf56, 7505d837-805d-4f08-b6c4-ab36978f7c17, 76de663c-84bf-40aa-b5f4-60c461e29f61, 771cdb0b-4d15-4d3f-b619-f6f3c2c19984, 7753f327-6d7f-482b-bcae-2fa8c137c795]
95509 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 36 for /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
95526 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet
95526 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 36 results, for file /tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_001.parquet => [785956e2-bb97-4c1d-9674-3ab90ca4703d, 7b7a92ab-e816-44d1-a70e-cc894690ca49, 7db276a8-af23-4968-ba98-a7141aa41210, 8628456d-2d42-43fe-a95a-2a8ee4b1fb0b, 8c2bafd2-d038-4da3-b116-aa08dd476e73, 92e71cf2-0204-4f4a-a0f4-16ce42c7bc51, 93146826-ddd3-4806-a637-b025de40d866, 94b63031-0890-445f-bba9-8411e385de17, 96552636-5ba0-449b-8007-ce037adfab60, 99116604-118f-4580-9d4a-27be86c52917, 9980e750-879d-4aee-97c4-7b4916328408, a03e29e3-a453-47b9-a049-d18195561d81, a475ab49-1acd-49de-b974-4d1da54a750f, a55898fd-57d1-4a3c-918f-7445dcfe5f90, b26bcdbe-85ee-492a-a9ce-a2f934b8b7d7, b4023ce4-e37a-42e5-aec5-dd8daf96692a, b6d42555-2398-490c-ac31-989b70ff3d91, ba0df039-ed40-40a1-b4a8-06e524668213, bcf33752-c5a1-4f17-9106-4285ef11a4a8, c06cca4e-f866-4f2d-aec7-df494f77ef7a, c3ee63c6-d2fe-4dfa-a0c3-2c6b53a05c2e, d65fdfad-a596-401f-a29d-d51dcc38cd16, d7159de3-e490-4f86-a404-c8596d92539e, dbe58092-d52c-4b9f-8fc0-28d1b665db37, e55ddb62-d4e4-42ee-814c-b468c326525a, e7713b5f-727b-4f15-aea8-34c0f5647df3, e8242168-d497-4428-9a6a-13a63041cf2d, ea46ba0a-5c49-4a19-a0d7-4213037a3b65, eb57f1bb-0484-49c0-a5fd-c7684838c1e0, eef98f3d-91b7-4413-8e2b-3c58b6d18a49, f28ce53b-c76c-407b-bd2b-2a0106dd9dd4, f6ace061-a1fa-4962-98dc-bfa99e8b8fd6, fb396e44-49b7-414a-835e-a635ef265561, fc4c25b2-4b6e-48ea-9edd-a0527ffcb717, fde54998-1e74-4d70-b5e8-12e9dde704aa, ff4364a3-b3a8-4075-9403-f14f403f35a3]
95547 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 51 for /tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet
95563 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 59 row keys from /tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet
95563 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 51 results, for file /tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_001.parquet => [0f6f84b5-28e4-43d1-881c-5156d57084cb, 13b287e7-f4d6-49bd-ba46-dde30557263d, 154060bf-7286-4b8b-9b07-f098ad12597e, 2557f6ae-14cb-4920-9063-1b85170c9e84, 265dd1cd-d023-4e0d-ae22-36073a79fb60, 2a0e666d-a106-43b8-9c27-87b2736c082f, 2e596a56-6994-407b-bdb6-cc866a7fd3da, 30a90a54-066e-4457-b1f2-f4059b1232c5, 34ce6c31-60fc-4465-af5b-08e0110afe69, 34cf877a-290c-4609-9efb-6c5a6d427933, 374a4a5d-8e3d-471e-8885-9f8f84c40404, 3a2e9e73-ed73-4a92-895a-4246e9eb4bcf, 432ce885-16aa-4ece-bc5e-2d6097437257, 5162df55-8972-43df-9858-8c501ab42cc0, 55022736-79c9-448e-8574-42413d4d697e, 5de61521-9061-449a-a508-7ea5dd824ac1, 657cc6b0-c82e-4437-af43-4180422762e3, 668a6b2f-8210-4445-9417-dc51de9ce9be, 673c4d15-7f2c-412b-881c-c8ebceae59e5, 6f867c18-2ad7-40e1-9b96-e9d27a476197, 7aa9b363-7631-4687-9fbb-9a9426ee7417, 7bca19ee-2231-40f5-bc0d-5aaeac042f8a, 80e102c0-7629-401e-8ecf-86528a1ff88c, 868bce5f-8bb2-4cae-bfb2-3faa25f8e83a, 899b9beb-9ed7-4c1d-a74b-f70be6d922a0, 89ec1c76-cbbb-47a7-8782-0b1a950068ce, 8f50252f-9830-48aa-9fb4-d40836295f3c, 90d1cc11-e651-4e53-b989-cffac00c9e25, 96437a24-fb6c-46b2-869b-cb26d22fcf87, a1f2d2c7-7414-4323-be4f-8b99ca2a737e, aa2faf94-11af-4e91-aeaa-28f04eb32e68, aae4b85b-c84d-4fe2-ad41-405217a65f32, ab945de0-daa9-4855-979a-78d5ca2e1aaa, adcffc70-fd5d-4ec2-bda9-286775279dc4, ae434a72-2ba6-4734-8826-64f3c6f37c62, b4ad49df-d48f-4368-88bc-378447172f53, b5f6edab-d65b-4c50-9684-2701bab3b481, c0014b32-05f7-4c5a-b0a5-3f038d685d1f, c0f77cef-b6cb-476c-baf3-5eb39d075b06, c143a77d-3efe-4fe4-8af6-c830a763713b, c3385363-3026-4a7e-9233-06136f8dbc05, c6d50df7-bf30-4e10-ae91-b6cce6d70195, c75b8c9f-4aae-4a07-b2da-f3f9efb0ebf8, d77feb12-45c1-4e0d-8aee-28ddb725ee7a, da03277a-78af-4a66-9a5b-ed2ea13a1302, e3385f2c-4e64-47f1-9ce8-6328fa9533f3, ee2bc68f-a736-4e14-b38f-10134a62f7d2, eeeec35f-bc14-4a72-a4ec-5777eeb69125, effecfd8-1777-4b73-9402-8a0178556269, f2ea87dd-8da8-4835-8f40-756165b200c0, fe4fc2a8-8f48-411d-9605-4a3a8ddda682]
95660 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=180}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=67}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=62}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=51}}}
96082 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
96082 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=65019474-2421-4585-a97b-1b57550f80fb}, 1=BucketInfo {bucketType=UPDATE, fileLoc=4b0ae810-1c96-4af3-988c-d1d4a86e0d7f}, 2=BucketInfo {bucketType=UPDATE, fileLoc=96edef2a-9048-4878-8371-270fe9e339ba}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{65019474-2421-4585-a97b-1b57550f80fb=0, 4b0ae810-1c96-4af3-988c-d1d4a86e0d7f=1, 96edef2a-9048-4878-8371-270fe9e339ba=2}
96104 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
96104 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
96316 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 65019474-2421-4585-a97b-1b57550f80fb
96824 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 4b0ae810-1c96-4af3-988c-d1d4a86e0d7f
97318 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 96edef2a-9048-4878-8371-270fe9e339ba
98227 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
98227 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
98242 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
98242 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
98502 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
98919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
98919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
99063 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082126
99078 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Checking if compaction needs to be run on /tmp/junit3076354357367460905
99078 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Compacting merge on read table /tmp/junit3076354357367460905
99248 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082126
99248 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082126
101311 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
101311 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
101327 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
101327 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
101596 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
102008 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082126 as complete
102008 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082126
102042 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [20180316082126]
102158 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
102162 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_1_20180316082126.parquet	true
102163 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
102165 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/96edef2a-9048-4878-8371-270fe9e339ba_2_20180316082126.parquet	true
102165 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
102167 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/65019474-2421-4585-a97b-1b57550f80fb_0_20180316082126.parquet	true
102174 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20180316082126]
102588 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20180316082126] rollback is complete
102588 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
102637 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
102766 [DataNode: [[[DISK]file:/tmp/1521184808859-0/dfs/data/data1/, [DISK]file:/tmp/1521184808859-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36206] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1907470652-172.17.0.2-1521184809441 (Datanode Uuid d071acd9-193a-44e6-b016-d32b62c6d8fa) service to localhost/127.0.0.1:36206 interrupted
102768 [DataNode: [[[DISK]file:/tmp/1521184808859-0/dfs/data/data1/, [DISK]file:/tmp/1521184808859-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36206] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1907470652-172.17.0.2-1521184809441 (Datanode Uuid d071acd9-193a-44e6-b016-d32b62c6d8fa) service to localhost/127.0.0.1:36206
102789 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@6d394690] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 7, Failures: 0, Errors: 2, Skipped: 1, Time elapsed: 81.401 sec <<< FAILURE! - in com.uber.hoodie.table.TestMergeOnReadTable
testCOWToMORConvertedDatasetRollback(com.uber.hoodie.table.TestMergeOnReadTable)  Time elapsed: 8.057 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 002
	at com.uber.hoodie.table.TestMergeOnReadTable.testCOWToMORConvertedDatasetRollback(TestMergeOnReadTable.java:383)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.table.TestMergeOnReadTable.testCOWToMORConvertedDatasetRollback(TestMergeOnReadTable.java:383)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testUpsertPartitioner(com.uber.hoodie.table.TestMergeOnReadTable)  Time elapsed: 7.333 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 002
	at com.uber.hoodie.table.TestMergeOnReadTable.testUpsertPartitioner(TestMergeOnReadTable.java:598)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:158)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.table.TestMergeOnReadTable.testUpsertPartitioner(TestMergeOnReadTable.java:598)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

Running com.uber.hoodie.index.TestHoodieIndex
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.index.TestHoodieIndex
Running com.uber.hoodie.index.TestHbaseIndex
Formatting using clusterid: testClusterID
105620 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
105633 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
105983 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
107725 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
107932 [spirals-vortex:43403.activeMasterManager] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
108011 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
110050 [spirals-vortex:43403.activeMasterManager] WARN  org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore  - Log directory not found: File hdfs://localhost:43779/user/root/test-data/ada592c6-cbe8-49f8-bb83-098f180bbda0/MasterProcWALs does not exist.
110205 [RS:0;spirals-vortex:33233] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
116520 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082143
116842 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
116994 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=60, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=74, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=66, numUpdates=0}}}
117405 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
117405 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
117405 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 60, totalInsertBuckets => 1, recordsPerBucket => 500000
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 74, totalInsertBuckets => 1, recordsPerBucket => 500000
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 66, totalInsertBuckets => 1, recordsPerBucket => 500000
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
117406 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
117431 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
117515 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117515 [pool-453-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117544 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117544 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118389 [pool-453-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118441 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118442 [pool-454-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118474 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119312 [pool-454-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119386 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119386 [pool-455-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119426 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119426 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120254 [pool-455-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120525 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
120660 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
120660 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
120670 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
120670 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
120842 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
121253 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
121253 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
121766 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082148
121949 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=58, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=73, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=69, numUpdates=0}}}
122361 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 58, totalInsertBuckets => 1, recordsPerBucket => 500000
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
122362 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
122384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180316082148
122520 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122521 [pool-456-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122556 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122556 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123395 [pool-456-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123426 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123426 [pool-457-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123987 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123987 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124415 [pool-457-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124443 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124443 [pool-458-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125313 [pool-458-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125341 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082148
125847 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
125847 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
125863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
125863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
126018 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
126434 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082148 as complete
126435 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082148
126940 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20180316082148]
126940 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20180316082148]
127015 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
127019 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:43779/tmp/junit172337767527739577/2015/03/16/1796b835-6a98-4394-b1f3-65e7fac2ff12_1_20180316082148.parquet	true
127020 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
127022 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:43779/tmp/junit172337767527739577/2015/03/17/2524cef6-7d47-4f20-93d2-55d9f992e601_2_20180316082148.parquet	true
127022 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
127023 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:43779/tmp/junit172337767527739577/2016/03/15/d9b1e0c0-c37c-494f-8e12-200a8fa1109d_0_20180316082148.parquet	true
127027 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20180316082148]
127027 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20180316082148]
127440 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20180316082148] rollback is complete
127440 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
128231 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082155
128418 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
128830 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
128830 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
128851 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180316082155
128922 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082155
129105 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
129518 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
129518 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
129539 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180316082155
129546 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082155
130001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
130001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
130010 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
130010 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
130010 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082155
131692 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
131817 [DataNode: [[[DISK]file:/root/workspace/uber/hudi/354187058/hoodie-client/target/test-data/d2eb6bd0-67ce-4f2f-aed2-4b126ca26a42/dfscluster_f3818e44-d65b-4b01-9693-27a3210cd60f/dfs/data/data1/, [DISK]file:/root/workspace/uber/hudi/354187058/hoodie-client/target/test-data/d2eb6bd0-67ce-4f2f-aed2-4b126ca26a42/dfscluster_f3818e44-d65b-4b01-9693-27a3210cd60f/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43779] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-304250384-172.17.0.2-1521184891085 (Datanode Uuid a1a1e744-0600-44e5-8283-588ff78011e5) service to localhost/127.0.0.1:43779 interrupted
131819 [DataNode: [[[DISK]file:/root/workspace/uber/hudi/354187058/hoodie-client/target/test-data/d2eb6bd0-67ce-4f2f-aed2-4b126ca26a42/dfscluster_f3818e44-d65b-4b01-9693-27a3210cd60f/dfs/data/data1/, [DISK]file:/root/workspace/uber/hudi/354187058/hoodie-client/target/test-data/d2eb6bd0-67ce-4f2f-aed2-4b126ca26a42/dfscluster_f3818e44-d65b-4b01-9693-27a3210cd60f/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43779] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-304250384-172.17.0.2-1521184891085 (Datanode Uuid a1a1e744-0600-44e5-8283-588ff78011e5) service to localhost/127.0.0.1:43779
131832 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@53398d3e] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.617 sec - in com.uber.hoodie.index.TestHbaseIndex
Running com.uber.hoodie.index.bloom.TestHoodieBloomIndex
132348 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
132349 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${0}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
133341 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
133704 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
133793 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 1
133793 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
134517 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3055335018427961024/2016/01/31/0980c31f-c10e-47d0-9f18-8bc11f21ecdb_1_20180316082200.parquet
134524 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3055335018427961024/2016/01/31/0980c31f-c10e-47d0-9f18-8bc11f21ecdb_1_20180316082200.parquet
134524 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3055335018427961024/2016/01/31/0980c31f-c10e-47d0-9f18-8bc11f21ecdb_1_20180316082200.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
134827 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
134874 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
134874 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
135424 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
135916 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
136484 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
136788 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
138019 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
138417 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
138417 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
138531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5491069632016549680/2015/01/31/15c1a37c-7ea8-4f04-abfc-a3a5fe3ab2a3_1_20180316082205.parquet
138540 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5491069632016549680/2015/01/31/15c1a37c-7ea8-4f04-abfc-a3a5fe3ab2a3_1_20180316082205.parquet
138540 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5491069632016549680/2015/01/31/15c1a37c-7ea8-4f04-abfc-a3a5fe3ab2a3_1_20180316082205.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
138544 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5491069632016549680/2016/01/31/8e77663a-1c19-44fd-9fa4-ba5314e0aa93_1_20180316082204.parquet
138549 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5491069632016549680/2016/01/31/8e77663a-1c19-44fd-9fa4-ba5314e0aa93_1_20180316082204.parquet
138549 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5491069632016549680/2016/01/31/8e77663a-1c19-44fd-9fa4-ba5314e0aa93_1_20180316082204.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
138551 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5491069632016549680/2016/01/31/f68d5e98-e921-4020-b9dc-5eaad823b65b_1_20180316082203.parquet
138555 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5491069632016549680/2016/01/31/f68d5e98-e921-4020-b9dc-5eaad823b65b_1_20180316082203.parquet
138555 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5491069632016549680/2016/01/31/f68d5e98-e921-4020-b9dc-5eaad823b65b_1_20180316082203.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
138876 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
138986 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
138986 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
140061 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
140089 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
141294 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
141450 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
142580 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
142581 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
142703 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8175335074960412790/2015/01/31/50a5f547-23c7-4d11-a279-64e66a453c9b_1_20180316082209.parquet
142712 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8175335074960412790/2015/01/31/50a5f547-23c7-4d11-a279-64e66a453c9b_1_20180316082209.parquet
142712 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8175335074960412790/2015/01/31/50a5f547-23c7-4d11-a279-64e66a453c9b_1_20180316082209.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
142716 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8175335074960412790/2016/01/31/6bcf5416-03dc-4877-93f4-49427174a556_1_20180316082208.parquet
142722 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8175335074960412790/2016/01/31/6bcf5416-03dc-4877-93f4-49427174a556_1_20180316082208.parquet
142722 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8175335074960412790/2016/01/31/6bcf5416-03dc-4877-93f4-49427174a556_1_20180316082208.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
142725 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8175335074960412790/2016/01/31/ea578fec-1f91-4c8e-85fa-1c76a92c3476_1_20180316082207.parquet
142732 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8175335074960412790/2016/01/31/ea578fec-1f91-4c8e-85fa-1c76a92c3476_1_20180316082207.parquet
142732 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8175335074960412790/2016/01/31/ea578fec-1f91-4c8e-85fa-1c76a92c3476_1_20180316082207.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
143068 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
143247 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
144125 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 2 row keys from /tmp/junit2251979856076927031/2016/01/31/7ce1cbb5-f766-4a0a-af61-e1ede616c50f_1_20180316082211.parquet
144125 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit2251979856076927031/2016/01/31/7ce1cbb5-f766-4a0a-af61-e1ede616c50f_1_20180316082211.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0, 2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
144504 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
144743 [Executor task launch worker-0] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit277924636591149631/2016/04/01/2_0_20160401010101.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1521184931000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
144761 [Executor task launch worker-0] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit277924636591149631/2015/03/12/1_0_20150312101010.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1521184931000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.689 sec - in com.uber.hoodie.index.bloom.TestHoodieBloomIndex
Running com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
144887 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
144968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
144974 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
144974 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
144974 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
144974 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
144974 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
144987 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
144987 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
145127 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
145128 [pool-610-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
145156 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
145167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
145167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
145184 [pool-610-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
145202 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
145203 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
145246 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
145246 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
145396 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
145402 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
145402 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
145530 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
145605 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=40, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=40, numUpdates=0}}}
145624 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 4438
145625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=69c83220-c8df-4485-8caa-ca756a012375}, sizeBytes=443790}]
145625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 40 inserts to new update bucket 0
145625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
145625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=69c83220-c8df-4485-8caa-ca756a012375}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{69c83220-c8df-4485-8caa-ca756a012375=0}
145638 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
145638 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
145792 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleInsertPartition(HoodieCopyOnWriteTable.java:520)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:436)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
145793 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_26_0 failed due to an exception
145793 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_26_0 could not be removed as it was not found on disk or in memory
145794 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 12.0 (TID 11)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleInsertPartition(HoodieCopyOnWriteTable.java:520)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:436)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 29 more
145796 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 12.0 (TID 11, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleInsertPartition(HoodieCopyOnWriteTable.java:520)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:436)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 29 more

145796 [task-result-getter-3] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 12.0 failed 1 times; aborting job
145890 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082212
146053 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
146115 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=151, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=188, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=161, numUpdates=0}}}
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 151, totalInsertBuckets => 1, recordsPerBucket => 500000
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 188, totalInsertBuckets => 1, recordsPerBucket => 500000
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
146122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
146123 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 161, totalInsertBuckets => 1, recordsPerBucket => 500000
146123 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
146123 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
146146 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082212
146146 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082212
146465 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
146465 [pool-628-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
146542 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
146542 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
146571 [pool-628-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
146591 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
146591 [pool-629-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
146695 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
146695 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
146725 [pool-629-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
146744 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
146744 [pool-630-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
146823 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
146823 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
146847 [pool-630-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
146865 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
146865 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
146953 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
146953 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
146972 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
147163 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
147169 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082212 as complete
147169 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082212
147344 [dispatcher-event-loop-15] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 9 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
147711 [dispatcher-event-loop-10] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 13 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
147732 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 500
147732 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
147787 [dispatcher-event-loop-18] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 14 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
147813 [dispatcher-event-loop-22] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 15 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
147936 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 151 for /tmp/junit3452656593956699127/2016/03/15/181e0627-ef24-4ac0-808a-3b7b713e1b13_0_20180316082212.parquet
147952 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 151 row keys from /tmp/junit3452656593956699127/2016/03/15/181e0627-ef24-4ac0-808a-3b7b713e1b13_0_20180316082212.parquet
147952 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 151 results, for file /tmp/junit3452656593956699127/2016/03/15/181e0627-ef24-4ac0-808a-3b7b713e1b13_0_20180316082212.parquet => [008668ea-cc00-4ae5-b4ca-e373e21cfb89, 0152f28f-0ff0-4557-a862-d090702d0bd5, 015907c3-f387-42b0-8b01-30791a33cfe0, 04a3a152-bb49-40aa-88db-7c11f2885b0f, 0826ba2b-daa5-4b5d-ae8a-34404c7f657b, 09ef330c-875b-4beb-8396-b297f281b7a5, 0a2d18f5-942c-4beb-9d6a-34b9d9e586da, 0b008ffa-b4d1-478a-83fd-0c386aff107f, 0c3d9029-fe8a-4890-9407-94cd3b17dcc3, 11853367-1224-48e3-99ab-7345d0bf307d, 11e465a8-f118-4a47-9b8a-d2e83bdb21e3, 12924b4f-ccaa-4381-9920-30aaaee4021d, 14d18b73-4e81-41cc-b091-72a977538b4e, 14deeeca-2ccc-4957-8e9d-f0b28c5dfb4e, 150febff-7832-451b-80d6-0140aea3464c, 15d5e66c-ef17-4431-80f4-2d77aa3a5814, 166a9428-7e1e-481a-b318-06d98d7f67dd, 17468ff6-91ac-4d47-8af2-3b9baf5b1493, 1884443c-b25b-4d2a-be80-b32df666175c, 1ca334aa-dd24-4c65-ac48-d017d225724a, 1d613913-18ec-4d4e-a47e-064577a24c9c, 2096b161-890c-4989-ac40-18f8ff757982, 21cdbba5-1815-464a-b5a0-079916ca81cf, 2343f64e-e1c7-4d04-84a5-17072fd56424, 2394fed2-c252-4615-af34-0d884855a4d0, 255ef9fe-8dd2-4b23-b75e-40100ca407b3, 258c31e7-1931-47b7-b97a-81ee9d3985a0, 279b2d44-d666-49b7-9795-4d7848b5530f, 27e2f0c3-fe43-4ea0-979b-dd3aa55ee32d, 28c70355-469d-4aa0-b4d7-c54747c9775b, 2b7e367f-1ca9-42eb-996f-2d4ca8292698, 2c7dae40-c2f6-468f-94a9-77020b29e691, 2da9777d-435f-476d-b9a9-c2e6c7ca7bc0, 311ebeda-e4a6-48db-8d9f-e76ed8841460, 32ac2071-2f88-4aa0-a028-92977992dc82, 33e33327-be17-4d62-8573-e29872050ddb, 37b9d350-2e46-4cc1-b9b7-a39e211fea38, 39f0497e-aa12-4477-a36d-8c6323b1d1bb, 3b99e9cb-2c44-4dfc-96c1-bd018e470bab, 3d280fca-6653-47bf-aaf5-95618dd5d166, 3ff14eb7-8929-4ce5-9ddb-a426b82bc9a9, 3ff6ac41-99ea-402c-837d-703cbde101cf, 41163794-d164-435f-9f67-e3d25ceccd87, 41915bdc-5c28-45fe-8aca-398f7764e45b, 4248a121-0841-42a1-a2ca-2e6d7514c341, 4282fad3-c3cd-46b1-8a1e-042deaf326f9, 4413301d-e915-4ab0-9277-0a2b5a96c333, 47f24e6c-e6da-4d50-8d9d-087537a7b7b6, 48d11b7c-c03d-4e1d-95a0-584e9396d324, 4b63e896-da93-4401-a4d7-c50e4c70209a, 4b7134ba-6b1d-4f8d-8979-e33c6e5dd8a0, 4bff27fc-6d7d-4271-bbf3-884fa9eb49d9, 4c28c1fb-9cf1-436b-b90d-9e5b8264d7f8, 4cb7b120-f6c1-49eb-ac32-975f81f4c88b, 4dc14a55-5278-431d-89ea-8f51fd8ed7a1, 4e0748e4-fed5-41a9-9d6d-3be6a09134c2, 4eecc2ea-1b4f-420d-8e9d-2d6b67668fc4, 50ef2cd0-5994-41e7-bdf9-be2d93839d96, 521f8cac-3879-49d9-adfb-e64f82c61d12, 54ffbc57-ea9a-4c2f-a96b-b052b7058b7a, 56867622-9525-4f96-a0b4-48e360ea3be8, 59820af0-4398-46e7-a10f-05ac5448f7e3, 5b560d78-a985-4979-a858-31508c171fbb, 5ea71792-14f0-41cc-a129-22000c8aedfd, 62bf776b-786d-4bfc-88e7-49172af8c2ac, 6bffa68c-6f50-4350-9bd0-5187117f5aa7, 6c7da830-eeb1-4234-a134-ac44246ca653, 6f1bb224-5a6a-47ee-9cdf-fed3d4059ba0, 6fbb3a12-b2b9-415c-9623-f825bb1b7420, 7df253fb-6a73-4492-803c-86af2fc289bc, 81585e92-df46-4fae-989f-876626d93ff4, 817ac45b-ac08-4c59-82d3-0326669ff74d, 82ffcb42-1937-4f53-843b-df1844dfdd6b, 86df49e7-621f-4cf9-ad25-25af4e3a70ec, 8a0fcd09-eb4a-4267-aeed-a6360e702ee8, 8c9726dc-6d3c-4ba7-8068-19448cac4b88, 8cf6cef2-d469-4ae2-a731-0c067367aab8, 8de8f663-d716-49d3-bd09-a0cc4174cccb, 8f7e136d-d74f-4ad5-99c2-c47c33e60dcc, 8fd5f8ae-8248-4760-a322-b759a7039280, 904de434-d674-4745-9781-03f36b2136e8, 9070e3d8-7186-455f-8394-1d828ab72def, 97283f9f-4d06-4277-aa22-f7a670caa5a7, 9791667e-dac0-422b-a80a-810bd6174fa7, 9bb0bfc6-dc9b-4da6-86f1-13ca43b7fcf2, 9c1e73d9-9c31-433a-8769-a6031b6d6283, 9cc2b74c-75d0-470e-8760-91649dd5b8a8, 9f45b1c5-7d43-4a51-a990-76e2b8d77b0f, 9f7f2510-3040-4011-bd04-917accdd0094, a0e1ce53-d86c-44e2-8e32-59bb250d4749, a1654ec3-e47d-4284-925e-993113377fd6, a2a451c6-77b6-4b83-ad79-bb48a64d09d8, a3a0b8ba-32d1-4462-b9d7-ee97229a47e1, a72d13b0-9159-412d-ac7c-a1be5f98ab95, a83a8b18-94ca-43b5-b9f4-94d43017848a, abdb1a57-6158-4e01-93e9-824de44a136b, b02d7d43-a366-4d58-bc1c-e160ea6f8557, b135a995-b531-45c7-9f45-f75cd66e8915, b6d11eee-1058-4582-89d1-58518a3ec000, b9bde73d-2bb2-4b42-a6d4-095ecd9bce05, ba7cd24d-af28-4f9d-853c-fca63e3b4d7f, bbc4bd80-ee44-4e68-957f-3fd51d790b22, be1d8682-bcf2-49e5-a5bd-52200929a5f9, c14c18c9-8a1a-430f-8a28-cd01bc01660b, c1507d8e-af6f-4bba-9af9-b0b7b47695e5, c3e05638-9860-4c04-b364-65448f955172, c65356c5-8dce-4e11-83d7-cf2710630f1b, c6b9ef21-da8b-4e9c-9b37-b7deac8a7f7c, c7832c76-4e1b-4c6a-a052-b0113e21af4d, c7853cfa-aac1-4884-9105-024b6c1a0873, ca26a50c-75f0-4c0a-aec2-a82884ad5aea, cacf5b4f-1abd-49e3-81df-6912746852f7, cbb45c26-43f0-4d02-b9bf-aa3779888897, cd2f6f52-425f-4d9d-a7f5-57b3240986b4, ce26064e-816f-4234-ab54-adc06f099306, d3d97e73-0dc0-4292-9951-787e9811cb42, d48bc5f5-f08d-422d-9e0b-c84147c7efe3, d4af4cd1-3d42-49b0-ba12-67414671bf55, d57aa824-27e9-4b4e-a425-7f755f64f198, d5f91067-9cfe-4bea-ba4b-e054a5450045, d609373f-5dfa-4106-a04f-f44d32b0e56a, d8e7b59c-0ed8-4a27-b625-e6e40653081c, d91338f8-8793-452b-9f3d-969ed7c62536, d9188663-e344-4338-b10f-201621d2f866, d9f26e00-63d2-459c-9e58-3e4f390ddad8, dadfed6e-6561-4397-b8e8-aebeb210b66a, db72b884-69c2-4d1d-a616-ed96ab9fa0e3, dbf56b26-201d-4c37-bfe5-ca3cea1afedf, dd63eb94-b60a-4961-8abb-c21bd9cf03fc, dfcea41a-3bee-44d3-a995-ffabf6bcdf6d, e6c1cbce-0060-4b79-9ef9-59e02f6c7656, ea2cc1ef-bca0-482a-825c-05f828cce2b9, eb171057-2074-409c-89c1-31ae2bc62786, eb2ce17c-27b1-4228-8cea-d579c278937f, ec6a1d4a-b544-4fc3-a9a5-bc6918d86bd1, ef27d727-7003-4084-9868-4ecb6304ec26, f00f1280-4d31-47c4-901f-0411bc4ed189, f1f24cb9-7a0e-4890-b521-3d2f0be76a9f, f3e7cd4a-749b-46f2-95a8-144d31d5b550, f3e90b52-fb61-4f5c-8bb5-5d0a8cebe957, f4b36be4-4768-4457-a97c-9b5b66446ac2, f573ad12-2ee0-4a97-8a79-98be121d7e46, f5df3d61-977b-45e9-bc87-55f5f862ebdb, f75d3c8d-7ad7-480d-b486-4ce535818bd9, f7cb4a92-9a63-48e0-b823-e54b0b945feb, f8026038-58fa-4faf-b0fb-b99a847fd94a, f9aa6dd6-059b-4bba-a04b-342293e4e437, fb149934-9b94-48ae-92ad-5e3d91364a22, fb720196-84c3-4b5f-834d-2822fe3212fe, fb9b9be5-52e4-4263-a02a-85856cf72111, ff587fda-2a52-453d-b78d-de8d5d66eeb6]
147968 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 161 for /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet
147984 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet
147984 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 161 results, for file /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet => [0193e62e-205b-4dae-ba5b-ee8c3fbf55d2, 0206e2fd-b371-407c-93f0-af58ae377fd6, 0292d846-f199-4e52-af2e-088157570024, 04535ffe-2386-4b29-9371-1b99aeca8575, 04e13e8f-f41c-45c9-8eef-148a02733c2e, 061cd54c-2dbe-405e-8399-05ca917a318d, 06de8b89-416b-4bb5-ac34-43f28343702b, 07307364-456c-4b5b-b48b-97b7383eb7e1, 092bb66f-ef50-48f3-b14c-72e6949f51bf, 0b0ab73b-ccdd-4ccf-aa74-274a02c216a5, 0c3e65a8-a9a8-47c1-8fa8-d16f5e2562f9, 0e6a4b0e-9ac2-44a9-b263-c3830c909369, 0eadaaf5-31ee-43e4-911a-12672e72f46b, 1003bb5d-c0db-4ee0-9574-3eb3264d5317, 106b6693-51b2-41f2-ad13-b8acc64e7740, 1173136a-472d-4030-ba43-264a0443038b, 12b18d21-1575-4566-90e1-ba5a478e7cca, 176f6c88-0267-4610-8862-0450661dd109, 19641fea-2ca6-4b0b-a8d2-bce46b968879, 19fcbfd9-4f81-4226-b38b-394b2544fd7d, 1a281c51-8ec3-4e09-ab99-50a5cd2aee37, 1ab35f4b-58dd-4a1b-8e35-3d4123e05d31, 1b1d7b0c-6640-4068-90fd-de4607797086, 1cac8e57-a1e1-4509-a169-9f86008c65fa, 1d2a902f-0cf8-4f37-ac28-cd7b48403cbe, 1de54f23-0685-45f9-b096-2a48eea34d2e, 22ba74c3-9b76-45e8-a98f-fa443f7cf34c, 2549b186-8a20-43b1-b645-f53d8b909ae2, 27088cb0-eee3-41c8-a4f5-941574c21069, 276fa843-6797-470d-bc1e-e3fd6f26af7e, 2cfd6c7a-6ea1-41f9-bc06-f6f6264d184b, 2ee0c0e4-c94d-49b5-9c7c-50ac0fe2e58c, 3264cbe9-00fe-41f9-948a-72155771ebc0, 33a5331a-b770-42f4-aecf-2be4f58e19cf, 3667c1ff-e68f-4f61-b70d-01262a57f52b, 367e5c8d-a57f-4066-aafb-ff4f7575dc7c, 367fb861-cc06-42c0-bc9e-4035647cd888, 39e65bec-0bb3-4f06-a42e-0e4365ee4b19, 3b510166-97e6-4d20-9d7e-08118fef2121, 3c9f6f5a-0258-481d-a594-2d2c9077d91f, 3d5d4c43-70b7-4ddf-9a42-7ebed8ff97a4, 3fbb82a5-0fb9-4583-8693-176ce962e563, 40950e75-3d46-44e4-87fb-6087f594ee4e, 415d8a36-46a2-4331-91b4-5180d4d8b380, 41778cad-fcaf-4a6c-8716-f212e1205934, 43e3fe6f-6939-4584-a7ed-c18567731552, 447f58b5-aa50-42e3-96b1-24918bb32f8e, 466524d6-6248-426e-8431-60e42f62af34, 4b6fbae3-cf9e-478a-882a-2d2a7babdb13, 4e1632ca-bf0e-4779-9fc6-b2090d200f74, 4e3ec9f3-f28c-4a47-a9ff-1abb484d2b09, 4eef5dec-8dc3-44cf-96cc-861468c7ed3e, 4f766d5f-f485-4b01-9e62-4865ad8febeb, 51265ced-22a8-4015-969f-cd10ccdf615f, 54adc4e8-68a8-4d6e-a79c-efd1678c8f78, 59d9f7c4-24cb-44a9-a79e-69ccbdf7e768, 5a729de0-99fc-44ad-a2ee-2fa6564b5f19, 5ad7f88e-47a8-47f7-a45a-080aa8ab6625, 5d0803d5-1ae6-422b-bca4-2213e698962d, 5d33140c-5c6f-4ddf-a306-eceba40b1c57, 5d87daea-b33b-40b9-918e-8c1f578109b3, 5e47405b-9808-436e-9b4c-afe3792dbcbe, 5ecf03bd-987b-4add-8f54-c6a8600a84df, 5f4eb340-31df-403b-af36-ac5d44f8babe, 606a3d80-7b6c-44f9-b24d-6885d2f9515e, 61507604-0b30-4c33-bdfb-bfa99245703f, 659db95b-4325-4b84-9c3e-f61135967c8b, 669bb16e-97af-4150-8cf8-1b17bb11ad15, 66d1bb9b-eb69-4be4-abe7-2004a34586bc, 6825e9d4-84b0-48be-b8a8-adfb28a0a302, 6952381f-7060-43e2-a74c-2f16410ca86a, 6a302223-2287-4c21-b9e8-7f6398bf308e, 6e2171ce-6e77-454b-87d8-d31cf0106407, 6e67d70d-e538-41c6-8f87-4027ccf8bd2b, 6f43703a-db06-44f9-9d98-3e28f081de80, 701ad16f-c352-4f44-b543-e35a757097a1, 72b9a4db-2393-4115-be07-af0c95d8c292, 73854cdd-9ed9-4f89-ae78-4d8375f12960, 75df80cd-4f4b-49d5-9c84-c0af727be237, 763d5c93-702d-40fb-a3f0-406ad69a26a9, 76b05909-4346-4c8c-9cbc-adf3e15a893d, 7b1e8bd8-6930-49a8-bb30-bbe43f7e4a2c, 7cd86524-c788-4ca2-b061-e2e6e829953c, 7cf581a9-921c-46eb-b4cf-02137783bcf4, 7d0860f4-2636-42d6-a956-9aa1f6c68b5d, 7d2c586a-e28a-4ebd-94d2-10c3a5c69bac, 7e09aeef-ee87-44f5-8f6a-fe4e4670919e, 87f61bd1-d951-412d-8ff6-fd45c97a427d, 89f16d38-f713-4e0f-80db-618a97b188d4, 8a9f9eb6-3325-4edf-a119-539ffda41af1, 913e5dd1-1bf8-419f-8da3-dac374d3828b, 91d69312-c0a1-499e-82f3-bf08ee837298, 927dfc06-77a6-4d65-b8d7-e8b0a555ad50, 92f556e8-edec-4d2b-aa21-bfe563555512, 9325a958-a3cb-483b-8a76-f590cd45d68f, 93d6618f-8ece-4c4e-97fd-d24b88f376dd, 96256d70-fcd9-47f7-9587-d400bd176d33, 96b6fa50-1fdc-4340-9cb1-6d37169585b1, 96b88299-1c99-4f77-9746-088695ecafd4, 97089f4e-66dc-48e4-8b08-d026f4d9825a, 970ffcdf-d5c8-4b81-8b57-ea45234a9c38, 9a1fccca-209f-41f2-976f-1cf1249fbc9f, 9a8bdab0-b481-4542-afcc-5b25aeeec18d, 9eea17bf-dd77-4da1-942c-856df576d57f, a3d6499f-9042-42d9-9806-436b991e3bf8, a45b7f1c-a75a-422b-b453-be8858a10068, a5bd4c90-e261-4b9e-ad28-de4fc66643a3, a618282e-2458-41eb-845f-fea62b2e0d8f, a6bf1817-6863-49a6-b868-8fc38b5f4226, a779c464-36e7-4f26-bbf8-569bc064218b, a7de519e-8d76-4b8a-85ae-e9697297ea15, a8b03a64-dd2a-4564-a19e-8e03b89ba4b2, a8c1e3d3-71ff-4a6c-9d36-b39eb649f2cf, a99fce83-3bbe-4c21-a194-2e64e1757ca0, b08d47f0-bda3-408b-9532-a91260cfd665, b0bf3b82-6656-42cc-a2ce-23456728fa8e, b17ee766-5218-4ad9-8837-5f187cb31016, b2a7dac4-fcea-4603-b44c-142dfbbbae90, b976e68e-47f9-4eae-8700-22ce1f55b350, b9f5bdd9-5c21-4755-8e89-8fa91d4398db, bb44441e-d43c-4420-830d-b03d2b1e2368, bf6b7f30-0cf8-4283-b747-ec9721fd3e34, bfd0854d-a1b6-4dbe-817a-e4c49a8475b0, c0004993-5f80-4fdb-b62f-8c5e8c19a280, c131b38d-792e-4f5b-8923-119e7d38662e, c178843f-c949-4c0f-9ccf-c4f77056cca5, c5bc7246-72f8-4129-b696-6b62dfd16c30, c5c93b7f-6e74-48c4-b5ab-80044c909745, cd030f9f-4b2d-454e-9e3c-86633e4b3245, d3aa302e-675e-4a4d-816a-033564fbc34d, d63237dc-4ebb-4a00-b6bb-153a949e16fd, d64cae7b-6e76-488c-854b-6ef01c3a672c, d70e5e86-fb3a-4e06-abbb-635761f040b8, d7cd25b4-7f62-4f57-9677-25873f4dac45, d87c331f-2ee3-4efe-8098-4a92b3cc2591, d8b44ddf-fb9a-4e00-8101-794b9945af58, d9072bcf-6d30-4e42-9990-5b24e0ac1e42, d9f659e1-04b0-4b59-a766-6a1b2c1fa2df, dc26fc85-7ae1-4d9f-bb77-4e943f001641, e2fb12d6-c09a-4c97-9b66-6129ce268cf6, e33e10dc-ffa9-4cb8-8c1c-275db1cc4150, e6868d77-ff7e-4f8b-8e07-c941ed87ac9e, e77dfd14-303c-4736-8538-ee2afd8d5633, e7a0461a-e0c8-4c0e-91f2-d52bd29ef90b, e8423fae-50db-4d22-94a5-ee3460fd9faf, e8d5db8e-32d4-47d5-81e3-7e0aeebab562, e8d6cf4f-b5ab-4f80-9431-0805e73a6cd7, e923f12b-b071-4e51-b844-070e74b1349f, e9916e49-da86-4b17-b77c-ff2db62b9c49, ea32673d-d7c9-48c2-906c-2eb6bd85460d, ea377919-0354-43bc-88ba-de8b6dd22551, ec1e32e9-f162-4e2c-b3af-bd58e408c127, ed015920-c5b0-4437-aa61-eb3851a595cd, ed1907af-9bbd-45d7-be76-3ad292a710c1, ef46bcac-5962-4c49-b6ac-4fea82e2fe9c, ef67bab1-c7cd-4b55-8400-685d7732bee0, f09fa83b-1852-4de7-ae65-6ffee7d08bf0, f32bde65-57ef-48f3-baee-efb41eb2b783, f7244c85-0151-49c5-97ae-7c24016889f8, f8c44ec7-d7b5-46b6-83be-f6ff3e1903f1, fc754b22-6d57-4012-afb0-6e02f77a669b]
148001 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 188 for /tmp/junit3452656593956699127/2015/03/16/ddc03e18-5296-49ad-b3f3-a760b5a2183e_1_20180316082212.parquet
148016 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 188 row keys from /tmp/junit3452656593956699127/2015/03/16/ddc03e18-5296-49ad-b3f3-a760b5a2183e_1_20180316082212.parquet
148016 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 188 results, for file /tmp/junit3452656593956699127/2015/03/16/ddc03e18-5296-49ad-b3f3-a760b5a2183e_1_20180316082212.parquet => [01c069f4-0b68-4a5c-a62a-f002336b0cb4, 02565b77-a5a4-45d1-a411-fa4957df7a45, 056e4195-1496-44f8-9639-fab44af8b6d0, 05bce6d9-f969-48b2-9a3f-95e54d38fe12, 06c6ec0a-ada6-4f98-846d-ffcad9f8c0a0, 06c7fa42-85de-4129-b373-3eb3a9f14ee2, 08b03a55-7d0e-4cd2-97fe-3e495234b319, 090a3799-cebe-4a83-b06b-89f07ffab6e7, 0cb7a0a0-e8be-4281-a1fd-fa0b59c25220, 0dfe88c7-e136-419b-a47a-be50321d1b0e, 0e06932a-88d1-42d6-8b61-574653d6065c, 0ec2eb51-da91-4758-820c-3456df8a2086, 0f2234ba-4e56-4d46-b133-a53364b46b16, 0f9b15f7-687a-47f7-88ed-7aaf8040c294, 0fa0bfb0-4168-48c9-8b98-b785bf63084e, 1053db6f-4de4-4999-a28f-6b433248d6df, 10618889-5af9-4ff1-9de8-0595dbae29cd, 11efa66c-5e75-48e6-9d12-fcb2330cb8de, 168edacb-faf9-49b9-b7cc-206104cfdcd4, 18eb41b3-beca-4cb5-b678-459a61b29c0c, 1b4617a4-1108-4c29-8757-8c762ad1ae73, 1bf1369a-83ff-4064-ae20-fdf6b42f3961, 1cad5e1b-8cbf-4960-90c6-56c2fb763e7a, 1cd78eaf-de24-4164-add6-a4fe1cce95d2, 1d298a1e-cb1c-4ff6-aa9c-13445e58ab98, 1ea2c2e1-9849-4330-893d-c6494bc7484a, 1ec9ae3a-b0f1-4e6c-9cd5-05fff693dc31, 20162a60-7c57-4399-9f77-43212669cc35, 22e94f75-48c7-417d-a776-9bf6c73148aa, 23dbac90-95cb-4a81-9352-9b59f8044500, 280d89f1-86c6-4235-8e6f-6b40ea3c4868, 294a88c6-02d8-4989-9d2b-10ecdf4d99a2, 29dad262-0407-4258-9565-b8d166ba67bc, 2a26a1e8-3db0-484a-97ae-1267dab9e5ff, 2a364d0b-4b03-4ce9-83c8-d6911a0ae784, 2aaec15b-bf7b-40d4-8cae-2ad8470c1f87, 2b24bc5e-2b97-4529-96c3-95ee9eb7fc3c, 2bbf920e-ecb1-44c5-b907-e9fc0178bbba, 2f9512fc-c937-4efb-afdd-4ac0ea8971ef, 30237fed-0d66-4016-aa56-4254ce5fd918, 32ac3f73-91ac-4c05-8c44-b1f26a6615fe, 33078d27-73db-482e-9e21-bbe85dee401b, 3361cca8-ce48-4817-9d5e-02057dba71c1, 368d7f72-3231-450b-ae4a-5e8ee23a9e46, 394a4f96-33c7-4299-88d5-6ba60ca08a45, 3b1dc151-c779-496f-b26c-b5c54b11ad2e, 4142e572-cf05-4889-bebf-3183b7bdad83, 4264b28e-284c-4815-8932-391104bd7f6f, 432093c7-bb7a-49f6-9cf9-b926cec39d56, 4866b81f-1b0f-402a-bda2-12d4bbe6fb96, 4d8e0174-0d1f-4eb5-93f2-665f047fe6fb, 4dfc0cca-10d0-4ae0-9d7b-79c40f28e46c, 4f308c5f-defc-47d5-afd8-cc396ee42066, 5128b820-6886-4d57-bf9e-60987a71a356, 522f74d9-57d2-46e1-8ecf-7e058e198db9, 52e121ae-65e5-40b6-a2c6-7484b6345cfb, 5368a7d2-9c0a-4d36-896f-4e621f1b8053, 54f9b511-0d9d-42eb-a7c2-de72b84dc05a, 55eb8dae-37be-4dc3-bc89-0ebd3f595189, 5b412abd-8b8d-4dc0-bbca-053711dc9085, 5e1c1d4c-0dc7-4ec8-bf01-39d92fba44f7, 5ec7248c-0baf-4659-b0ce-2b1cab644b5d, 5eeac694-a8ef-48b6-a3a3-16630a895672, 5f6b2627-8c3d-4e96-abf7-b07dc3b2667d, 605c2159-8454-4bbc-99a0-44f373ab231b, 629b0797-5409-4e72-ad08-9bace48f5374, 63dd1576-4d9c-4fda-b801-9e92ca246b93, 63efe5fa-b787-4ca8-80a4-545f7f9c0cb8, 63fd2968-968c-43d9-befa-1875842374b0, 65c8862c-7b60-4349-b94c-8d5f20dbaa5f, 667d46b7-7d27-4d09-b55c-c37127860ecc, 680f4d75-7f99-4245-945a-71286e0bffb4, 691eb334-7240-46fd-bce6-96013b76abab, 69cf42c5-bc4c-4a10-9d63-1250fb36e511, 6acf4f16-7580-4741-87f9-62965d771904, 6c80cee2-3947-4656-a2f3-c371706f480f, 6da5b318-88da-400e-ae80-7dc176d32bb7, 708be0bd-9467-4adb-84c5-c8ee7b0f47ee, 71c174e8-dd6a-48ff-822c-79f3be168b81, 73092f3f-8635-4101-b4c5-5f5fa8d54ecd, 7442cf3d-51e0-480e-b527-0495de885d57, 76e2243a-294d-4a06-b423-40b6fe838241, 797ec82f-355c-4bfd-9c2d-cb9fbefb5ce5, 7b1172ee-a8bb-4ecf-9dab-7507d5f4e9db, 7c6cbac1-178b-4ef1-96f9-2957a048e824, 7c89a4e3-ea32-4d37-ab2a-99f5fd71c1b1, 7e4d1401-d05d-4f80-96aa-4858f202d8cb, 7e4fbf79-8f0e-4912-bb12-269e49a991cf, 7ee3bb69-25f6-445c-a9e8-9f7f7031a152, 7fb0a22a-335c-412f-b4f1-050615296d7e, 7ffb79ab-ce55-4634-ad33-56ba4db80918, 804caea4-06f0-43e1-8258-762ff82203d9, 8060b783-d4b1-41eb-8b89-de4dc218ef66, 8120c1ae-7df8-4748-9ba1-1083518350f2, 81d2efb4-33f6-4c54-9032-ba8855d503e4, 85d546f2-1e20-4b7f-b416-2adc16a7b17c, 86515ddd-e8a2-473f-8e2b-9b230ea9f8d6, 8683e725-d2f0-42be-b564-5e9394257814, 86927543-a520-4664-abf5-a2a4f00c3f1b, 885bab5f-912d-4176-8855-02ad71dcb508, 8b3ab254-b506-4aa3-8215-b49c10e232d9, 8bf69d72-209e-4c41-953e-0d1a59c54b57, 8e8f20c4-7bc1-4253-bae3-ce53d8e49fd1, 914beeba-7935-411e-a562-6dededfa854f, 920f275f-5557-427b-8e2e-183fdbca9f87, 92678c1c-bc9c-4a7c-97f0-31844aa36e67, 92cb957e-2871-410a-a7c0-c4a94d8d0d9b, 9367e881-5757-4356-8162-fbfdb1c45612, 945e2c61-171d-4726-a477-24f6f137f360, 957aed47-30ff-46d3-a778-456dd72d1ef4, 987c36c4-cae3-46a0-a415-a51d506cbdd6, 99944194-b5e8-4664-93b9-755a4772e3ce, 9bc8a8d6-c830-40ad-97cc-799d4f7311fe, 9c08769a-7586-4060-8d5a-b859ca015eea, 9c428b73-4315-4f3e-8b55-06d37c7d6b2d, 9cdd2656-2f83-4ffe-b08d-dde0349988e9, 9d0ef400-54ae-44c0-a20c-ca52de40a93b, a041c48c-62f2-4818-a7e2-c31daaf3111b, a074d94a-82fb-40bf-abea-46c6dc68a992, a495d859-2d66-4eee-b7e4-605906431c7f, a558299d-9495-4f17-b91e-b9a51dbc88b2, a61dc93f-f954-4af5-9c56-437544ed05b4, a61e7026-1d85-4338-a10f-d799159d50bd, a6aa6afb-2ac1-4797-81bd-1778d1ce5402, ad798abc-6a7c-44f0-9569-d0c325c20251, ae08fcc9-b176-4ae2-b966-38a50a22c8b1, b071529f-55f9-4587-b729-ebc589790695, b37bc43c-04de-4b4b-b95a-2e099220284d, b61d728d-6e23-4827-b3e3-f07e305c3d01, b6e444cb-9a33-4575-ac08-8ec4a53a5b2b, b6e4a612-2063-423a-ba71-5f42dd45ebfa, b76064f2-74f6-4506-9da3-ca861b80e2f1, bc536a3f-40a7-4e02-9a02-21fad1a468ce, bc81f187-bea3-4124-adf4-0a5a7225230f, bd177c62-7516-4cb5-900a-fc9de7061473, c118e93f-52ec-453f-80e1-2cf48060b2fc, c20f4279-31a9-4018-ac7e-c76deb14aeca, c27fee91-3100-434e-9e40-d9607c2a2ef2, c5678839-8e06-4a31-9188-e6fa0f7d8095, c628723a-ec8b-4a37-a55b-90c8e38efd04, c8551e19-6aac-4b3d-a73a-39d86e9f5964, ccebd49f-dc7c-4964-8d0f-203f390c49be, ce2576f0-a72d-44bc-bb5d-5229e4f5e2e7, cef64ff0-4594-4f38-ba41-01b7801da5da, d045400f-3884-435c-a1ec-050bad4b67f1, d3655c56-6f90-4a70-a520-4967ebf2c528, d3b72d99-13ba-435e-b70c-b5b62cc638a5, d3bd198d-6dab-4661-9be1-078162a187e2, d3cdbc77-7fa3-479c-a0f2-bbdf10fd4e2e, d54d507d-d696-42cd-92a6-27fabae4e824, d5bdec87-5b4c-43d6-9276-dcbc949f723a, d5c619e9-95ce-4913-a579-b5cb9a8cac6e, d7166cdf-d25f-4482-ab03-fccc36ee892f, d7dfa959-5953-44c4-afbf-0f44c6c8a1e0, d7f5ec95-40c3-4df7-9563-b29b41d7966a, d94ea073-aeaf-4332-9808-8ead5dda7d73, d97fe9b0-347a-4d72-8002-d45455dce4e5, dbec6e57-9d02-4162-9bd9-eafcf31da080, ddfccc0e-f1ad-462a-8c20-843f6f5952f6, dfb6a0ce-0947-463c-8257-3fccce7041a9, dfdeb19f-2d93-4969-bb19-62b4cb3cb692, e080166b-3fa4-4302-8ce3-4813ef20d057, e182808f-dc14-44ee-a256-335f2d17586b, e42ab534-5325-4596-b04c-dc4f0e66e84e, e4508311-271b-439a-a01c-46ccc9eed2d7, e4691964-f9fb-4175-a6f5-012ba3696c4b, e4f51fb7-e245-4210-b58a-9759eaa4f8de, e5abd950-d7af-4fd5-817c-5ad91ad75bd5, e9174cc2-7fda-455f-b33c-250f50dea4de, eae09143-f5a4-42b5-8460-1890a7ddb6a8, eaee401f-360a-4b37-8cf7-6237c2106e42, eb0409b1-d7bc-4ff8-970c-7246f7197500, ed802fcf-8426-41eb-b16b-1672b2776e08, ef036368-9e99-46ee-aed6-523402e4b68a, ef65e6f3-4f54-4000-8a9d-2fd4d8469701, f1066f5a-03d1-402a-a38b-2f00c298f356, f35d73b9-4d98-4673-bd07-26f54ea2525d, f40cb721-9973-4d79-a33c-333d8aecd229, f47a2c64-d9cc-48aa-b39b-47cf5f9855c4, f7872a81-414b-41d0-8413-f724323f648b, f81d71fb-9bad-4dc8-a575-2b8f40c00ade, f82115bb-c367-4a4f-9cac-8e98c0dd1db9, fa26a8c0-03e4-4855-b4bb-d4d05b85c3c3, fb4d662e-2b34-49dc-acfc-052481439e39, fc3d87d1-1b20-4e1f-b8f0-adbadd0bae00, fd64e1bc-c573-4166-9905-7e20060a222c, ff3417a3-0560-49df-aee5-f881ef01e916, ff486146-5606-475e-a8fe-21d49e01fafc]
148077 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
148097 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
149197 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082216
149545 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
149599 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 95
149599 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
149730 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
149778 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit3452656593956699127/2016/03/15/181e0627-ef24-4ac0-808a-3b7b713e1b13_0_20180316082212.parquet
149789 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 151 row keys from /tmp/junit3452656593956699127/2016/03/15/181e0627-ef24-4ac0-808a-3b7b713e1b13_0_20180316082212.parquet
149790 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit3452656593956699127/2016/03/15/181e0627-ef24-4ac0-808a-3b7b713e1b13_0_20180316082212.parquet => [0152f28f-0ff0-4557-a862-d090702d0bd5, 015907c3-f387-42b0-8b01-30791a33cfe0, 0c3d9029-fe8a-4890-9407-94cd3b17dcc3, 12924b4f-ccaa-4381-9920-30aaaee4021d, 1d613913-18ec-4d4e-a47e-064577a24c9c, 21cdbba5-1815-464a-b5a0-079916ca81cf, 255ef9fe-8dd2-4b23-b75e-40100ca407b3, 258c31e7-1931-47b7-b97a-81ee9d3985a0, 2c7dae40-c2f6-468f-94a9-77020b29e691, 2da9777d-435f-476d-b9a9-c2e6c7ca7bc0, 39f0497e-aa12-4477-a36d-8c6323b1d1bb, 4282fad3-c3cd-46b1-8a1e-042deaf326f9, 54ffbc57-ea9a-4c2f-a96b-b052b7058b7a, 6c7da830-eeb1-4234-a134-ac44246ca653, 7df253fb-6a73-4492-803c-86af2fc289bc, 82ffcb42-1937-4f53-843b-df1844dfdd6b, 86df49e7-621f-4cf9-ad25-25af4e3a70ec, 8a0fcd09-eb4a-4267-aeed-a6360e702ee8, 8de8f663-d716-49d3-bd09-a0cc4174cccb, 9cc2b74c-75d0-470e-8760-91649dd5b8a8, a72d13b0-9159-412d-ac7c-a1be5f98ab95, b135a995-b531-45c7-9f45-f75cd66e8915, ce26064e-816f-4234-ab54-adc06f099306, d3d97e73-0dc0-4292-9951-787e9811cb42, d4af4cd1-3d42-49b0-ba12-67414671bf55, d57aa824-27e9-4b4e-a425-7f755f64f198, d5f91067-9cfe-4bea-ba4b-e054a5450045, d9f26e00-63d2-459c-9e58-3e4f390ddad8, f4b36be4-4768-4457-a97c-9b5b66446ac2, f7cb4a92-9a63-48e0-b823-e54b0b945feb, fb720196-84c3-4b5f-834d-2822fe3212fe]
149816 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet
149831 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet
149831 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet => [04535ffe-2386-4b29-9371-1b99aeca8575, 07307364-456c-4b5b-b48b-97b7383eb7e1, 0eadaaf5-31ee-43e4-911a-12672e72f46b, 1173136a-472d-4030-ba43-264a0443038b, 1d2a902f-0cf8-4f37-ac28-cd7b48403cbe, 22ba74c3-9b76-45e8-a98f-fa443f7cf34c, 2549b186-8a20-43b1-b645-f53d8b909ae2, 276fa843-6797-470d-bc1e-e3fd6f26af7e, 2ee0c0e4-c94d-49b5-9c7c-50ac0fe2e58c, 367fb861-cc06-42c0-bc9e-4035647cd888, 39e65bec-0bb3-4f06-a42e-0e4365ee4b19, 54adc4e8-68a8-4d6e-a79c-efd1678c8f78, 5a729de0-99fc-44ad-a2ee-2fa6564b5f19, 5ad7f88e-47a8-47f7-a45a-080aa8ab6625, 6e67d70d-e538-41c6-8f87-4027ccf8bd2b, 701ad16f-c352-4f44-b543-e35a757097a1, 76b05909-4346-4c8c-9cbc-adf3e15a893d]
149875 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet
149890 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 161 row keys from /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet
149891 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit3452656593956699127/2015/03/17/379aa730-ac07-49a9-926a-528b7a2bc5bd_2_20180316082212.parquet => [7cd86524-c788-4ca2-b061-e2e6e829953c, 87f61bd1-d951-412d-8ff6-fd45c97a427d, 91d69312-c0a1-499e-82f3-bf08ee837298, 92f556e8-edec-4d2b-aa21-bfe563555512, 96b88299-1c99-4f77-9746-088695ecafd4, 9a8bdab0-b481-4542-afcc-5b25aeeec18d, 9eea17bf-dd77-4da1-942c-856df576d57f, a7de519e-8d76-4b8a-85ae-e9697297ea15, bb44441e-d43c-4420-830d-b03d2b1e2368, c178843f-c949-4c0f-9ccf-c4f77056cca5, cd030f9f-4b2d-454e-9e3c-86633e4b3245, d63237dc-4ebb-4a00-b6bb-153a949e16fd, d64cae7b-6e76-488c-854b-6ef01c3a672c, d8b44ddf-fb9a-4e00-8101-794b9945af58, e2fb12d6-c09a-4c97-9b66-6129ce268cf6, e7a0461a-e0c8-4c0e-91f2-d52bd29ef90b, e8423fae-50db-4d22-94a5-ee3460fd9faf, ed015920-c5b0-4437-aa61-eb3851a595cd]
149912 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit3452656593956699127/2015/03/16/ddc03e18-5296-49ad-b3f3-a760b5a2183e_1_20180316082212.parquet
149926 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 188 row keys from /tmp/junit3452656593956699127/2015/03/16/ddc03e18-5296-49ad-b3f3-a760b5a2183e_1_20180316082212.parquet
149926 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit3452656593956699127/2015/03/16/ddc03e18-5296-49ad-b3f3-a760b5a2183e_1_20180316082212.parquet => [05bce6d9-f969-48b2-9a3f-95e54d38fe12, 06c7fa42-85de-4129-b373-3eb3a9f14ee2, 090a3799-cebe-4a83-b06b-89f07ffab6e7, 0f2234ba-4e56-4d46-b133-a53364b46b16, 168edacb-faf9-49b9-b7cc-206104cfdcd4, 1cd78eaf-de24-4164-add6-a4fe1cce95d2, 1ea2c2e1-9849-4330-893d-c6494bc7484a, 2a26a1e8-3db0-484a-97ae-1267dab9e5ff, 2a364d0b-4b03-4ce9-83c8-d6911a0ae784, 5128b820-6886-4d57-bf9e-60987a71a356, 522f74d9-57d2-46e1-8ecf-7e058e198db9, 680f4d75-7f99-4245-945a-71286e0bffb4, 708be0bd-9467-4adb-84c5-c8ee7b0f47ee, 7b1172ee-a8bb-4ecf-9dab-7507d5f4e9db, 7ffb79ab-ce55-4634-ad33-56ba4db80918, 8060b783-d4b1-41eb-8b89-de4dc218ef66, 8e8f20c4-7bc1-4253-bae3-ce53d8e49fd1, 9367e881-5757-4356-8162-fbfdb1c45612, 9d0ef400-54ae-44c0-a20c-ca52de40a93b, b61d728d-6e23-4827-b3e3-f07e305c3d01, bd177c62-7516-4cb5-900a-fc9de7061473, c5678839-8e06-4a31-9188-e6fa0f7d8095, cef64ff0-4594-4f38-ba41-01b7801da5da, d3cdbc77-7fa3-479c-a0f2-bbdf10fd4e2e, d5c619e9-95ce-4913-a579-b5cb9a8cac6e, f35d73b9-4d98-4673-bd07-26f54ea2525d, f40cb721-9973-4d79-a33c-333d8aecd229, fb4d662e-2b34-49dc-acfc-052481439e39, ff3417a3-0560-49df-aee5-f881ef01e916]
150001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=95}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=35}}}
150014 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2699
150014 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=379aa730-ac07-49a9-926a-528b7a2bc5bd}, 1=BucketInfo {bucketType=UPDATE, fileLoc=ddc03e18-5296-49ad-b3f3-a760b5a2183e}, 2=BucketInfo {bucketType=UPDATE, fileLoc=181e0627-ef24-4ac0-808a-3b7b713e1b13}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{379aa730-ac07-49a9-926a-528b7a2bc5bd=0, ddc03e18-5296-49ad-b3f3-a760b5a2183e=1, 181e0627-ef24-4ac0-808a-3b7b713e1b13=2}
150029 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082216
150029 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082216
150201 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
150203 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_78_0 failed due to an exception
150203 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_78_0 could not be removed as it was not found on disk or in memory
150203 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 38.0 (TID 70)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
150206 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 38.0 (TID 70, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

150206 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 38.0 failed 1 times; aborting job
150221 [Executor task launch worker-0] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
150221 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_78_1 failed due to an exception
150222 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_78_1 could not be removed as it was not found on disk or in memory
150222 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 38.0 (TID 71)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
150222 [dispatcher-event-loop-20] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 37586, None),rdd_78_1,StorageLevel(1 replicas),0,0))
150229 [Executor task launch worker-0] WARN  org.apache.spark.rpc.netty.NettyRpcEnv  - RpcEnv already stopped.
150299 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082217
150623 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
150623 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
150676 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
150898 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082217
150898 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082217
151180 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151181 [pool-660-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151182 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151182 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151227 [pool-660-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151256 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151256 [pool-661-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151257 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151257 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151287 [pool-661-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151313 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151313 [pool-662-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151352 [pool-662-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151377 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151377 [pool-663-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151630 [pool-663-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151649 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151649 [pool-664-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151649 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151649 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151683 [pool-664-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151708 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151708 [pool-665-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151709 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151709 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151716 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
151732 [pool-665-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151753 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151753 [pool-666-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151753 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151754 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151777 [pool-666-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151802 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151802 [pool-667-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151803 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151803 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151824 [pool-667-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151846 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151846 [pool-668-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151846 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151846 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151870 [pool-668-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151890 [pool-669-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151913 [pool-669-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151932 [pool-670-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151933 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151933 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151956 [pool-670-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151980 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151980 [pool-671-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151981 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151981 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152011 [pool-671-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152036 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152036 [pool-672-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152067 [pool-672-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152092 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152092 [pool-673-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152093 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152093 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152140 [pool-673-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152165 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
152168 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152168 [pool-674-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152169 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152169 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152208 [pool-674-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152234 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152234 [pool-675-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152286 [pool-675-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152313 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152314 [pool-676-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152354 [pool-676-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152382 [pool-677-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152383 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152383 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152417 [pool-677-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152444 [pool-678-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152445 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152445 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152481 [pool-678-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152509 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152509 [pool-679-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152510 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152510 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152537 [pool-679-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152564 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152564 [pool-680-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152564 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152565 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152591 [pool-680-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152619 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152619 [pool-681-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152619 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152620 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152653 [pool-681-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152678 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152678 [pool-682-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152679 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152679 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152704 [pool-682-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152727 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152728 [pool-683-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152728 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152728 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152763 [pool-683-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152785 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152786 [pool-684-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152786 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152786 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152812 [pool-684-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152829 [pool-685-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152829 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152829 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152863 [pool-685-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152880 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152881 [pool-686-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152906 [pool-686-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152923 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152923 [pool-687-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152923 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152923 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152949 [pool-687-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
152968 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
152968 [pool-688-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
152968 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
152969 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
152992 [pool-688-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153013 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153013 [pool-689-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153014 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153014 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153037 [pool-689-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153063 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153063 [pool-690-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153087 [pool-690-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153110 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153110 [pool-691-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153111 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153111 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153146 [pool-691-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153166 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153166 [pool-692-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153190 [pool-692-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153209 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153209 [pool-693-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153239 [pool-693-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153260 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
153260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153261 [pool-694-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153261 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153261 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153287 [pool-694-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153314 [pool-695-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153315 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153340 [pool-695-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153363 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153363 [pool-696-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153364 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153364 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153396 [pool-696-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153424 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153425 [pool-697-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153425 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153425 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153455 [pool-697-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153481 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153481 [pool-698-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153482 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153482 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153505 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
153516 [pool-698-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153540 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153540 [pool-699-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153541 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153541 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153572 [pool-699-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153595 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153595 [pool-700-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153595 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153596 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153622 [pool-700-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153645 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153645 [pool-701-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153646 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153646 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153669 [pool-701-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153691 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153691 [pool-702-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153692 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153692 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153713 [pool-702-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153735 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153735 [pool-703-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153735 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153736 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153765 [pool-703-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153793 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153793 [pool-704-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153794 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153794 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153817 [pool-704-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153839 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153839 [pool-705-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153840 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153840 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153862 [pool-705-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153883 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153883 [pool-706-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153884 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153884 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153906 [pool-706-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153927 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153927 [pool-707-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153928 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153928 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153950 [pool-707-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
153972 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
153972 [pool-708-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
153973 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
153973 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
153994 [pool-708-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154015 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154016 [pool-709-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154016 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154016 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154039 [pool-709-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154056 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154056 [pool-710-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154057 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154057 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154085 [pool-710-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154102 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154102 [pool-711-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154102 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154102 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154128 [pool-711-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154146 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154146 [pool-712-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154146 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154146 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154170 [pool-712-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154192 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154192 [pool-713-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154193 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154193 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154216 [pool-713-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154245 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154245 [pool-714-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154246 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154246 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154285 [pool-714-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154303 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154303 [pool-715-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154304 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154304 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154328 [pool-715-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154344 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154344 [pool-716-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154345 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154345 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154377 [pool-716-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154395 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154395 [pool-717-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154395 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154395 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154423 [pool-717-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154440 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154440 [pool-718-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154441 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154441 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154467 [pool-718-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154485 [pool-719-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154507 [pool-719-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154523 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154523 [pool-720-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154524 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154524 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154556 [pool-720-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154577 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154578 [pool-721-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154578 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154578 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154613 [pool-721-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154634 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154634 [pool-722-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154634 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154634 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154666 [pool-722-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154688 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154688 [pool-723-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154688 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154688 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154711 [pool-723-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154730 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154730 [pool-724-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154731 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154731 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154763 [pool-724-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154785 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154785 [pool-725-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154786 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154786 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154812 [pool-725-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154830 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154830 [pool-726-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154831 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154831 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154863 [pool-726-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154889 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154890 [pool-727-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154924 [pool-727-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
154952 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
154952 [pool-728-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
154952 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
154953 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
154994 [pool-728-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155021 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
155021 [pool-729-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
155021 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
155021 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
155057 [pool-729-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155083 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
155083 [pool-730-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
155084 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
155084 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
155111 [pool-730-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155138 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
155138 [pool-731-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
155139 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
155139 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
155168 [pool-731-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155192 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
155192 [pool-732-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
155193 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
155193 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
155193 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
155229 [pool-732-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155259 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
155259 [pool-733-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
155260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
155260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
155279 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
155295 [pool-733-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155320 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
155320 [pool-734-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
155321 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
155321 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
155356 [pool-734-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
155384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
155384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
155815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
155815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
155961 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
155968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082217 as complete
155968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082217
156351 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
157333 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
157811 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
158296 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 75
158296 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
158417 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/0347204e-3626-4508-b815-80dca7824772_40_20180316082217.parquet
158432 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/0347204e-3626-4508-b815-80dca7824772_40_20180316082217.parquet
158432 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/0347204e-3626-4508-b815-80dca7824772_40_20180316082217.parquet => [41489156-09aa-4d7a-ac9f-2f8d63424b95]
158447 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/07c5525b-e323-4479-8013-0c5786900b55_65_20180316082217.parquet
158460 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/07c5525b-e323-4479-8013-0c5786900b55_65_20180316082217.parquet
158460 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/07c5525b-e323-4479-8013-0c5786900b55_65_20180316082217.parquet => [b6f4f91f-192a-4f9b-a4ff-3572e380bd81]
158474 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/0a5ee928-ea8f-43a7-bf32-383aa9be9cbe_21_20180316082217.parquet
158486 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/0a5ee928-ea8f-43a7-bf32-383aa9be9cbe_21_20180316082217.parquet
158486 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/0a5ee928-ea8f-43a7-bf32-383aa9be9cbe_21_20180316082217.parquet => [a9276b61-1b3a-470f-b88a-2908800e27f7]
158499 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/0bdb834b-1c0d-484f-b35f-1ba98eec7011_16_20180316082217.parquet
158510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/0bdb834b-1c0d-484f-b35f-1ba98eec7011_16_20180316082217.parquet
158510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/0bdb834b-1c0d-484f-b35f-1ba98eec7011_16_20180316082217.parquet => [899f8082-e5fd-4155-a3ec-3e268738528c]
158526 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/0f2863bd-03d4-4201-b348-d185e27e2ef6_36_20180316082217.parquet
158538 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/0f2863bd-03d4-4201-b348-d185e27e2ef6_36_20180316082217.parquet
158538 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/0f2863bd-03d4-4201-b348-d185e27e2ef6_36_20180316082217.parquet => [feffd04e-b590-43fe-8242-c54280b8a292]
158554 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/10daddec-70b4-4cd3-97b0-fb8dfc5b3ae6_1_20180316082217.parquet
158567 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/10daddec-70b4-4cd3-97b0-fb8dfc5b3ae6_1_20180316082217.parquet
158567 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/10daddec-70b4-4cd3-97b0-fb8dfc5b3ae6_1_20180316082217.parquet => [163dc090-09cf-4cf7-8775-8012daefbd50]
158581 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/148e1969-eb40-41f6-ab4a-30f501f82295_64_20180316082217.parquet
158594 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/148e1969-eb40-41f6-ab4a-30f501f82295_64_20180316082217.parquet
158594 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/148e1969-eb40-41f6-ab4a-30f501f82295_64_20180316082217.parquet => [b5fe592d-3673-49bd-9ede-81fa63a63f9e]
158607 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/152119ef-36e8-43bc-b8b7-9598ce4238fa_74_20180316082217.parquet
158621 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/152119ef-36e8-43bc-b8b7-9598ce4238fa_74_20180316082217.parquet
158621 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/152119ef-36e8-43bc-b8b7-9598ce4238fa_74_20180316082217.parquet => [fefc20aa-757c-44d3-a112-93949291ca72]
158635 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/1c619356-5e02-4f6b-844b-76524ba90b3e_48_20180316082217.parquet
158648 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/1c619356-5e02-4f6b-844b-76524ba90b3e_48_20180316082217.parquet
158648 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/1c619356-5e02-4f6b-844b-76524ba90b3e_48_20180316082217.parquet => [99305843-e619-46d5-b174-29612ba73ef6]
158663 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/1cb73ee1-0a81-45c7-bed8-501c2c5a3254_47_20180316082217.parquet
158678 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/1cb73ee1-0a81-45c7-bed8-501c2c5a3254_47_20180316082217.parquet
158678 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/1cb73ee1-0a81-45c7-bed8-501c2c5a3254_47_20180316082217.parquet => [7e4b8425-b671-46bd-876e-1780b5f1ae46]
158694 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/1f91a94d-bd8f-43ee-857a-f816f3b20194_18_20180316082217.parquet
158715 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/1f91a94d-bd8f-43ee-857a-f816f3b20194_18_20180316082217.parquet
158715 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/1f91a94d-bd8f-43ee-857a-f816f3b20194_18_20180316082217.parquet => [8b52b4d4-c4b4-46b2-b7a0-f359d8570ecd]
158730 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/20462e8f-d4de-41a5-aca8-08e0194806a6_41_20180316082217.parquet
158743 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/20462e8f-d4de-41a5-aca8-08e0194806a6_41_20180316082217.parquet
158743 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/20462e8f-d4de-41a5-aca8-08e0194806a6_41_20180316082217.parquet => [525d3652-2458-42ab-852d-1b904faa9e81]
158758 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/257c829b-04ba-4c0f-ba84-d4310d52f356_23_20180316082217.parquet
158770 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/257c829b-04ba-4c0f-ba84-d4310d52f356_23_20180316082217.parquet
158770 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/257c829b-04ba-4c0f-ba84-d4310d52f356_23_20180316082217.parquet => [c4b0a240-6d05-4eab-85c7-094e80ec42a2]
158791 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/375c381c-c31c-4b79-9bab-5e3e12cdfe30_10_20180316082217.parquet
158809 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/375c381c-c31c-4b79-9bab-5e3e12cdfe30_10_20180316082217.parquet
158809 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/375c381c-c31c-4b79-9bab-5e3e12cdfe30_10_20180316082217.parquet => [57e6417e-bfd8-4092-98ae-9b8930fe131d]
158826 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/39d8c681-7b1f-4ac4-a187-cb8cb0d6cbef_60_20180316082217.parquet
158842 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/39d8c681-7b1f-4ac4-a187-cb8cb0d6cbef_60_20180316082217.parquet
158842 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/39d8c681-7b1f-4ac4-a187-cb8cb0d6cbef_60_20180316082217.parquet => [76db5bfe-7ed1-4be2-9061-0eebfd41bb01]
158857 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/405df4a7-3b69-4051-841d-68cdb0566ad5_59_20180316082217.parquet
158874 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/405df4a7-3b69-4051-841d-68cdb0566ad5_59_20180316082217.parquet
158874 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/405df4a7-3b69-4051-841d-68cdb0566ad5_59_20180316082217.parquet => [6e8fc2d6-b50e-42c8-809a-467bebc37765]
158890 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/4225a757-d322-4ea7-a220-05893d44bd11_33_20180316082217.parquet
158906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/4225a757-d322-4ea7-a220-05893d44bd11_33_20180316082217.parquet
158906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/4225a757-d322-4ea7-a220-05893d44bd11_33_20180316082217.parquet => [f20c4f57-dfb4-45f8-9726-5800b5bccd79]
158922 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/427091a2-ac13-46f4-a46e-1c43ad5013a9_20_20180316082217.parquet
158938 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
158939 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/427091a2-ac13-46f4-a46e-1c43ad5013a9_20_20180316082217.parquet
158939 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/427091a2-ac13-46f4-a46e-1c43ad5013a9_20_20180316082217.parquet => [a6a43af9-2d64-4dda-8d5f-a0cf0d8c2d75]
158954 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/45457bc8-c776-4dd8-a42c-ec99c5ec4e14_38_20180316082217.parquet
158969 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/45457bc8-c776-4dd8-a42c-ec99c5ec4e14_38_20180316082217.parquet
158969 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/45457bc8-c776-4dd8-a42c-ec99c5ec4e14_38_20180316082217.parquet => [08670bad-62a8-4499-b89a-7febbceae1dd]
158986 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/4833bea4-8b57-460d-ab1e-ab76699d729b_13_20180316082217.parquet
158997 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/4833bea4-8b57-460d-ab1e-ab76699d729b_13_20180316082217.parquet
158997 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/4833bea4-8b57-460d-ab1e-ab76699d729b_13_20180316082217.parquet => [7210ec6a-41c5-44ab-af1f-515f105065cf]
158998 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
159013 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/51046e67-bcd1-4bbf-bb4d-37e3d65dfe17_66_20180316082217.parquet
159024 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/51046e67-bcd1-4bbf-bb4d-37e3d65dfe17_66_20180316082217.parquet
159025 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/51046e67-bcd1-4bbf-bb4d-37e3d65dfe17_66_20180316082217.parquet => [b9f0647f-d18e-42a5-9dbf-6cd1ac56b6e0]
159040 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/54125b0e-f6b0-483b-afa8-d47d084ae91a_26_20180316082217.parquet
159053 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/54125b0e-f6b0-483b-afa8-d47d084ae91a_26_20180316082217.parquet
159053 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/54125b0e-f6b0-483b-afa8-d47d084ae91a_26_20180316082217.parquet => [d36d0f05-0590-4adf-97c0-7e05cf230cab]
159067 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/56d3ecdb-47e1-44f6-8058-85993d1ec14c_31_20180316082217.parquet
159077 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/56d3ecdb-47e1-44f6-8058-85993d1ec14c_31_20180316082217.parquet
159077 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/56d3ecdb-47e1-44f6-8058-85993d1ec14c_31_20180316082217.parquet => [eb657b94-68bb-4b8d-9006-fefefcff7081]
159091 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/58789d33-2721-4581-a451-da4757edfca1_37_20180316082217.parquet
159101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/58789d33-2721-4581-a451-da4757edfca1_37_20180316082217.parquet
159101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/58789d33-2721-4581-a451-da4757edfca1_37_20180316082217.parquet => [00388db3-dde3-472d-85c5-f7a2c72e1370]
159115 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/5a24497a-7b8c-4574-81a8-d03cbf75c5cb_3_20180316082217.parquet
159126 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/5a24497a-7b8c-4574-81a8-d03cbf75c5cb_3_20180316082217.parquet
159126 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/5a24497a-7b8c-4574-81a8-d03cbf75c5cb_3_20180316082217.parquet => [2bb112ca-b1c7-4e03-b881-c9b6fb193d23]
159139 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/5b8980cf-d0d7-4c51-b80b-1ca3eb1be920_14_20180316082217.parquet
159149 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/5b8980cf-d0d7-4c51-b80b-1ca3eb1be920_14_20180316082217.parquet
159150 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/5b8980cf-d0d7-4c51-b80b-1ca3eb1be920_14_20180316082217.parquet => [7a525379-c9a0-46db-b171-26e096578f76]
159163 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/5d232f7e-1d04-4c5e-982e-153d51afd762_42_20180316082217.parquet
159173 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/5d232f7e-1d04-4c5e-982e-153d51afd762_42_20180316082217.parquet
159174 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/5d232f7e-1d04-4c5e-982e-153d51afd762_42_20180316082217.parquet => [540cdac6-bde3-4077-8bee-25bfc909cbe9]
159187 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/5eefc55f-5b36-49c0-a6a8-542881ead907_8_20180316082217.parquet
159196 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/5eefc55f-5b36-49c0-a6a8-542881ead907_8_20180316082217.parquet
159196 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/5eefc55f-5b36-49c0-a6a8-542881ead907_8_20180316082217.parquet => [52799e58-c205-4293-9207-fa1705d1182c]
159210 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/6374658f-4f9e-4555-9602-51d353dbf83e_53_20180316082217.parquet
159219 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/6374658f-4f9e-4555-9602-51d353dbf83e_53_20180316082217.parquet
159219 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/6374658f-4f9e-4555-9602-51d353dbf83e_53_20180316082217.parquet => [ec64a188-af37-4291-9b29-77d5a8e46484]
159233 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/65d7e9ca-bc8b-49d9-becc-5b50511ff4df_62_20180316082217.parquet
159242 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/65d7e9ca-bc8b-49d9-becc-5b50511ff4df_62_20180316082217.parquet
159242 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/65d7e9ca-bc8b-49d9-becc-5b50511ff4df_62_20180316082217.parquet => [8a850fd9-3697-4147-86cd-6583ee1969b4]
159256 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/65e9aabe-bd63-4171-a0d5-50626f5e84fc_0_20180316082217.parquet
159265 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/65e9aabe-bd63-4171-a0d5-50626f5e84fc_0_20180316082217.parquet
159265 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/65e9aabe-bd63-4171-a0d5-50626f5e84fc_0_20180316082217.parquet => [148f20bc-783c-40e4-8d54-73299e61a4c5]
159279 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/674655f5-0fd1-45c1-afd0-07def8fc82c0_28_20180316082217.parquet
159288 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/674655f5-0fd1-45c1-afd0-07def8fc82c0_28_20180316082217.parquet
159288 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/674655f5-0fd1-45c1-afd0-07def8fc82c0_28_20180316082217.parquet => [d72cc438-e73e-4332-a212-0a2754f868a1]
159302 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/6807b178-160a-4a43-b30a-483cd4beca1e_49_20180316082217.parquet
159312 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/6807b178-160a-4a43-b30a-483cd4beca1e_49_20180316082217.parquet
159312 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/6807b178-160a-4a43-b30a-483cd4beca1e_49_20180316082217.parquet => [9ad184e1-888c-4db4-bfa0-12aa6bbf67bb]
159326 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/6f58f566-1e6b-4cf3-9fef-813c1feedefa_68_20180316082217.parquet
159336 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/6f58f566-1e6b-4cf3-9fef-813c1feedefa_68_20180316082217.parquet
159336 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/6f58f566-1e6b-4cf3-9fef-813c1feedefa_68_20180316082217.parquet => [bae6e5d8-fb64-4348-8bb0-6314b9b0a946]
159350 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/70f473df-f449-47e2-8125-6da74ff13832_70_20180316082217.parquet
159367 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/70f473df-f449-47e2-8125-6da74ff13832_70_20180316082217.parquet
159367 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/70f473df-f449-47e2-8125-6da74ff13832_70_20180316082217.parquet => [c832e008-73ac-433c-ab5c-6c1cd56163cc]
159382 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/7243a547-e522-4b9b-8bc8-6ca5b6e70aee_56_20180316082217.parquet
159393 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/7243a547-e522-4b9b-8bc8-6ca5b6e70aee_56_20180316082217.parquet
159393 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/7243a547-e522-4b9b-8bc8-6ca5b6e70aee_56_20180316082217.parquet => [0f8ee12f-91d5-4e07-9ebf-f4b67b74145e]
159406 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/772be91f-c381-4608-82d4-225884efc667_57_20180316082217.parquet
159423 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/772be91f-c381-4608-82d4-225884efc667_57_20180316082217.parquet
159423 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/772be91f-c381-4608-82d4-225884efc667_57_20180316082217.parquet => [380e7879-6276-4e5e-9ac2-d194466ff02d]
159436 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/7825e37f-2e35-4a05-8bac-b33a431a47b9_34_20180316082217.parquet
159446 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/7825e37f-2e35-4a05-8bac-b33a431a47b9_34_20180316082217.parquet
159446 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/7825e37f-2e35-4a05-8bac-b33a431a47b9_34_20180316082217.parquet => [f2d3253d-2b02-45b7-b0da-4e36225922fb]
159460 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/7870d391-f794-4e76-ae14-2d9a73692ab3_35_20180316082217.parquet
159473 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/7870d391-f794-4e76-ae14-2d9a73692ab3_35_20180316082217.parquet
159473 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/7870d391-f794-4e76-ae14-2d9a73692ab3_35_20180316082217.parquet => [f6cb82ef-2dce-40e9-89a5-07b98dad8c64]
159487 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/78ac8a98-3fcc-43ec-8ae0-8576290c7b90_61_20180316082217.parquet
159497 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/78ac8a98-3fcc-43ec-8ae0-8576290c7b90_61_20180316082217.parquet
159497 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/78ac8a98-3fcc-43ec-8ae0-8576290c7b90_61_20180316082217.parquet => [77df5a81-05c3-4cc9-ab88-c705a453dee8]
159510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/7a7a514a-b83c-40c8-8a32-0989598eb4ef_54_20180316082217.parquet
159519 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/7a7a514a-b83c-40c8-8a32-0989598eb4ef_54_20180316082217.parquet
159519 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/7a7a514a-b83c-40c8-8a32-0989598eb4ef_54_20180316082217.parquet => [09ad6851-c444-46b0-969e-d4cd70d91021]
159533 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/7bc25fec-521f-4620-a124-405c92126821_12_20180316082217.parquet
159541 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/7bc25fec-521f-4620-a124-405c92126821_12_20180316082217.parquet
159541 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/7bc25fec-521f-4620-a124-405c92126821_12_20180316082217.parquet => [7045320a-2bc8-4704-96a1-367cbfd42e46]
159554 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/7c09aadf-b395-46b7-80b5-569f9d43f273_5_20180316082217.parquet
159562 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/7c09aadf-b395-46b7-80b5-569f9d43f273_5_20180316082217.parquet
159562 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/7c09aadf-b395-46b7-80b5-569f9d43f273_5_20180316082217.parquet => [3ad1b1d3-7ae3-43ce-bd3c-85535f1ef441]
159576 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/7d2335c1-f3d8-47a3-bfe8-c6cd4cceff98_71_20180316082217.parquet
159584 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/7d2335c1-f3d8-47a3-bfe8-c6cd4cceff98_71_20180316082217.parquet
159584 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/7d2335c1-f3d8-47a3-bfe8-c6cd4cceff98_71_20180316082217.parquet => [cef09f4a-e7e2-44de-9d27-f7ba26a345cf]
159597 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/835851ce-25cf-4b77-8232-de2a50fb635b_4_20180316082217.parquet
159606 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/835851ce-25cf-4b77-8232-de2a50fb635b_4_20180316082217.parquet
159606 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/835851ce-25cf-4b77-8232-de2a50fb635b_4_20180316082217.parquet => [3462b72d-30ab-4885-8b4e-080c65d9569e]
159619 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/8492f686-092f-4d0a-811d-dc8a2aa7ce89_27_20180316082217.parquet
159628 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/8492f686-092f-4d0a-811d-dc8a2aa7ce89_27_20180316082217.parquet
159628 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/8492f686-092f-4d0a-811d-dc8a2aa7ce89_27_20180316082217.parquet => [d6d572cf-f223-44bf-ba08-28197037c3a7]
159641 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/91a0abeb-27cd-4f58-b653-0111210da69f_46_20180316082217.parquet
159650 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/91a0abeb-27cd-4f58-b653-0111210da69f_46_20180316082217.parquet
159650 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/91a0abeb-27cd-4f58-b653-0111210da69f_46_20180316082217.parquet => [70f88531-2f81-4693-9bad-ef80df8be001]
159663 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/9668f54e-e999-4771-aa2e-7b23076d4cb0_9_20180316082217.parquet
159671 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/9668f54e-e999-4771-aa2e-7b23076d4cb0_9_20180316082217.parquet
159671 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/9668f54e-e999-4771-aa2e-7b23076d4cb0_9_20180316082217.parquet => [54f656a0-5a74-474f-a7dc-60f586f97bb7]
159686 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/9d688cb0-8fa1-4caa-96a7-204bfbf81124_15_20180316082217.parquet
159695 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/9d688cb0-8fa1-4caa-96a7-204bfbf81124_15_20180316082217.parquet
159695 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/9d688cb0-8fa1-4caa-96a7-204bfbf81124_15_20180316082217.parquet => [8854872d-9f23-4825-b0fa-6c7b261d3e59]
159708 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/a0dbec49-c276-4356-a8b0-9e244badeea8_39_20180316082217.parquet
159717 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/a0dbec49-c276-4356-a8b0-9e244badeea8_39_20180316082217.parquet
159717 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/a0dbec49-c276-4356-a8b0-9e244badeea8_39_20180316082217.parquet => [2bb01c70-7321-437e-bd54-3dc170348176]
159730 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/a2ec605c-cad4-49e3-a0ae-c9c1fb9cb915_29_20180316082217.parquet
159738 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/a2ec605c-cad4-49e3-a0ae-c9c1fb9cb915_29_20180316082217.parquet
159738 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/a2ec605c-cad4-49e3-a0ae-c9c1fb9cb915_29_20180316082217.parquet => [e18c2453-8f76-4789-a8a8-17fc94f91c89]
159751 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/a5c1c65a-1ef1-460c-acb1-6adbd9f21935_44_20180316082217.parquet
159760 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/a5c1c65a-1ef1-460c-acb1-6adbd9f21935_44_20180316082217.parquet
159760 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/a5c1c65a-1ef1-460c-acb1-6adbd9f21935_44_20180316082217.parquet => [645f635f-20a8-4fad-99eb-ae45c9f776ca]
159773 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/a6b13998-d124-4ad2-bfe6-3a162f6975e6_2_20180316082217.parquet
159782 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/a6b13998-d124-4ad2-bfe6-3a162f6975e6_2_20180316082217.parquet
159782 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/a6b13998-d124-4ad2-bfe6-3a162f6975e6_2_20180316082217.parquet => [1be2783e-0b4d-481f-8203-e4e5c46786f7]
159795 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/a8229c57-820e-40b0-87ab-671eade3185a_6_20180316082217.parquet
159807 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/a8229c57-820e-40b0-87ab-671eade3185a_6_20180316082217.parquet
159807 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/a8229c57-820e-40b0-87ab-671eade3185a_6_20180316082217.parquet => [43b108af-a930-40e2-82d7-f4fda114c498]
159820 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/a8b117e4-e759-4b49-a5e8-8675910eac8f_25_20180316082217.parquet
159831 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/a8b117e4-e759-4b49-a5e8-8675910eac8f_25_20180316082217.parquet
159831 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/a8b117e4-e759-4b49-a5e8-8675910eac8f_25_20180316082217.parquet => [cf69f307-c082-4568-a317-2da95bd13b08]
159845 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/a97d4c60-5d76-46ca-9a2d-f272f78ef463_30_20180316082217.parquet
159856 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/a97d4c60-5d76-46ca-9a2d-f272f78ef463_30_20180316082217.parquet
159856 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/a97d4c60-5d76-46ca-9a2d-f272f78ef463_30_20180316082217.parquet => [e570d6ad-be96-4ddd-ba5f-5106ba30751e]
159870 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/a9d2e872-12db-4154-8744-99135f2a693f_22_20180316082217.parquet
159881 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/a9d2e872-12db-4154-8744-99135f2a693f_22_20180316082217.parquet
159881 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/a9d2e872-12db-4154-8744-99135f2a693f_22_20180316082217.parquet => [a97db169-36ba-465e-adcf-c12caff17efa]
159894 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/ad32475e-cda1-41fa-b08b-ee1af750fcd6_52_20180316082217.parquet
159905 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/ad32475e-cda1-41fa-b08b-ee1af750fcd6_52_20180316082217.parquet
159905 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/ad32475e-cda1-41fa-b08b-ee1af750fcd6_52_20180316082217.parquet => [d2eb28f6-960d-457c-b587-99cd46b51d41]
159919 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/b0f6e9c5-ba73-415f-bd9a-a7d1f0072584_11_20180316082217.parquet
159932 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/b0f6e9c5-ba73-415f-bd9a-a7d1f0072584_11_20180316082217.parquet
159932 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/b0f6e9c5-ba73-415f-bd9a-a7d1f0072584_11_20180316082217.parquet => [60fd1411-97c0-4c0a-81c1-c2bd5817f3ef]
159947 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/b4f887c4-9181-46b8-a96e-f71cfe16d879_24_20180316082217.parquet
159957 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/b4f887c4-9181-46b8-a96e-f71cfe16d879_24_20180316082217.parquet
159957 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/b4f887c4-9181-46b8-a96e-f71cfe16d879_24_20180316082217.parquet => [c64f3d08-6ee0-4da3-8972-d9988bb8b7de]
159971 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/bd215af8-dfd4-4b0d-b4ae-abc620c8d579_7_20180316082217.parquet
159983 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/bd215af8-dfd4-4b0d-b4ae-abc620c8d579_7_20180316082217.parquet
159983 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/bd215af8-dfd4-4b0d-b4ae-abc620c8d579_7_20180316082217.parquet => [4caa7c3a-6023-43ed-bc9d-6a79c0a81a82]
159997 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/bfa05596-d0ea-42d8-8153-bc8df27f8dff_17_20180316082217.parquet
160009 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/bfa05596-d0ea-42d8-8153-bc8df27f8dff_17_20180316082217.parquet
160009 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/bfa05596-d0ea-42d8-8153-bc8df27f8dff_17_20180316082217.parquet => [8a38bc74-9693-421c-af18-d9b5cc6312c4]
160023 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/c542b2c1-eb89-4fb9-93e1-b3526a14cad8_51_20180316082217.parquet
160033 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/c542b2c1-eb89-4fb9-93e1-b3526a14cad8_51_20180316082217.parquet
160033 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/c542b2c1-eb89-4fb9-93e1-b3526a14cad8_51_20180316082217.parquet => [d1843f3b-b3da-4a15-b828-caa74abf3c09]
160047 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/c670be82-f49a-485b-bb73-0265eaace9a2_19_20180316082217.parquet
160057 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/c670be82-f49a-485b-bb73-0265eaace9a2_19_20180316082217.parquet
160057 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/c670be82-f49a-485b-bb73-0265eaace9a2_19_20180316082217.parquet => [9e832c8f-2394-4b25-a031-f45e0315240f]
160072 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/d1aa5e7e-a67d-4948-8c12-5d2e922263c7_72_20180316082217.parquet
160082 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/d1aa5e7e-a67d-4948-8c12-5d2e922263c7_72_20180316082217.parquet
160082 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/d1aa5e7e-a67d-4948-8c12-5d2e922263c7_72_20180316082217.parquet => [d4c0b4bd-17e7-46ed-8883-00a0144db23e]
160097 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/d39bc7f7-4cdd-48bf-aa53-aa4c2edaa432_45_20180316082217.parquet
160108 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/d39bc7f7-4cdd-48bf-aa53-aa4c2edaa432_45_20180316082217.parquet
160108 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/d39bc7f7-4cdd-48bf-aa53-aa4c2edaa432_45_20180316082217.parquet => [6ecf4284-bb4d-49ee-b7a2-394df36269b3]
160122 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/d84d1416-ba8f-412a-8d9e-d6c17f09c49f_73_20180316082217.parquet
160135 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/d84d1416-ba8f-412a-8d9e-d6c17f09c49f_73_20180316082217.parquet
160135 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/d84d1416-ba8f-412a-8d9e-d6c17f09c49f_73_20180316082217.parquet => [fa3f86f9-a563-4e12-8744-a4035331e50b]
160155 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/e226e995-bf67-4d9d-bcba-3e58100401b0_55_20180316082217.parquet
160167 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/e226e995-bf67-4d9d-bcba-3e58100401b0_55_20180316082217.parquet
160167 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/e226e995-bf67-4d9d-bcba-3e58100401b0_55_20180316082217.parquet => [0eaf7c75-708b-457e-b798-4ba353b61e8a]
160181 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/e3c90e6f-204b-4d50-b233-de96bf1b11a4_69_20180316082217.parquet
160192 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/e3c90e6f-204b-4d50-b233-de96bf1b11a4_69_20180316082217.parquet
160192 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/e3c90e6f-204b-4d50-b233-de96bf1b11a4_69_20180316082217.parquet => [be70d1b5-25dd-47cc-b4ba-c28d61c023a0]
160206 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/ea3ca3b7-5faf-4c36-a268-60fc6105c072_63_20180316082217.parquet
160216 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/ea3ca3b7-5faf-4c36-a268-60fc6105c072_63_20180316082217.parquet
160216 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/ea3ca3b7-5faf-4c36-a268-60fc6105c072_63_20180316082217.parquet => [ae6115c2-08c1-4386-9f2c-ea9a8e486153]
160230 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/f401b28d-172e-4852-8c33-1f5edda4544a_50_20180316082217.parquet
160240 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/f401b28d-172e-4852-8c33-1f5edda4544a_50_20180316082217.parquet
160240 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/f401b28d-172e-4852-8c33-1f5edda4544a_50_20180316082217.parquet => [c24faa0f-1092-4b49-a251-b9a4ee6184ef]
160254 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/17/fa74b789-6926-4046-9464-c1c610d5926d_43_20180316082217.parquet
160267 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/17/fa74b789-6926-4046-9464-c1c610d5926d_43_20180316082217.parquet
160267 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/17/fa74b789-6926-4046-9464-c1c610d5926d_43_20180316082217.parquet => [645a150e-78a5-48aa-b627-79ac8a00d4b1]
160283 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/fbac6869-0917-4e1f-9f15-ab7c243b0119_58_20180316082217.parquet
160294 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/fbac6869-0917-4e1f-9f15-ab7c243b0119_58_20180316082217.parquet
160295 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/fbac6869-0917-4e1f-9f15-ab7c243b0119_58_20180316082217.parquet => [3cd58815-e157-432e-bd31-4d81dd311566]
160313 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2016/03/15/fc806f16-1d68-48b9-99a4-f394ea46f14b_67_20180316082217.parquet
160325 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2016/03/15/fc806f16-1d68-48b9-99a4-f394ea46f14b_67_20180316082217.parquet
160325 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2016/03/15/fc806f16-1d68-48b9-99a4-f394ea46f14b_67_20180316082217.parquet => [bad8b859-1eeb-4b72-a741-6a7138687ef6]
160340 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit257097701857872318/2015/03/16/fe8fca9c-9ce4-44dd-bae0-8869b44fa9e7_32_20180316082217.parquet
160351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit257097701857872318/2015/03/16/fe8fca9c-9ce4-44dd-bae0-8869b44fa9e7_32_20180316082217.parquet
160351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit257097701857872318/2015/03/16/fe8fca9c-9ce4-44dd-bae0-8869b44fa9e7_32_20180316082217.parquet => [ec694f89-ca2c-4678-a22e-cbeccaf7bd18]
160487 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
160522 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
160621 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
160622 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
160944 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161380 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161380 [pool-900-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161381 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161381 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161429 [pool-900-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161455 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161455 [pool-901-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161456 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161456 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161487 [pool-901-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161509 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161509 [pool-902-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161509 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161510 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161540 [pool-902-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161565 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161565 [pool-903-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161565 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161566 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161589 [pool-903-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161610 [pool-904-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161645 [pool-904-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161669 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161669 [pool-905-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161670 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161670 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161705 [pool-905-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161729 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161729 [pool-906-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161730 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161730 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161760 [pool-906-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161782 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161782 [pool-907-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161816 [pool-907-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161836 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161836 [pool-908-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161871 [pool-908-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161890 [pool-909-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161890 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161919 [pool-909-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161937 [pool-910-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
161944 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161966 [pool-910-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161984 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
161985 [pool-911-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
161985 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
161985 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162007 [pool-911-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162027 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162027 [pool-912-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162028 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162028 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162049 [pool-912-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162070 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162070 [pool-913-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162092 [pool-913-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162118 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162118 [pool-914-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162119 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162119 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162141 [pool-914-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162166 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162166 [pool-915-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162209 [pool-915-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162237 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162237 [pool-916-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162238 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162238 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162275 [pool-916-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162304 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162304 [pool-917-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162305 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162305 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162335 [pool-917-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162348 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
162362 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162362 [pool-918-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162363 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162363 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162394 [pool-918-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162415 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162415 [pool-919-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162415 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162415 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162440 [pool-919-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162463 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162463 [pool-920-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162463 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162463 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162499 [pool-920-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162521 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162521 [pool-921-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162522 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162522 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162547 [pool-921-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162566 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162566 [pool-922-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162567 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162567 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162594 [pool-922-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162612 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162612 [pool-923-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162613 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162613 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162638 [pool-923-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162663 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162663 [pool-924-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162695 [pool-924-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162728 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162728 [pool-925-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162729 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162729 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162756 [pool-925-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162799 [pool-926-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162800 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162800 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162832 [pool-926-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162860 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162860 [pool-927-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162899 [pool-927-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162935 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162935 [pool-928-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162936 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162936 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
162960 [pool-928-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
162982 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
162982 [pool-929-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
162983 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
162983 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163004 [pool-929-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163022 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163022 [pool-930-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163022 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163022 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163044 [pool-930-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163067 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163067 [pool-931-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163068 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163068 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163091 [pool-931-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163115 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163115 [pool-932-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163116 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163116 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163138 [pool-932-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163179 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163179 [pool-933-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163181 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163181 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163204 [pool-933-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163212 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
163229 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163230 [pool-934-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163230 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163230 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163252 [pool-934-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163292 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163292 [pool-935-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163321 [pool-935-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163352 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163352 [pool-936-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163377 [pool-936-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163411 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163411 [pool-937-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163412 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163412 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163437 [pool-937-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163465 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163465 [pool-938-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163466 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163466 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163488 [pool-938-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163514 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163514 [pool-939-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163514 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163515 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163539 [pool-939-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163548 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
163566 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163566 [pool-940-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163567 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163567 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163589 [pool-940-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163613 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163613 [pool-941-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163614 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163614 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163648 [pool-941-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163666 [pool-942-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163693 [pool-942-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163711 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163711 [pool-943-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163712 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163712 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163737 [pool-943-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163756 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163756 [pool-944-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163757 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163757 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163781 [pool-944-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163799 [pool-945-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163800 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163800 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163835 [pool-945-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163854 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163855 [pool-946-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163855 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163855 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163896 [pool-946-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163923 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163923 [pool-947-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163924 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163924 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
163959 [pool-947-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
163990 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
163990 [pool-948-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
163991 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
163991 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164015 [pool-948-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164041 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164041 [pool-949-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164042 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164042 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164064 [pool-949-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164089 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164089 [pool-950-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164090 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164090 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164112 [pool-950-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164135 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164135 [pool-951-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164136 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164136 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164184 [pool-951-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164203 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164203 [pool-952-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164203 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164203 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164241 [pool-952-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164262 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164263 [pool-953-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164294 [pool-953-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164313 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164313 [pool-954-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164314 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164345 [pool-954-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164371 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164371 [pool-955-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164372 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164372 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164405 [pool-955-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164428 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164429 [pool-956-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164429 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164429 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164464 [pool-956-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164474 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
164487 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164487 [pool-957-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164488 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164488 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164517 [pool-957-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164536 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164536 [pool-958-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164537 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164537 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164562 [pool-958-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164580 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164580 [pool-959-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164581 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164581 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164606 [pool-959-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164625 [pool-960-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164625 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164648 [pool-960-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164666 [pool-961-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164694 [pool-961-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164711 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164711 [pool-962-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164712 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164712 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164743 [pool-962-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164760 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164761 [pool-963-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164761 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164761 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164794 [pool-963-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164822 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164822 [pool-964-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164823 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164823 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164859 [pool-964-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164885 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164885 [pool-965-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164917 [pool-965-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164938 [pool-966-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164938 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
164970 [pool-966-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
164990 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
164990 [pool-967-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
164990 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
164990 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165022 [pool-967-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165041 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165041 [pool-968-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165042 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165042 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165080 [pool-968-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165099 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165099 [pool-969-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165100 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165100 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165123 [pool-969-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165143 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165143 [pool-970-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165143 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165143 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165144 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
165176 [pool-970-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165204 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165204 [pool-971-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165234 [pool-971-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165258 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165258 [pool-972-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165259 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165259 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165288 [pool-972-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165311 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165312 [pool-973-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165312 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165312 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165337 [pool-973-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165356 [pool-974-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165357 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165357 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165386 [pool-974-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165406 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165406 [pool-975-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165406 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165406 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165443 [pool-975-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165462 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165462 [pool-976-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165462 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165462 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165483 [pool-976-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165501 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165501 [pool-977-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165502 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165502 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165522 [pool-977-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165540 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165540 [pool-978-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165541 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165541 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165568 [pool-978-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165588 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165589 [pool-979-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165589 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165589 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165621 [pool-979-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165639 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165639 [pool-980-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165640 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165640 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165665 [pool-980-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165684 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165684 [pool-981-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165685 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165685 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165711 [pool-981-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165731 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165731 [pool-982-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165732 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165759 [pool-982-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165785 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165785 [pool-983-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165786 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165786 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165826 [pool-983-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165846 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165847 [pool-984-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165847 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165847 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165888 [pool-984-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165906 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165906 [pool-985-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165907 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165907 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165935 [pool-985-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
165955 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
165955 [pool-986-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
165956 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
165956 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
165983 [pool-986-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166007 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166007 [pool-987-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166008 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166008 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166037 [pool-987-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166057 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166057 [pool-988-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166058 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166058 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166085 [pool-988-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166102 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
166107 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166108 [pool-989-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166108 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166108 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166140 [pool-989-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166166 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166166 [pool-990-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166190 [pool-990-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166211 [pool-991-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166211 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166212 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166239 [pool-991-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166259 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166259 [pool-992-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166295 [pool-992-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166324 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166324 [pool-993-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166325 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166325 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166350 [pool-993-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166378 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166378 [pool-994-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166379 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166379 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166410 [pool-994-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166437 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166437 [pool-995-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166471 [pool-995-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166497 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166497 [pool-996-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166498 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166498 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166525 [pool-996-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166553 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166553 [pool-997-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166554 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166554 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166580 [pool-997-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166605 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166605 [pool-998-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166606 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166606 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166631 [pool-998-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166656 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166656 [pool-999-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166657 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166657 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166690 [pool-999-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166709 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166709 [pool-1000-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166710 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166710 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166741 [pool-1000-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166744 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
166764 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166764 [pool-1001-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166765 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166765 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166790 [pool-1001-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166813 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166813 [pool-1002-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166813 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166813 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166835 [pool-1002-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166855 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166855 [pool-1003-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166879 [pool-1003-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166898 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166898 [pool-1004-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166899 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166922 [pool-1004-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166940 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166940 [pool-1005-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166941 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166941 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
166968 [pool-1005-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
166985 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
166985 [pool-1006-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
166986 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
166986 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167013 [pool-1006-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167031 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167031 [pool-1007-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167032 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167032 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167056 [pool-1007-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167073 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167074 [pool-1008-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167074 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167074 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167099 [pool-1008-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167117 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167117 [pool-1009-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167117 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167117 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167140 [pool-1009-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167160 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167160 [pool-1010-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167160 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167160 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167192 [pool-1010-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167209 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167209 [pool-1011-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167210 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167237 [pool-1011-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167255 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167255 [pool-1012-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167255 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167255 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167280 [pool-1012-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167300 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167300 [pool-1013-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167301 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167301 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167324 [pool-1013-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167349 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167350 [pool-1014-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167382 [pool-1014-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167407 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167407 [pool-1015-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167408 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167408 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167440 [pool-1015-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167459 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167459 [pool-1016-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167459 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167459 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167493 [pool-1016-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167521 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167521 [pool-1017-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167521 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167522 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167555 [pool-1017-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167586 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167586 [pool-1018-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167621 [pool-1018-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167653 [pool-1019-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167653 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167654 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167690 [pool-1019-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167714 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167714 [pool-1020-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167715 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167715 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167745 [pool-1020-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167773 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167773 [pool-1021-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167774 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167774 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167805 [pool-1021-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167827 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167827 [pool-1022-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167856 [pool-1022-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167877 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167877 [pool-1023-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167878 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167878 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167907 [pool-1023-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167929 [pool-1024-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167930 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167930 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
167954 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
167956 [pool-1024-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
167977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
167977 [pool-1025-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
167977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
167978 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168002 [pool-1025-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168025 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168025 [pool-1026-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168026 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168026 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168053 [pool-1026-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168078 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168079 [pool-1027-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168080 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168080 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168115 [pool-1027-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168143 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168144 [pool-1028-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168144 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168144 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168179 [pool-1028-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168212 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168213 [pool-1029-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168214 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168214 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168245 [pool-1029-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168281 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168281 [pool-1030-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168311 [pool-1030-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168355 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168356 [pool-1031-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168377 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
168381 [pool-1031-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168408 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168408 [pool-1032-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168409 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168409 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168443 [pool-1032-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168467 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168467 [pool-1033-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168467 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168468 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168492 [pool-1033-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168512 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168512 [pool-1034-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168512 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168512 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168537 [pool-1034-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168557 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168557 [pool-1035-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168557 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168558 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168591 [pool-1035-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168609 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168610 [pool-1036-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168645 [pool-1036-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168665 [pool-1037-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168666 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168694 [pool-1037-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168714 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168715 [pool-1038-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168715 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168715 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168755 [pool-1038-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168778 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168778 [pool-1039-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168779 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168779 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168807 [pool-1039-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168834 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168834 [pool-1040-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168835 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168835 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168876 [pool-1040-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168908 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168908 [pool-1041-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168909 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168909 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168936 [pool-1041-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
168959 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
168960 [pool-1042-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
168960 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
168960 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
168998 [pool-1042-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169028 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169028 [pool-1043-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169029 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169029 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169058 [pool-1043-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169084 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169084 [pool-1044-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169084 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169084 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169115 [pool-1044-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169136 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169136 [pool-1045-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169137 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169137 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169174 [pool-1045-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169204 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169204 [pool-1046-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169205 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169322 [pool-1046-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169350 [pool-1047-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169351 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169351 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169371 [pool-1047-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169389 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169389 [pool-1048-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169390 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169390 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169419 [pool-1048-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169436 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169436 [pool-1049-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169437 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169437 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169460 [pool-1049-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169478 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169478 [pool-1050-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169479 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169481 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169517 [pool-1050-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169533 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169533 [pool-1051-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169534 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169534 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169563 [pool-1051-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169579 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169579 [pool-1052-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169579 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169579 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169580 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
169607 [pool-1052-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169623 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169623 [pool-1053-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169624 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169624 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169650 [pool-1053-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169665 [pool-1054-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169665 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169684 [pool-1054-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169699 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169699 [pool-1055-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169700 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169700 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169718 [pool-1055-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169733 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169733 [pool-1056-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169734 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169734 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169753 [pool-1056-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169769 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169769 [pool-1057-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169770 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169770 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169790 [pool-1057-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169805 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169806 [pool-1058-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169806 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169806 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169824 [pool-1058-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169840 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169840 [pool-1059-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169840 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169840 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169878 [pool-1059-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169901 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169902 [pool-1060-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169902 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169902 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169922 [pool-1060-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169933 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
169940 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169940 [pool-1061-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169940 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169940 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
169963 [pool-1061-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
169979 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
169979 [pool-1062-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
169980 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
169980 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170001 [pool-1062-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170018 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170018 [pool-1063-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170019 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170019 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170058 [pool-1063-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170074 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170074 [pool-1064-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170075 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170075 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170096 [pool-1064-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170114 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170114 [pool-1065-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170114 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170114 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170136 [pool-1065-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170153 [pool-1066-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170182 [pool-1066-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170199 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170199 [pool-1067-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170199 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170199 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170243 [pool-1067-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170272 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170272 [pool-1068-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170273 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170273 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170307 [pool-1068-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170333 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170333 [pool-1069-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170334 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170334 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170378 [pool-1069-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170402 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170402 [pool-1070-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170402 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170402 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170431 [pool-1070-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170450 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170450 [pool-1071-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170450 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170450 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170475 [pool-1071-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170502 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170503 [pool-1072-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170503 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170503 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170536 [pool-1072-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170563 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170563 [pool-1073-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170564 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170564 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170586 [pool-1073-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170606 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170606 [pool-1074-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170607 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170607 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170641 [pool-1074-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170662 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170662 [pool-1075-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170663 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170663 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170702 [pool-1075-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170738 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170738 [pool-1076-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170739 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170739 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170760 [pool-1076-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170783 [pool-1077-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170784 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170784 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170803 [pool-1077-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170828 [pool-1078-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170851 [pool-1078-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170852 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
170881 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170881 [pool-1079-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170882 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170882 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170906 [pool-1079-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170932 [pool-1080-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
170955 [pool-1080-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
170984 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
170984 [pool-1081-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
170986 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
170986 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171007 [pool-1081-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171036 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171036 [pool-1082-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171058 [pool-1082-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171075 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171075 [pool-1083-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171076 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171076 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171096 [pool-1083-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171115 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171115 [pool-1084-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171116 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171116 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171135 [pool-1084-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171153 [pool-1085-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171174 [pool-1085-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171194 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171194 [pool-1086-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171195 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171195 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171215 [pool-1086-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171234 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171234 [pool-1087-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171235 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171255 [pool-1087-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171273 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171273 [pool-1088-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171274 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171274 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171294 [pool-1088-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171312 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171313 [pool-1089-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171313 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171313 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171334 [pool-1089-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171353 [pool-1090-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171381 [pool-1090-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171409 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171409 [pool-1091-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171410 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171410 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171434 [pool-1091-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171464 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171464 [pool-1092-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171464 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171464 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171519 [pool-1092-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171545 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171545 [pool-1093-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171546 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171546 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171570 [pool-1093-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171596 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171596 [pool-1094-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171597 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171597 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171620 [pool-1094-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171636 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171636 [pool-1095-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171637 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171637 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171656 [pool-1095-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171671 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171671 [pool-1096-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171671 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171671 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171705 [pool-1096-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171723 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171723 [pool-1097-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171724 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171724 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171750 [pool-1097-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171764 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
171770 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171770 [pool-1098-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171771 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171771 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171811 [pool-1098-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171829 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171829 [pool-1099-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171830 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171830 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171856 [pool-1099-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171881 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
171881 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
172563 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
172788 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
172788 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
172944 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
172948 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
172949 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
172949 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
173711 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
173849 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
175334 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
175793 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
176238 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 81
176238 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
176390 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/06340a7d-f1ef-4caa-827c-845704233473_60_001.parquet
176405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/06340a7d-f1ef-4caa-827c-845704233473_60_001.parquet
176405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/06340a7d-f1ef-4caa-827c-845704233473_60_001.parquet => [e32567e8-5be8-45de-b7e8-45c6c3163e33]
176431 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/064f90eb-2275-4f86-bb74-51b8b1cf65b4_110_001.parquet
176447 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/064f90eb-2275-4f86-bb74-51b8b1cf65b4_110_001.parquet
176447 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/064f90eb-2275-4f86-bb74-51b8b1cf65b4_110_001.parquet => [8b4f893b-79ab-47f8-8f99-13ad82b16f18]
176462 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/079ebe9e-89d2-4728-8bb9-54b29f08b150_171_001.parquet
176473 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/079ebe9e-89d2-4728-8bb9-54b29f08b150_171_001.parquet
176473 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/079ebe9e-89d2-4728-8bb9-54b29f08b150_171_001.parquet => [96c2463f-12a2-4396-a063-e9e7a40a8038]
176486 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/0aa59145-ba03-46c4-b195-a76298f7bd6e_49_001.parquet
176494 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/0aa59145-ba03-46c4-b195-a76298f7bd6e_49_001.parquet
176494 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/0aa59145-ba03-46c4-b195-a76298f7bd6e_49_001.parquet => [c07b2e98-c7d9-427d-81f6-92f26a808c03]
176507 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/0c151cbf-838a-4d25-ae6e-8e1c28a9c429_2_001.parquet
176519 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/0c151cbf-838a-4d25-ae6e-8e1c28a9c429_2_001.parquet
176519 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/0c151cbf-838a-4d25-ae6e-8e1c28a9c429_2_001.parquet => [11e701d7-a793-4a60-9431-35bbe9f8095e]
176532 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/11f98c91-700f-407b-aa9e-0f76f817ae12_178_001.parquet
176544 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/11f98c91-700f-407b-aa9e-0f76f817ae12_178_001.parquet
176544 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/11f98c91-700f-407b-aa9e-0f76f817ae12_178_001.parquet => [aa9823bf-a7e9-4692-8a9f-a4ba6ec11401]
176558 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/129482ae-c91f-4aef-9d18-d448b7db9990_112_001.parquet
176569 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/129482ae-c91f-4aef-9d18-d448b7db9990_112_001.parquet
176569 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/129482ae-c91f-4aef-9d18-d448b7db9990_112_001.parquet => [94f55ec1-dd6c-499e-9d0f-8ecf1590fc80]
176582 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/1ec64489-6a4f-413c-ba65-7207a229f43e_93_001.parquet
176590 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/1ec64489-6a4f-413c-ba65-7207a229f43e_93_001.parquet
176590 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/1ec64489-6a4f-413c-ba65-7207a229f43e_93_001.parquet => [5280228d-5b61-4908-986f-238461305f9a]
176604 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/1ed64d42-3dd0-45dd-ae0b-51f4baf43e57_199_001.parquet
176613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/1ed64d42-3dd0-45dd-ae0b-51f4baf43e57_199_001.parquet
176613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/1ed64d42-3dd0-45dd-ae0b-51f4baf43e57_199_001.parquet => [f6540fa3-9386-4ca5-a3b0-56b4aec8e953]
176627 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/215f9697-7a02-45b0-8918-01e29099aa1f_142_001.parquet
176642 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/215f9697-7a02-45b0-8918-01e29099aa1f_142_001.parquet
176642 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/215f9697-7a02-45b0-8918-01e29099aa1f_142_001.parquet => [1fd7172d-7176-4e8e-9a50-01b3f309aed0]
176656 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/26707c76-bfa8-4c8c-ad46-5b02c14c3478_138_001.parquet
176668 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/26707c76-bfa8-4c8c-ad46-5b02c14c3478_138_001.parquet
176668 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/26707c76-bfa8-4c8c-ad46-5b02c14c3478_138_001.parquet => [0c41de2b-c3eb-4bc7-b6e6-ec342f0a5a7d]
176681 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/27903732-e7f3-4304-8f6a-135167dfbb68_154_001.parquet
176692 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/27903732-e7f3-4304-8f6a-135167dfbb68_154_001.parquet
176692 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/27903732-e7f3-4304-8f6a-135167dfbb68_154_001.parquet => [51a091c1-c8e4-4556-bccb-4ce61d7bc365]
176706 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/28201c58-be55-414d-b3d3-bd02833ea414_103_001.parquet
176717 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/28201c58-be55-414d-b3d3-bd02833ea414_103_001.parquet
176717 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/28201c58-be55-414d-b3d3-bd02833ea414_103_001.parquet => [74625e79-4cce-46ec-b8de-2eb992ae0b9f]
176731 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/2839957c-74a2-4ebd-b7e6-22f42a0ff600_164_001.parquet
176740 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/2839957c-74a2-4ebd-b7e6-22f42a0ff600_164_001.parquet
176740 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/2839957c-74a2-4ebd-b7e6-22f42a0ff600_164_001.parquet => [8803b205-01ba-4203-ae20-13aceb711f91]
176754 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/299341ea-d1c4-482b-a6bf-0fea7df399cf_74_001.parquet
176767 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/299341ea-d1c4-482b-a6bf-0fea7df399cf_74_001.parquet
176767 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/299341ea-d1c4-482b-a6bf-0fea7df399cf_74_001.parquet => [0a588c86-30bb-498e-9a73-e83f15376299]
176780 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/2c017570-77d3-4114-8d75-392f364a99b3_162_001.parquet
176791 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/2c017570-77d3-4114-8d75-392f364a99b3_162_001.parquet
176791 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/2c017570-77d3-4114-8d75-392f364a99b3_162_001.parquet => [80be3b6f-6639-4a84-918f-0a8ac121e01e]
176816 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/30477b8a-8d01-403b-bf6a-783468c54365_87_001.parquet
176831 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/30477b8a-8d01-403b-bf6a-783468c54365_87_001.parquet
176831 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/30477b8a-8d01-403b-bf6a-783468c54365_87_001.parquet => [3fe662da-b967-432a-ab17-95c007f9ab39]
176852 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/31288041-11d1-4333-99a1-670e80ed3b94_5_001.parquet
176867 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/31288041-11d1-4333-99a1-670e80ed3b94_5_001.parquet
176867 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/31288041-11d1-4333-99a1-670e80ed3b94_5_001.parquet => [21e96265-961b-4147-adb5-549b2f667ef1]
176885 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/33856898-0915-4e3e-89f8-329c8a6ed637_36_001.parquet
176899 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/33856898-0915-4e3e-89f8-329c8a6ed637_36_001.parquet
176899 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/33856898-0915-4e3e-89f8-329c8a6ed637_36_001.parquet => [982a7a83-1a5f-476a-aa23-68dfdd01ccf1]
176915 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/3748be5e-9ec8-48f7-8c0c-02232990d5b5_67_001.parquet
176928 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/3748be5e-9ec8-48f7-8c0c-02232990d5b5_67_001.parquet
176928 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/3748be5e-9ec8-48f7-8c0c-02232990d5b5_67_001.parquet => [fc4a303e-32d3-425c-b4a8-a96050948a14]
176944 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/3d706d21-029c-4091-b6fe-f555a543de4f_165_001.parquet
176959 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/3d706d21-029c-4091-b6fe-f555a543de4f_165_001.parquet
176959 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/3d706d21-029c-4091-b6fe-f555a543de4f_165_001.parquet => [884fd8e8-661f-41e8-bb52-946bf38c0b97]
176976 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/4450fb74-241d-4a67-b56e-0b2b1f5deed4_191_001.parquet
176990 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/4450fb74-241d-4a67-b56e-0b2b1f5deed4_191_001.parquet
176990 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/4450fb74-241d-4a67-b56e-0b2b1f5deed4_191_001.parquet => [dcba5b1b-66ce-4120-ba87-042c0fb5354b]
177014 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/453a4ad3-47de-41e9-9e4f-bf08b3db3f73_95_001.parquet
177025 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/453a4ad3-47de-41e9-9e4f-bf08b3db3f73_95_001.parquet
177025 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/453a4ad3-47de-41e9-9e4f-bf08b3db3f73_95_001.parquet => [58753a2e-787b-4c28-a631-dca24dabd768]
177044 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/4dab0d6e-7736-4a49-9c43-b327bf1b68dc_50_001.parquet
177056 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/4dab0d6e-7736-4a49-9c43-b327bf1b68dc_50_001.parquet
177056 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/4dab0d6e-7736-4a49-9c43-b327bf1b68dc_50_001.parquet => [c40fae69-d2a0-4540-b83e-c75cfe8233b7]
177073 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/4e198182-1402-4827-a7ca-5a6da9220df1_46_001.parquet
177083 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/4e198182-1402-4827-a7ca-5a6da9220df1_46_001.parquet
177083 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/4e198182-1402-4827-a7ca-5a6da9220df1_46_001.parquet => [b5595e49-fbf7-47ae-9485-e5cde6bf46ff]
177099 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/52b7a2f2-e791-437d-8fdb-53716a1ff226_20_001.parquet
177109 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/52b7a2f2-e791-437d-8fdb-53716a1ff226_20_001.parquet
177109 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/52b7a2f2-e791-437d-8fdb-53716a1ff226_20_001.parquet => [599f0b83-2c04-4382-8cc5-261296777329]
177124 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/53827ec1-bc5c-49dd-b853-047f318d19cb_107_001.parquet
177132 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
177133 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/53827ec1-bc5c-49dd-b853-047f318d19cb_107_001.parquet
177133 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/53827ec1-bc5c-49dd-b853-047f318d19cb_107_001.parquet => [7fc4f646-5d1e-47af-b03a-2d9799a1e640]
177148 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/5d4cc127-d79e-44cb-a6fc-32c6a1273008_78_001.parquet
177156 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/5d4cc127-d79e-44cb-a6fc-32c6a1273008_78_001.parquet
177156 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/5d4cc127-d79e-44cb-a6fc-32c6a1273008_78_001.parquet => [14b336f1-65b4-479d-a038-16b60c05f00f]
177170 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/6428d374-052b-412b-8a76-bdb98d9137ee_13_001.parquet
177178 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/6428d374-052b-412b-8a76-bdb98d9137ee_13_001.parquet
177178 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/6428d374-052b-412b-8a76-bdb98d9137ee_13_001.parquet => [3bfbac25-e2ab-40a4-9582-9a31e7b419a2]
177192 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/6798e9c6-3228-44b1-bf34-695c0e8b9ab9_155_001.parquet
177200 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/6798e9c6-3228-44b1-bf34-695c0e8b9ab9_155_001.parquet
177200 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/6798e9c6-3228-44b1-bf34-695c0e8b9ab9_155_001.parquet => [53de1666-6c03-489a-8be3-89f33386fb96]
177216 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/6883cfab-7e96-4503-bf7e-9ac04161ddec_123_001.parquet
177226 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/6883cfab-7e96-4503-bf7e-9ac04161ddec_123_001.parquet
177226 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/6883cfab-7e96-4503-bf7e-9ac04161ddec_123_001.parquet => [d56469a8-328f-40c4-ab87-752a36cd4765]
177239 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/6c29c00b-2847-4b6b-888d-a7a1c6e509ce_140_001.parquet
177248 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/6c29c00b-2847-4b6b-888d-a7a1c6e509ce_140_001.parquet
177248 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/6c29c00b-2847-4b6b-888d-a7a1c6e509ce_140_001.parquet => [1d004a38-272f-4dec-bda4-1fc9db29b0bf]
177256 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
177262 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/6f0cc61e-c01d-44b6-9cd7-552aff50641d_19_001.parquet
177271 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/6f0cc61e-c01d-44b6-9cd7-552aff50641d_19_001.parquet
177271 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/6f0cc61e-c01d-44b6-9cd7-552aff50641d_19_001.parquet => [580392f1-e2f4-4e92-8e85-091ad9db9e79]
177285 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/712ad067-209b-4f58-8ce0-6e37e18b5836_97_001.parquet
177293 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/712ad067-209b-4f58-8ce0-6e37e18b5836_97_001.parquet
177293 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/712ad067-209b-4f58-8ce0-6e37e18b5836_97_001.parquet => [5d6c56c5-4450-44db-ad79-a7d597bd6794]
177307 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/746ce790-2e38-4071-bc45-5e013573869e_132_001.parquet
177316 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/746ce790-2e38-4071-bc45-5e013573869e_132_001.parquet
177316 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/746ce790-2e38-4071-bc45-5e013573869e_132_001.parquet => [f60e9f44-5214-458c-951f-4b4ba96f7c4c]
177329 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/7478fc5b-c421-4036-b57a-81b5d366c0bb_30_001.parquet
177338 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/7478fc5b-c421-4036-b57a-81b5d366c0bb_30_001.parquet
177338 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/7478fc5b-c421-4036-b57a-81b5d366c0bb_30_001.parquet => [758ddfa0-e3ce-4b1c-8fd7-b16d56037499]
177352 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/76c89ca0-2706-4c10-bf69-03449baf1171_151_001.parquet
177360 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/76c89ca0-2706-4c10-bf69-03449baf1171_151_001.parquet
177360 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/76c89ca0-2706-4c10-bf69-03449baf1171_151_001.parquet => [34493ef7-30aa-48d6-a0b2-afa3187fbf87]
177374 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/79d07ade-f3e4-45ed-b21f-039390fd2e71_136_001.parquet
177383 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/79d07ade-f3e4-45ed-b21f-039390fd2e71_136_001.parquet
177383 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/79d07ade-f3e4-45ed-b21f-039390fd2e71_136_001.parquet => [06d014e5-37ff-4fa9-b929-648967920a1e]
177397 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/7cea76f5-f37b-49e8-a7e0-f08f13a1cc98_109_001.parquet
177406 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/7cea76f5-f37b-49e8-a7e0-f08f13a1cc98_109_001.parquet
177406 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/7cea76f5-f37b-49e8-a7e0-f08f13a1cc98_109_001.parquet => [898830a1-7a4c-4a26-914a-a15dc75bc45a]
177420 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/7d751220-107a-47c6-aeaa-31cf9bff69de_83_001.parquet
177428 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/7d751220-107a-47c6-aeaa-31cf9bff69de_83_001.parquet
177428 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/7d751220-107a-47c6-aeaa-31cf9bff69de_83_001.parquet => [31e9be44-557c-491e-b97e-099cc187586e]
177442 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/824b0fac-25bf-499c-8c6c-a3623ba470a7_1_001.parquet
177450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/824b0fac-25bf-499c-8c6c-a3623ba470a7_1_001.parquet
177450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/824b0fac-25bf-499c-8c6c-a3623ba470a7_1_001.parquet => [0c0fc20c-746e-4222-a02e-615eed5aae56]
177481 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/864a26e9-0cd7-4673-8027-c7a3bc82de7f_11_001.parquet
177491 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/864a26e9-0cd7-4673-8027-c7a3bc82de7f_11_001.parquet
177491 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/864a26e9-0cd7-4673-8027-c7a3bc82de7f_11_001.parquet => [39ae1c86-0946-445e-9a68-24ad9f565db0]
177507 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/875def08-bc72-4835-8ba8-da571b82d896_54_001.parquet
177517 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/875def08-bc72-4835-8ba8-da571b82d896_54_001.parquet
177517 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/875def08-bc72-4835-8ba8-da571b82d896_54_001.parquet => [cd958ee2-edc6-4003-8c2e-2ceefa0f227f]
177531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/8b8b051e-e2a4-4f88-8b7c-1d5095f3219c_85_001.parquet
177541 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/8b8b051e-e2a4-4f88-8b7c-1d5095f3219c_85_001.parquet
177541 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/8b8b051e-e2a4-4f88-8b7c-1d5095f3219c_85_001.parquet => [36ca5ca5-636d-46ce-b553-b542dc1405f5]
177556 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/8bc7f97a-1f8c-4c79-9578-f328b9d4ce4f_31_001.parquet
177566 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/8bc7f97a-1f8c-4c79-9578-f328b9d4ce4f_31_001.parquet
177566 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/8bc7f97a-1f8c-4c79-9578-f328b9d4ce4f_31_001.parquet => [81e24e7c-ad01-494e-b6fe-c07fb22e718a]
177581 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/92ff01fc-d3b5-42d9-833d-01cc88b6fadd_179_001.parquet
177590 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/92ff01fc-d3b5-42d9-833d-01cc88b6fadd_179_001.parquet
177590 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/92ff01fc-d3b5-42d9-833d-01cc88b6fadd_179_001.parquet => [b17abfa2-a8a0-428d-aacc-0cd69d832031]
177604 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/959f1aa9-2f08-488d-9050-9bc3d04ee49c_197_001.parquet
177613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/959f1aa9-2f08-488d-9050-9bc3d04ee49c_197_001.parquet
177613 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/959f1aa9-2f08-488d-9050-9bc3d04ee49c_197_001.parquet => [e7302683-7f2c-4ecb-98c9-2edece18d9fb]
177627 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/95c0edb8-d25c-49d4-99f2-caa701728e1b_6_001.parquet
177637 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/95c0edb8-d25c-49d4-99f2-caa701728e1b_6_001.parquet
177637 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/95c0edb8-d25c-49d4-99f2-caa701728e1b_6_001.parquet => [330b35b5-652f-4432-9641-817b6a5c3913]
177651 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/96cb81db-5ccd-4a23-8ed1-0a913d902e0c_66_001.parquet
177660 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/96cb81db-5ccd-4a23-8ed1-0a913d902e0c_66_001.parquet
177660 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/96cb81db-5ccd-4a23-8ed1-0a913d902e0c_66_001.parquet => [fc0f15b2-d685-4e67-bc8d-cdefafb231d6]
177674 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/9ad6b0eb-7ab9-41da-ae25-5c5c010f85a0_44_001.parquet
177683 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/9ad6b0eb-7ab9-41da-ae25-5c5c010f85a0_44_001.parquet
177683 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/9ad6b0eb-7ab9-41da-ae25-5c5c010f85a0_44_001.parquet => [b230c273-b47b-4507-97b5-fc26948c6c7c]
177699 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/9bed3a74-4a78-4ae7-a09c-3c4dee0f8e1d_72_001.parquet
177709 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/9bed3a74-4a78-4ae7-a09c-3c4dee0f8e1d_72_001.parquet
177709 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/9bed3a74-4a78-4ae7-a09c-3c4dee0f8e1d_72_001.parquet => [07da88a0-4c4e-4d29-b4c8-7da1dcab3fb8]
177723 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/a2052565-e54d-4509-9e6f-91039d1a787b_69_001.parquet
177732 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/a2052565-e54d-4509-9e6f-91039d1a787b_69_001.parquet
177732 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/a2052565-e54d-4509-9e6f-91039d1a787b_69_001.parquet => [024832ab-6b09-4d17-ae2f-46982ac1480c]
177747 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/a26edf2c-635a-4dd5-8c52-e5fae42f7250_182_001.parquet
177757 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/a26edf2c-635a-4dd5-8c52-e5fae42f7250_182_001.parquet
177757 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/a26edf2c-635a-4dd5-8c52-e5fae42f7250_182_001.parquet => [b768268a-e489-492a-b839-228c16de67e7]
177772 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/a4be6e9e-8785-4e17-8ff8-b16c23e1b9e0_51_001.parquet
177781 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/a4be6e9e-8785-4e17-8ff8-b16c23e1b9e0_51_001.parquet
177781 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/a4be6e9e-8785-4e17-8ff8-b16c23e1b9e0_51_001.parquet => [c8381528-a7fd-4e3d-a0bf-0bd99cfe570c]
177795 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/a4e8049e-5943-4eb2-8323-366f89b77d5f_119_001.parquet
177803 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/a4e8049e-5943-4eb2-8323-366f89b77d5f_119_001.parquet
177803 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/a4e8049e-5943-4eb2-8323-366f89b77d5f_119_001.parquet => [bd7674cd-c375-415e-9556-d6965c8c3ffd]
177817 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/a5f3b388-ad6d-4fde-9322-6e0f78a031d8_188_001.parquet
177825 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/a5f3b388-ad6d-4fde-9322-6e0f78a031d8_188_001.parquet
177825 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/a5f3b388-ad6d-4fde-9322-6e0f78a031d8_188_001.parquet => [c6ac0efc-6d62-4bd7-b571-b124fb6514fc]
177839 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/b23b6134-e77d-42c5-85ca-b259be7d5fd6_174_001.parquet
177847 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/b23b6134-e77d-42c5-85ca-b259be7d5fd6_174_001.parquet
177848 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/b23b6134-e77d-42c5-85ca-b259be7d5fd6_174_001.parquet => [a1ddd28f-a8a1-449d-8ebe-9b39ef6a5cd1]
177861 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/b2a104c2-cbf2-4bbe-899b-3f113dd81e8c_118_001.parquet
177870 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/b2a104c2-cbf2-4bbe-899b-3f113dd81e8c_118_001.parquet
177870 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/b2a104c2-cbf2-4bbe-899b-3f113dd81e8c_118_001.parquet => [bc2c29bb-ac58-4acd-a4ce-0ab874721aa5]
177886 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/b9110126-75bc-4759-b378-310ded41b684_192_001.parquet
177898 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/b9110126-75bc-4759-b378-310ded41b684_192_001.parquet
177898 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/b9110126-75bc-4759-b378-310ded41b684_192_001.parquet => [dcee785e-5244-44fa-9ae3-82baff4d0bd6]
177912 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/c4cf608b-096a-44b9-8fd1-0661672e8831_127_001.parquet
177922 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/c4cf608b-096a-44b9-8fd1-0661672e8831_127_001.parquet
177922 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/c4cf608b-096a-44b9-8fd1-0661672e8831_127_001.parquet => [dee49897-9a96-4162-9984-b927fc0b6b53]
177936 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/c67a202e-39bb-4eb0-b01e-d131b9c9021c_68_001.parquet
177946 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/c67a202e-39bb-4eb0-b01e-d131b9c9021c_68_001.parquet
177946 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/c67a202e-39bb-4eb0-b01e-d131b9c9021c_68_001.parquet => [01b34784-4a98-4419-ae3a-aba7c8aec39b]
177960 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/cb24921a-900a-4d37-bca9-a6aa0ed1ee84_108_001.parquet
177970 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/cb24921a-900a-4d37-bca9-a6aa0ed1ee84_108_001.parquet
177970 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/cb24921a-900a-4d37-bca9-a6aa0ed1ee84_108_001.parquet => [888d996c-2530-4a59-8469-67c870724271]
177984 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/cb797359-6bb6-45a0-8ea7-8a36ac7db10f_77_001.parquet
177993 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/cb797359-6bb6-45a0-8ea7-8a36ac7db10f_77_001.parquet
177993 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/cb797359-6bb6-45a0-8ea7-8a36ac7db10f_77_001.parquet => [10315230-e4fb-4460-9c79-34bae3f696e0]
178007 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/ce4f1e53-640f-4102-b40b-0a06b7e2ea0f_61_001.parquet
178017 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/ce4f1e53-640f-4102-b40b-0a06b7e2ea0f_61_001.parquet
178017 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/ce4f1e53-640f-4102-b40b-0a06b7e2ea0f_61_001.parquet => [e62eb1fe-752b-49ff-b23e-9085f03bc732]
178031 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/d46ac3d5-4419-4bb8-8a10-5c33b004c4f4_186_001.parquet
178045 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/d46ac3d5-4419-4bb8-8a10-5c33b004c4f4_186_001.parquet
178045 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/d46ac3d5-4419-4bb8-8a10-5c33b004c4f4_186_001.parquet => [c2cb56a0-9efa-45a8-88e2-96116235f6f9]
178059 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/dadb6852-079a-434d-8a58-bf9f34f6ce10_111_001.parquet
178068 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/dadb6852-079a-434d-8a58-bf9f34f6ce10_111_001.parquet
178068 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/dadb6852-079a-434d-8a58-bf9f34f6ce10_111_001.parquet => [91f28c91-30f8-480a-a7fb-c284041d75e4]
178083 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/dd6b8f5b-151c-485b-bb58-cc9d91bb3f39_172_001.parquet
178092 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/dd6b8f5b-151c-485b-bb58-cc9d91bb3f39_172_001.parquet
178092 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/dd6b8f5b-151c-485b-bb58-cc9d91bb3f39_172_001.parquet => [9dcc9d90-90fc-4a6f-b227-c7a8ba213160]
178107 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/de415c2d-a14e-4298-8126-d3be6b3a1a49_75_001.parquet
178116 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/de415c2d-a14e-4298-8126-d3be6b3a1a49_75_001.parquet
178116 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/de415c2d-a14e-4298-8126-d3be6b3a1a49_75_001.parquet => [0da2e8c4-f6d5-4f74-a71f-936c21ab7bb4]
178131 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/df7cba1c-02eb-41ff-a823-e238659e8872_26_001.parquet
178140 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/df7cba1c-02eb-41ff-a823-e238659e8872_26_001.parquet
178140 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/df7cba1c-02eb-41ff-a823-e238659e8872_26_001.parquet => [69e09c27-8a3b-45f5-8dc2-26494c4440c9]
178155 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/e15cc65e-d82f-431d-858a-7ea431e3e42b_173_001.parquet
178164 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/e15cc65e-d82f-431d-858a-7ea431e3e42b_173_001.parquet
178164 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/e15cc65e-d82f-431d-858a-7ea431e3e42b_173_001.parquet => [9eb8fffe-5a03-4b9f-a35d-5da68b63c350]
178179 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/e29fe604-6150-4f86-b511-587b8d405452_56_001.parquet
178188 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/e29fe604-6150-4f86-b511-587b8d405452_56_001.parquet
178188 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/e29fe604-6150-4f86-b511-587b8d405452_56_001.parquet => [d1f69003-e7a7-4333-8c20-7c94075a1ecf]
178202 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/e5d62300-d1b1-4e3f-b00b-1ef58bb25c05_27_001.parquet
178214 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/e5d62300-d1b1-4e3f-b00b-1ef58bb25c05_27_001.parquet
178214 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/e5d62300-d1b1-4e3f-b00b-1ef58bb25c05_27_001.parquet => [6bc8c90c-10b2-47a0-b3e9-fc638570ceec]
178228 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/e772e016-1835-4a19-bfc1-e7796bfce165_9_001.parquet
178237 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/e772e016-1835-4a19-bfc1-e7796bfce165_9_001.parquet
178237 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/e772e016-1835-4a19-bfc1-e7796bfce165_9_001.parquet => [37c72752-ae4d-4c12-82a9-59ca2d60ae53]
178252 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/ea21b6bb-545e-4f0d-a99e-599592f0101a_48_001.parquet
178261 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/ea21b6bb-545e-4f0d-a99e-599592f0101a_48_001.parquet
178262 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/ea21b6bb-545e-4f0d-a99e-599592f0101a_48_001.parquet => [bdf2f1e1-855e-4ab9-ae56-c1d05d265f5f]
178276 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/ebade5d1-bb9d-46ba-8893-d70eefe0dd6f_195_001.parquet
178286 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/ebade5d1-bb9d-46ba-8893-d70eefe0dd6f_195_001.parquet
178286 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/ebade5d1-bb9d-46ba-8893-d70eefe0dd6f_195_001.parquet => [e3b64673-1a81-4599-815c-ce478fe6ec3d]
178300 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/ec9ef00a-74a2-448e-acaa-6de27db1e1e2_117_001.parquet
178310 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/ec9ef00a-74a2-448e-acaa-6de27db1e1e2_117_001.parquet
178310 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/ec9ef00a-74a2-448e-acaa-6de27db1e1e2_117_001.parquet => [af2eeb8a-8bb3-449f-9e3a-f62a84919b95]
178325 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/ecabbba7-2a3c-4d30-90af-0f19f273f249_163_001.parquet
178334 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/ecabbba7-2a3c-4d30-90af-0f19f273f249_163_001.parquet
178334 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/ecabbba7-2a3c-4d30-90af-0f19f273f249_163_001.parquet => [87cd1ee7-e41b-44a3-9569-cc2335b65bb2]
178348 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2016/03/15/eef0dd26-13d7-4d8a-af9a-19c0cb8e8c43_169_001.parquet
178357 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2016/03/15/eef0dd26-13d7-4d8a-af9a-19c0cb8e8c43_169_001.parquet
178357 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2016/03/15/eef0dd26-13d7-4d8a-af9a-19c0cb8e8c43_169_001.parquet => [9054d0c6-954a-407d-876f-cfd2b64690ae]
178371 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/ef55df4a-f6f4-45de-9365-25578872c6a8_14_001.parquet
178380 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/ef55df4a-f6f4-45de-9365-25578872c6a8_14_001.parquet
178380 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/ef55df4a-f6f4-45de-9365-25578872c6a8_14_001.parquet => [46fde01e-9ace-4e21-b3be-c06ace5f8553]
178396 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/17/f5bcaaa8-5148-4d63-ba42-31c518ba5d9f_90_001.parquet
178405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/17/f5bcaaa8-5148-4d63-ba42-31c518ba5d9f_90_001.parquet
178405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/17/f5bcaaa8-5148-4d63-ba42-31c518ba5d9f_90_001.parquet => [4baaf6b1-c539-4641-9ceb-63461df5ccff]
178419 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit132203066327801689/2015/03/16/f813902e-effa-42b5-8296-5bf724805717_3_001.parquet
178430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit132203066327801689/2015/03/16/f813902e-effa-42b5-8296-5bf724805717_3_001.parquet
178430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit132203066327801689/2015/03/16/f813902e-effa-42b5-8296-5bf724805717_3_001.parquet => [13e055ea-455a-4cd2-9b99-0d1b7e3f4c56]
178516 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=81}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=26}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=27}}}
178543 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 435021
178544 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :81, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=e15cc65e-d82f-431d-858a-7ea431e3e42b}, 1=BucketInfo {bucketType=UPDATE, fileLoc=e772e016-1835-4a19-bfc1-e7796bfce165}, 2=BucketInfo {bucketType=UPDATE, fileLoc=53827ec1-bc5c-49dd-b853-047f318d19cb}, 3=BucketInfo {bucketType=UPDATE, fileLoc=d46ac3d5-4419-4bb8-8a10-5c33b004c4f4}, 4=BucketInfo {bucketType=UPDATE, fileLoc=ea21b6bb-545e-4f0d-a99e-599592f0101a}, 5=BucketInfo {bucketType=UPDATE, fileLoc=215f9697-7a02-45b0-8918-01e29099aa1f}, 6=BucketInfo {bucketType=UPDATE, fileLoc=76c89ca0-2706-4c10-bf69-03449baf1171}, 7=BucketInfo {bucketType=UPDATE, fileLoc=0c151cbf-838a-4d25-ae6e-8e1c28a9c429}, 8=BucketInfo {bucketType=UPDATE, fileLoc=7478fc5b-c421-4036-b57a-81b5d366c0bb}, 9=BucketInfo {bucketType=UPDATE, fileLoc=30477b8a-8d01-403b-bf6a-783468c54365}, 10=BucketInfo {bucketType=UPDATE, fileLoc=453a4ad3-47de-41e9-9e4f-bf08b3db3f73}, 11=BucketInfo {bucketType=UPDATE, fileLoc=ef55df4a-f6f4-45de-9365-25578872c6a8}, 12=BucketInfo {bucketType=UPDATE, fileLoc=cb797359-6bb6-45a0-8ea7-8a36ac7db10f}, 13=BucketInfo {bucketType=UPDATE, fileLoc=06340a7d-f1ef-4caa-827c-845704233473}, 14=BucketInfo {bucketType=UPDATE, fileLoc=79d07ade-f3e4-45ed-b21f-039390fd2e71}, 15=BucketInfo {bucketType=UPDATE, fileLoc=079ebe9e-89d2-4728-8bb9-54b29f08b150}, 16=BucketInfo {bucketType=UPDATE, fileLoc=ce4f1e53-640f-4102-b40b-0a06b7e2ea0f}, 17=BucketInfo {bucketType=UPDATE, fileLoc=6798e9c6-3228-44b1-bf34-695c0e8b9ab9}, 18=BucketInfo {bucketType=UPDATE, fileLoc=4dab0d6e-7736-4a49-9c43-b327bf1b68dc}, 19=BucketInfo {bucketType=UPDATE, fileLoc=3748be5e-9ec8-48f7-8c0c-02232990d5b5}, 20=BucketInfo {bucketType=UPDATE, fileLoc=b9110126-75bc-4759-b378-310ded41b684}, 21=BucketInfo {bucketType=UPDATE, fileLoc=cb24921a-900a-4d37-bca9-a6aa0ed1ee84}, 22=BucketInfo {bucketType=UPDATE, fileLoc=6428d374-052b-412b-8a76-bdb98d9137ee}, 23=BucketInfo {bucketType=UPDATE, fileLoc=746ce790-2e38-4071-bc45-5e013573869e}, 24=BucketInfo {bucketType=UPDATE, fileLoc=ecabbba7-2a3c-4d30-90af-0f19f273f249}, 25=BucketInfo {bucketType=UPDATE, fileLoc=92ff01fc-d3b5-42d9-833d-01cc88b6fadd}, 26=BucketInfo {bucketType=UPDATE, fileLoc=c4cf608b-096a-44b9-8fd1-0661672e8831}, 27=BucketInfo {bucketType=UPDATE, fileLoc=4e198182-1402-4827-a7ca-5a6da9220df1}, 28=BucketInfo {bucketType=UPDATE, fileLoc=4450fb74-241d-4a67-b56e-0b2b1f5deed4}, 29=BucketInfo {bucketType=UPDATE, fileLoc=b23b6134-e77d-42c5-85ca-b259be7d5fd6}, 30=BucketInfo {bucketType=UPDATE, fileLoc=52b7a2f2-e791-437d-8fdb-53716a1ff226}, 31=BucketInfo {bucketType=UPDATE, fileLoc=1ed64d42-3dd0-45dd-ae0b-51f4baf43e57}, 32=BucketInfo {bucketType=UPDATE, fileLoc=dd6b8f5b-151c-485b-bb58-cc9d91bb3f39}, 33=BucketInfo {bucketType=UPDATE, fileLoc=2c017570-77d3-4114-8d75-392f364a99b3}, 34=BucketInfo {bucketType=UPDATE, fileLoc=95c0edb8-d25c-49d4-99f2-caa701728e1b}, 35=BucketInfo {bucketType=UPDATE, fileLoc=26707c76-bfa8-4c8c-ad46-5b02c14c3478}, 36=BucketInfo {bucketType=UPDATE, fileLoc=ec9ef00a-74a2-448e-acaa-6de27db1e1e2}, 37=BucketInfo {bucketType=UPDATE, fileLoc=33856898-0915-4e3e-89f8-329c8a6ed637}, 38=BucketInfo {bucketType=UPDATE, fileLoc=b2a104c2-cbf2-4bbe-899b-3f113dd81e8c}, 39=BucketInfo {bucketType=UPDATE, fileLoc=a4be6e9e-8785-4e17-8ff8-b16c23e1b9e0}, 40=BucketInfo {bucketType=UPDATE, fileLoc=959f1aa9-2f08-488d-9050-9bc3d04ee49c}, 41=BucketInfo {bucketType=UPDATE, fileLoc=9bed3a74-4a78-4ae7-a09c-3c4dee0f8e1d}, 42=BucketInfo {bucketType=UPDATE, fileLoc=6c29c00b-2847-4b6b-888d-a7a1c6e509ce}, 43=BucketInfo {bucketType=UPDATE, fileLoc=6f0cc61e-c01d-44b6-9cd7-552aff50641d}, 44=BucketInfo {bucketType=UPDATE, fileLoc=864a26e9-0cd7-4673-8027-c7a3bc82de7f}, 45=BucketInfo {bucketType=UPDATE, fileLoc=31288041-11d1-4333-99a1-670e80ed3b94}, 46=BucketInfo {bucketType=UPDATE, fileLoc=ebade5d1-bb9d-46ba-8893-d70eefe0dd6f}, 47=BucketInfo {bucketType=UPDATE, fileLoc=7cea76f5-f37b-49e8-a7e0-f08f13a1cc98}, 48=BucketInfo {bucketType=UPDATE, fileLoc=11f98c91-700f-407b-aa9e-0f76f817ae12}, 49=BucketInfo {bucketType=UPDATE, fileLoc=7d751220-107a-47c6-aeaa-31cf9bff69de}, 50=BucketInfo {bucketType=UPDATE, fileLoc=a26edf2c-635a-4dd5-8c52-e5fae42f7250}, 51=BucketInfo {bucketType=UPDATE, fileLoc=1ec64489-6a4f-413c-ba65-7207a229f43e}, 52=BucketInfo {bucketType=UPDATE, fileLoc=a2052565-e54d-4509-9e6f-91039d1a787b}, 53=BucketInfo {bucketType=UPDATE, fileLoc=a4e8049e-5943-4eb2-8323-366f89b77d5f}, 54=BucketInfo {bucketType=UPDATE, fileLoc=8b8b051e-e2a4-4f88-8b7c-1d5095f3219c}, 55=BucketInfo {bucketType=UPDATE, fileLoc=824b0fac-25bf-499c-8c6c-a3623ba470a7}, 56=BucketInfo {bucketType=UPDATE, fileLoc=a5f3b388-ad6d-4fde-9322-6e0f78a031d8}, 57=BucketInfo {bucketType=UPDATE, fileLoc=6883cfab-7e96-4503-bf7e-9ac04161ddec}, 58=BucketInfo {bucketType=UPDATE, fileLoc=9ad6b0eb-7ab9-41da-ae25-5c5c010f85a0}, 59=BucketInfo {bucketType=UPDATE, fileLoc=0aa59145-ba03-46c4-b195-a76298f7bd6e}, 60=BucketInfo {bucketType=UPDATE, fileLoc=129482ae-c91f-4aef-9d18-d448b7db9990}, 61=BucketInfo {bucketType=UPDATE, fileLoc=eef0dd26-13d7-4d8a-af9a-19c0cb8e8c43}, 62=BucketInfo {bucketType=UPDATE, fileLoc=875def08-bc72-4835-8ba8-da571b82d896}, 63=BucketInfo {bucketType=UPDATE, fileLoc=712ad067-209b-4f58-8ce0-6e37e18b5836}, 64=BucketInfo {bucketType=UPDATE, fileLoc=e5d62300-d1b1-4e3f-b00b-1ef58bb25c05}, 65=BucketInfo {bucketType=UPDATE, fileLoc=de415c2d-a14e-4298-8126-d3be6b3a1a49}, 66=BucketInfo {bucketType=UPDATE, fileLoc=299341ea-d1c4-482b-a6bf-0fea7df399cf}, 67=BucketInfo {bucketType=UPDATE, fileLoc=f813902e-effa-42b5-8296-5bf724805717}, 68=BucketInfo {bucketType=UPDATE, fileLoc=f5bcaaa8-5148-4d63-ba42-31c518ba5d9f}, 69=BucketInfo {bucketType=UPDATE, fileLoc=c67a202e-39bb-4eb0-b01e-d131b9c9021c}, 70=BucketInfo {bucketType=UPDATE, fileLoc=27903732-e7f3-4304-8f6a-135167dfbb68}, 71=BucketInfo {bucketType=UPDATE, fileLoc=df7cba1c-02eb-41ff-a823-e238659e8872}, 72=BucketInfo {bucketType=UPDATE, fileLoc=064f90eb-2275-4f86-bb74-51b8b1cf65b4}, 73=BucketInfo {bucketType=UPDATE, fileLoc=e29fe604-6150-4f86-b511-587b8d405452}, 74=BucketInfo {bucketType=UPDATE, fileLoc=3d706d21-029c-4091-b6fe-f555a543de4f}, 75=BucketInfo {bucketType=UPDATE, fileLoc=dadb6852-079a-434d-8a58-bf9f34f6ce10}, 76=BucketInfo {bucketType=UPDATE, fileLoc=5d4cc127-d79e-44cb-a6fc-32c6a1273008}, 77=BucketInfo {bucketType=UPDATE, fileLoc=8bc7f97a-1f8c-4c79-9578-f328b9d4ce4f}, 78=BucketInfo {bucketType=UPDATE, fileLoc=2839957c-74a2-4ebd-b7e6-22f42a0ff600}, 79=BucketInfo {bucketType=UPDATE, fileLoc=28201c58-be55-414d-b3d3-bd02833ea414}, 80=BucketInfo {bucketType=UPDATE, fileLoc=96cb81db-5ccd-4a23-8ed1-0a913d902e0c}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{e15cc65e-d82f-431d-858a-7ea431e3e42b=0, e772e016-1835-4a19-bfc1-e7796bfce165=1, 53827ec1-bc5c-49dd-b853-047f318d19cb=2, d46ac3d5-4419-4bb8-8a10-5c33b004c4f4=3, ea21b6bb-545e-4f0d-a99e-599592f0101a=4, 215f9697-7a02-45b0-8918-01e29099aa1f=5, 76c89ca0-2706-4c10-bf69-03449baf1171=6, 0c151cbf-838a-4d25-ae6e-8e1c28a9c429=7, 7478fc5b-c421-4036-b57a-81b5d366c0bb=8, 30477b8a-8d01-403b-bf6a-783468c54365=9, 453a4ad3-47de-41e9-9e4f-bf08b3db3f73=10, ef55df4a-f6f4-45de-9365-25578872c6a8=11, cb797359-6bb6-45a0-8ea7-8a36ac7db10f=12, 06340a7d-f1ef-4caa-827c-845704233473=13, 79d07ade-f3e4-45ed-b21f-039390fd2e71=14, 079ebe9e-89d2-4728-8bb9-54b29f08b150=15, ce4f1e53-640f-4102-b40b-0a06b7e2ea0f=16, 6798e9c6-3228-44b1-bf34-695c0e8b9ab9=17, 4dab0d6e-7736-4a49-9c43-b327bf1b68dc=18, 3748be5e-9ec8-48f7-8c0c-02232990d5b5=19, b9110126-75bc-4759-b378-310ded41b684=20, cb24921a-900a-4d37-bca9-a6aa0ed1ee84=21, 6428d374-052b-412b-8a76-bdb98d9137ee=22, 746ce790-2e38-4071-bc45-5e013573869e=23, ecabbba7-2a3c-4d30-90af-0f19f273f249=24, 92ff01fc-d3b5-42d9-833d-01cc88b6fadd=25, c4cf608b-096a-44b9-8fd1-0661672e8831=26, 4e198182-1402-4827-a7ca-5a6da9220df1=27, 4450fb74-241d-4a67-b56e-0b2b1f5deed4=28, b23b6134-e77d-42c5-85ca-b259be7d5fd6=29, 52b7a2f2-e791-437d-8fdb-53716a1ff226=30, 1ed64d42-3dd0-45dd-ae0b-51f4baf43e57=31, dd6b8f5b-151c-485b-bb58-cc9d91bb3f39=32, 2c017570-77d3-4114-8d75-392f364a99b3=33, 95c0edb8-d25c-49d4-99f2-caa701728e1b=34, 26707c76-bfa8-4c8c-ad46-5b02c14c3478=35, ec9ef00a-74a2-448e-acaa-6de27db1e1e2=36, 33856898-0915-4e3e-89f8-329c8a6ed637=37, b2a104c2-cbf2-4bbe-899b-3f113dd81e8c=38, a4be6e9e-8785-4e17-8ff8-b16c23e1b9e0=39, 959f1aa9-2f08-488d-9050-9bc3d04ee49c=40, 9bed3a74-4a78-4ae7-a09c-3c4dee0f8e1d=41, 6c29c00b-2847-4b6b-888d-a7a1c6e509ce=42, 6f0cc61e-c01d-44b6-9cd7-552aff50641d=43, 864a26e9-0cd7-4673-8027-c7a3bc82de7f=44, 31288041-11d1-4333-99a1-670e80ed3b94=45, ebade5d1-bb9d-46ba-8893-d70eefe0dd6f=46, 7cea76f5-f37b-49e8-a7e0-f08f13a1cc98=47, 11f98c91-700f-407b-aa9e-0f76f817ae12=48, 7d751220-107a-47c6-aeaa-31cf9bff69de=49, a26edf2c-635a-4dd5-8c52-e5fae42f7250=50, 1ec64489-6a4f-413c-ba65-7207a229f43e=51, a2052565-e54d-4509-9e6f-91039d1a787b=52, a4e8049e-5943-4eb2-8323-366f89b77d5f=53, 8b8b051e-e2a4-4f88-8b7c-1d5095f3219c=54, 824b0fac-25bf-499c-8c6c-a3623ba470a7=55, a5f3b388-ad6d-4fde-9322-6e0f78a031d8=56, 6883cfab-7e96-4503-bf7e-9ac04161ddec=57, 9ad6b0eb-7ab9-41da-ae25-5c5c010f85a0=58, 0aa59145-ba03-46c4-b195-a76298f7bd6e=59, 129482ae-c91f-4aef-9d18-d448b7db9990=60, eef0dd26-13d7-4d8a-af9a-19c0cb8e8c43=61, 875def08-bc72-4835-8ba8-da571b82d896=62, 712ad067-209b-4f58-8ce0-6e37e18b5836=63, e5d62300-d1b1-4e3f-b00b-1ef58bb25c05=64, de415c2d-a14e-4298-8126-d3be6b3a1a49=65, 299341ea-d1c4-482b-a6bf-0fea7df399cf=66, f813902e-effa-42b5-8296-5bf724805717=67, f5bcaaa8-5148-4d63-ba42-31c518ba5d9f=68, c67a202e-39bb-4eb0-b01e-d131b9c9021c=69, 27903732-e7f3-4304-8f6a-135167dfbb68=70, df7cba1c-02eb-41ff-a823-e238659e8872=71, 064f90eb-2275-4f86-bb74-51b8b1cf65b4=72, e29fe604-6150-4f86-b511-587b8d405452=73, 3d706d21-029c-4091-b6fe-f555a543de4f=74, dadb6852-079a-434d-8a58-bf9f34f6ce10=75, 5d4cc127-d79e-44cb-a6fc-32c6a1273008=76, 8bc7f97a-1f8c-4c79-9578-f328b9d4ce4f=77, 2839957c-74a2-4ebd-b7e6-22f42a0ff600=78, 28201c58-be55-414d-b3d3-bd02833ea414=79, 96cb81db-5ccd-4a23-8ed1-0a913d902e0c=80}
178561 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 002
178562 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
178753 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
178820 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
178820 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_53_0 failed due to an exception
178820 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_53_0 could not be removed as it was not found on disk or in memory
178821 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 26.0 (TID 435)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
178824 [task-result-getter-3] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 26.0 (TID 435, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

178824 [task-result-getter-3] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 26.0 failed 1 times; aborting job
178834 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :1
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
178835 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_53_1 failed due to an exception
178835 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_53_1 could not be removed as it was not found on disk or in memory
178835 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 26.0 (TID 436)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :1
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
178835 [dispatcher-event-loop-20] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 34739, None),rdd_53_1,StorageLevel(1 replicas),0,0))
178928 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
179065 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
179407 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
179407 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
180035 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
180087 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=60, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=75, numUpdates=0}}}
180094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
180094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
180094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
180094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
180094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
180095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 60, totalInsertBuckets => 1, recordsPerBucket => 500000
180095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
180095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
180095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 75, totalInsertBuckets => 1, recordsPerBucket => 500000
180095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
180095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
180111 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
180111 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
180273 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
180273 [pool-1277-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
180327 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
180327 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
180351 [pool-1277-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
180369 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
180369 [pool-1278-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
180584 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
180584 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
180601 [pool-1278-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
180624 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
180624 [pool-1279-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
180673 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
180673 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
180702 [pool-1279-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
180719 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
180719 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
180804 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
180804 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
180887 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
180969 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
180975 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
180975 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
181061 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
181468 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
181468 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
181670 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit1535367383186147016/2015/03/16/23e5f87d-15a7-4242-ab6b-52ca8a19b1e7_1_001.parquet
181686 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit1535367383186147016/2015/03/16/23e5f87d-15a7-4242-ab6b-52ca8a19b1e7_1_001.parquet
181686 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit1535367383186147016/2015/03/16/23e5f87d-15a7-4242-ab6b-52ca8a19b1e7_1_001.parquet => [00382894-7e91-4ae6-8fba-810e98b3642b, 04995904-aa2e-4dbd-91a5-77268a1e80ed, 066d4b54-e1ba-405a-b8bd-9cbbdac1f106, 066e5cbe-6bb0-439e-bc85-67b1c82f1b40, 0a6840e6-e60d-4239-9b0b-6b43bf39bbd4, 1bb9afb1-3d7e-467f-91f5-38282e8b1289, 1c07d80f-188b-426f-983d-430d58dc53b8, 24ee0486-3077-4b43-98b1-229b80ee5816, 280a8238-2efa-4416-9c72-693fd240890a, 34c6e844-2934-4219-938f-b3fa4f82e248, 387b5ad5-d091-469c-8f2b-1744aecee071, 3b8dded5-2f9f-4b12-9386-bea2281b5b21, 41aaf94c-34e0-4b0d-9330-505840034ad0, 455577c5-5a52-4da9-a72f-fe9820354ce7, 4784d311-9582-4a60-9caa-f9fa31487c4e, 47d5b650-65df-41ed-a924-ae5fee5da25a, 4824e2bc-d13d-4f22-abf0-d353dced72b7, 53346608-98a1-4055-ae06-05d2db8e0a3a, 5ed411c8-0f91-457e-b593-d54f5b2aa5fe, 5f99455b-3b0b-4c3e-93ff-edd9368b6bb3, 60652263-4a1f-497e-a861-4fed82f12de9, 60d731ce-346c-41fa-966f-c9b3b8f172ad, 6482da52-303d-426d-8e10-95192ffe7701, 690068bc-bc19-47bd-af8f-7c134641644d, 6b0ddc1e-3c1d-4cd2-b29e-3bf74f4671c6, 6cf10122-ff3c-4e68-b660-6300a5c9ac0e, 6dabb065-4881-4584-b826-cd2ebb83b5d4, 70cad6e7-219a-4ba6-a31e-1c0149180ada, 765c99a0-13da-40e5-81e9-c0b4d005452a, 77a9da4f-9ded-4b5a-a2d8-1b5316d8b83d, 7d8698e9-73e3-4ca9-8b63-c4d9d990b26b, 7f16e12d-f130-416e-b040-ec4773203c1b, 82651960-5e7d-47d8-8b05-f36c875edfea, 84f3d67f-5231-4a57-8391-d1b04c9ccf43, 872cb8b5-238b-4a4a-b953-27402d3b884d, 8e76422c-65b3-46f3-977a-8395ef724756, 8f9826bc-5e20-48b7-9eca-a9403f500ab0, 9ade0a49-0835-4f50-98cf-db4f57d3bb62, 9ae360db-4eb4-43af-81d8-4689671db9ea, 9fd48e2e-8f50-451f-a084-305a24b1ad6c, a3ad837c-d52f-468a-859c-e330bb7d03c2, a41fc7f2-365c-4bd4-82b4-80a62e734ff3, a6f25caf-b19c-4e7b-bce4-7289192da92f, b485f9c4-de3a-45bc-b6b1-53e7f7dab5fe, c32169df-e6d2-4b67-a630-c1bfb81eca67, c979bea8-2234-45e0-a3ea-e4642aaa5fc4, c9a3c878-421e-42b0-bd3a-3ad76e20b2e1, ccc09193-db4a-4019-b6fa-d19be09822a3, d3d3004e-da70-4eea-ad31-6c6c7e35db93, d474799a-c36d-4734-8083-f1d573fd8e14, d6fa823e-6014-4dcc-8c1d-2e9ee9f9c6ee, dea615b6-46ce-444b-be42-2c01c3fe1fc9, def3659d-fc9c-49b0-a8cc-bab02d35106f, e197d1b8-6a5b-439a-87fd-f55c25b5ab7e, e3a7c364-66da-487d-a82c-6f862c98abbf, eeb00836-fc64-4811-ba71-17c82c1fbf90, f037a89f-4d13-4a65-8f5e-bc810b00b212, f04d2730-dc4d-4a21-8116-fd1a09f90531, f0877f9f-1b7e-43d5-9a55-6637f09c72ac, fdf3d574-39d0-47fe-86a3-8b58573f1662]
181702 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 41 for /tmp/junit1535367383186147016/2016/03/15/39ab9af0-f2c7-48b6-abff-549ab5b8e1a4_0_001.parquet
181714 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit1535367383186147016/2016/03/15/39ab9af0-f2c7-48b6-abff-549ab5b8e1a4_0_001.parquet
181714 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 41 results, for file /tmp/junit1535367383186147016/2016/03/15/39ab9af0-f2c7-48b6-abff-549ab5b8e1a4_0_001.parquet => [04389e90-083e-4f0f-be8d-602f1cb4ff01, 07a7b382-2646-4c50-89b2-e00010a5fb7a, 090caa26-6a7d-40de-a196-8cde5776c116, 1019f873-4fa3-4d83-81f5-2c7fba50f257, 1374cbd9-764b-4c5f-ba5b-bf4332754448, 13a0ae91-4628-47c2-8f5f-d3507b726e54, 15cba035-34e3-4120-a08f-bbf91129033f, 19c6a4a5-248f-47b2-9c8f-46c58652df2d, 211ff65b-7041-4c7a-942b-55d10317dc5c, 22187c8a-a5c6-4570-858f-adc27c0f1dbc, 26b229d4-c868-4c08-a98c-dbf183c001ee, 35882b55-563d-48f7-bd7e-217d8639bf48, 3f3e5942-198c-4341-b522-b784067e6134, 42ae6d5f-0cf2-449d-89c2-de1f66536bff, 43713e90-0985-4227-afc2-83f4b7629737, 480e7a83-ac38-432b-86c2-2eda05b75bc8, 48297295-b2ff-4f3b-8962-977d49e4eedc, 48af7b06-8935-4bad-83fd-78b505cec8f1, 49fe4d98-8219-4a56-b56a-fbdf98d7db6d, 4a8557e2-4003-4ea9-9e49-0e58596a6d1c, 5b852904-b236-4ae9-9ab6-e6a594de1ff1, 5d2dad76-8097-4373-957d-f8599138b393, 5f50f6f5-7eb2-45ab-aadc-8ba0a38216ff, 5ffc8fc3-7994-42d0-bb17-c18135497cc7, 629d3eb0-332a-4423-a054-7d153219694a, 646c26c0-3a78-4ab3-aac8-b608459a2e8c, 6bda1556-753d-49af-b55d-c326d4d5d6b1, 6c14d0c8-d1c7-4876-8273-f283b71d5fd8, 7097a112-b6e9-4e07-a1c7-baa3f87acc27, 7746363d-5447-4204-a115-00cc08c9006b, 799bad7a-0a73-4a1f-9ac2-d07397fc0f66, 7c78bfb9-19ed-4517-ad52-2b7df3f6c0b6, 8463e6f4-b413-4ef9-ab23-13643670899a, 866ef8cc-ed86-4bb2-a745-b7cbcba88e4e, 86e58614-a7f3-4fec-8e30-36dfa0d43f4d, 91dae623-f35b-4b62-a847-c629a9b3771d, 923f5b9d-a582-4c63-bf50-cfd57119f035, 94cb3896-4a4c-4e9b-8d05-0cba07c415dd, 96592d78-f6d0-4eac-a409-ad6a8f69fa02, 9d2d2b28-a454-4028-ad26-67cc3fd4e9d6, 9e007384-2805-465d-ac6d-a094597d018e]
181746 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit1535367383186147016/2016/03/15/39ab9af0-f2c7-48b6-abff-549ab5b8e1a4_0_001.parquet
181759 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit1535367383186147016/2016/03/15/39ab9af0-f2c7-48b6-abff-549ab5b8e1a4_0_001.parquet
181759 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit1535367383186147016/2016/03/15/39ab9af0-f2c7-48b6-abff-549ab5b8e1a4_0_001.parquet => [a2e93590-3df1-4a6f-87ab-7d6ce5715879, a46b737b-1b61-48c9-8174-caaaa769d426, a83e5582-d656-411c-ab7f-b26e13a59b79, a8fe14ec-d8bb-4904-b3f6-ad9659f9391b, b2195578-e67f-46fd-8c45-69aa4324740d, b9c4797b-2726-453a-8bf3-529b9b9b7dc3, ba953980-9b21-469c-ad6e-2632e9cbbf4f, c084d15c-9ca2-4c7e-b5f3-84198d642820, c59ec5c9-a99a-46e7-8a97-6fc0c10670d9, c86f6264-142c-4d23-a494-f8d21146448b, cb28081c-9c8d-4088-8335-fe98c6ff0990, cd1cee50-0cdf-4d22-a12a-615fb439c8db, ced77e93-6347-4d0b-84c1-b90656bb0f1a, d2f6c30b-dcf1-4e75-9d8d-a6b4bfae006f, de8f5c9e-3393-464c-97e6-fbb46ae6698f, e291bbb1-d07a-4b3d-9f5a-4280ae2f8f27, e516d5a9-d8a3-48fa-9db8-1101d34c960f, f5d34f02-5b9c-4f69-9f78-f40d3508bc31, f697ace8-7334-4cc5-bd15-9ed9a5a8003d, f72b03dc-57cf-490d-ac36-52cd29f6ef18, f9b86d2a-d6e8-47e9-aa7e-e812cfcbbb6b, fcb476c5-7d40-4053-b0bb-68d8ba6ddfa4, fd840f21-d441-46c8-a6b4-931c76bc6b99, fe0c1b34-5126-4d1d-839f-494daf0fd796]
181776 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit1535367383186147016/2015/03/17/cf518437-de75-4e8e-9534-d33f5003b449_2_001.parquet
181789 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit1535367383186147016/2015/03/17/cf518437-de75-4e8e-9534-d33f5003b449_2_001.parquet
181790 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit1535367383186147016/2015/03/17/cf518437-de75-4e8e-9534-d33f5003b449_2_001.parquet => [029881d4-90e5-4ec7-ac8b-82d72dd88c58, 09e394e5-881c-4b5c-9253-8c86740e5185, 10b62516-6ec3-4cf4-90dd-9e73056e1191, 10fc8d07-c016-45f5-93f8-0a77e87bd420, 1219524b-62d8-454c-a999-a2a8e14eb9d0, 16021846-c169-4562-a634-72fb5cf838e2, 17c227a5-8492-4f05-8e9f-2019fdf6739a, 1c9d8146-3edc-4de3-8f0a-21fff036573e, 2217745a-9e73-43ee-bb59-c28d189e0a7f, 25e7a854-e384-430d-9804-dc22e93c39b6, 2709aa72-3a0e-4fdd-8930-bfaff3d37635, 2a69e5c3-80ed-470b-9a89-0ab8268302f3, 2f81e543-b16c-4828-bd8d-b91f65f8006b, 3060f55e-06fe-4cd0-a083-ffd220ab1f08, 3069174e-7b89-4828-8331-00a3b1870810, 3334d063-85ba-43bc-a27c-1c651da16015, 33e79f5e-a86b-414a-8a99-06b164d5bf3a, 346be2ac-876a-4381-afca-2021293ad427, 41b8d7b0-254c-4b07-9341-52ca0b640996, 47543c1f-47bc-4e07-8c35-f6067838fafd, 47f6b383-8cad-4fa4-ac0c-a3c8c5459cb8, 4abd0fe6-c6dd-4710-87ec-3a2fbabcfcf9, 544a47e8-16eb-4197-95f7-c6ee22f73055, 55148eaf-f92e-448d-a377-5aee80c1e6fb, 5822bb09-e86a-4630-96b7-309ac1a12e03, 58f73bcd-14ed-44eb-8705-cf7ad77716f3, 59183455-85c2-4c50-be0d-629c54cbea71, 598da038-54d8-48e8-be99-c3894213fcd6, 5c4f4c1c-eafa-4943-83f8-5927efff6adb, 60f09de5-7ac9-4d90-830c-18adaec6796d, 6401e67b-8e82-444c-a404-0f57159e1948, 6609d408-0873-4775-a6e1-19d4f022fe47, 6aa620a4-625a-4d05-a18d-56c640ea8abd, 70e8db1e-fd07-4c99-bc47-1a6b66926c01, 73aee19b-7714-42fd-ad3c-4cdd3c64b081, 786bd9f5-7669-4c9d-bd30-a28e7d43ab7a, 7939420f-2a1c-46ab-a78e-198fa09d3d6f, 7ac2c0e5-a5bf-45d9-a55b-b22aa76ce241, 7c5b9e67-c0ea-453f-bc47-0489dec63f69, 7e23c038-f396-4e2c-bc98-21ebb5238712, 7ec50b3f-c681-4197-8fc3-028c82eeeace, 7f5643a6-8110-4415-a9c1-77112bbe70f3, 7f9d70c3-898c-4ae1-a905-03b9ce81323b, 975732b4-f3a2-4a20-80fe-3c2a10ed9b68, 98c9d15d-824b-4a1d-aac8-3a25d9b29a34, 99093b45-c1a9-4ed7-bbed-2a50b9b2be26, 9e443d2f-9b3c-4473-bdf3-c4460cfbf7a6, a0e763e1-3d10-4dc8-83ae-abd57d708faf, a38f64f7-d764-454c-84b1-0c633091fc12, aa176ca2-b021-4a58-bddd-a85b121cc0c7, b094c0b7-1bd5-4065-b21f-560a3a92857d, b2863f7c-d82a-40b2-a012-03f6b3a70c9a, b5e24cf3-6040-447b-8a23-855d16c3ef69, b700a6ad-344c-4f3f-8fb6-0ebcaed65937, b792756b-ee0c-400c-a111-6137f00fcf83, b8dd7d58-5bbe-4af3-bb77-3f852b2934c9, c412c8c1-85c3-43f1-af0e-732c22e7fcec, c67d2d8c-c535-4f74-9647-41fb33766a77, c7af98f4-a232-4696-b9f2-56872cfe1723, cdf41297-de48-4723-83c5-ef345878abec, cf819681-5a6d-48e4-86b2-3ef62ac50929, d7cce30d-23ed-4b7e-9e74-04d98dcb454c, d983d9a6-fa24-4dc1-a3b5-00d00932407d, da179b2a-67fb-41b6-8b87-b9c05adb3e8e, e050c710-6c19-4536-9203-0a768733f524, e32b6db1-fe5b-4585-8e4b-6ebb2369f7cd, e3af983e-06fd-49aa-9067-5515133a03c7, e6a16900-db55-4281-8629-38d5c61f0af8, eb4d96d6-4281-4fff-9e2c-ae5423a2268a, ed53deaf-06d1-4973-af30-16cd1818f5b6, ed691f69-2560-42b8-b68d-57251ff59778, f50d512e-a745-4c16-bb94-76f47a8a1af0, f5e3848b-ea2c-4184-8546-3939a0397de9, f66788b6-2b75-4e9c-a9f0-4b7ae459aede, fa7d1c8e-2630-40ae-93e2-1b3b15c9c7e2]
181896 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=60}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=75}}}
181909 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
181909 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=39ab9af0-f2c7-48b6-abff-549ab5b8e1a4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=cf518437-de75-4e8e-9534-d33f5003b449}, 2=BucketInfo {bucketType=UPDATE, fileLoc=23e5f87d-15a7-4242-ab6b-52ca8a19b1e7}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{39ab9af0-f2c7-48b6-abff-549ab5b8e1a4=0, cf518437-de75-4e8e-9534-d33f5003b449=1, 23e5f87d-15a7-4242-ab6b-52ca8a19b1e7=2}
181928 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
181928 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
182052 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
182054 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_0 failed due to an exception
182055 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_0 could not be removed as it was not found on disk or in memory
182055 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 50.0 (TID 66)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
182057 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

182058 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 50.0 failed 1 times; aborting job
182067 [Executor task launch worker-0] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
182069 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_1 failed due to an exception
182069 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_1 could not be removed as it was not found on disk or in memory
182069 [dispatcher-event-loop-24] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 46003, None),rdd_82_1,StorageLevel(1 replicas),0,0))
182069 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 50.0 (TID 67)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
182102 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
182113 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
182177 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01], with policy KEEP_LATEST_FILE_VERSIONS
182177 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
182377 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
182454 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=69, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=59, numUpdates=0}}}
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 59, totalInsertBuckets => 1, recordsPerBucket => 500000
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
182459 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
182471 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
182471 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
182566 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
182566 [pool-1318-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
182596 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
182596 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
182618 [pool-1318-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
182639 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
182639 [pool-1319-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
182668 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
182668 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
182687 [pool-1319-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
182708 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
182708 [pool-1320-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
182730 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
182730 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
182747 [pool-1320-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
182764 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
182764 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
182843 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
182843 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
182996 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
183003 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
183003 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
183374 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
183624 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
183625 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
183762 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 69 for /tmp/junit3759763799676180278/2015/03/16/085e9102-3475-4615-8ca9-d2b5385626d3_1_001.parquet
183778 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit3759763799676180278/2015/03/16/085e9102-3475-4615-8ca9-d2b5385626d3_1_001.parquet
183778 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 69 results, for file /tmp/junit3759763799676180278/2015/03/16/085e9102-3475-4615-8ca9-d2b5385626d3_1_001.parquet => [0106da27-615d-4cae-b849-3443ba1e9018, 013ee3b5-aacb-4110-a9f6-86c38318400d, 0b33260f-d6ec-453c-bd23-f68e3e1798a7, 115a8697-33a4-4114-8020-d6ae335cebb8, 118b710d-13de-4929-bf6c-72e49ac34131, 129cd5d5-cf62-49ff-a1a3-cf25e887984f, 183d0b0b-533e-4f9a-9030-15723b94a79e, 1910291e-1499-4475-b868-a62f4e17bdcf, 2241e7ab-ddd6-45f8-a382-9bc572e1f17d, 22e2875f-79e6-4985-90f6-750acf2543c0, 2367d626-48eb-4a18-8b5b-033071a01f2f, 29c32fc8-d5e4-422e-9bcb-e01c37e25dc9, 2e1f8676-ca17-4581-9457-76441cba7310, 31908456-df6e-48e5-925c-bc7ad5ed436c, 31fbd5e8-9836-4f76-b4f8-4461943ea26d, 32570225-1969-4150-85e8-29f0454561fc, 3288473e-cadf-4240-a860-2f674f3f054d, 3483d4b6-b091-4758-ae8e-c5591cc19556, 3500d4d9-98b9-439c-8092-5120133a4622, 3947ad6f-14c4-4b25-b79c-6b5ebd10e6b6, 3b6a2dd4-752b-4fc4-855e-e04ddb898ce7, 3c81cb3d-acc2-4d4b-9802-2880df5eb972, 3cac3b29-828a-44bb-82e7-8dfd1e6a582b, 3d047523-c86f-4803-9ac4-341756a79e47, 436ec47c-f25f-4475-9c0a-405f92cb88f8, 47db7a64-ff3f-4a06-83eb-cc4afbc22788, 497a039b-8ebe-451c-9ed7-f42f34262387, 4acc2c6e-3693-4a6c-8846-14ed79f585a5, 50b968c3-39b5-403b-8f06-acd867744523, 52194b2c-eeeb-4593-a122-cce573dde603, 52b15dcf-edad-4f1b-bc98-511b9c6093ea, 54a72c9c-5330-42bb-804c-3e91b70ac4d0, 559a20c8-80a9-459e-8537-f970adcdfa09, 5c915f45-5709-4026-b261-eb6282e2643f, 635dc2ee-c00a-4d49-bdc0-b6f1dde7b7c4, 68333408-1654-49a5-8915-80854353767a, 6977bba4-e926-4666-992a-3d5644a8e697, 6ab5c1a1-4e7f-4d61-865b-7a5270eaa70e, 6ac8717e-7e29-4db3-9f6c-ca84618d82d1, 75d1a2b8-57a3-4ddf-8550-96e30fd3729b, 7a43a9b6-1dc1-4200-9a04-98d37f40345b, 7b953be9-43c0-46e7-9f64-7b7b5e959da7, 7d083b71-2ae3-459e-b974-82020d4b2906, 7d37b691-190a-4a40-9d3a-641a6b706368, 806d2e16-f97c-4445-b308-e0c4ae7809bc, 8447ceed-960b-4980-a711-7956cc63f643, 8a77a1d8-640a-4d85-83d7-4316a16a640c, 93de3aa1-4d56-45e5-a3a0-fc74e5d403e2, 9ec98396-b127-468d-9dc0-91cb7e23c434, 9f9a004f-45b8-49d8-ae3b-f1e7410e971c, a7addca7-f122-4163-b257-3410450a40b2, aba183ad-0cc3-4944-a60c-4457cd879d3b, b0486429-9625-4ce2-9443-867414f45bc1, b69a9c09-1692-4190-890c-c56941fb6381, b6cd9532-67db-4020-aa5a-dfebd696f8af, b782ec12-74e6-4c68-a682-a3dc8603a0eb, c2632f68-2724-4256-8fae-a80598bfbe85, c73765b0-01f4-401d-ac45-2b7ba0254b7a, c82c42b6-9fb4-4cb4-892d-c3c9650fe0e7, c82e0b9c-2c91-4817-a790-95dfe5d56bac, d1b5881d-115a-47d9-9fbf-287fdad54d58, deeae67d-44b1-470d-975e-7281673e35cb, dfeccdaa-1f31-4bc0-abf0-597b0722cc18, e04ed9f8-6acd-4903-a10f-15c1daddeb37, e2cff5d7-06be-41c2-ad97-d51074516ec0, f35efbd6-4766-4f20-b06a-0211167449e5, f570ab21-cb4a-4ad7-9001-f549d96ef3f2, f98b2f91-ea77-4ce6-b865-6b54e669841a, f9c156ba-f092-4fdf-b43f-3777adc272ce]
183794 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 72 for /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet
183805 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet
183805 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 72 results, for file /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet => [00617a8c-dbaa-4eef-99b8-e8ba0b7e9454, 077efae5-bc98-4feb-ae58-25967b4b28d8, 0a13b00b-959e-41d1-a9a3-1dd2eed684a2, 11d45c6a-4c15-4710-a008-84e9c30a0ce2, 14b63828-1844-4f0a-b705-f366e327c833, 1aa2969c-47d8-4d0c-9afb-0b8c3a5068d3, 2693d73e-443c-4d49-b58b-3bf4fc97f538, 298339ee-6464-429c-af43-3bd4ed5f7a08, 33736ab7-5748-46b3-b94e-b4fa6ac4e864, 34edf00b-f31a-4169-a1f3-f03f7629856b, 35b905ce-195b-4852-a6aa-ac38ed8acb96, 3a9ec2f2-8a3a-4aa8-854c-3dc867e3dda4, 3aa2aa8f-79c0-487a-a54e-093e5e573c8e, 3bc924ae-96d4-4b29-bf1b-27b325edb463, 40afa687-8aa0-4427-8b0d-0d0a8530912f, 41aa9cd8-4a10-47a4-b64d-db382f75f3cd, 45cf3783-a721-40ac-a115-7042d31a98ce, 493d24cc-795b-4ecb-83b2-9d42288756d3, 4a61a8cd-eff0-4306-9fee-6b9b9e67492a, 4ab27294-387a-49f8-96bc-deea6d8f6e64, 4c3b8dcb-2e4a-424d-b27f-0f2888305fb6, 54e1c52f-1841-4f7b-a23c-42930798ebec, 56614b02-c769-4c1c-86d4-e1c66c8bd268, 60be879a-712e-4243-96db-c0dcf5132203, 656c1ff0-461a-4a53-ac82-016466ed4149, 6619f48d-df0d-4c9d-804c-9da2697432d4, 697254db-8fc1-4363-b5d9-bfcab57d0510, 6a39694a-620f-4a05-8836-49277c85ff11, 6cea91ba-9bc7-402c-8224-0c8b5aa59d68, 6d57fcf6-0e8c-47a7-8021-53626b3375e9, 714815a8-1b9f-43dd-9df3-a247e5bc3fc2, 72f67d8b-9f4c-45a0-9f54-f9eca2370db9, 7a9b03ac-570e-4222-b9da-1b5cdaddfb0f, 85344052-accd-47ac-8c4a-75cda54302f1, 994b7321-515e-4e8f-806d-08a67707bfbf, a2240e69-e4ad-42fe-9be3-bf5a57c5f344, a304a6a5-4efa-4b8a-946b-a8c1efd63f33, a6119a6c-bbc8-494a-b16c-df92ca356d6c, a68f1459-06b8-44a0-b1df-c454977f1189, a7d44059-57d3-4231-a041-62c0cc9970b0, a930648e-be74-4c33-93d6-99f764dd8487, ab3d8197-631a-40b0-ac48-5056a619ce68, ae1464a5-a81c-4db1-bb54-5ea29295da1f, b0ac0deb-2edf-4383-9199-62430e501ebb, b40ec7e1-c7c4-49bb-92e6-d394b6f91f51, b4bcc046-f973-4a55-879a-f2610a4b06d8, b735e4c2-8345-4f74-a9d7-90c9751eb9dc, b816482e-7eb5-4285-9b45-c8d89a731dce, bd09eb0a-6b45-460d-9057-8381642a3a25, c6066831-67e3-4ea0-a16a-0d7c27a0845d, ca200d92-9f87-45c2-8334-41f1abd80df1, ce100e4d-d1fd-4e07-8e22-0be133306b32, d04883c5-c65b-499d-b531-b9c34d876aed, d12fe6ff-2c91-40bb-ae25-c3985f534b25, d2f006b6-436f-4fc7-9c09-8b049b8c0824, d4c1a128-393d-4f71-8561-8dc131e41759, d9ab8cc6-829f-49b9-9a32-817daa32eb2b, deac1823-423b-4208-877c-ba911a1ceed2, df4e896f-4e78-482f-820f-3783efa62e18, e1c400c6-c552-47e1-8269-13d717768d8e, e8d0a8ae-956a-4551-8706-80bf1a48809b, f12e4c70-181b-4606-85e5-d5ad39261490, f13e52b2-7ad5-4151-8738-caba93667eaf, f2961a90-757e-4e30-a049-d72a3c33f92a, f540d280-d06f-486c-997b-6a3769bc70eb, f56953e2-564b-4969-b632-407ac7b83127, f972139e-387d-43d0-bbc1-6d160a76c912, faba3d9a-8332-4d79-ae35-100f07b951bc, fb24f6dd-0889-46d0-9cfc-c1db373fea95, fc96678c-de3d-42db-b52e-83d09802de66, fdcb2d04-9095-414a-bcb6-6e64424e73d0, fff73ab8-753d-43f7-8457-e36bb0ca42af]
183819 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 59 for /tmp/junit3759763799676180278/2015/03/17/56efdc04-204b-4ab9-8d80-1c225d65f969_2_001.parquet
183829 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 59 row keys from /tmp/junit3759763799676180278/2015/03/17/56efdc04-204b-4ab9-8d80-1c225d65f969_2_001.parquet
183829 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 59 results, for file /tmp/junit3759763799676180278/2015/03/17/56efdc04-204b-4ab9-8d80-1c225d65f969_2_001.parquet => [03c98ba8-e9cd-4abf-8a9e-9e557fd700b2, 0ae2758b-8d08-4a47-86ab-9a451fcb64ee, 100942b0-709a-47c8-bffa-836ebb2de170, 10aa8446-feab-43c4-b5a1-19fb8bc3e32d, 11190067-84bd-468d-970e-1898429e0527, 1866861e-1244-4b75-b5d2-f90a864be0cc, 1891192e-e642-4472-981a-cdf59ca56f30, 19d7230e-fe4e-4402-a23e-0813db6449d8, 1ac3f3f5-13bf-4e74-8c9f-d89b88d4016d, 1e5804ea-4dbe-42be-b1b5-d80966da2dfb, 2372fb61-afcd-46db-8fd7-960acc3e5031, 23929345-8d01-4782-a866-8fadfba2c1b3, 23b68afa-0bc7-48d9-b21e-fe0d9476eefb, 2514b5f1-a181-4f73-9961-e5c249b3c64d, 2cb2b04f-2ce4-4f0f-ba2f-d95cdf615d03, 312fff78-afff-49de-9af4-0bd48ca36645, 33b8d22e-417f-4116-8ec8-67815199b31c, 35c2c01b-ea8f-4f17-8ea4-a17a92466a96, 485487c0-802c-4f95-972b-6249fba4e860, 492d8b44-e579-4a08-bdcb-3cc492779858, 4aa61ff2-cde2-4133-bd91-45c49be8ee35, 4faae6aa-cd0e-4890-b869-11e13f4d7d27, 60ceeed6-bdc0-4b53-a62f-d62b1f43a5f3, 713745f1-0148-4ca1-bf02-d8b2f302bd5d, 713e73b2-eaac-48b9-ad5f-91eb8709d633, 72bac8fd-127d-4123-9a1e-5e11707b6bb3, 7532645e-742e-45c1-8c24-ab8aeecfe97c, 84c78136-f315-487b-99a1-c34f0666aef8, 85ed3418-2283-4600-a204-903b073ddb01, 881ba716-abe5-41ac-82df-e477f309e473, 91fca6c7-0abe-4737-868b-1b8db1556f61, 929373c5-bb5e-462f-bbde-c25f8b2d15c6, 92f225e7-3f6f-458c-a255-efe7fa0e9eac, 97e590ce-11ca-4350-b7e4-04e585b93c53, 98187cdc-45e0-49d3-ac8d-3a9a4c983d96, a1887674-ed2f-4cee-a267-3bbf52fb7097, a76e6896-3265-4c14-81c8-751393e0f022, a9d2ba27-a44b-43f8-9505-6cca9baa77fe, aa542459-58ec-48a3-bb4f-548065c79c4c, b145c2af-f9bb-45c9-8c3a-46cd2ad497fd, b46293f5-3937-4641-9afa-6ecc7443a5b6, ba23e6a9-f6fe-407e-8469-7b9e78142ae2, bd626761-5c5f-455e-be37-b2ac357e8d16, c6508d1f-4d3d-4009-bcbf-751572cb103a, c6d94667-eb40-414a-b337-954a9aec6b4d, c8826c1b-119a-4c0d-8e2c-d21aa6b1731c, c8a236de-0fcb-47ea-a3c0-c26f30011158, cc590bc9-3728-4e28-ae91-bcfe0f95c36c, cd1ac507-c776-4894-9570-b45dc0efc3f4, ce743961-6b25-4527-946c-b943ca043964, d78012b7-94a4-4e0b-8667-5456230910ac, ddffa18a-a2e2-44f3-b890-16c10e3663b0, de5ca0cc-5002-4868-a742-3db5afe83468, e1921e11-2535-4c78-8025-4fbc48a7fda3, e1d5e887-846e-4bda-9128-d50ef87aec77, e327af2d-42a7-4613-a9d2-dad3ac2a4feb, e601187d-2416-478f-bc8a-992c15bc6b4b, f74b7464-696a-4404-ae3a-1e590a0c08ad, f78c80ed-9456-4307-8cb2-6a022592fced]
183893 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
184166 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
184224 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 81
184224 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
184362 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit3759763799676180278/2015/03/16/085e9102-3475-4615-8ca9-d2b5385626d3_1_001.parquet
184373 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit3759763799676180278/2015/03/16/085e9102-3475-4615-8ca9-d2b5385626d3_1_001.parquet
184373 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit3759763799676180278/2015/03/16/085e9102-3475-4615-8ca9-d2b5385626d3_1_001.parquet => [2241e7ab-ddd6-45f8-a382-9bc572e1f17d, 22e2875f-79e6-4985-90f6-750acf2543c0, 2367d626-48eb-4a18-8b5b-033071a01f2f, 29c32fc8-d5e4-422e-9bcb-e01c37e25dc9, 2e1f8676-ca17-4581-9457-76441cba7310, 31fbd5e8-9836-4f76-b4f8-4461943ea26d, 3947ad6f-14c4-4b25-b79c-6b5ebd10e6b6, 3d047523-c86f-4803-9ac4-341756a79e47, 47db7a64-ff3f-4a06-83eb-cc4afbc22788, 497a039b-8ebe-451c-9ed7-f42f34262387, 54a72c9c-5330-42bb-804c-3e91b70ac4d0, 559a20c8-80a9-459e-8537-f970adcdfa09, 5c915f45-5709-4026-b261-eb6282e2643f, 635dc2ee-c00a-4d49-bdc0-b6f1dde7b7c4, 68333408-1654-49a5-8915-80854353767a, 6ab5c1a1-4e7f-4d61-865b-7a5270eaa70e, 75d1a2b8-57a3-4ddf-8550-96e30fd3729b, 7b953be9-43c0-46e7-9f64-7b7b5e959da7, 7d083b71-2ae3-459e-b974-82020d4b2906, 8a77a1d8-640a-4d85-83d7-4316a16a640c, 9ec98396-b127-468d-9dc0-91cb7e23c434, 9f9a004f-45b8-49d8-ae3b-f1e7410e971c, b6cd9532-67db-4020-aa5a-dfebd696f8af, c2632f68-2724-4256-8fae-a80598bfbe85, c82c42b6-9fb4-4cb4-892d-c3c9650fe0e7, e04ed9f8-6acd-4903-a10f-15c1daddeb37, f35efbd6-4766-4f20-b06a-0211167449e5, f570ab21-cb4a-4ad7-9001-f549d96ef3f2, f9c156ba-f092-4fdf-b43f-3777adc272ce]
184390 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet
184399 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet
184400 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet => [077efae5-bc98-4feb-ae58-25967b4b28d8, 0a13b00b-959e-41d1-a9a3-1dd2eed684a2, 11d45c6a-4c15-4710-a008-84e9c30a0ce2, 34edf00b-f31a-4169-a1f3-f03f7629856b, 35b905ce-195b-4852-a6aa-ac38ed8acb96, 3bc924ae-96d4-4b29-bf1b-27b325edb463, 493d24cc-795b-4ecb-83b2-9d42288756d3, 4a61a8cd-eff0-4306-9fee-6b9b9e67492a, 4ab27294-387a-49f8-96bc-deea6d8f6e64, 4c3b8dcb-2e4a-424d-b27f-0f2888305fb6, 56614b02-c769-4c1c-86d4-e1c66c8bd268, 60be879a-712e-4243-96db-c0dcf5132203]
184424 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet
184433 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet
184433 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit3759763799676180278/2016/03/15/2a6639b0-489d-47d9-a849-0928b9f2f050_0_001.parquet => [656c1ff0-461a-4a53-ac82-016466ed4149, 714815a8-1b9f-43dd-9df3-a247e5bc3fc2, 7a9b03ac-570e-4222-b9da-1b5cdaddfb0f, 85344052-accd-47ac-8c4a-75cda54302f1, a2240e69-e4ad-42fe-9be3-bf5a57c5f344, ab3d8197-631a-40b0-ac48-5056a619ce68, ae1464a5-a81c-4db1-bb54-5ea29295da1f, b40ec7e1-c7c4-49bb-92e6-d394b6f91f51, b4bcc046-f973-4a55-879a-f2610a4b06d8, b816482e-7eb5-4285-9b45-c8d89a731dce, bd09eb0a-6b45-460d-9057-8381642a3a25, d04883c5-c65b-499d-b531-b9c34d876aed, d2f006b6-436f-4fc7-9c09-8b049b8c0824, df4e896f-4e78-482f-820f-3783efa62e18, e8d0a8ae-956a-4551-8706-80bf1a48809b, f2961a90-757e-4e30-a049-d72a3c33f92a, fb24f6dd-0889-46d0-9cfc-c1db373fea95, fc96678c-de3d-42db-b52e-83d09802de66]
184447 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 22 for /tmp/junit3759763799676180278/2015/03/17/56efdc04-204b-4ab9-8d80-1c225d65f969_2_001.parquet
184455 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 59 row keys from /tmp/junit3759763799676180278/2015/03/17/56efdc04-204b-4ab9-8d80-1c225d65f969_2_001.parquet
184455 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 22 results, for file /tmp/junit3759763799676180278/2015/03/17/56efdc04-204b-4ab9-8d80-1c225d65f969_2_001.parquet => [100942b0-709a-47c8-bffa-836ebb2de170, 10aa8446-feab-43c4-b5a1-19fb8bc3e32d, 11190067-84bd-468d-970e-1898429e0527, 1866861e-1244-4b75-b5d2-f90a864be0cc, 1891192e-e642-4472-981a-cdf59ca56f30, 1ac3f3f5-13bf-4e74-8c9f-d89b88d4016d, 23929345-8d01-4782-a866-8fadfba2c1b3, 35c2c01b-ea8f-4f17-8ea4-a17a92466a96, 492d8b44-e579-4a08-bdcb-3cc492779858, 60ceeed6-bdc0-4b53-a62f-d62b1f43a5f3, 713745f1-0148-4ca1-bf02-d8b2f302bd5d, 7532645e-742e-45c1-8c24-ab8aeecfe97c, 84c78136-f315-487b-99a1-c34f0666aef8, 92f225e7-3f6f-458c-a255-efe7fa0e9eac, a76e6896-3265-4c14-81c8-751393e0f022, aa542459-58ec-48a3-bb4f-548065c79c4c, ba23e6a9-f6fe-407e-8469-7b9e78142ae2, c6508d1f-4d3d-4009-bcbf-751572cb103a, cc590bc9-3728-4e28-ae91-bcfe0f95c36c, de5ca0cc-5002-4868-a742-3db5afe83468, e1921e11-2535-4c78-8025-4fbc48a7fda3, f74b7464-696a-4404-ae3a-1e590a0c08ad]
184538 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=81}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=22}}}
184550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
184550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=085e9102-3475-4615-8ca9-d2b5385626d3}, 1=BucketInfo {bucketType=UPDATE, fileLoc=2a6639b0-489d-47d9-a849-0928b9f2f050}, 2=BucketInfo {bucketType=UPDATE, fileLoc=56efdc04-204b-4ab9-8d80-1c225d65f969}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{085e9102-3475-4615-8ca9-d2b5385626d3=0, 2a6639b0-489d-47d9-a849-0928b9f2f050=1, 56efdc04-204b-4ab9-8d80-1c225d65f969=2}
184566 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
184566 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
184587 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
184678 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
184679 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_86_0 failed due to an exception
184679 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_86_0 could not be removed as it was not found on disk or in memory
184679 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 41.0 (TID 61)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
184682 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 41.0 (TID 61, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

184682 [task-result-getter-1] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 41.0 failed 1 times; aborting job
184690 [Executor task launch worker-0] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
184691 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_86_1 failed due to an exception
184691 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_86_1 could not be removed as it was not found on disk or in memory
184691 [dispatcher-event-loop-1] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 37380, None),rdd_86_1,StorageLevel(1 replicas),0,0))
184691 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 41.0 (TID 62)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
184759 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
184759 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
184869 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
184869 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
185007 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
185007 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
185119 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
185119 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
185328 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
185328 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
185381 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
185619 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
185619 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
185749 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
185778 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
185814 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
185821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
185821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
185821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
185821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
185821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
185838 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
185838 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
185954 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
185954 [pool-1381-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
185991 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
185991 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
186023 [pool-1381-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
186042 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
186042 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
186095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
186095 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
186207 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
186213 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
186213 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
186293 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
186593 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 139
186594 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
186783 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 46 for /tmp/junit5391345119613166450/2016/09/26/c45ee078-beb2-466d-be18-8db58b8f938d_0_001.parquet
186799 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 100 row keys from /tmp/junit5391345119613166450/2016/09/26/c45ee078-beb2-466d-be18-8db58b8f938d_0_001.parquet
186799 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 46 results, for file /tmp/junit5391345119613166450/2016/09/26/c45ee078-beb2-466d-be18-8db58b8f938d_0_001.parquet => [02c244aa-2e95-4fbd-9a80-77036429d141, 07e89614-372a-42a3-81cc-13c404f118c8, 0bb89e37-6d2b-4020-b7d2-e2b850be4793, 0cc5bdc8-ba34-4ff4-beba-aedae644bb25, 0e759679-aece-4bce-b758-2a15135f9bfb, 1142c53d-3ef1-450b-b5c0-3c640ca3dbf0, 176cb2dd-77cc-45e1-b9be-e244481dc29e, 1afb0770-bc6b-48e5-aaa0-c5b305fe8b65, 1f705928-0054-425f-9b72-7a6656b3fb33, 1fa3873e-2b1a-4694-9ffc-ffc3236a9191, 211ca89d-ae02-4f64-af1b-d8ce0744b900, 24ee71eb-e50a-4130-a8d0-b35cca4c2fe7, 258e9e5d-7aa9-4a98-9f01-dc76fbeef08e, 27b7a720-1c18-431e-b973-993869fc10e1, 29035909-08b0-423f-8170-1aa95f2e9ffe, 2cb4f77f-38a5-4990-815a-a163ef626c15, 2d9e914a-90a5-49ca-93c7-4b8a80e13899, 34b66efb-c2be-4383-a918-7e50c23df6ec, 352e941f-bc32-43af-9105-786550ee1959, 37ed267d-bf2a-4e1a-a5bf-d97486d75924, 39093f8e-9409-40e9-b18f-2d33462f484a, 39a8c6f1-cfdd-41b9-9d95-9b555634cc39, 3dad5156-3d7b-4402-8c71-9cca1b516cea, 3e32224e-14d2-4369-88e0-f3be8ad599ad, 43823d70-3a41-48b0-85d3-d4a7cca1f30d, 494753fb-76cf-4da8-82c1-5c8c36508362, 50dd7c89-158d-4ad8-962e-8878a3a53984, 58b740de-1bb7-4242-9ee8-9c39163f9cf6, 595d1989-8e32-4a18-8fa9-fb3167604034, 5cdc22e8-4ae3-44f6-9853-20c1b621d3bc, 61c94f9d-087d-4ac8-b004-d5039adcd17c, 62e4ff6d-6c14-45af-aa76-b79a49cc0830, 63f6760e-4bfd-4e80-b8d0-ff1e37a94e8e, 64f8e6e2-ecd5-40f8-8509-7b80b3eb2335, 66376f98-d23f-4b6a-9642-7bd23475fa29, 68108b22-8a7c-4a50-ac12-5b13736aafb1, 69f67d35-35cc-4191-b1bb-b0d95154a0b3, 6a52eb7a-4419-409c-8117-08aad420bc3e, 6d08ba15-fa9c-44bc-93ff-8ff4ca2e15b6, 6d390ab9-2449-4b03-b938-0d540302358d, 6d809cbb-7461-4152-8e08-b3cf9b7c8c38, 75320386-7a80-442f-8298-32cd4ef783a6, 7692a63c-180a-43f4-91a2-4b0b7f5e71e3, 780bca1d-f136-4c93-8366-776cea3c1696, 78ade67d-aba8-4180-8cb1-541e9eeed6f0, 797bf584-6a52-4530-9dd9-0070f23bb5be]
186845 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 54 for /tmp/junit5391345119613166450/2016/09/26/c45ee078-beb2-466d-be18-8db58b8f938d_0_001.parquet
186858 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 100 row keys from /tmp/junit5391345119613166450/2016/09/26/c45ee078-beb2-466d-be18-8db58b8f938d_0_001.parquet
186858 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 54 results, for file /tmp/junit5391345119613166450/2016/09/26/c45ee078-beb2-466d-be18-8db58b8f938d_0_001.parquet => [7f7c8a81-8653-4883-afa5-84b7f2e273c7, 8211322e-b4c8-4920-87c8-96c722454462, 828f5a18-d8c4-4a2c-8b60-5b3a9ceec952, 849f0235-746c-4425-9715-f84ac4b47f8c, 87dcd212-991b-4931-91b7-f29cf3246046, 8b1d13f2-0766-4533-abbb-fcfdb1f01bdc, 8d13c2e0-6adb-46f6-b4d1-c3f73d8ce688, 8e0663e5-f985-412d-ad8b-d7499cc8df03, 8fc5b186-34fc-4167-aa54-ea9cf5c9444f, 901e100c-1c9a-44e3-ad08-ca4ef40436d8, 91ca25b0-22af-4151-9148-3fb5f906bea0, 9436b810-4e2c-465e-99ec-9c99f49c87b3, 9711f4d9-4bfa-49b3-8014-83b056292640, a1abbd29-d16a-4620-a2d7-b4ca8006de11, a20ceaa0-e0ec-4720-81fe-b2210ce291d3, a52415da-9103-43f6-865b-b2123f545012, a60f0534-2179-488c-8f14-6a315e42abcd, a626c210-de9c-4c73-a050-2ee24f8a487b, aa6606a6-8cf1-448f-8432-c6ee5d21fa32, aa6ba7cf-1660-42db-81cd-38ff1760bd20, ade5cb74-e9d1-4503-b59f-47df3766d13d, b076ce3b-dfee-4254-a14e-c4043313ad99, b26f4dff-bd29-4198-9ccf-ce5aeaaea7ef, b59ca332-d5ee-4779-b108-46d5647f711f, b5ce20ad-6837-4cbf-b522-b19cc9486690, b916d62a-4979-4944-9f74-de7d096b2d5a, bad7b962-cfc3-48a0-8b5c-644d042084f4, c42614d2-1d11-4fc7-9a57-0b319c0d006c, c5722f9e-b034-4187-a9b5-3b9e93978592, c5a2be85-f275-44f5-ac14-787f65596a67, c64451c6-87a1-41f7-955d-e28ae6ad29a8, ca4efe29-456b-4c00-9ee3-25cfb42a150f, cd1a026d-4bd3-4cff-8138-aa0259822b93, cdb818e8-0779-4529-a36c-ba3f635827fa, d10855ac-22e6-4fc6-945e-c520eeac9b65, d892c7d0-7de3-4627-9758-e9c2b5770b20, db04a776-9eca-40a0-840c-c96ff8bdfc72, de75b150-c102-4115-a4d2-8186c486c5e0, deaf5548-b471-4a26-8364-52c9c415afea, e633fcff-4d7a-481b-9acb-ee2fa409cc64, e8db5ff5-42ed-4b38-bf05-29e31d29a3b7, ea0ccc51-7f03-4ab8-8121-461d1c5e0f0e, ea1fdaa3-fabe-4844-87e3-397599e948b9, eb7ff635-45c4-45fb-9546-dfb8f6d97cf0, eea23c5d-b072-43e9-bdbf-5fe219514dd0, eea7a8da-d587-4f19-9e5a-2aa20db4d92c, ef6d7987-6008-446b-b98c-a89466d6a2d7, ef92e9f3-53dc-4dac-a413-2c3e4ffe337e, f1b069fa-3c50-4196-a588-6bea4b102a79, f92f7eff-4b35-419d-ad14-cd40b457d839, f989d021-1ecc-4b4a-89f9-08773822c844, fb1c7fca-389a-41fb-957f-e4b42142513c, fce8038f-4d93-4d80-bbe3-0df22f8b0604, fe8c7027-34ef-4e72-b052-ba8beade4ddb]
186947 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=40, numUpdates=100}, partitionStat={2016/09/26=WorkloadStat {numInserts=40, numUpdates=100}}}
186958 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 4438
186959 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=c45ee078-beb2-466d-be18-8db58b8f938d}, sizeBytes=443793}]
186959 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 40 inserts to existing update bucket 0
186959 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
186959 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=c45ee078-beb2-466d-be18-8db58b8f938d}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{c45ee078-beb2-466d-be18-8db58b8f938d=0}
186973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
186973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
187042 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
187066 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
187066 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_0 failed due to an exception
187067 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_0 could not be removed as it was not found on disk or in memory
187068 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 50.0 (TID 50)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
187069 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 50.0 (TID 50, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

187070 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 50.0 failed 1 times; aborting job
187161 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
187164 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
187228 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=58, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=78, numUpdates=0}}}
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 58, totalInsertBuckets => 1, recordsPerBucket => 500000
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 78, totalInsertBuckets => 1, recordsPerBucket => 500000
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
187233 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
187244 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
187244 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
187308 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
187308 [pool-1403-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
187325 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
187325 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
187348 [pool-1403-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
187368 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
187368 [pool-1404-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
187389 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
187389 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
187409 [pool-1404-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
187425 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
187425 [pool-1405-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
187451 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
187451 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
187467 [pool-1405-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
188084 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit931322379055617523/.hoodie/.temp/2016-03-15_42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001_3_3.parquet to /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet
188334 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
188512 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit931322379055617523/.hoodie/.temp/2015-03-16_a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001_3_4.parquet to /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
188947 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit931322379055617523/.hoodie/.temp/2015-03-17_b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001_3_5.parquet to /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet
189071 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
189453 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
190764 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
191011 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
191011 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
191101 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
191101 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
191200 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
191285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
191292 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
191292 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
191932 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
191932 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
192081 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 64 for /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet
192096 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet
192096 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 64 results, for file /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet => [01f1d583-1022-4899-9318-ab8880218466, 0d1bc870-e43e-4a7b-92c5-38999f0aa763, 0e118be1-408b-4817-9f39-6fd85002b6a4, 113490ee-2198-4241-a329-4d54a63977e1, 13c45221-2652-49b0-ab95-faca73084b24, 14bb7c35-07bb-4d1b-b429-d70db8789dd3, 17df7f94-c200-4e30-a57c-eba8bd1e72b4, 19f1106f-f58d-4a8f-855b-6a4b50e7b772, 1f4c11a1-bdba-456a-9bb6-483603d72e58, 2d7b7245-93bf-4647-a68a-37db8b192949, 320cdbe1-6107-4bb2-8db1-797de23543de, 33c6bc55-9284-4085-80a2-c5ca83bd1fb6, 36818e9c-3baf-41c5-9a89-956f697281ac, 415ce27c-9bea-4d91-9f9a-136e6e39a903, 439b0e37-cde7-4293-a20e-31ee8221dbce, 4b1a3b27-f688-4185-bacb-abfc87ab2184, 4d18d17a-d808-44ae-ba02-9c529a8e950d, 521acb3e-4e24-46e9-b6e4-997cbdf081d6, 5e66f2cc-73bb-4630-8cba-efa18508ef1b, 5ed8e4db-0683-43a2-8a62-fd1025313f10, 6b852288-b274-4772-a38f-7ad465a739a9, 7540ae41-bbbe-44ba-b3f6-8a2d657d048c, 7957a728-6263-41b8-8c95-9df1830e0c73, 7ad1fd37-91a2-408c-8617-2dc6327a6148, 7ce6536e-7333-4703-91d6-4f8d64012295, 7e537168-6ceb-4d91-857e-cd82450aaf40, 89fe3eda-e19d-4035-8ee0-a1b391db9c50, 8baeb43a-919b-4d34-9758-7bca24776e71, 92c7cbc0-f8ae-4645-89f8-8150e083fd9f, 92d498bd-8cea-45bf-ad32-9b452ed5e5e8, 948d8316-5385-453c-9241-d3fcc333e651, 97769958-43f9-436a-8348-7b5d8c62e1c1, 9857b1b3-9fcf-4023-8fc3-2b5b4a40346a, 9b36aff3-0e9a-4026-a5e4-29f2e4e43108, a9040b26-725a-4acb-85d8-52cb612ebb06, a91d8d87-f123-4f6a-a348-5ef82791f942, aa356abe-ce01-4196-8ebd-3d430f47bded, ad1bca72-0a64-4d87-8a93-ea82806470f7, ad70745d-f61f-4010-9631-f6501810ce17, aea65771-8988-4ab8-84c5-06d899dcfd24, aea87aa9-42fd-45a6-9630-8584c32f851b, b085237d-1d8b-4502-a80a-dca6cc9d3e8e, b7dc812a-1e3e-43d1-8a3d-2156f55cfaf6, ba0bfd6b-c4f7-40cd-ad01-abf413d84864, bb81b777-17ed-4a86-9042-445780e1aaea, c0c01edd-368a-4c19-8c8d-8a0f989d26a7, c6f44481-2eeb-4971-aa4c-5163a235d59c, cc550a00-8193-4631-a15f-24a4c5fa406d, d2d8de7d-d9cf-44fd-aa62-7369a3f6ec85, d52d2815-e667-42d9-812d-09ceeaf144e3, d756735a-ba45-45e2-8c3a-a3a55acc4d5a, dc9c769b-e58f-4dcc-a65c-bd5081c43235, dce6c392-7d78-46c8-b3b1-3cd25cd65bcd, df864924-4b8d-4238-9c33-80f17ed2ba7f, dfc7a5f7-1d2a-4103-90d2-771e6fbd83a6, e2656040-e7b3-433c-9ca1-f3568a62657d, e2c9c7d9-430d-4eb9-88d7-f76f9f4ce6c0, ec193353-2f9b-4ece-873b-de89a8ba1efe, ee07a266-e42a-452f-a386-619c3484b2e2, f412bdfb-aac1-4a61-956f-cca9dab77ef0, f41ffde9-5d5e-4329-8c9d-1745778a3921, fc471b7e-0b81-4785-8be9-0fb856bf70cb, ff2b8eb9-5050-49e6-bebb-813c696bbfd6, ffc28a1b-b28f-4622-a6f4-002693583bcb]
192114 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 58 for /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
192126 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
192126 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 58 results, for file /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet => [02003992-4b35-4341-b93c-e727f47f86e3, 0327326e-21b3-4dab-aaa3-cf1c39f4393b, 0827adb7-0f89-45e3-a062-bad374a9c372, 0a03f6d0-88c6-458e-b0d0-c3f91440b908, 0d731149-be3d-4ad8-a7da-944cdadd58cc, 0da2e6a4-4295-4876-8532-a948558ea714, 0e4836be-2a8e-4643-84a1-cc1ec25fca51, 0edc3742-de07-45c1-8628-b6aa7538c6c0, 143b21c8-a748-41e5-bd29-5cc33f0d24e0, 15c05cb0-2201-4c64-a718-7ad0ce673b43, 1b23885a-5e10-4f40-a06c-049eb064e2d9, 1b596f5f-ffa6-431b-a6e1-3ade643e55a4, 1cc587e5-9ba4-4e3a-973d-f7e7983b6142, 20e7a337-d168-44cc-aa42-f9eceb4ded75, 23468e40-3c0f-4cb4-855b-88f057df3400, 29891a55-6ea9-4faf-bd64-c8834a5839bd, 2a3ebd0a-cd16-45a8-ad80-bca87f0c70ec, 2ba78d2b-232e-4b38-9d2a-c66b668866d1, 36c2ff3b-84c3-4c18-8e37-0467b07f2f74, 3cb5f035-d59d-4266-8fa0-e4d856a6d5c0, 45e7fa75-cea5-4a3b-b3c3-5883051a4d88, 47312670-0511-4526-aa64-a44ff4ba04ad, 4aa1d8ad-80ae-4097-ba35-77514bf5c781, 4c054e7e-0926-4018-8133-f5aec051ca85, 4e65aa2b-f564-4ff4-878c-33c8b741b97c, 53384abb-43fa-435a-92b5-8986d1e48af4, 5717929e-9065-4891-8939-b8a0b577aa65, 5990f5ac-d104-4d6c-ae6c-de886a80c2e0, 622cc401-30f0-4e32-b145-dbe671e2784d, 64acb0ae-52d6-4868-9224-8b2557ba3253, 7a252fea-1b61-429d-bcf5-7cecbf63ac5a, 7d79ec77-ad8c-4e22-b350-ea99365fb151, 7d9e180b-80fe-467e-915c-eaf55d6aaccf, 7e2d0f3b-a454-4b33-929e-7478ddf69f12, 818237de-e430-4660-9db2-fe54b7d90349, 87cef3c1-9a96-4fd6-9da5-ba706f340b82, 8f6f5605-2456-4145-9cd9-c811e754ef91, 901b6518-cd6b-4be4-bf02-c207b6207f82, a05a872c-8cbb-4a61-a233-aeaa2e8d773e, a1adc160-a29c-420e-ad87-c62b7ff45eae, a4097dbe-658b-4c5d-859c-dcf6d520f7c6, a4724c4d-3490-436b-ba9f-6cd764bf47f3, a59207bf-575d-46bd-91d4-401b94fae306, affcfefb-4a3d-4b71-a126-0ddbedfd4a3b, b41538a7-bdfd-49b3-97ff-034957a3d773, bb739551-a549-4870-987e-aba24b7aa1d8, c2c02e87-8cfc-44d7-ad62-cdd43730b750, c8d48022-4154-46f9-93ad-dec4573f2ebf, cfb7b6a2-e19d-4909-9c1d-31736431e76c, daa11f3e-e831-44d2-9ba1-c9fb655a07cb, db09c113-285c-4dea-93f6-ecc502ff325a, dbde27b6-b912-4148-8417-069e2a4a2edf, dbe3a2be-c9e6-4f68-9792-99d3197a2992, ec9ea0b9-c666-4fe4-9b40-8ea83f1a106b, f00db8b9-164e-4f30-a2a1-ed857344f684, f2e7dd63-87ac-44d3-8310-e3522b11b4d7, fa600af2-247e-4d6f-a61c-518927e3021a, fe067721-f70e-4ec8-9fe7-591e828411a2]
192143 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 78 for /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet
192153 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 78 row keys from /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet
192153 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 78 results, for file /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet => [00d17d81-9219-4114-ac3a-1aa52f915e87, 035d9c70-6ab0-4b95-b6fb-edd6db07b7a4, 0569b06f-6438-4f9e-a5c4-2da3f0282dca, 08256999-d29c-44d2-94bd-bd4cebbdc405, 092e96bc-771b-4261-927d-35a39ad929a1, 0cbf2636-d597-4f9d-ab5c-45f7aeb7dc77, 129a590a-f927-4317-8721-faaaa55d6dd2, 131ccffc-c26b-4f6f-8984-70a6283f53e3, 13897b2f-4d2f-40d6-b8d2-e9ccd0b1503f, 15a1535f-0044-4ccf-9fd6-88b52bea5e1e, 15b1a689-32ae-4fef-b2f6-ba8ad784ccc2, 19d5f815-b1ae-4206-956a-d364354a0999, 1a0dae05-3c89-4036-9135-5714e199ede0, 1ca0ded7-6c28-4eef-b43f-a34336d64b33, 25a1125c-8d33-4581-bd06-c0c48acce58f, 2d45869e-db3a-4377-bede-313f8f1428cd, 2e95ee4f-f0fa-435e-a38d-3e6e784efbf1, 3024143b-d79f-4380-8b2a-9aac17404775, 358f074b-a8ea-41ae-b261-44d71ac82ecf, 3a104a94-d1f3-464e-9344-e29ae0c2d263, 450d7867-f254-4837-8911-317bb2a39194, 4614faa2-21c7-4471-9363-772783181f24, 4b09c0da-1c48-4c85-81c4-2f6dee68eb63, 4f14581f-c8d6-4e7f-95c1-290440137851, 5080798d-dba9-4c20-8c3a-d4d99ed61ad8, 54d66938-7822-44f7-b45c-acdaaff3b5f5, 5f5822c9-6d6b-46ba-9787-f32eda221843, 607750f2-6a83-4c34-bd19-f7628f475306, 6439c834-2e24-4814-b733-bf34f43aba19, 64acf5b5-91e2-44a9-b0de-18a4427e232e, 6e67afaf-60f5-448b-bbeb-2ce5a128483c, 717c13ff-5172-4db4-b330-b833dfe39c5f, 759db0be-b24f-462c-98a3-c9d59fd5c13f, 7a4a22f9-1f63-44ab-af15-265f7394b581, 7e34826e-bb47-464d-a09c-8f459a13ff3a, 86fc6d36-ea10-44b3-8fdd-35f030ef9ebd, 8997ddb6-3587-40f9-bbc2-7c4ad16e13f9, 96f8fdc5-51fc-44ed-be9e-9f48341c2904, 9a9cb97f-870f-4912-9875-888dbe0ab6e6, 9af96784-3c3b-4e28-b496-4ef3c8ca8cd2, 9f09999f-2c50-4fe0-960d-53ce6164beff, a1968de8-c53b-42d1-87eb-03fa6dfb983c, a257f575-c4a7-43c0-b89c-d9b5a7271722, a38cd465-0f26-4007-b9b0-f0492b0d94df, a6306dd8-a01b-495c-a793-a387d8130c52, a7be8c2f-0929-4cf0-a6f5-9f25cafc300b, ac367d40-9a29-467e-8de6-ad07efe38383, afe27db9-519d-4286-9ba3-bcb4786f50f7, b217ec77-0bba-4188-ba77-ec24d8848994, b508c966-2ea2-4542-be66-e88e55f4ea5b, b6d51737-7a78-4fa8-8b33-f5aab698eaa2, b7815e47-cf56-42c6-98e4-fe12c5a3fa00, bcda8253-5df9-4fd2-8100-37f79c755b58, bcf4a013-4834-4a58-9467-6ee5b8bb5bb6, bd5a819c-f185-428e-88ba-5aac005dee2b, c127d334-c5d9-465f-8eac-ac440085d6e8, c3d0171c-b72f-430b-b387-675c42034d4b, c57d126f-c684-446e-a19a-32db80fd138b, c6267c5a-58b9-470d-ace7-1288b4974910, d172aeb4-5bf7-4f82-8e1b-9065d8807f00, d342f871-fbed-4049-bae7-9f04655cae26, d5daf01a-4677-4861-984c-037ab2eb804a, d636e645-34c9-4bb5-b910-f751e059319d, d7e0d411-4da5-47a9-920b-e73561a2d207, d8691c28-278e-4e1e-80ae-1ecbbf72d3bb, d8a485c5-e229-4da6-8601-dabe3157fdba, de234499-fe85-4cde-b831-ad0940a0f650, df0e4755-0a45-4bf2-8e5b-5f12518d460b, e32fd249-53e8-4f04-8a9f-3585440e4bbe, e4d32d84-2c25-4c8e-8a7a-f2f66fa22566, e6ad3c73-7329-4463-8b0f-676cc3876a95, eb0b60f8-36b5-4742-aa24-9fbb5a597589, ef13e34f-6cac-48ca-a16d-dbb474a9ca89, f20c28aa-d743-4b93-971a-a0970e27ca02, f64874bd-cf2a-43a9-b6dd-231a0fa36b17, f85aba9f-e54b-4029-b032-b1d5e93607a3, f95d87bf-497a-45b5-8944-ca4cb0923546, fccdbeaa-faed-4b61-a252-f8c6241fba8b]
192201 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
192334 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
192574 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 82
192574 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
192736 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet
192749 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet
192749 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit931322379055617523/2016/03/15/42df5dd4-a6a4-4538-983e-6b050afdc07c_0_001.parquet => [01f1d583-1022-4899-9318-ab8880218466, 0d1bc870-e43e-4a7b-92c5-38999f0aa763, 13c45221-2652-49b0-ab95-faca73084b24, 14bb7c35-07bb-4d1b-b429-d70db8789dd3, 2d7b7245-93bf-4647-a68a-37db8b192949, 33c6bc55-9284-4085-80a2-c5ca83bd1fb6, 36818e9c-3baf-41c5-9a89-956f697281ac, 415ce27c-9bea-4d91-9f9a-136e6e39a903, 4b1a3b27-f688-4185-bacb-abfc87ab2184, 521acb3e-4e24-46e9-b6e4-997cbdf081d6, 7ce6536e-7333-4703-91d6-4f8d64012295, 92d498bd-8cea-45bf-ad32-9b452ed5e5e8, 97769958-43f9-436a-8348-7b5d8c62e1c1, 9857b1b3-9fcf-4023-8fc3-2b5b4a40346a, 9b36aff3-0e9a-4026-a5e4-29f2e4e43108, a91d8d87-f123-4f6a-a348-5ef82791f942, ad70745d-f61f-4010-9631-f6501810ce17, c6f44481-2eeb-4971-aa4c-5163a235d59c, cc550a00-8193-4631-a15f-24a4c5fa406d, d2d8de7d-d9cf-44fd-aa62-7369a3f6ec85, dce6c392-7d78-46c8-b3b1-3cd25cd65bcd, df864924-4b8d-4238-9c33-80f17ed2ba7f, dfc7a5f7-1d2a-4103-90d2-771e6fbd83a6, e2c9c7d9-430d-4eb9-88d7-f76f9f4ce6c0, ec193353-2f9b-4ece-873b-de89a8ba1efe, ee07a266-e42a-452f-a386-619c3484b2e2, f412bdfb-aac1-4a61-956f-cca9dab77ef0, ff2b8eb9-5050-49e6-bebb-813c696bbfd6]
192766 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 13 for /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
192779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
192779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 13 results, for file /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet => [0327326e-21b3-4dab-aaa3-cf1c39f4393b, 0d731149-be3d-4ad8-a7da-944cdadd58cc, 0da2e6a4-4295-4876-8532-a948558ea714, 0edc3742-de07-45c1-8628-b6aa7538c6c0, 15c05cb0-2201-4c64-a718-7ad0ce673b43, 1cc587e5-9ba4-4e3a-973d-f7e7983b6142, 20e7a337-d168-44cc-aa42-f9eceb4ded75, 29891a55-6ea9-4faf-bd64-c8834a5839bd, 2a3ebd0a-cd16-45a8-ad80-bca87f0c70ec, 36c2ff3b-84c3-4c18-8e37-0467b07f2f74, 45e7fa75-cea5-4a3b-b3c3-5883051a4d88, 53384abb-43fa-435a-92b5-8986d1e48af4, 5717929e-9065-4891-8939-b8a0b577aa65]
192806 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
192817 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet
192817 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit931322379055617523/2015/03/16/a04c4df0-d212-4fff-ad89-6b7ff295fc7b_1_001.parquet => [64acb0ae-52d6-4868-9224-8b2557ba3253, 7d79ec77-ad8c-4e22-b350-ea99365fb151, a1adc160-a29c-420e-ad87-c62b7ff45eae, a59207bf-575d-46bd-91d4-401b94fae306, bb739551-a549-4870-987e-aba24b7aa1d8, dbde27b6-b912-4148-8417-069e2a4a2edf, f00db8b9-164e-4f30-a2a1-ed857344f684]
192835 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet
192841 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
192848 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 78 row keys from /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet
192848 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit931322379055617523/2015/03/17/b54f4896-a1e0-495e-9f35-35e9cd9c861a_2_001.parquet => [00d17d81-9219-4114-ac3a-1aa52f915e87, 035d9c70-6ab0-4b95-b6fb-edd6db07b7a4, 092e96bc-771b-4261-927d-35a39ad929a1, 15a1535f-0044-4ccf-9fd6-88b52bea5e1e, 15b1a689-32ae-4fef-b2f6-ba8ad784ccc2, 1a0dae05-3c89-4036-9135-5714e199ede0, 25a1125c-8d33-4581-bd06-c0c48acce58f, 2d45869e-db3a-4377-bede-313f8f1428cd, 2e95ee4f-f0fa-435e-a38d-3e6e784efbf1, 358f074b-a8ea-41ae-b261-44d71ac82ecf, 450d7867-f254-4837-8911-317bb2a39194, 4614faa2-21c7-4471-9363-772783181f24, 5080798d-dba9-4c20-8c3a-d4d99ed61ad8, 6439c834-2e24-4814-b733-bf34f43aba19, 717c13ff-5172-4db4-b330-b833dfe39c5f, 759db0be-b24f-462c-98a3-c9d59fd5c13f, 7a4a22f9-1f63-44ab-af15-265f7394b581, 9f09999f-2c50-4fe0-960d-53ce6164beff, a6306dd8-a01b-495c-a793-a387d8130c52, a7be8c2f-0929-4cf0-a6f5-9f25cafc300b, afe27db9-519d-4286-9ba3-bcb4786f50f7, b217ec77-0bba-4188-ba77-ec24d8848994, b508c966-2ea2-4542-be66-e88e55f4ea5b, bd5a819c-f185-428e-88ba-5aac005dee2b, c127d334-c5d9-465f-8eac-ac440085d6e8, c3d0171c-b72f-430b-b387-675c42034d4b, d342f871-fbed-4049-bae7-9f04655cae26, d5daf01a-4677-4861-984c-037ab2eb804a, d8691c28-278e-4e1e-80ae-1ecbbf72d3bb, e32fd249-53e8-4f04-8a9f-3585440e4bbe, e4d32d84-2c25-4c8e-8a7a-f2f66fa22566, f64874bd-cf2a-43a9-b6dd-231a0fa36b17, f95d87bf-497a-45b5-8944-ca4cb0923546, fccdbeaa-faed-4b61-a252-f8c6241fba8b]
192910 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=82}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=20}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=34}}}
192921 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6615
192921 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=42df5dd4-a6a4-4538-983e-6b050afdc07c}, 1=BucketInfo {bucketType=UPDATE, fileLoc=a04c4df0-d212-4fff-ad89-6b7ff295fc7b}, 2=BucketInfo {bucketType=UPDATE, fileLoc=b54f4896-a1e0-495e-9f35-35e9cd9c861a}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{42df5dd4-a6a4-4538-983e-6b050afdc07c=0, a04c4df0-d212-4fff-ad89-6b7ff295fc7b=1, b54f4896-a1e0-495e-9f35-35e9cd9c861a=2}
192934 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
192934 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
193044 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
193045 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_90_0 failed due to an exception
193045 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_90_0 could not be removed as it was not found on disk or in memory
193045 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 43.0 (TID 461)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
193048 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 43.0 (TID 461, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

193048 [task-result-getter-1] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 43.0 failed 1 times; aborting job
193059 [Executor task launch worker-0] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
193060 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_90_1 failed due to an exception
193060 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_90_1 could not be removed as it was not found on disk or in memory
193060 [dispatcher-event-loop-22] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 39960, None),rdd_90_1,StorageLevel(1 replicas),0,0))
193060 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 43.0 (TID 462)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
193231 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02, 2016/06/02], with policy KEEP_LATEST_COMMITS
193231 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
193526 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082300
193568 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
193736 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=146, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=180, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=174, numUpdates=0}}}
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 146, totalInsertBuckets => 1, recordsPerBucket => 500000
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 180, totalInsertBuckets => 1, recordsPerBucket => 500000
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 174, totalInsertBuckets => 1, recordsPerBucket => 500000
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
193743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
193759 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082300
193759 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082300
193959 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
193959 [pool-1451-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
194018 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
194018 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
194043 [pool-1451-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
194066 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
194066 [pool-1452-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
194149 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
194150 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
194168 [pool-1452-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
194194 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
194194 [pool-1453-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
194260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
194260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
194276 [pool-1453-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
194294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
194294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
194384 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
194384 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
194554 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
194561 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082300 as complete
194561 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082300
194693 [dispatcher-event-loop-15] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 9 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
194917 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
194989 [dispatcher-event-loop-10] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 13 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
195008 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 500
195008 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
195064 [dispatcher-event-loop-18] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 14 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
195099 [dispatcher-event-loop-22] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 16 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
195202 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 146 for /tmp/junit8381636235460367963/2016/03/15/0903d041-8afc-4f19-820b-9ec207c7eff5_0_20180316082300.parquet
195213 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit8381636235460367963/2016/03/15/0903d041-8afc-4f19-820b-9ec207c7eff5_0_20180316082300.parquet
195214 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 146 results, for file /tmp/junit8381636235460367963/2016/03/15/0903d041-8afc-4f19-820b-9ec207c7eff5_0_20180316082300.parquet => [0142d0ac-50ba-4e29-92ba-9d7349447c7b, 02fd36d1-b574-4010-be10-5cd5916e9279, 061bda4a-381a-42a0-bd90-a9461c184fe2, 06e93633-92fc-4ba3-af8d-00b8c792d9b2, 0755321b-084a-42c5-9364-af748f756288, 07f524c4-6b4c-4c42-a904-2b1c87b00921, 09b6b593-204e-40d2-b40b-b6c8f9c86913, 0c7e1b58-82b8-4cac-b899-e622fea9c5da, 0f703d1c-703d-4dfc-8646-e48dc4baf9fa, 12265b0d-1ed0-47ac-ad9c-810866dfea89, 12274212-e3ad-4e25-b3d4-fe4eeac4f316, 171043d9-a9d1-418d-9800-9a26b9417aff, 1e4dfa78-26c3-46a0-9d9d-a02522e9675b, 20304dd7-45e1-418b-806b-96bffc0cd5b6, 22059a4f-2850-42e5-8ae0-cb7a50fee513, 23b487e5-6062-44a6-b36a-4ecd06419d91, 24059c15-f2d3-432a-8a79-e53519910e5d, 2447ebc3-0ebd-4e79-ab69-75c1f5dea766, 254c8a46-5c19-4a61-989c-8681c19cc4ee, 25801be5-97d5-40da-8afc-4b8ebd1a6a11, 265dab77-62e6-40e0-9fc3-1062e5ccd606, 2772750d-7c1b-4bc4-bc47-f053e84ea473, 289a2251-a914-487a-a1c0-9b5709d1189e, 28d45ea9-7a1f-44fc-901c-fbec19f2babb, 2c22226e-d221-4d8c-a869-3128810eb9d6, 2d66125b-4e08-4187-8017-eefafe34fd79, 2eb31eb1-b93b-40dc-98ae-6777bd6f2717, 2fe7d527-19ab-4c9f-b06f-5be2d90c4ed9, 302f2856-e4d6-4d6e-a201-8b276925f32a, 340654ab-516c-44c6-9e07-6defab6256cc, 352c33da-b1d6-40e4-9940-2a7823f8f555, 364e693a-4ad6-4846-9e60-8006e15f5ce4, 3682efe8-6a5c-4528-a47f-dc23dde2e1e7, 3889e865-c5fd-405b-84f0-11ece99f434c, 3c5a3cd1-e853-4bff-9df0-9f2c87d59398, 3c716be4-9b7b-4309-947b-1ef30801cd9d, 3cd035a6-599a-4304-b078-6e301f8fe262, 3d07205c-d486-43cc-91a1-51082e763f36, 3d9a99e7-3c4c-4aac-a3cd-44e0a5fb8220, 40f92de6-8f76-4b64-b900-46aeba11b06b, 41aee4d6-98ba-459a-818b-54cf90557d24, 41c2d4eb-147c-4f3c-9547-61cd57712a77, 4508fa53-9ffc-45c2-a333-42ab724eb06b, 480ef725-ef41-4c31-b6a9-eef8f77fcea6, 48d7651e-4b1a-48b4-b757-7df8374a4eda, 49d2fb85-8724-49e3-8f61-41ce86fa79c8, 4a03e3e3-8cee-485d-a2c4-aaab6a00ad18, 4b729e53-b737-4ded-9603-01bc06da70f8, 4d64a372-2687-4b52-bbaf-f5aa8a5c335c, 4fe5d1ff-b9a1-448a-aff5-a43e4026cea7, 508f0453-acb7-48a0-926e-92f7384b056d, 5481e734-4f55-49ff-8ad4-2360caba708f, 55c8f4e8-06fa-4144-bc1c-f3a6f5876c90, 5bc6304c-fac4-470d-9052-c6187e578cbf, 5ce48457-7bfd-4aaa-b5a3-ce9aa872dced, 5ce7fd19-b2f1-41c8-9466-d0d502018d89, 5dfaff5f-9247-43af-a0f5-2c088fa49ecb, 5e32b7e0-37a2-49ed-b86e-b68c09d6c090, 642a0fc4-0f08-4706-9044-ad853810d9fb, 64ede577-ea47-43d9-bfb0-5f5c3dc90c04, 6548066f-9cbb-4956-873f-7bdc55c0ccb4, 656e395b-8362-4f4d-a195-ee04f2b85400, 6585ddbe-cd4a-4b98-9f9c-95622851051e, 6dde14f9-b316-404e-ad75-4c8ce53d37c0, 7077e150-659a-47bf-b26c-2f82f0b41257, 768acd1c-2e35-4691-81c5-94bc21a753ca, 76a70225-c251-4939-9681-e4717cbf5b9c, 7ce8fde9-3312-4fdb-afe1-d42357943b9e, 7ed36fec-000c-441f-8c86-c50d7b2c2864, 7f1417e2-b0bc-4d3f-9a62-d5746859c50a, 80fb0fe6-1287-4039-87c8-4ebfb7313cc7, 812fcdd1-e6b7-40c7-b4af-7672682149e2, 8430a8dd-9232-43ba-82ab-2bba6b0f7b51, 8453d668-edac-459f-8fa5-bfbca66446e0, 8473d5ae-b5f6-492a-a8bb-ce6a21935af6, 848b3613-42e4-42c7-88f4-70cc6535b812, 84ca45bf-abc4-45cc-af5f-702c217f4017, 863fd0f8-9bee-4c57-b133-568e6571159f, 87d6a1da-7617-4652-a837-88885462db59, 8ac8802b-f82b-4752-8e3f-e6f0591d19eb, 8f36120b-577e-421c-b63c-6dd23fbf7f4f, 9118e98e-074f-411d-a6b4-14b581cd5ca5, 93c29703-bcb0-41af-8837-806bf534103c, 951d38d8-73ae-4bf0-9105-e017e7322314, 977ae9a3-eb9b-4e60-8254-b47383c4d9de, 97b9e758-5736-475f-a4ba-419dfe1027a9, 99963c9e-fd60-489c-93c9-14d6f0472cc9, 99bca778-916f-4812-b460-9d1dcd04a6bc, 9b31ee01-7a4e-4eea-88e9-c5e069ca638d, 9d162533-31ff-4c0f-9cf8-2ff10c88cea5, 9d26fbab-7aa6-47f7-879e-7015c93c00f1, 9dec7413-9088-4282-bf31-4ffd9cb60f5a, 9dfad830-5d0d-43ab-a3bc-faf7df23622f, 9ea39f8c-430f-4ae3-96f6-4f9af776b981, a21f0c73-785a-480f-a44c-6554a6d5618e, a7b9fcef-be64-48f0-ae13-15b8c4a831cd, a94f2fd3-9714-4181-a52e-fc27f76e4949, ab59cae3-94ee-46af-b6a7-3e396bf5ccda, b195373a-7b1c-4bd2-991d-3090d595b7aa, b2839271-82c3-4252-bc14-e2aa79301f3b, b41542e1-8474-4886-8428-7f93632cf5c7, b4d23d9e-7b84-4ead-8beb-2c1656990c64, b53d0c67-8cc3-4be7-bbbe-0dbc2135d9e5, b6912538-57b0-4f1f-ad6b-56683588be64, b7a01a45-e54e-4f7a-8df4-2566b30a4298, b94f7d9a-97e1-4f5b-9352-a8478627dcee, b9dc901b-2c87-450d-a3c5-47631f5c90e5, ba45a452-cb51-4595-8495-1d56212d263a, be06262c-17c4-44ac-b38a-9a6f9ecc21f4, be16b8b5-8358-435e-bd03-cab9930aad5b, bf25da6a-3fe4-4e57-8f8c-1048d9dfb1ab, c31b02dc-6de9-4595-840c-15a57e103126, c4aefd0b-0862-495f-b52d-5f89e56f0693, c5d3312f-27e8-4cc8-b305-3729d1b4799d, cab049b7-853f-4833-99d9-7074af82d408, cb1832b1-5ade-4b40-baff-1b157a0955a2, cb878884-6dcf-40f5-a0c7-8fcddfe73953, cd25306c-f47c-4bdd-9968-cedf45d88ca7, d5a135ea-8a47-496f-bbca-343c43cc489e, d68d076a-2910-46a6-9cc4-495f9c369e36, d79cb7ae-2470-4842-85f7-c84c50c01998, d7b43e7f-c843-4792-87f9-7927277fa174, d8b842b0-db6e-4eb6-8854-618986a59a64, d8fa721c-3c41-409f-9bf4-e8064b038c48, da43a043-558e-4ecb-b67b-5378aea9f0dc, db666ebd-4f4b-49ab-a1f6-6a21a76dec28, de3fbe4a-548e-4ad4-8e0d-92b6fa42f238, df6d3d04-91a8-41be-9be2-2911f41cc224, e4e15fb8-263c-40a4-8298-bedcdd09caf0, e548634e-012f-4fdb-a5a7-2bd2d3e9d177, e684ff93-5bd2-4671-81a0-0427aecd6c41, e920e9e4-b1bf-4ca0-a525-e1c399048233, ec0cc578-ebaa-49a6-b3f3-662d763df975, ecb98d78-5c0e-4721-a608-b7ef7d6509f6, ee874e63-c9de-4749-8cd2-2588b4de5847, f40434cc-7ef2-41c3-9d1d-b0f40d031527, f4e26247-6a2e-40df-8615-4b8c704c709e, f5ca68c4-dcb6-4662-abfc-775999c6c14d, f60bedb1-f0b2-4fe0-a97e-8b2074e7349f, f6e21675-0df0-4b34-bbb9-9130424bcb38, f8819b5c-edb2-4942-848a-a2f2f90f3564, fbf2edc1-193e-4503-b375-f0822929bf04, fc979b60-0668-48cd-a32b-f9e400b40e97, fcb8326f-9e7c-4ecf-98ab-23c0e05f1031, fccf6ae1-2cb1-46c7-b3ea-f9218b79ca71, ff8a4caf-2c8c-42c1-915b-32c72d206023]
195234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 174 for /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet
195246 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 174 row keys from /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet
195247 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 174 results, for file /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet => [009b277a-9c93-47d9-8489-55d37edd131a, 01ead285-92fa-4448-97ad-864ea85114e3, 02e191a3-2edd-4847-902a-f9c190b71a7f, 0316b828-6432-47b7-9841-a28f29de0551, 0332efe0-954f-4328-a88d-cbb9f5500ea7, 095ef82a-536c-4e0d-8a05-ba57167a7ddd, 0a4d2a2a-de9e-4eac-8ec2-529b42660f9a, 0c7f4e0f-5830-438e-bdac-d90782172d1c, 0cac82b8-abe8-4716-8b34-f30dd6481be7, 0f199eaf-589e-4405-a6b7-c73ad1942cec, 0f869090-ebff-48f2-8068-b8c1a6b40230, 0fb45370-5c2d-4502-a917-00763792de3a, 118e516b-b60f-4e9d-991c-a8aaa07dda40, 12db4a68-492a-4184-b721-ee75cd4cb317, 1446f804-79ff-491a-8c00-114de3055cb0, 150da65b-b952-45be-bb04-916e368b25bc, 1714c50d-f948-4334-a1ee-19118231912d, 184a1af0-2d6b-48c9-b7d7-1f2bd320d58f, 18d9d4b9-b2b2-4dc5-9996-6b3bbae7a1f1, 1b1a4f83-66c2-4547-bd98-1c9496908fe7, 1c89fd21-16af-4809-b1b4-04c538bc8d98, 1d607600-3afb-47e1-851d-08161dd87331, 200c1d2c-23fd-4ce9-9e64-20f2a014a63b, 217a2d32-d079-4441-a4da-e749eb2f1bfc, 22bc88dd-b29e-4f58-b0dc-4a86a1f6fbee, 25c0b818-c9b9-49ed-b567-8cda97368fd5, 25d5de28-967e-4ee9-b788-c840667c9981, 27c9be59-fd5e-408c-ad64-00b729eb551d, 2c983318-de43-4f88-8ab4-9b339c6d1bfd, 2eb97aab-3fee-4f78-af38-07cacb2a2934, 2f30bf8f-2858-403e-bbbf-4c3a85fb3752, 2f9c2ae3-95cd-44be-b87e-7d88ba51428e, 2fc59113-39f4-429e-861d-7e281f36306f, 31c15cd7-2e1d-4239-8b05-e8770a87b1c3, 326bc173-9b35-4561-acf7-7cdcaa8829ce, 34f7e72f-4867-4066-b4f9-90bd4a5ea8be, 35c1d3b3-1c95-4ea2-b210-10086a799283, 36af496e-fb1c-4aee-8eb9-0bec22df4f96, 37c9c8c5-659b-4ee9-81b3-31881c4501c0, 3915c09c-6450-4b33-966c-c43f7bb42d15, 3ab7c7ca-5983-4307-b6aa-dd8edd6b0159, 3bac5578-9656-4098-b7c1-e8af5c51b1d3, 3c28a042-4d53-407f-89a6-40f4718f26c6, 3ed44492-e974-4d4f-8791-9d21e7a9961d, 3ef9df41-3392-4386-964a-eeb09d1def5c, 406ae874-c6e3-41ae-ae11-e99b000669b3, 40d590cb-04b3-4ae0-a3f7-457ed8545fc3, 415c64df-ca1d-4928-afa3-8b475c705672, 4379abe0-8294-4995-97be-a43cded3677a, 43c722b2-402d-47a8-8eb1-c3baf4ec9cc2, 43e6a88d-fcef-4f27-867d-9f64e40b0ff7, 477103bf-83cf-414d-91d8-6b39d768d5be, 48a87a49-f87f-4df7-b217-199d17f6b1c6, 492bd6e2-d94e-431f-b070-4885c26c6fb3, 4dc90c02-8ae7-4098-9501-78c1327d9209, 4e7d2d3d-532c-46a6-bb1c-ed3f8cd9a387, 510fc44a-1f9d-4d24-95ac-ed7869ecdb77, 5215b1c0-42a9-4b3a-ba16-fcc97c1af72c, 5216cd39-3cac-41ca-81b4-818da642c813, 52f7efa2-732d-4868-8875-df9676a0ea20, 5993f85c-7fa4-478b-b4cd-c6ae05937d48, 5ad524f7-ba2b-4c52-b930-46c8ec687bf8, 5afb6e1e-a817-4994-acca-1aa7bbbff9bc, 5b00407e-aa41-4161-bcfa-31e954aed29f, 5baadf27-2dd4-465c-8db8-9dbf9344d0e3, 61b1c0e4-f837-40bc-8396-ddf40f6d55dc, 62e91805-7f04-4dde-aa30-04da6b69d188, 6368b41c-2bd9-4133-828a-b8773f3bc4c5, 63da4a87-2bc8-4cb5-af8b-a7ebbe20ce7a, 6567d9ef-fa57-437f-8b13-542d3766fcae, 67a5972b-4a30-4dec-8552-02fd9d703827, 6962ded8-a2a6-459c-898a-1e3a71a75214, 6b2552db-4c0e-4dd4-af15-534952f8ad9e, 6dc864f1-a015-4d8f-bb83-d946ffa3f8c4, 6ee8bcbb-05cd-44ba-a113-d1006d737fdb, 6ef23917-82f0-4e01-a369-0c6dbe7baf4d, 6fdf67c4-8108-445b-ad0b-85fc2ef1e559, 70a46f75-6a9e-4364-b62b-bf75736a2c59, 70efb546-380a-4980-bedd-9f79a82b5537, 750d5681-694f-4835-b83f-57a6181a0abb, 75939929-62ab-4ad2-aa3d-2755d326d219, 7910917f-6483-4b69-a08f-046d8844bd8a, 7a399e7f-5606-48ee-a5bb-55adacc39b21, 7ae1c09e-cb20-4799-b459-6cc8acc3d5da, 7d1081d2-739d-4960-b253-1bddc1630261, 7e4dcdf8-5083-484c-807f-b4bf7a40eab4, 7ebba231-2c1f-4162-933a-2eca04ae838b, 7edc8429-b5b4-43d4-9fd5-e85f5615e4d1, 7f766cd1-a871-4cc4-a3eb-10595be792ff, 7f945cb6-ba2a-42ab-a0e6-b9f8698aa634, 80860419-74ff-41da-bae4-af36cce20bdf, 80a6083f-49a9-4b72-bf0e-98c8113b056c, 81564538-d97a-4b31-9920-2e535af46176, 82ab8487-1e4e-49eb-8bf7-c976318c7999, 84314200-7973-478f-9b40-56f1025de0f0, 875158a6-bdce-4c63-8e1a-9521ec750908, 87accc8d-8842-41ad-b844-cbdeeffdb382, 88a47f68-59bb-4ff0-8992-33f32ff3d205, 8b240534-1d56-4e05-9505-437623231a06, 8b3223fa-e926-40be-8a0e-025941abaf11, 8b338c16-883f-47ac-b93b-694afef99976, 8f38be11-d012-44f2-91b7-f3652774a213, 8f59d541-ea8e-497f-9cb8-f7b90870d22a, 90cb5912-234d-4c96-b34e-9292e9d53dec, 92215809-a1cb-470d-a991-64054184edc3, 93116790-76ab-455d-b87e-268ce6f75753, 937b60d2-e0e4-4bdc-b94b-87127da21861, 954f6fd9-d9d9-488a-8130-bb2810aba23b, 9684746d-474e-4be8-bb0c-a5bac521b812, 99a2aadc-e86d-4f60-ae46-7f8cbab2c315, 99d880c3-cb04-4107-9f25-1ece2c24508f, 9a6815c6-35e6-4e18-8cab-f4c77873de09, 9c2abca1-426d-4468-9960-88d62706f72c, 9d6275c3-078a-4b49-9e94-3c5f489e81dc, 9e2b6cc1-ab11-47ee-a802-8de22ef98fd2, a025b7ff-4d23-480f-bc76-bb8d5c5d1e7f, a0420bd9-56f8-4d04-8279-ff7aed0cb66e, a303acaf-db09-49e1-8b5e-3511a6c4b699, a305a400-a1c2-4476-9647-c9855229c7b3, a474b925-35d0-468e-9663-c6c40347c4f9, a9428b61-69d9-4b11-8105-1c7a1795ca47, a982531f-3966-4f30-9a3f-cca14df4b2e0, a9dcf452-5c2b-4652-9a3d-b41fc67c422a, aa14cb9b-2c22-4c5b-bcfe-98a8e13bb2f4, ae2d9f1a-a265-421c-81ca-27a7b9fa1f89, affe376e-c199-4b49-9b96-8b28628bf622, b0513bf9-87ed-4a67-b0c6-8d631d678c7b, b07150bc-2d24-4d5e-85bd-aa532ddd5357, b54f373d-b76b-4951-b32d-00eb6e1067fc, b775524c-7a45-44ab-b27f-bce1e65c7ce9, b7b488d4-e599-4574-9ea9-1ef634492b0e, b981a332-4848-4d0e-8f47-af9487ed09e8, ba37f0b5-5f29-4525-b5c5-3f92862eed73, bce0d6f0-faf8-43b3-9c6d-a3e7452262c7, be046aa7-fa4b-4ecf-8750-f303942c6a17, c054332c-98f7-4937-92dd-96137b0710bb, c106269a-b6ce-4704-aeeb-5a6653a5a288, c1243ca8-b4ad-42aa-b0be-771db8c5e88b, c5150cad-687d-466f-81b6-49ea89cc9664, c5e72149-9716-4ba6-a51e-bd5761ae8b0f, c6b06335-1ad6-45a2-8b65-2b830cb3bdfc, c9ccc41f-eced-44b5-809d-ca5f6ee17056, ca3f51ac-5a52-4cd2-9691-b38419e73150, cbda16ed-ae26-4e5f-a940-7bc233d7f131, cc232792-a2a7-4d16-8366-1203302cb076, cc40b807-3c67-4b16-816c-2344348438fe, ccb4ab26-cbc1-4eb3-ae7c-9d5fd6c09160, cf3eed0e-f668-45c7-8300-75be3435d629, d1a33f1e-bbeb-41e3-92ab-1b5fd9be723d, d1efc3a5-652b-4ec4-bb23-b9a240028379, d58b1f10-d289-4c38-983a-f686f8290b38, d9229003-ba66-4a17-95ae-2b8d3b4c88e1, d95c566d-b375-44f5-b887-3aa06551d217, dac67fe7-e899-4f6c-9338-dd25e69c4f16, daf4a502-11ed-41be-b761-634e8a0a99b9, dcad33b6-ae49-4912-9541-691942b1c258, ddf77290-4828-449c-93e8-2836844b361f, de319f1a-ee4f-44e4-aca6-8ae45a9b56a8, de86989b-327a-407b-89c6-88c184096ab8, de94ad7e-10fc-4a91-b622-dc037eea7619, e02ab6df-f436-4ee4-a615-1e930c277d8c, e1831c5a-5111-40c8-9152-dde7dab3a6c3, e2cfad30-7fcf-409f-9968-8ae76e33b033, e4b7c08f-2b98-453e-89fc-453aa23904b3, e4bbba53-7202-4ef1-b7cf-f35c9f10ded5, e6560cf8-942b-4b48-9774-88fb88df1912, e7676918-2b16-4751-b26c-8334ca3e1fae, e8639564-155b-4e88-9f7a-c0b50c3fd8b6, e8dade5f-d9e2-464e-ab66-2cd6a7946b31, edb84f97-46bd-435e-86a0-b8c1871e21de, eff611ec-c763-4142-9a2a-aa6812f754a4, f62249da-b286-4743-b2f1-fdcb8f838e75, fb7ab988-44b1-4caa-8855-141a98e99528, fe1ba749-e603-43dd-9794-a277ae9f7393]
195268 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 180 for /tmp/junit8381636235460367963/2015/03/16/c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9_1_20180316082300.parquet
195281 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 180 row keys from /tmp/junit8381636235460367963/2015/03/16/c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9_1_20180316082300.parquet
195281 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 180 results, for file /tmp/junit8381636235460367963/2015/03/16/c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9_1_20180316082300.parquet => [00456224-8fbc-4b9f-b43d-60dc10936dd7, 0476f8b7-0a8b-4cff-9546-88a1b9b2df26, 052d8665-2a19-4ffa-a258-b63ef681ec89, 06003691-e075-4e3a-a1d5-b8a854a5c1c7, 06a23a93-8b28-481a-b6e5-c63e0dbd8d4b, 07ca2ff5-42d9-4cf9-ab25-8fd33574071c, 0a7d717a-a69b-4e1c-89a6-ad542fce7e86, 0c26297c-382e-45d2-8690-b714605efdfc, 0d2cc976-05e7-4f7b-ae0b-e7110e3b1209, 0deaa269-4081-4599-a805-6e6e956d1a50, 0e54ab12-ffee-4334-b48d-931c628ee3a0, 0f6bbbc0-c1b4-4505-9094-be4110109613, 0f851f5e-d797-4cda-afed-2b77069b6524, 0fc3fa0e-d31b-4dde-9558-04696f893ca7, 0fd16256-9570-4189-9e88-b11223eea937, 100888eb-2ad8-469e-8421-a62909f9983d, 12a66b23-deaa-4d90-a65e-2a14942ef001, 153218b2-c037-4e62-becf-3756543801c1, 16c3786b-2039-46e3-9a06-9060c7cd33b1, 17591eb6-5c4a-475b-90c5-0c05a95eb33b, 1806c5b7-1de7-4a84-90a4-ff42038fb9ac, 186d4178-b848-45c3-a20a-a3da95776812, 19814da0-5e9f-4c0b-8e71-953591e0f9e4, 1a4d0100-ce04-4445-b0b4-7a9792fb6f34, 1b0365fe-6e3e-4727-a531-7ca550824b7c, 1b413c2b-8591-4713-a672-251d8cc26989, 1b8e8590-502d-4ba3-be5c-215a1ff6a058, 1c7e069c-5561-41e0-ac20-d8f83051915c, 1d8c8aa0-675f-42f8-8a1e-cbc64064a21c, 1f94b4e7-2672-4d4c-a25f-f1a9145d8265, 20b25560-977b-40fb-a9d7-6e9539e26b17, 2217b5a2-f801-4eb7-9ab1-5d438b8a90b0, 2219e190-c1f0-4bf0-99e9-b2bf976faa1b, 22645f30-21a5-4072-8ae5-e0f994ba0d2e, 232c2a7a-d920-4e74-942e-8a4d5a8beefa, 237bd557-51ef-4c51-b338-97af65d3773f, 2403773b-a249-4b79-ad39-6aecd68e629f, 242d974e-0468-44b5-8c58-c9abb3779a25, 245a04ce-9dee-4f92-a071-59f70d243f4c, 24612263-6570-4ee4-8a4d-e24dd3b74c91, 26ee26e8-5a3f-43a0-abf6-503140e5f8e6, 27f8b0a1-378b-41f8-85f5-7b9239e3cab8, 28a35da5-eb52-48a9-880d-a2aca8657588, 2bacc869-28f2-4c6c-a307-0db670e6682e, 2c5d3b02-bf18-4d1e-8c91-a668c2302d0f, 2c669698-7504-41e6-b9c9-c56d22c80340, 2cae4975-d832-4dac-a5a3-b0d14d45dbbd, 2e6699bb-4af6-4477-8813-e2dfbad10b50, 3038f716-4128-4e7f-8224-7b1f5e8f958d, 33a7281c-3423-4933-a834-18e3db0b6ac0, 34a71397-cf88-411e-9cbf-fe1cdb2bda4a, 34cd9a4d-5611-4713-afd4-d614f5985475, 35274cd3-99fa-4673-ac67-b4b335f847e4, 36bc143a-8429-466b-88da-379115f2be51, 379fd6d5-64c5-4e15-9e12-105218edfe31, 3872749d-d8d4-4138-80df-a24f46759f51, 3887610a-b174-48b3-a6da-1a58a80721ae, 38ea047e-8e43-4c98-a8e1-b9ee206b37f9, 3a089bb8-884e-4f5c-b96d-3a0ad72a2ed9, 3dafb974-7c52-4766-8778-5d8a301f8e5c, 3e99fde8-5809-47da-affb-da526091e846, 3f004141-fccd-41a7-8de2-c9e129faa596, 4061fb6e-cbed-4983-9365-b61f3db9ca04, 41a6c66c-e670-41f6-b1a6-c8d39b8bbbac, 41f7cb27-6124-4067-83bb-535bdc56135c, 42fdff09-d344-49dd-932c-16662efddcd4, 4591b9ed-fdf4-47f2-9f97-cfbcc401a4fc, 478b35f1-ddad-432b-aaa5-02c4abb272e8, 487ba7df-8fc7-49f8-83fd-fd4f67850d84, 4951db1c-da9c-4332-896e-65766f6c2ea9, 4982d219-4ac7-4643-a187-e82ce45e224a, 4cd0468b-5c21-4db5-bf21-01f581db48f3, 4d8caea3-a067-4e8b-bdbe-895b941c0da0, 4ed6b394-fd29-4d11-b029-375af14730e0, 51f0f4b5-56ef-47a3-9ef0-6d682b5ed20d, 52d07571-778a-48c1-aa42-db5b9c8f2bed, 55a99059-a440-4efb-924b-9248c3126744, 56457f95-e2d0-4255-b004-09adb16bd193, 56ec3ac4-df56-4dfe-878b-3b7f5557767d, 58375a0f-fa1a-470f-baea-d1c17ec3083b, 596abb08-73ea-4fb6-966d-905e55e4032d, 5b57b45b-0b05-4da8-8653-cb05ec95f1dc, 5c5a494e-d57c-4e98-925e-54e5c7d37779, 5ee3d884-081f-42ec-b90c-0dc2d79922fb, 5fd72a8b-b1f4-43bd-aaf4-bb44ac75573c, 640fc4a2-1741-420c-a474-0958fb855c6b, 69d692ab-fe73-44ec-ae4c-33af46864fca, 6b7d566d-8ca4-4be1-8862-17f1e1c6de35, 6ca729b8-40f1-404f-a044-92c50a476b51, 6d24f3df-c9c2-4224-ac84-9264f23d71d5, 6ef0fc4a-e4b4-411d-946a-0837e183c31a, 6f8c3682-b2f3-4462-8052-713a47c2cf6b, 70cd9c1b-fbe3-4048-ae4e-e8ff963a710a, 73753372-fbbe-4fdf-a48a-7683de51f542, 751f2480-d481-4eb0-b94f-53068732f9ce, 75e07a58-dace-46a3-a12c-6737d54cfc71, 77dc0985-4c7c-4a66-b012-20c0447ebe0e, 7a461b97-10b6-465e-bdcc-732271556ff9, 7a9047c6-4211-460e-b965-514e3c93b444, 7af4a485-064b-47a7-b56a-1548fdc67038, 7d7a3a31-c96b-435e-95f4-9f513eb58d37, 7e4dd652-d981-482c-aab9-5360b96cd899, 827e9603-a668-44db-a92b-35931381feff, 83f8c55b-de7c-4e72-9857-f727e4f8935d, 86081d90-bcf8-46fe-970f-d4374bdc52d9, 9035d661-270e-46e1-8100-11957d56e482, 90b9af83-c8f8-4ce2-b866-0b70a5e7f45d, 9155206f-41b0-479e-947a-4f9d8532ef10, 91bef275-f9ce-40d3-ab6e-a8bf96e3605d, 923ca6d8-a9eb-4261-8f0f-85177d141c8d, 951bc191-973d-419a-8227-753da83ad969, 964886c9-f719-4bd2-949e-c1d477b0c407, 9ab91274-bd8c-4fe0-8f97-dcf5501e8a77, 9c31b044-e7a3-442e-9f0c-c60001b67c8e, 9e892a88-3033-4f73-b4f9-192ef883f40d, a0de4ac9-f6de-4428-a197-76e85f6e569c, a1d99cba-955b-4c5e-87c2-8bacd0b921fe, a3f4aaec-9766-4b32-9cb8-83810b4347f5, a5697f85-2426-411b-888f-7ec5106c292e, a58d2a90-e3eb-4ccf-9834-3359c0741fdf, a60933e4-359a-4628-bb22-6619a62ec86f, a626edd9-7576-47fb-850b-ea1131597729, a7cca51d-0109-4087-930a-494c4fb7bbbe, a8230c0f-acf4-4f0d-b7ac-33b7fd87d056, acf2c070-0c1b-462e-b436-ef14e9b08c45, add81ba9-988f-40a0-abbc-35b4e85bc81f, b04240ef-f210-497b-863a-60d7b92ee701, b33d3d04-2a9e-4190-9706-dff595120b35, b6ca50d1-7372-4f45-9b9f-5a3958fb346c, b6d65c7d-fdf0-4443-80b9-88ceae43360b, b80a9ba7-8f54-4c86-bf5e-6310efdea3d0, b8a8394a-d9be-48a4-ad69-4b9b1c2625e6, b8af1cc9-e551-4e0d-ae11-c3e4a7d95374, b944b654-7b98-4aeb-989a-327ddb71b862, ba33b932-f184-4253-a1db-9c9364462291, bcfa780e-b3d2-4e25-a099-838133e78acc, bfa99c9f-1b27-44bd-8d45-395d937e64f2, c0c1a3d5-97e3-43a3-ad4a-224c0e5010ac, c2e253a1-1798-4cb3-9488-174546875ec6, c462832a-7ecd-438e-91e2-2ce8ff733e50, c80d6b6a-b00a-4d15-b996-0d84e1f81a89, c82a5c80-63d3-4e0c-9382-7d9f96fb6445, cbd0d86a-a8cd-4fc7-8edb-10833d269d37, cc05175d-0af1-4488-86e6-0aa1da3f2108, cc28082a-898c-4f39-ad4b-58058489c81f, cecc3970-edd7-4729-a3fb-11a79a1d2d58, cf02701e-6d8a-429b-b731-2b305cb34546, d0860d71-32cb-4ee4-9f6b-90c666002d3f, d12d0826-7b2b-4c3f-bc62-348a2f74c83e, d58d2de7-d7ae-4cdc-8638-d2497fd8bf45, d6183bf0-7897-425d-9127-4404a92fcba3, d65c08c5-f0ec-41c3-a65b-056f3e6f8b31, d7927243-97fa-4c33-8a74-84907cba8e6d, d7c2467e-7921-4238-b1ce-f6650b40666e, daacc8fc-bf7c-4a31-97bd-507cec637fe0, dd5d630f-ef6b-42c4-9a07-39cafb658fd4, e040167f-d529-4c88-9aa5-f7fff8a5fba2, e072127f-2bdb-464d-8884-01f14104ea5c, e11138dc-10b7-4ce4-86a7-c6cfa74edaf4, e1ecd724-1ef6-4acf-bf9c-ca1d906f0e5a, e263420c-bd53-4604-a435-70168b7239dd, e2b8a439-98d5-43ca-8f87-3eebfcec30f3, e5a4510e-6d0b-410d-8b94-f901353b18ec, e77488cf-c953-416f-bd72-80ba8f1b0787, eae38c05-7302-40b4-ad21-a4228c6ffa1a, ebd1d874-276a-405f-b04f-b4c974404f53, ebf5eb9f-4a91-4841-9c48-4d63c2792c39, ef87a218-83a0-4f95-9081-ee9b02274e4a, f1a9695d-298b-4b53-a63e-7c6f2463a2c1, f2089ed2-ec93-4292-9c66-c48a72bf06ec, f419926f-040b-48c5-812a-8b97a1ff13b3, f56231af-c630-4307-8ce4-0a61cf0cb3fd, f61416bc-8779-4e58-90d6-39f4accdb50b, f6fe0a86-7bcb-45d6-97e3-e186dc54c715, fa524d3f-c6c9-41fd-81d1-f8c2efc5f466, fa767478-05ab-451a-9622-44ca1e0ee7d9, fbb54846-ce98-47c7-8871-8f3120f3733c, fd0f3229-5219-4243-99e0-53906a506b8b, fd7c1963-5e9a-4cbb-9f1e-5e35eae3ba1a, ff8a2858-7f29-422f-b13a-517055ef8ff8]
195486 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
196486 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082303
196686 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
196930 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 86
196931 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
197101 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit8381636235460367963/2016/03/15/0903d041-8afc-4f19-820b-9ec207c7eff5_0_20180316082300.parquet
197114 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit8381636235460367963/2016/03/15/0903d041-8afc-4f19-820b-9ec207c7eff5_0_20180316082300.parquet
197114 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit8381636235460367963/2016/03/15/0903d041-8afc-4f19-820b-9ec207c7eff5_0_20180316082300.parquet => [23b487e5-6062-44a6-b36a-4ecd06419d91, 2c22226e-d221-4d8c-a869-3128810eb9d6, 2eb31eb1-b93b-40dc-98ae-6777bd6f2717, 302f2856-e4d6-4d6e-a201-8b276925f32a, 3682efe8-6a5c-4528-a47f-dc23dde2e1e7, 3889e865-c5fd-405b-84f0-11ece99f434c, 3d07205c-d486-43cc-91a1-51082e763f36, 4fe5d1ff-b9a1-448a-aff5-a43e4026cea7, 5481e734-4f55-49ff-8ad4-2360caba708f, 5e32b7e0-37a2-49ed-b86e-b68c09d6c090, 93c29703-bcb0-41af-8837-806bf534103c, 97b9e758-5736-475f-a4ba-419dfe1027a9, 9ea39f8c-430f-4ae3-96f6-4f9af776b981, a7b9fcef-be64-48f0-ae13-15b8c4a831cd, b9dc901b-2c87-450d-a3c5-47631f5c90e5, d8fa721c-3c41-409f-9bf4-e8064b038c48, ee874e63-c9de-4749-8cd2-2588b4de5847, f40434cc-7ef2-41c3-9d1d-b0f40d031527, f4e26247-6a2e-40df-8615-4b8c704c709e]
197131 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet
197141 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 174 row keys from /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet
197141 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet => [0332efe0-954f-4328-a88d-cbb9f5500ea7, 0a4d2a2a-de9e-4eac-8ec2-529b42660f9a, 0fb45370-5c2d-4502-a917-00763792de3a, 217a2d32-d079-4441-a4da-e749eb2f1bfc, 27c9be59-fd5e-408c-ad64-00b729eb551d, 2f30bf8f-2858-403e-bbbf-4c3a85fb3752, 326bc173-9b35-4561-acf7-7cdcaa8829ce, 3c28a042-4d53-407f-89a6-40f4718f26c6, 5993f85c-7fa4-478b-b4cd-c6ae05937d48, 5afb6e1e-a817-4994-acca-1aa7bbbff9bc, 67a5972b-4a30-4dec-8552-02fd9d703827, 750d5681-694f-4835-b83f-57a6181a0abb, 75939929-62ab-4ad2-aa3d-2755d326d219, 7ae1c09e-cb20-4799-b459-6cc8acc3d5da, 80a6083f-49a9-4b72-bf0e-98c8113b056c, 8f38be11-d012-44f2-91b7-f3652774a213, 93116790-76ab-455d-b87e-268ce6f75753, affe376e-c199-4b49-9b96-8b28628bf622, b07150bc-2d24-4d5e-85bd-aa532ddd5357, ba37f0b5-5f29-4525-b5c5-3f92862eed73, c054332c-98f7-4937-92dd-96137b0710bb, c5e72149-9716-4ba6-a51e-bd5761ae8b0f, d58b1f10-d289-4c38-983a-f686f8290b38, d9229003-ba66-4a17-95ae-2b8d3b4c88e1]
197169 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 6 for /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet
197179 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 174 row keys from /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet
197179 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 6 results, for file /tmp/junit8381636235460367963/2015/03/17/3f8d963f-e94f-4657-abfa-652b8b121b13_2_20180316082300.parquet => [dac67fe7-e899-4f6c-9338-dd25e69c4f16, de94ad7e-10fc-4a91-b622-dc037eea7619, e02ab6df-f436-4ee4-a615-1e930c277d8c, e4b7c08f-2b98-453e-89fc-453aa23904b3, e7676918-2b16-4751-b26c-8334ca3e1fae, eff611ec-c763-4142-9a2a-aa6812f754a4]
197196 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 37 for /tmp/junit8381636235460367963/2015/03/16/c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9_1_20180316082300.parquet
197205 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 180 row keys from /tmp/junit8381636235460367963/2015/03/16/c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9_1_20180316082300.parquet
197205 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 37 results, for file /tmp/junit8381636235460367963/2015/03/16/c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9_1_20180316082300.parquet => [052d8665-2a19-4ffa-a258-b63ef681ec89, 07ca2ff5-42d9-4cf9-ab25-8fd33574071c, 1b413c2b-8591-4713-a672-251d8cc26989, 2403773b-a249-4b79-ad39-6aecd68e629f, 245a04ce-9dee-4f92-a071-59f70d243f4c, 26ee26e8-5a3f-43a0-abf6-503140e5f8e6, 28a35da5-eb52-48a9-880d-a2aca8657588, 2c669698-7504-41e6-b9c9-c56d22c80340, 379fd6d5-64c5-4e15-9e12-105218edfe31, 38ea047e-8e43-4c98-a8e1-b9ee206b37f9, 3a089bb8-884e-4f5c-b96d-3a0ad72a2ed9, 487ba7df-8fc7-49f8-83fd-fd4f67850d84, 4982d219-4ac7-4643-a187-e82ce45e224a, 4ed6b394-fd29-4d11-b029-375af14730e0, 51f0f4b5-56ef-47a3-9ef0-6d682b5ed20d, 55a99059-a440-4efb-924b-9248c3126744, 58375a0f-fa1a-470f-baea-d1c17ec3083b, 5b57b45b-0b05-4da8-8653-cb05ec95f1dc, 5c5a494e-d57c-4e98-925e-54e5c7d37779, 5fd72a8b-b1f4-43bd-aaf4-bb44ac75573c, 6b7d566d-8ca4-4be1-8862-17f1e1c6de35, 751f2480-d481-4eb0-b94f-53068732f9ce, 7d7a3a31-c96b-435e-95f4-9f513eb58d37, 7e4dd652-d981-482c-aab9-5360b96cd899, 90b9af83-c8f8-4ce2-b866-0b70a5e7f45d, 91bef275-f9ce-40d3-ab6e-a8bf96e3605d, a8230c0f-acf4-4f0d-b7ac-33b7fd87d056, add81ba9-988f-40a0-abbc-35b4e85bc81f, c82a5c80-63d3-4e0c-9382-7d9f96fb6445, d0860d71-32cb-4ee4-9f6b-90c666002d3f, dd5d630f-ef6b-42c4-9a07-39cafb658fd4, e040167f-d529-4c88-9aa5-f7fff8a5fba2, e11138dc-10b7-4ce4-86a7-c6cfa74edaf4, eae38c05-7302-40b4-ad21-a4228c6ffa1a, fa524d3f-c6c9-41fd-81d1-f8c2efc5f466, fa767478-05ab-451a-9622-44ca1e0ee7d9, ff8a2858-7f29-422f-b13a-517055ef8ff8]
197296 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=86}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=19}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=37}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=30}}}
197317 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2699
197317 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=3f8d963f-e94f-4657-abfa-652b8b121b13}, 1=BucketInfo {bucketType=UPDATE, fileLoc=0903d041-8afc-4f19-820b-9ec207c7eff5}, 2=BucketInfo {bucketType=UPDATE, fileLoc=c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{3f8d963f-e94f-4657-abfa-652b8b121b13=0, 0903d041-8afc-4f19-820b-9ec207c7eff5=1, c342d6d6-1fcb-4269-a25b-9aa14bc6ebe9=2}
197334 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082303
197334 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082303
197513 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
197514 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_78_0 failed due to an exception
197514 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_78_0 could not be removed as it was not found on disk or in memory
197514 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 38.0 (TID 70)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
197516 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 38.0 (TID 70, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

197516 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 38.0 failed 1 times; aborting job
197536 [Executor task launch worker-1] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
197537 [Executor task launch worker-1] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_78_1 failed due to an exception
197538 [Executor task launch worker-1] WARN  org.apache.spark.storage.BlockManager  - Block rdd_78_1 could not be removed as it was not found on disk or in memory
197538 [dispatcher-event-loop-21] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 34418, None),rdd_78_1,StorageLevel(1 replicas),0,0))
197539 [Executor task launch worker-1] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 38.0 (TID 71)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
197566 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
197677 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
197677 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
197858 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
197859 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/01/id31_1_20160506030611.parquet	true
197859 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
197860 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/02/id32_1_20160506030611.parquet	true
197860 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
197860 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/06/id33_1_20160506030611.parquet	true
197867 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
197867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
197874 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
197877 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
197877 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
198037 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
198038 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
198038 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
198044 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
198044 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
198050 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
198052 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
198052 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
198261 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
198262 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/01/id21_1_20160502020601.parquet	true
198262 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
198262 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/02/id22_1_20160502020601.parquet	true
198262 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
198262 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/06/id23_1_20160502020601.parquet	true
198266 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
198267 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
198267 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
198272 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
198273 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
198273 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
198435 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
198435 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/01/id21_1_20160502020601.parquet	true
198435 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
198435 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/02/id22_1_20160502020601.parquet	true
198435 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
198436 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/06/id23_1_20160502020601.parquet	true
198441 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
198441 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
198447 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
198448 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160501010101]
198448 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160501010101]
198620 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
198620 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/01/id11_1_20160501010101.parquet	true
198621 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
198621 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/02/id12_1_20160501010101.parquet	true
198621 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
198622 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2040610336916804085/2016/05/06/id13_1_20160501010101.parquet	true
198625 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160501010101]
198625 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160501010101]
198630 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160501010101] rollback is complete
198705 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
198887 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
199108 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
199108 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
199353 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=300, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=112, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=90, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=98, numUpdates=0}}}
199358 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 112, totalInsertBuckets => 1, recordsPerBucket => 500000
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 90, totalInsertBuckets => 1, recordsPerBucket => 500000
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 98, totalInsertBuckets => 1, recordsPerBucket => 500000
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
199359 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
199372 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
199372 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
199507 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
199507 [pool-1498-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
199537 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
199537 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
199556 [pool-1498-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
199573 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
199573 [pool-1499-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
199592 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
199592 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
199609 [pool-1499-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
199626 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
199626 [pool-1500-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
199652 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
199652 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
199679 [pool-1500-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
199696 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
199696 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
199775 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
199775 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
199998 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
200003 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
200003 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
200059 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
200636 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
200760 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
200760 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
200885 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 80 for /tmp/junit8702654220717866196/2016/03/15/7da4f472-7b3e-49b8-ad6c-ec0740429c34_0_001.parquet
200900 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 80 row keys from /tmp/junit8702654220717866196/2016/03/15/7da4f472-7b3e-49b8-ad6c-ec0740429c34_0_001.parquet
200900 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 80 results, for file /tmp/junit8702654220717866196/2016/03/15/7da4f472-7b3e-49b8-ad6c-ec0740429c34_0_001.parquet => [01d42a19-5532-4127-8adf-32dee0e611ea, 022002b7-8658-4ec1-88a9-170996e365cb, 05114f2c-8043-41e5-8b79-42d17570b563, 052a09a7-53a7-4926-8f29-bcdf1f3b8f4c, 0abf8190-33b8-4658-b483-8afa43971904, 0e1d9689-9fbe-460e-bda9-af0c17ff20b0, 0f175814-bb52-43c2-b8df-85a40f6f0aea, 0f6366d6-d472-4c1d-bd9d-41992793845a, 0f985ba3-42d6-482b-a847-1c13da5dc818, 1256d01c-e1e0-4f68-89c0-9e35b2ac1b62, 1656e428-d0b7-4452-8608-23e211852473, 1ded8bef-1e5a-4023-81d1-78a3471c243d, 1fc5bd05-6018-4cdb-935f-c02b634ee3ca, 2bf233b0-d735-4b17-ab7e-a8723cd2f778, 2d0ffe61-6c50-48f7-ac85-57b45f542123, 2ff878b4-f937-40aa-ae1a-d96955ed4f17, 39f17e4e-a4b2-46ec-b21a-557c9b6cea43, 3b4d9b92-7359-477d-9cb9-7d9e21b70e63, 3c32c321-33a1-4d18-9155-2d066cf1237e, 3de563ad-6686-430b-8e28-20f3cf2a9d94, 45708dd5-e884-42f0-8ef0-fe8eceb6205a, 4858d881-e2d4-4d6c-9c6c-e27453fa04c0, 49274b8e-a9d3-4dd2-962e-7afe21cf2f4d, 501b252b-d1ae-4bcb-8b7f-2a22995d99b7, 55e5ef65-1d45-44ec-a332-c452cdf8a8c2, 57c34059-a664-411a-9b85-84bd1f4e5d8f, 5c1944a6-494f-4ff7-abd4-20a3bb9fa312, 5e9ece93-6251-441f-aa5d-6e74ed93e4f4, 62c79600-abcc-460f-9005-5fbe650f731f, 6360e0df-056b-4f48-9caa-1252fc3a7818, 64870888-f6b6-45ab-9e9a-712fb4fd9508, 66033617-d57f-4ed1-90fe-7ec8985e6d67, 664bb6f9-7918-4061-a054-8bdc0f4a5d90, 6a341e19-125b-4f7f-9b7a-847ff7300f52, 6ae11cb1-9b00-4ab4-a410-5f5334170db5, 6fb48f74-a6a1-4148-bdf5-ca7a9353e331, 722a26b4-02fb-4c67-8a92-88d009947ef9, 72665c4f-cdf4-47e4-bc03-b20aea9bfabb, 72784c71-5fae-44e8-b0a5-2cee4f1c3375, 7436b1e3-ae0b-4e4a-9f64-b4d54f885680, 7573b3a5-d199-4ad9-a49a-be8a4353abb3, 76e81326-abbc-423c-a67f-22a12e35278c, 79670390-c34c-4585-9bb2-9918cac7efba, 7c0f57be-351f-4fd9-8cf6-e6d2f5097b7b, 82dd8e2a-2946-4d24-a01a-e162019aafb3, 83b47d39-6a28-4741-8784-c92a0f4445d9, 85787589-08b3-4514-8e7a-545763fb3690, 86965de5-e29f-4902-9405-360cdcceb249, 88b5c407-cf7a-470b-8a0e-c8370cc137bc, 8e72f61c-ba51-43c7-b0cc-e0a8d1ae11d6, 8f4689b3-8b11-414e-8dbe-d57a934fcbfb, 913d0318-233c-4d0c-a766-d006251a8846, 960ab12c-025b-4c38-8a3e-9820d17707e8, 989b0a6f-d8b0-479e-950d-229ffbcdad72, 9a21386c-f654-4efc-a74e-b1e0e480fea2, 9cabb070-8d07-439c-8ecd-3ed843cec7a0, 9f009ec7-74cb-4e95-9365-fa7b758e56cd, a09117b1-e559-433b-942f-a5e8da451427, a42574ed-0c5d-4e00-8507-06588982beed, a61c0fa4-b212-4c56-9405-2ade633d488b, ac5c18d2-d38f-4a4b-b951-24fdb220331d, ae200e71-f6b1-44b9-8516-10e4d5f1042c, bc26d190-5113-4869-8d85-7c7a6590ad2d, c1c36852-5036-4747-8496-c2f279ae5137, c2c66c50-4b64-499c-bb7d-49d884df94b5, c41d3c31-3950-4879-bb63-15f207485294, c8fcd62c-7de2-445a-bbf2-c5041b461406, ce56b325-cffe-4a4e-87bf-7f968b892bbb, d3dc6233-ce14-4c9f-80e3-afd83ae82446, d47370eb-1492-4a2d-8838-14a69388864c, ddc13fb9-2a37-46a2-98ec-124f1f46145d, e664e4f0-4cb5-4a5d-b3cc-200552236e8c, e9bc52b8-e8f0-41b0-ad53-083c5d16dabe, ea50da99-4977-4075-a402-a6f6b05c8b0d, eb453de2-528c-43c7-be17-bdf376524fe5, ebb21244-ec47-478a-a87b-62d143c26c82, ec4c5de6-c4e1-4536-a83b-fa7b1dfab08e, ed4f53af-8c34-44d7-8927-05a3514d09e4, f37bb5be-b389-43bf-8028-10138afb99bd, fb2a210e-86e9-4caa-a9fc-e42c1798eac2]
200915 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 65 for /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet
200926 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet
200926 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 65 results, for file /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet => [08ab6839-e76a-4806-bb5b-c6fb62372c23, 11c416af-f704-4f38-83ae-8e6ad1198f23, 12a4a005-a643-4d2a-939e-8223df8882af, 22d923be-07e7-4430-a012-890ed31b8c7f, 252ddba6-f627-4d05-b9f4-bb6028a6c7f2, 377a78cf-079e-4228-b176-3fb70b0bde37, 39461058-1e86-4487-8fa9-8044f06eac87, 49c2e30a-e42b-4685-a58a-18cd9863ff7a, 4b64ea11-fc61-4eb6-b763-4889d42896f8, 4dc1dfc6-4df2-4fe9-bac2-b9861c42897c, 59965ea0-ee12-4eae-8402-adebccf900ce, 5c80f4e0-fd47-4a85-a9ca-11db14bd1db4, 637adbe1-4fc0-4bb2-bf5f-1ebd859d2bf3, 64d92999-01e5-4446-9151-f1beadf570e5, 695cd31a-52c4-48ee-8dbd-08bbd63e289f, 6e29b012-f2bc-4a1d-b2d1-cf26fb070aab, 710ca74a-1dbd-4325-9292-f2f069fb2ce0, 71b09916-4961-44b8-b741-431b11a80bba, 77cbff6d-70ee-4e5c-b439-35f7042ee25d, 7f50f38e-9c35-47aa-a788-92a33c85d5e9, 7facc50b-4eb7-49f1-8e06-d3b85588678b, 818e4c65-3354-42ea-b186-5c1ceb960d07, 82d92098-f4a9-4265-b513-2c04a4916cc8, 83e29113-364d-4bd3-8c40-bd08848e2fa6, 85a9662e-7713-46a7-97a4-79e396641472, 8a742095-32ef-495c-8621-60e85499b18f, 9544ea46-f98c-499e-8304-2d68da596589, 974aac49-8a2f-46d3-9f86-877c42c3fc3d, 983ec410-56ba-4314-a63f-5a3c18f11e5b, 9bb2307c-5b2f-4e31-a255-aa915a4f9994, 9df2a318-0f6b-4c5e-b6fa-ef03934a9250, a894cc8a-8691-443c-981a-8a8b81de10ae, ab7fcab1-677e-4992-9ef6-08a43fdb2df0, ac1958aa-7921-4971-af5a-5936d0b72a7e, adaff9d6-4f25-4041-955f-824d41717fb9, b0063ffd-ce96-4be0-9eb4-5702212a7f55, b1432c2e-2e1e-450b-b110-75187e4b1d9a, b237fe8e-f380-4ff2-b972-ad28c40bf704, b414032f-d4cd-4d55-a005-ac0bc6cc6d36, b57b5093-3a69-4337-b337-28d2e9a57c35, b6798de0-201c-47a3-9fd3-fa708e08bce9, b75aac9c-56b6-4f25-b40a-1fc82475664c, b8aef267-c304-4251-940d-032ff81cb256, c4d8d6c1-7877-40ae-afba-c9cba570ffc0, c51881bd-2050-42fe-a488-f231a6347871, c83f896f-2199-49e4-a3c9-a115da4a1b9f, c9ea0f45-22d5-4189-97c1-3bc62cb5c598, cb2bff7d-9c8a-4fd1-8e5e-48af71d4ffab, cee27a3f-743b-4533-a245-93fa37f9c732, d13aa2c8-caf3-4e03-9a6a-9b981249f735, d291dca4-ff8d-43de-b077-827159adc5e8, d50b988e-5b27-4f76-88d5-2831e276f33a, d92523b8-5463-4937-94f6-9815246ba6f5, e5522c59-75f1-4092-badd-8cea95e61c2f, e5b80377-b6d8-445c-81fd-c9a75d58b277, e9410b6e-18de-4267-adc3-c3132e6f0a85, eb2c6e69-770b-47d8-9e64-31f8618423a5, edba5b91-942b-4081-a5a2-d9d57919f9c4, ede3820a-d6eb-4445-8304-d12b1a3ebc73, f2a6dc28-3176-409e-88f0-310e2b60922f, f462c980-e3b5-46f1-8918-547211d35e9f, f8ca349a-5546-4f34-a1cb-e6fdae2a7c54, fbd18d20-91ca-4466-bb35-4f8287adf886, fbd9ebb1-36fd-4b3c-907d-6caa8fcbf302, fd562da8-5c88-4a2e-be22-be4c8547cec1]
200940 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 55 for /tmp/junit8702654220717866196/2015/03/16/b2b3c37c-eb60-43e7-8149-47ea4650d622_1_001.parquet
200951 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 55 row keys from /tmp/junit8702654220717866196/2015/03/16/b2b3c37c-eb60-43e7-8149-47ea4650d622_1_001.parquet
200951 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 55 results, for file /tmp/junit8702654220717866196/2015/03/16/b2b3c37c-eb60-43e7-8149-47ea4650d622_1_001.parquet => [005fac33-9f4e-4a6a-afd7-55c590f1d6b4, 021ed65f-1222-4419-add7-119acf154122, 0a30b725-7881-42ed-8cfe-886179a54a04, 0d1db853-d719-4329-a70e-246c029917ac, 0f62ccaf-474e-4ef2-b853-0a901d4a838e, 1057fddb-5f7a-42de-b94e-07945591f500, 144b32a4-0e04-4bd0-a714-a2850b1a10e1, 1cc93ab0-c98e-446c-93c6-1bfc213ad8fb, 2091298b-c051-45b1-be3c-ff3834307861, 2b28739d-c52a-42ce-8134-d05b923346c9, 2c75a30a-4178-4a06-840f-e2ec7d2967a1, 3246193e-4261-4268-89ee-9924900b2329, 3a7c8d14-44bb-4fdc-b164-da777c01c4d4, 3f1e3aec-48f6-4428-bba8-d8c8010f33b9, 40e05a9e-6584-454e-98ea-614c1e716319, 432410c4-a552-4b5b-8991-3d9efb1b54b2, 442a19c1-4366-44d1-be44-fa61f696b868, 461c1731-a872-41e1-b9e3-53217f7d0aa0, 4c290db0-8d7b-4de2-9226-dcf4b64545ab, 4e5f0b6b-3f5a-4f1c-baa7-fcb50d104a9d, 5449e574-2a15-4780-8be5-774ebe90c513, 55609def-7acd-4a1b-925d-e1313681b9c6, 5b82aa57-6cad-4b74-801c-4ecf6a6f341e, 5f4581da-0abb-4a9e-bd5e-c19126bc3b3e, 5fa2b61c-4e44-404d-b0a5-834f223854e0, 780dd948-bc2f-4f47-85fe-867c1da9ba23, 79387ba5-b50d-4e25-a796-8c01ec3595f7, 7a47c2da-f29d-4ca2-a81c-be2ce0b3f587, 7b5c0ee0-e3ed-4017-ade6-a72012c399ee, 875ab269-5714-4037-b8bb-eb2a66766eed, 890967a3-ea34-4769-81bb-b5faa5d955a8, 9522062c-afc8-491e-a048-3cabe542ae5a, 96f86f7a-e6cd-4d4f-a56c-c246569a70a8, 97b6fe13-7697-4ca7-9621-59ff3858cd1a, 981a9008-3f7d-4889-80bd-21cc235f823f, 9a10b217-51c4-4e4f-ac0f-62f217af69e5, 9c6f1325-b549-4f34-9198-8fd6305fa5c5, 9e29b69a-489b-4ce1-8ed2-67ada9f95ee6, a4944a2e-4107-4a27-84c1-ee8556168916, b70d9f8f-ec07-44aa-9b35-f84d1a94d57c, ba6b4400-683d-4032-8fa5-cb5ed0ccbce7, bb589a38-bbf7-4f05-a8f6-68c239b8ca65, bea6e760-d4cf-44bb-ba84-a3cc1e8db278, cf5c10cc-43fc-4226-af55-21b505f30f6a, d6ee398f-1f96-4c12-82ae-6eee59b43a1b, e21374a8-042c-49f1-a7bf-b03cbc721093, e21d217b-65e9-439e-9142-dabc95b21262, e30e6b49-3ca4-4a88-83e7-2160154cdeba, e43a96b8-e1db-4c4a-87ea-7509af25dbb0, e49686b1-9740-47c0-a76a-c8318b3c3536, ef8ec854-e445-4f5b-af30-b728d66e5271, f0fe0579-653e-44f6-ac4b-17cacb854656, f63968e7-87b8-498f-b187-389a96b35afd, f98bce48-915e-4088-a436-5b1ed95d8cc9, fdd24acf-8ef9-4b09-9359-5dcc4d7dfc80]
200998 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
201331 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
201331 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
201348 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
201499 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 39 for /tmp/junit8702654220717866196/2016/03/15/7da4f472-7b3e-49b8-ad6c-ec0740429c34_0_001.parquet
201510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 80 row keys from /tmp/junit8702654220717866196/2016/03/15/7da4f472-7b3e-49b8-ad6c-ec0740429c34_0_001.parquet
201510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 39 results, for file /tmp/junit8702654220717866196/2016/03/15/7da4f472-7b3e-49b8-ad6c-ec0740429c34_0_001.parquet => [01d42a19-5532-4127-8adf-32dee0e611ea, 05114f2c-8043-41e5-8b79-42d17570b563, 0f175814-bb52-43c2-b8df-85a40f6f0aea, 1656e428-d0b7-4452-8608-23e211852473, 1ded8bef-1e5a-4023-81d1-78a3471c243d, 1fc5bd05-6018-4cdb-935f-c02b634ee3ca, 2bf233b0-d735-4b17-ab7e-a8723cd2f778, 2d0ffe61-6c50-48f7-ac85-57b45f542123, 45708dd5-e884-42f0-8ef0-fe8eceb6205a, 501b252b-d1ae-4bcb-8b7f-2a22995d99b7, 5c1944a6-494f-4ff7-abd4-20a3bb9fa312, 62c79600-abcc-460f-9005-5fbe650f731f, 6360e0df-056b-4f48-9caa-1252fc3a7818, 66033617-d57f-4ed1-90fe-7ec8985e6d67, 664bb6f9-7918-4061-a054-8bdc0f4a5d90, 6ae11cb1-9b00-4ab4-a410-5f5334170db5, 6fb48f74-a6a1-4148-bdf5-ca7a9353e331, 72665c4f-cdf4-47e4-bc03-b20aea9bfabb, 7436b1e3-ae0b-4e4a-9f64-b4d54f885680, 7573b3a5-d199-4ad9-a49a-be8a4353abb3, 76e81326-abbc-423c-a67f-22a12e35278c, 79670390-c34c-4585-9bb2-9918cac7efba, 82dd8e2a-2946-4d24-a01a-e162019aafb3, 83b47d39-6a28-4741-8784-c92a0f4445d9, 86965de5-e29f-4902-9405-360cdcceb249, 8e72f61c-ba51-43c7-b0cc-e0a8d1ae11d6, 9a21386c-f654-4efc-a74e-b1e0e480fea2, 9cabb070-8d07-439c-8ecd-3ed843cec7a0, a09117b1-e559-433b-942f-a5e8da451427, a61c0fa4-b212-4c56-9405-2ade633d488b, ae200e71-f6b1-44b9-8516-10e4d5f1042c, c41d3c31-3950-4879-bb63-15f207485294, d3dc6233-ce14-4c9f-80e3-afd83ae82446, ddc13fb9-2a37-46a2-98ec-124f1f46145d, e9bc52b8-e8f0-41b0-ad53-083c5d16dabe, ea50da99-4977-4075-a402-a6f6b05c8b0d, eb453de2-528c-43c7-be17-bdf376524fe5, f37bb5be-b389-43bf-8028-10138afb99bd, fb2a210e-86e9-4caa-a9fc-e42c1798eac2]
201531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet
201542 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet
201542 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet => [08ab6839-e76a-4806-bb5b-c6fb62372c23, 11c416af-f704-4f38-83ae-8e6ad1198f23, 22d923be-07e7-4430-a012-890ed31b8c7f, 252ddba6-f627-4d05-b9f4-bb6028a6c7f2, 39461058-1e86-4487-8fa9-8044f06eac87, 49c2e30a-e42b-4685-a58a-18cd9863ff7a, 4dc1dfc6-4df2-4fe9-bac2-b9861c42897c, 59965ea0-ee12-4eae-8402-adebccf900ce, 5c80f4e0-fd47-4a85-a9ca-11db14bd1db4, 71b09916-4961-44b8-b741-431b11a80bba, 82d92098-f4a9-4265-b513-2c04a4916cc8]
201573 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet
201592 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet
201593 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit8702654220717866196/2015/03/17/93a879be-4296-4adc-9aac-757f9e1aa88c_2_001.parquet => [8a742095-32ef-495c-8621-60e85499b18f, 9544ea46-f98c-499e-8304-2d68da596589, 974aac49-8a2f-46d3-9f86-877c42c3fc3d, 983ec410-56ba-4314-a63f-5a3c18f11e5b, 9bb2307c-5b2f-4e31-a255-aa915a4f9994, b1432c2e-2e1e-450b-b110-75187e4b1d9a, b237fe8e-f380-4ff2-b972-ad28c40bf704, b75aac9c-56b6-4f25-b40a-1fc82475664c, b8aef267-c304-4251-940d-032ff81cb256, c9ea0f45-22d5-4189-97c1-3bc62cb5c598, cee27a3f-743b-4533-a245-93fa37f9c732, d13aa2c8-caf3-4e03-9a6a-9b981249f735, d291dca4-ff8d-43de-b077-827159adc5e8, d92523b8-5463-4937-94f6-9815246ba6f5, ede3820a-d6eb-4445-8304-d12b1a3ebc73, f2a6dc28-3176-409e-88f0-310e2b60922f, f462c980-e3b5-46f1-8918-547211d35e9f, fbd9ebb1-36fd-4b3c-907d-6caa8fcbf302]
201612 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 32 for /tmp/junit8702654220717866196/2015/03/16/b2b3c37c-eb60-43e7-8149-47ea4650d622_1_001.parquet
201622 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 55 row keys from /tmp/junit8702654220717866196/2015/03/16/b2b3c37c-eb60-43e7-8149-47ea4650d622_1_001.parquet
201622 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 32 results, for file /tmp/junit8702654220717866196/2015/03/16/b2b3c37c-eb60-43e7-8149-47ea4650d622_1_001.parquet => [021ed65f-1222-4419-add7-119acf154122, 0a30b725-7881-42ed-8cfe-886179a54a04, 0f62ccaf-474e-4ef2-b853-0a901d4a838e, 1cc93ab0-c98e-446c-93c6-1bfc213ad8fb, 2c75a30a-4178-4a06-840f-e2ec7d2967a1, 3246193e-4261-4268-89ee-9924900b2329, 3a7c8d14-44bb-4fdc-b164-da777c01c4d4, 3f1e3aec-48f6-4428-bba8-d8c8010f33b9, 40e05a9e-6584-454e-98ea-614c1e716319, 442a19c1-4366-44d1-be44-fa61f696b868, 4e5f0b6b-3f5a-4f1c-baa7-fcb50d104a9d, 5449e574-2a15-4780-8be5-774ebe90c513, 55609def-7acd-4a1b-925d-e1313681b9c6, 5b82aa57-6cad-4b74-801c-4ecf6a6f341e, 5f4581da-0abb-4a9e-bd5e-c19126bc3b3e, 5fa2b61c-4e44-404d-b0a5-834f223854e0, 780dd948-bc2f-4f47-85fe-867c1da9ba23, 79387ba5-b50d-4e25-a796-8c01ec3595f7, 7a47c2da-f29d-4ca2-a81c-be2ce0b3f587, 875ab269-5714-4037-b8bb-eb2a66766eed, 890967a3-ea34-4769-81bb-b5faa5d955a8, 96f86f7a-e6cd-4d4f-a56c-c246569a70a8, 97b6fe13-7697-4ca7-9621-59ff3858cd1a, 981a9008-3f7d-4889-80bd-21cc235f823f, 9a10b217-51c4-4e4f-ac0f-62f217af69e5, a4944a2e-4107-4a27-84c1-ee8556168916, b70d9f8f-ec07-44aa-9b35-f84d1a94d57c, bb589a38-bbf7-4f05-a8f6-68c239b8ca65, bea6e760-d4cf-44bb-ba84-a3cc1e8db278, e43a96b8-e1db-4c4a-87ea-7509af25dbb0, ef8ec854-e445-4f5b-af30-b728d66e5271, f0fe0579-653e-44f6-ac4b-17cacb854656]
201701 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=100}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=39}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=32}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=29}}}
201712 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
201712 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7da4f472-7b3e-49b8-ad6c-ec0740429c34}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b2b3c37c-eb60-43e7-8149-47ea4650d622}, 2=BucketInfo {bucketType=UPDATE, fileLoc=93a879be-4296-4adc-9aac-757f9e1aa88c}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7da4f472-7b3e-49b8-ad6c-ec0740429c34=0, b2b3c37c-eb60-43e7-8149-47ea4650d622=1, 93a879be-4296-4adc-9aac-757f9e1aa88c=2}
201725 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
201725 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
201822 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
201896 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
201897 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_114_0 failed due to an exception
201897 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_114_0 could not be removed as it was not found on disk or in memory
201897 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 62.0 (TID 82)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
201899 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 62.0 (TID 82, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

201899 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 62.0 failed 1 times; aborting job
201907 [Executor task launch worker-0] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
201908 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_114_1 failed due to an exception
201908 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_114_1 could not be removed as it was not found on disk or in memory
201908 [dispatcher-event-loop-15] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 36491, None),rdd_114_1,StorageLevel(1 replicas),0,0))
201908 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 62.0 (TID 83)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
201987 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
201987 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
202164 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
202164 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
202354 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
202354 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
202584 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
202584 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
202773 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
202773 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
202868 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
203032 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
203153 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
203437 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
203437 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
203676 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=75, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=56, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=69, numUpdates=0}}}
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 75, totalInsertBuckets => 1, recordsPerBucket => 500000
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
203682 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
203695 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
203695 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
203828 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
203828 [pool-1546-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
203861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
203861 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
203887 [pool-1546-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
203905 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
203906 [pool-1547-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
203931 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
203931 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
203953 [pool-1547-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
203972 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
203972 [pool-1548-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
204003 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
204003 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
204028 [pool-1548-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
204044 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
204044 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
204128 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
204128 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
204272 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
204336 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
204341 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
204341 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
204431 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
205028 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
205195 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
205195 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
205348 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 56 for /tmp/junit5790122458697468505/2015/03/16/1373f5f1-2ea2-481f-8cac-714dcf575784_1_001.parquet
205362 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 56 row keys from /tmp/junit5790122458697468505/2015/03/16/1373f5f1-2ea2-481f-8cac-714dcf575784_1_001.parquet
205362 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 56 results, for file /tmp/junit5790122458697468505/2015/03/16/1373f5f1-2ea2-481f-8cac-714dcf575784_1_001.parquet => [062c1285-bfc1-4ae8-afb6-862e0dc69390, 08ffdcc0-dcee-469d-9e24-e1230e17af71, 0cc1febc-f7db-40fa-affa-3e060a4f48fc, 1faa9381-4a5a-4c27-8912-6fcd7ae94fe5, 29b2d47b-d7ae-4d29-9ef1-20ec968dc202, 34e52623-63d0-4f6b-ac5e-dff456d3e7a2, 35096d2f-8149-47fa-b98f-3eea545c1c2c, 3a0727ef-d1c2-4d70-afbf-8fdf94eb11ee, 3df5299b-c2d8-41e3-877c-c6e0c59ef882, 3e59aa46-a228-450a-ad4e-a2ee26450c16, 41f4efed-f2ea-4704-96b7-298f1003e3d8, 44af2aaf-965e-4b7e-880c-12398326d2e4, 46f80f9d-9237-4039-9e21-5dd937fedc6b, 5a00dd5a-28ca-4f34-bd70-10589332d990, 5b539c6d-7e73-4122-8619-5e1333ff54a2, 5bc1c68a-202b-45ab-8a30-ebaaf2c7ad65, 631b1975-5133-4625-9adc-3f3fb550894a, 6fcfe792-5e38-4224-897f-37d532a78114, 6ff5168f-7e38-41cd-b9df-fc94d5738515, 70caa6ca-b3f6-4146-b5bc-311ad549b62c, 720c946b-4704-486d-93f9-e79cbc1f9812, 730c0df8-9080-4ab8-8324-4af519730a21, 7394121b-2e9d-4376-b977-4f238c9a3e4f, 74833e24-2322-4004-8fcf-9f8fe1340177, 749ae296-72f1-4cac-9354-100405c162d4, 76e4b558-c9df-49dd-86db-1f90d5e0459c, 7e413e35-8cdd-4872-ab68-718fdc0b925c, 7f50a421-028c-4881-81c4-a78f29269fbf, 81d8a008-ba92-4bc9-a80f-0d5b0b17718e, 8c4b34bd-8ae9-46e4-8ca8-dcd2b364b1b4, 8d7c8f05-fa8c-4a27-9d99-60be8753dec3, 9a2c77a3-47b1-42c7-af30-6e0f943649aa, 9c66ae74-2aef-449e-895a-bbb8d1f8cf2a, a3c1293e-f034-491f-adea-10a8de20081c, a7139537-03c2-44e3-b544-7d17ed482322, ab14c297-d4ee-4d4f-9edb-ac8ee564fdf8, b198fea8-a2bb-47c2-999b-1e1b13c9fd9d, b2dd51cc-226d-4404-b70c-09448bef240d, bb2f52ad-301d-4f5d-8655-be23f3e94484, bdd14473-3e8f-4543-ab30-4a8def5f18e8, c1860105-3a29-4bdc-8d4f-84b0960b406f, c1a56cb3-1582-4313-89e3-f9037141f383, c36e15cb-f5f6-48c7-92d2-9095fbef3312, c5331250-3ebf-49e4-9c27-a0bc5fc30082, c71bd528-c762-4283-a2d7-64ad00ff5be8, c7a1568a-4833-4924-8f6c-8b7fffada611, c9f39f6a-b7d4-4727-aea5-b39cf3a717d4, cc34bb37-3ac9-45bb-afb2-09264dbb8ac8, ccc3ed6b-d8be-40ae-822d-20f6a9c662e3, cf4e4374-9a89-4605-849e-5b03243696df, d061dc82-6be5-4cd1-9f6b-fe35a1cbde22, dfe5b6f7-7a31-402b-8835-4acdb0db3b21, e2261881-bb07-4277-a318-8c061bf501f8, e952fdd1-f027-4bef-a636-2161f3964a5f, edc0e52f-5a53-46fa-8a38-5787cd84f42d, f7675f97-7d5f-4e30-a49d-580eef0b048d]
205378 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 51 for /tmp/junit5790122458697468505/2015/03/17/446c654a-b960-49c8-99af-bb2dd554a46b_2_001.parquet
205389 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit5790122458697468505/2015/03/17/446c654a-b960-49c8-99af-bb2dd554a46b_2_001.parquet
205390 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 51 results, for file /tmp/junit5790122458697468505/2015/03/17/446c654a-b960-49c8-99af-bb2dd554a46b_2_001.parquet => [0194cf2d-3638-494b-9a85-7db1cf43efd6, 04d896a7-c87e-47f5-91f3-cef5106513dc, 064a6f68-956e-4b66-ad4b-7d005acdd852, 0d038705-1e5c-494d-947f-1a63530b2169, 12663ea0-85dc-422d-9ff0-ebf3efc9a04e, 12816612-1804-49c0-930e-4fc78c691111, 1afcca6c-f5fb-4524-95e1-25c9b778dd47, 2558641f-b6ee-4a31-aefa-b89defe531be, 28484dc2-0dac-48ee-bf3f-c9640917539d, 2876046f-1cfc-423d-9f51-880597cece77, 29ca0fb4-c7b0-468d-a0b8-849355ec7288, 2c01d4d2-7574-4896-ab88-4bf8a4623798, 2fd0347a-435b-48b6-a302-a55464573d39, 36620979-53a5-468b-a94a-ba945714a540, 3b7dee37-7989-4dd6-98eb-91ef5f18233f, 3cf0cfc5-73e1-4d8d-adb2-383b56b4c527, 3e0b398d-425f-4699-a0ee-0eeefc9777ba, 432d0ccf-17b9-4f04-bfca-2ff1620d19af, 46aead30-f884-46ea-a299-51ba3ba75986, 4787f8e3-a1de-4933-9cd4-013ea2252240, 483e74d3-aba2-47d6-82cd-60612112571b, 539f8d79-b771-4ef2-bd51-e669774c445f, 5857df94-a30d-4a22-977d-bbfb5e233dd1, 5df7a93f-10cc-4d18-9880-9391fc966be0, 5e3c2400-0583-4631-a957-0c3ee804e4c4, 5f04ee6c-dfec-4964-9f21-4d506d087f5b, 62bf178c-d69d-4d2a-9036-75b15ee06ec3, 692ea2fe-4089-4fdf-9ff7-e75de766c244, 6fa845a4-b382-46a6-a6d1-2a7b4e81c4bf, 733a7839-f8c8-4684-9002-135d637491d6, 743cab75-c3ed-4de3-96c3-172a4b3310ab, 7abf2bf3-4c1c-42c4-87b7-9b748d138f93, 7cef446f-23ce-4f1e-b365-d30d1ce857bc, 7d74abbf-4ce9-4b95-baaf-f74b79a235c9, 872b35e1-d23d-4a44-9253-d3bc4f4971ea, 8b55ee54-f270-4199-ae39-30969d5a2a3c, 8b725a98-495f-4855-be8b-43560389f430, 8d3bc388-d5a0-4e57-a205-1873956ab330, 8fbe9b44-03ee-45f1-b73f-76f4755a9df7, 91618762-0d75-42d6-bdec-4559a3ca10bb, 94ade82e-61c8-4113-b359-336773ce6901, a1c401be-b018-40d6-8880-7c00201ff778, a8208e40-b762-4764-bb3b-5d2bdd524af8, a93ae830-1b9e-4f97-9101-2a0351db5c9e, adce80af-ca3d-40d3-8b92-6ae8865bc36e, ae157cd6-1c22-48ea-b232-61ce85b730a6, b2209c74-bae3-4727-be5d-eee508ccc839, b74479af-ac93-43fc-b444-1bb97b4e43a1, b7dc5673-db83-42b2-953a-723318d092d2, b8dfad59-0385-45c9-b1ce-adb383ebf387, b9bfe644-8a90-47aa-8889-da47bbdd21f9]
205421 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit5790122458697468505/2015/03/17/446c654a-b960-49c8-99af-bb2dd554a46b_2_001.parquet
205431 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit5790122458697468505/2015/03/17/446c654a-b960-49c8-99af-bb2dd554a46b_2_001.parquet
205431 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit5790122458697468505/2015/03/17/446c654a-b960-49c8-99af-bb2dd554a46b_2_001.parquet => [c571e8f7-1c9c-4b3e-b639-630beb3a2d6a, ce66cd75-fa56-4106-b426-beb140927eda, cf4bbf41-3762-4fa2-ae7f-46572c23aedc, cfa3fc06-8800-40b3-b031-3ebb7a0bb8c4, d2ac0c56-de46-4d61-a60e-86d040264a1b, d2c58cf5-879e-4bfd-ac21-7614ba13ff48, d723e728-a5b8-4851-886e-24098afa08f3, db759962-b7a5-4f2b-956b-2a663fb4c7f8, dc33d555-8ca7-47bf-8378-8dffd771000c, e0153e50-07bc-4c33-9b35-576335492cd2, e02df8a8-ad48-4319-9267-16a1e736c72d, e7cc07d1-381b-42ea-88cb-86f54b5efa59, e992915e-66bf-43b2-8917-4636610d5a1f, eea66684-ed33-4b33-b4c0-57f12eb500aa, f13eb3ff-4d90-4f7e-b5e1-2d5c8b84f008, f438b5fd-f0d4-46aa-a7d8-2f3da428c029, f68b0900-5275-4e93-9f3f-cafa1d21c953, fddef4ed-820c-4fb3-aec4-8ee05a04bc51]
205447 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit5790122458697468505/2016/03/15/edcdc104-891a-4d50-83c9-32d8579ef180_0_001.parquet
205457 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit5790122458697468505/2016/03/15/edcdc104-891a-4d50-83c9-32d8579ef180_0_001.parquet
205457 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit5790122458697468505/2016/03/15/edcdc104-891a-4d50-83c9-32d8579ef180_0_001.parquet => [00fb8682-c512-4799-be36-db88a7206ace, 0369475b-1272-4170-b267-671cbd5c1135, 05e3b020-37b3-4cfd-bc45-d2c91a826ec9, 0c0eb395-4f0a-4bda-a87f-902203cc2a73, 10d37d32-077e-498f-b38a-ee0aefbd3ee2, 1705808e-078e-4953-b737-d0b35e4a4b9e, 1a788e08-d7db-48db-b585-5c41602d1384, 21b5c4d6-5fe6-4b6d-8636-625963f60b7f, 2444add3-1fe0-4e0a-89aa-8ae64670240f, 2ab4268f-e3e6-4bea-bad3-6577f77d7dfb, 2b3efd43-5683-42c3-a0e0-1d9659b6eacf, 2c43cea4-0f5b-47b0-87c6-8fe0c2cd772a, 2fba1302-035a-46f3-af32-045022eced05, 31253943-da69-4089-bc0c-7221349b7f2f, 368e4206-8d12-4765-9cd8-108a0fcc0843, 36ab7bdc-7b5c-4198-a07e-a0d80dc91acf, 3a764851-1666-40b3-b007-f6b95605e1b1, 3c257bc1-972d-471f-9c87-dea114154cd2, 3c48d927-5046-4a0b-b812-ef8147fd3cb1, 3c80beac-f67f-4da6-aef7-3972d4dc088d, 41198208-5d6a-4c0b-8ea0-1ca70a14567d, 435c5fb7-fcde-4952-ba67-02603cb1d09d, 436e1170-565f-49db-9f9a-36db3d7856b7, 47780377-08c1-4b46-985a-77af4bee4ece, 4e311c86-73c1-43ea-8383-4a6ac18e4ce6, 52fe1b90-6061-49e5-8676-dfc84ec31a41, 55d6c380-ca6e-487a-9aab-2d87203d78fc, 594cb1c4-c086-46ab-bc09-7c819574823d, 5a188337-9ffe-42f8-8f88-62cedbca018c, 60202a56-816a-445b-b239-c34742f82ff2, 6a3162dc-89e6-4295-8b43-81e11d3ad568, 6a471a02-dbcc-43c9-9aac-8d897b7fc28e, 6ac99228-4243-4084-b497-1a16dfda89ad, 6da354b0-524c-4fed-8337-36cd2c00efc2, 717c7ef0-0cb1-46a6-807e-0dd13276761a, 7b56a435-f113-4b0a-b665-ba63f4ab6aba, 7c08b8db-5796-48a3-a0df-77ce9138c36b, 7c43b3d4-497b-409c-bc8b-a68b95eb51d6, 7ca68479-e287-4287-bb4c-65176d8229da, 7cae0926-9aed-415d-bc4f-2bdea61360a5, 7d341b8e-3e9f-4b2c-8b2c-22dc3b7c4c6c, 82bcda98-4761-43ba-97bb-779ffa6d9ec4, 840c14e8-81b4-4674-aef5-55dc30bc24e6, 89540a6c-b40d-42fa-9e48-2a7dfb91866f, 975bb7eb-8a7b-40aa-bd72-6ce23bcb1284, 9a7909e0-3199-4063-92e3-db984d15589b, 9b459f37-0ae5-49ad-9175-a62a9987b621, 9fbe7a4e-e67d-45ab-b8b3-54845ed621e6, a0de0b18-8339-4661-b108-58c0d2fd600e, a141a795-36ee-44d3-9200-4c8ad6f3ef8b, a1890104-3e92-493b-83e8-8653a6c68ac3, a7905118-482e-43cc-bbf6-2bc87a9eac65, afd84932-25ea-466b-a320-cff556426de0, b3695398-abcc-4817-85da-0af6858fc805, ba54fd1e-3ecc-4dce-a6dc-320d47800e98, c211d0fb-93b6-4d8d-9e5d-975cd23df828, c6917cd7-199e-4f68-9c52-bd328af7f795, c720fd49-d1ae-4313-930e-82b8623f9186, cb3389bd-619b-40b0-8e7f-56dcbe065bba, cb8d1f03-98d8-4c29-8388-6b6540073538, cce0550c-b4a4-4cd2-985b-c177108ded74, ced83155-6d78-450a-b890-846dc644df5d, d4cf0399-dde8-4010-9552-90b06960e849, d9345a6b-d40d-46be-b4e3-5f10066eaf48, da8df628-9cad-42ab-8f65-1665a5d37b81, de882e89-8ee2-4cac-befd-8df9ea8981ce, e1c14474-91b5-44c1-94ee-616267382665, e4138ddb-a01a-4f70-9fc6-6d138fa74466, e987dd8d-200e-4fc9-9128-e5cc419c8f5f, ea2e5325-963a-410e-9976-1d2ca37ec9ce, f23d88ca-edda-4b93-bde9-358b91cbd813, f906868f-ddab-47f1-ad31-354a97a31bef, fa334d92-24a7-4d03-974d-13bc5c4cd133, fdd95cb3-a28a-4a59-b072-7400cbb1675f, ff53a091-b4aa-4071-8201-ad06ea536a1e]
205564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=75}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=56}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=69}}}
205575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
205575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=446c654a-b960-49c8-99af-bb2dd554a46b}, 1=BucketInfo {bucketType=UPDATE, fileLoc=1373f5f1-2ea2-481f-8cac-714dcf575784}, 2=BucketInfo {bucketType=UPDATE, fileLoc=edcdc104-891a-4d50-83c9-32d8579ef180}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{446c654a-b960-49c8-99af-bb2dd554a46b=0, 1373f5f1-2ea2-481f-8cac-714dcf575784=1, edcdc104-891a-4d50-83c9-32d8579ef180=2}
205592 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
205592 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
205669 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable  - Error upserting bucketType UPDATE for partition :0
java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
205670 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_0 failed due to an exception
205670 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_0 could not be removed as it was not found on disk or in memory
205671 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 50.0 (TID 66)
com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more
205672 [task-result-getter-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

205672 [task-result-getter-2] ERROR org.apache.spark.scheduler.TaskSetManager  - Task 0 in stage 50.0 failed 1 times; aborting job
205679 [Executor task launch worker-0] ERROR org.apache.spark.MapOutputTrackerMaster  - Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
205680 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Putting block rdd_82_1 failed due to an exception
205680 [Executor task launch worker-0] WARN  org.apache.spark.storage.BlockManager  - Block rdd_82_1 could not be removed as it was not found on disk or in memory
205680 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor  - Exception in task 1.0 in stage 50.0 (TID 67)
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:106)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	... 32 more
205680 [dispatcher-event-loop-9] ERROR org.apache.spark.scheduler.LiveListenerBus  - SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 172.17.0.2, 34371, None),rdd_82_1,StorageLevel(1 replicas),0,0))
205770 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
205770 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
205919 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
205920 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit898872506947389944/2016/05/01/id31_1_20160506030611.parquet	true
205920 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
205920 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit898872506947389944/2016/05/02/id32_1_20160506030611.parquet	true
205920 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
205920 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit898872506947389944/2016/05/06/id33_1_20160506030611.parquet	true
205925 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
205925 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
205930 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
205931 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
205931 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
206029 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
206056 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
206056 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit898872506947389944/2016/05/01/id21_1_20160502020601.parquet	true
206056 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
206056 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit898872506947389944/2016/05/02/id22_1_20160502020601.parquet	true
206057 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
206057 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit898872506947389944/2016/05/06/id23_1_20160502020601.parquet	true
206062 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
206062 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
206066 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
206137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
206137 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
206229 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
206248 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
206249 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
206249 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
206581 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/dce47aa4-019b-461d-a9c6-a3367413851f_1_000.parquet	true
206758 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/30f14d77-745e-46a8-9d5f-0ba6ccc15f1b_1_000.parquet	true
206924 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/49ce752b-141d-4111-a18b-f1170c09ecd7_1_000.parquet	true
207115 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/789643d0-b003-4bb9-a496-9961d791573f_1_000.parquet	true
207300 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/86a4850a-1406-428b-85c2-dedecc2abab7_1_000.parquet	true
207482 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/e526fff0-1290-4c31-98b4-cd6948c58ce4_1_000.parquet	true
207662 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/3e34ff4f-21aa-4382-930c-640763096716_1_000.parquet	true
207741 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
207811 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/b38be4c8-3173-4e14-8f1d-18da7ea8a8a1_1_000.parquet	true
207972 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/82ab7594-e331-44be-b2f5-e4a8636edb74_1_000.parquet	true
208049 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
208142 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3351943771064605807/.hoodie/.temp/f5f37c9b-6d8d-4227-9afa-1052c2d4ee43_1_000.parquet	true
208144 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
Tests run: 19, Failures: 0, Errors: 10, Skipped: 0, Time elapsed: 63.338 sec <<< FAILURE! - in com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
testSmallInsertHandlingForInserts(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 1 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieInsertException: Failed to insert for commit time 002
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForInserts(TestHoodieClientOnCopyOnWriteStorage.java:1126)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 11, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleInsertPartition(HoodieCopyOnWriteTable.java:520)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:436)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 29 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForInserts(TestHoodieClientOnCopyOnWriteStorage.java:1126)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testInsertAndCleanByCommits(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 4.415 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 20180316082216
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByCommits(TestHoodieClientOnCopyOnWriteStorage.java:764)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 70, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByCommits(TestHoodieClientOnCopyOnWriteStorage.java:764)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testAutoCommit(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 18.428 sec  <<< ERROR!
org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 435, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testAutoCommit(TestHoodieClientOnCopyOnWriteStorage.java:211)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testCreateSavepoint(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 3.231 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 002
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testCreateSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:432)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testCreateSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:432)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testUpserts(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 2.405 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 004
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:273)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpserts(TestHoodieClientOnCopyOnWriteStorage.java:218)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 61, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:273)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpserts(TestHoodieClientOnCopyOnWriteStorage.java:218)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testSmallInsertHandlingForUpserts(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 1.756 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 002
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForUpserts(TestHoodieClientOnCopyOnWriteStorage.java:1016)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 50, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForUpserts(TestHoodieClientOnCopyOnWriteStorage.java:1016)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testUpsertsWithFinalizeWrite(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 5.986 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 004
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:273)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsWithFinalizeWrite(TestHoodieClientOnCopyOnWriteStorage.java:313)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 461, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:273)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsWithFinalizeWrite(TestHoodieClientOnCopyOnWriteStorage.java:313)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testInsertAndCleanByVersions(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 4.096 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 20180316082303
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByVersions(TestHoodieClientOnCopyOnWriteStorage.java:662)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 70, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByVersions(TestHoodieClientOnCopyOnWriteStorage.java:662)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testDeletes(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 3.281 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 004
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testDeletes(TestHoodieClientOnCopyOnWriteStorage.java:372)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 82, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testDeletes(TestHoodieClientOnCopyOnWriteStorage.java:372)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

testRollbackToSavepoint(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 2.737 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 002
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testRollbackToSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:531)
Caused by: org.apache.spark.SparkException: 
Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 66, localhost, executor driver): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:512)
	at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:433)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException: spark.executor.memory
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:243)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.SparkConf.get(SparkConf.scala:243)
	at com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:112)
	at com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:154)
	at com.uber.hoodie.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:73)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:479)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:429)
	at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:504)
	... 28 more

Driver stacktrace:
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testRollbackToSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:531)
Caused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
Caused by: java.util.NoSuchElementException: spark.executor.memory

Running com.uber.hoodie.TestMultiFS
Formatting using clusterid: testClusterID
209390 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
209804 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
210607 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
210907 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
211061 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
211445 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082318
211454 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting commit 20180316082318
211572 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
211736 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
211736 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
211948 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=28, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=37, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=35, numUpdates=0}}}
212221 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 28, totalInsertBuckets => 1, recordsPerBucket => 500000
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 35, totalInsertBuckets => 1, recordsPerBucket => 500000
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
212363 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
212376 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082318
212376 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082318
212472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
212472 [pool-1607-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
212490 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
212490 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
212922 [pool-1607-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
212937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
212937 [pool-1608-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
212951 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
212951 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
213309 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
213582 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
213788 [pool-1608-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
213811 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
213811 [pool-1609-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
213832 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
213832 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
214680 [pool-1609-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
214705 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
215110 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
215110 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
215122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
215122 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
215257 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
215272 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
215667 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082318 as complete
215667 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082318
215959 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180316082322
215966 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting write commit 20180316082322
215974 [main] INFO  com.uber.hoodie.TestMultiFS  - Writing to path: file:///tmp/hoodie/sample-table
216003 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
216237 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
216237 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
216441 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=37, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=32, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=31, numUpdates=0}}}
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 32, totalInsertBuckets => 1, recordsPerBucket => 500000
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 31, totalInsertBuckets => 1, recordsPerBucket => 500000
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
216449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
216461 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180316082322
216461 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180316082322
216550 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
216550 [pool-1611-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
216570 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
216570 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
216587 [pool-1611-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
216610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
216610 [pool-1612-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
216613 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
216621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
216621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
216641 [pool-1612-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
216660 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
216660 [pool-1613-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
216672 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
216672 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
216704 [pool-1613-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
216721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
216721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
216815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
216815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
216965 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
216970 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180316082322 as complete
216970 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180316082322
216970 [main] INFO  com.uber.hoodie.TestMultiFS  - Reading from path: file:///tmp/hoodie/sample-table
217204 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
217323 [DataNode: [[[DISK]file:/tmp/1521184995119-0/dfs/data/data1/, [DISK]file:/tmp/1521184995119-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36997] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-618208731-172.17.0.2-1521184995212 (Datanode Uuid 05d9559b-3e09-40bb-bffb-8d6839e2920b) service to localhost/127.0.0.1:36997 interrupted
217325 [DataNode: [[[DISK]file:/tmp/1521184995119-0/dfs/data/data1/, [DISK]file:/tmp/1521184995119-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36997] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-618208731-172.17.0.2-1521184995212 (Datanode Uuid 05d9559b-3e09-40bb-bffb-8d6839e2920b) service to localhost/127.0.0.1:36997
217333 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@4e53488e] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
217540 [spirals-vortex:43403.activeMasterManager-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.994 sec - in com.uber.hoodie.TestMultiFS
Running com.uber.hoodie.metrics.TestHoodieMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.036 sec - in com.uber.hoodie.metrics.TestHoodieMetrics
217603 [Thread-903] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit1794041572377723184/2015/03/16/.a582e15b-9dda-4089-8525-39292810d658_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217603 [Thread-673] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit8385931549291184186/2016/03/15/.d2e0826c-18e2-4241-af98-9bbb52517944_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1132] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/.96edef2a-9048-4878-8371-270fe9e339ba_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-681] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit8385931549291184186/2015/03/16/.af89a756-09fc-483c-8b55-387b558d8bcc_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217603 [Thread-1166] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/.4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1211] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/.65019474-2421-4585-a97b-1b57550f80fb_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1120] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/.65019474-2421-4585-a97b-1b57550f80fb_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-665] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit8385931549291184186/2015/03/17/.c69b3c20-c3d6-4b90-a50f-6ff5746641b0_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1219] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/.4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1172] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/.96edef2a-9048-4878-8371-270fe9e339ba_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-897] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit1794041572377723184/2016/03/15/.623a3c0d-c7fb-4051-ad14-5e3067ce879f_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1160] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2016/03/15/.65019474-2421-4585-a97b-1b57550f80fb_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-909] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit1794041572377723184/2015/03/17/.4e921783-1296-4aad-bef1-fcfc38a9a51a_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1126] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/16/.4b0ae810-1c96-4af3-988c-d1d4a86e0d7f_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
217605 [Thread-1227] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:36206/tmp/junit3076354357367460905/2015/03/17/.96edef2a-9048-4878-8371-270fe9e339ba_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
218245 [Executor task launch worker-0-SendThread(localhost:54917)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x1622daf69590009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)

Results :

Tests in error: 
  TestHoodieClientOnCopyOnWriteStorage.testAutoCommit:211 » Spark Job aborted du...
  TestHoodieClientOnCopyOnWriteStorage.testCreateSavepoint:432 » HoodieUpsert Fa...
  TestHoodieClientOnCopyOnWriteStorage.testDeletes:372 » HoodieUpsert Failed to ...
  TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByCommits:764 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByVersions:662 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testRollbackToSavepoint:531 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForInserts:1126 » HoodieInsert
  TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForUpserts:1016 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testUpserts:218->testUpsertsInternal:273 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testUpsertsWithFinalizeWrite:313->testUpsertsInternal:273 » HoodieUpsert
  TestUpdateMapFunction.testSchemaEvolutionOnUpdate:106 » NullPointer
  TestCopyOnWriteTable.testUpdateRecords:197 » NoSuchElement spark.executor.memo...
  TestMergeOnReadTable.testCOWToMORConvertedDatasetRollback:383 » HoodieUpsert F...
  TestMergeOnReadTable.testUpsertPartitioner:598 » HoodieUpsert Failed to upsert...

Tests run: 66, Failures: 0, Errors: 14, Skipped: 1

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hoodie ............................................. SUCCESS [  0.064 s]
[INFO] hoodie-common ...................................... SUCCESS [01:23 min]
[INFO] hoodie-hadoop-mr ................................... SUCCESS [  5.155 s]
[INFO] hoodie-client ...................................... FAILURE [03:41 min]
[INFO] hoodie-spark ....................................... SKIPPED
[INFO] hoodie-hive ........................................ SKIPPED
[INFO] hoodie-utilities ................................... SKIPPED
[INFO] hoodie-cli ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 05:10 min
[INFO] Finished at: 2018-03-16T08:23:26+01:00
[INFO] Final Memory: 59M/1307M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hoodie-client: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/uber/hudi/354187058/hoodie-client/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hoodie-client
