[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-common:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/341719691/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-client:jar:0.4.1-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hbase:hbase-client:jar -> version (?) vs 1.2.3 @ com.uber.hoodie:hoodie-client:[unknown-version], /root/workspace/uber/hudi/341719691/hoodie-client/pom.xml, line 195, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/341719691/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-cli:jar:0.4.1-SNAPSHOT
[WARNING] 'dependencies.dependency.systemPath' for dnl.utils:textutils:jar should not point at files within the project directory, ${basedir}/lib/dnl/utils/textutils/0.3.3/textutils-0.3.3.jar will be unresolvable by dependent projects @ com.uber.hoodie:hoodie-cli:[unknown-version], /root/workspace/uber/hudi/341719691/hoodie-cli/pom.xml, line 164, column 19
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/341719691/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hadoop-mr:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/341719691/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hive:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/341719691/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-utilities:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie-utilities:[unknown-version], /root/workspace/uber/hudi/341719691/hoodie-utilities/pom.xml, line 37, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie:pom:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ line 156, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Hoodie
[INFO] hoodie-common
[INFO] hoodie-hadoop-mr
[INFO] hoodie-client
[INFO] hoodie-spark
[INFO] hoodie-hive
[INFO] hoodie-utilities
[INFO] hoodie-cli
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hoodie 0.4.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-common 0.4.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-common ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/341719691/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/341719691/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- avro-maven-plugin:1.7.6:schema (default) @ hoodie-common ---
[INFO] Importing File: /root/workspace/uber/hudi/341719691/hoodie-common/src/main/avro/HoodieCommitMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/341719691/hoodie-common/src/main/avro/HoodieSavePointMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/341719691/hoodie-common/src/main/avro/HoodieCompactionMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/341719691/hoodie-common/src/main/avro/HoodieCleanMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/341719691/hoodie-common/src/main/avro/HoodieRollbackMetadata.avsc
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/341719691/hoodie-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-common ---
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom (4 KB at 5.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom (3 KB at 80.7 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar (74 KB at 549.6 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.common.table.log.HoodieLogFormatTest
0    [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - Cleaning HDFS cluster data at: /tmp/1518663702566-0/dfs and starting fresh.
73   [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS force binding to ip: 127.0.0.1
2698 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
4648 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
11594 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS Minicluster service started.
11732 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper force binding to: 127.0.0.1
11868 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper Minicluster service started on client port: 2828
11952 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
12995 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
13018 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
13118 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
13118 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
13252 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
13252 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4218004374521104591
13296 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4218004374521104591 as 1
13296 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4218004374521104591/.test-fileid1_100.log.1
13310 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4218004374521104591/.test-fileid1_100.log.1} does not exist. Create a new file
13942 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
13977 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14009 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14010 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14224 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14244 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14257 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14269 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14269 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14269 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14270 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14270 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
14281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit4218004374521104591/.test-fileid1_100.log.1
14281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
14325 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
14493 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14502 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14508 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14509 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
14509 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14510 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9064419565558678913
14512 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9064419565558678913 as 1
14513 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9064419565558678913/.test-fileid1_100.log.1
14515 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9064419565558678913/.test-fileid1_100.log.1} does not exist. Create a new file
15000 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
15554 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15567 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15577 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15577 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
15577 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15577 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1552371367608308041
15582 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1552371367608308041 as 1
15582 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1552371367608308041/.test-fileid1_100.log.1
15584 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1552371367608308041/.test-fileid1_100.log.1} does not exist. Create a new file
16121 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16152 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16161 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16167 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16168 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16168 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16168 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1992863384273474860
16170 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1992863384273474860 as 1
16170 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1992863384273474860/.test-fileid1_100.log.1
16172 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1992863384273474860/.test-fileid1_100.log.1} does not exist. Create a new file
16273 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16273 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1992863384273474860
16280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1992863384273474860 as 1
16280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1992863384273474860/.test-fileid1_100.log.1
16283 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1992863384273474860/.test-fileid1_100.log.1} exists. Appending to existing file
16309 [IPC Server handler 1 on 37195] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit1992863384273474860/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1646901710_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
16311 [IPC Server handler 1 on 37195] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit1992863384273474860/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1646901710_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
16319 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit1992863384273474860/.test-fileid1_100.log.1
16326 [IPC Server handler 2 on 37195] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1992863384273474860/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1009 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-ab0af316-c1f4-4a97-b4db-dac77d2f5bbd:NORMAL:127.0.0.1:50010|RBW]]}
17348 [IPC Server handler 4 on 37195] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit1992863384273474860/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1010 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-ab0af316-c1f4-4a97-b4db-dac77d2f5bbd:NORMAL:127.0.0.1:50010|RBW]]}
17411 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741832_1008] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741832_1008
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
17411 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42636 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741832_1008]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42636 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:42636]. 58839 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
18351 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit1992863384273474860/.test-fileid1_100.log.1
18469 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18493 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18501 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18506 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18506 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18507 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18507 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5050915965010174481
18509 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5050915965010174481 as 1
18510 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5050915965010174481/.test-fileid1_100.log.1
18511 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5050915965010174481/.test-fileid1_100.log.1} does not exist. Create a new file
18609 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18609 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5050915965010174481
18612 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5050915965010174481 as 1
18614 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5050915965010174481/.test-fileid1_100.log.1
18615 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5050915965010174481/.test-fileid1_100.log.1} exists. Appending to existing file
18669 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18677 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18681 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18681 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18691 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1} has a corrupted block at 7417
18754 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1} starts at 7448
18771 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18775 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18775 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18775 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Invalid or extra rollback command block in hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18775 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
18787 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18808 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit5050915965010174481/.test-fileid1_100.log.1
18834 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19040 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19084 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19102 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19103 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19168 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19173 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit517857222707542213/append_test
19174 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit517857222707542213/append_test as 1
19174 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit517857222707542213/append_test/.commits.archive_.archive.1
19175 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit517857222707542213/append_test/.commits.archive_.archive.1} does not exist. Create a new file
19229 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19229 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit517857222707542213/append_test
19230 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit517857222707542213/append_test as 1
19230 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit517857222707542213/append_test/.commits.archive_.archive.1
19231 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit517857222707542213/append_test/.commits.archive_.archive.1} exists. Appending to existing file
19231 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
19286 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19417 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19426 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19432 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19432 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19433 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19433 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3948066251261472367
19437 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3948066251261472367 as 1
19437 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3948066251261472367/.test-fileid1_100.log.1
19441 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3948066251261472367/.test-fileid1_100.log.1} does not exist. Create a new file
19734 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19777 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19801 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19821 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19830 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19835 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19835 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19835 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
19845 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19846 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit3948066251261472367/.test-fileid1_100.log.1
19888 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
20030 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
20036 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
20045 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
20045 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
20045 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20045 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2002795953051446531
20052 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2002795953051446531 as 1
20052 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2002795953051446531/.test-fileid1_100.log.1
20054 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2002795953051446531/.test-fileid1_100.log.1} does not exist. Create a new file
20099 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20099 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2002795953051446531
20102 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2002795953051446531 as 1
20102 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2002795953051446531/.test-fileid1_100.log.1
20103 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2002795953051446531/.test-fileid1_100.log.1} exists. Appending to existing file
20176 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20177 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2002795953051446531
20179 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2002795953051446531 as 1
20179 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2002795953051446531/.test-fileid1_100.log.1
20181 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2002795953051446531/.test-fileid1_100.log.1} exists. Appending to existing file
20251 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
20847 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
20853 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
20859 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
20859 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
20860 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20860 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8537853466765738547
20862 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8537853466765738547 as 1
20862 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8537853466765738547/.test-fileid1_100.log.1
20870 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8537853466765738547/.test-fileid1_100.log.1} does not exist. Create a new file
21022 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21022 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8537853466765738547
21027 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8537853466765738547 as 1
21028 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8537853466765738547/.test-fileid1_100.log.1
21037 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8537853466765738547/.test-fileid1_100.log.1} exists. Appending to existing file
21278 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21278 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8537853466765738547
21290 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8537853466765738547 as 1
21290 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8537853466765738547/.test-fileid1_100.log.1
21298 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8537853466765738547/.test-fileid1_100.log.1} exists. Appending to existing file
21501 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
21643 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
21657 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
21666 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
21667 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
21667 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21667 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4152340727242669317
21672 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4152340727242669317 as 1
21673 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4152340727242669317/.test-fileid1_100.log.1
21675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} does not exist. Create a new file
22027 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} has a corrupted block at 2051
22133 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} starts at 2082
22283 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22283 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4152340727242669317
22286 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4152340727242669317 as 1
22287 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4152340727242669317/.test-fileid1_100.log.1
22297 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} exists. Appending to existing file
22506 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} has a corrupted block at 2051
22581 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} starts at 2082
22583 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} has a corrupted block at 2094
22627 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {/tmp/junit4152340727242669317/.test-fileid1_100.log.1} starts at 2130
22638 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22672 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22678 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22682 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22682 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2072779912075903099
22685 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2072779912075903099 as 1
22685 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2072779912075903099/.test-fileid1_100.log.1
22686 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2072779912075903099/.test-fileid1_100.log.1} does not exist. Create a new file
22713 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22719 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22727 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22727 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22742 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22743 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22745 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22745 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22745 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22745 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
22748 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit2072779912075903099/.test-fileid1_100.log.1
22748 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
22754 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23177 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23183 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23187 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23188 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23188 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23192 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8933120100885901284
23198 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8933120100885901284 as 1
23198 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8933120100885901284/.test-fileid1_100.log.1
23203 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8933120100885901284/.test-fileid1_100.log.1} does not exist. Create a new file
23342 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23377 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23478 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23479 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit8933120100885901284/.test-fileid1_100.log.1
23526 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit8933120100885901284/.test-fileid1_100.log.1
23528 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit8933120100885901284/.test-fileid1_100.log.1
23528 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Invalid or extra rollback command block in hdfs://localhost:37195/tmp/junit8933120100885901284/.test-fileid1_100.log.1
23528 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
23528 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit8933120100885901284/.test-fileid1_100.log.1
23545 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23696 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23724 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23730 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23730 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23730 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23731 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2765028083794081162
23739 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2765028083794081162 as 1
23739 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2765028083794081162/.test-fileid1_100.log.1
23747 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2765028083794081162/.test-fileid1_100.log.1} does not exist. Create a new file
24616 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
24616 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2765028083794081162
24619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2765028083794081162 as 1
24622 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2765028083794081162/.test-fileid1_100.log.1
24624 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2765028083794081162/.test-fileid1_100.log.1} exists. Appending to existing file
25318 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25319 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2765028083794081162
25321 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2765028083794081162 as 1
25322 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2765028083794081162/.test-fileid1_100.log.1
25324 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2765028083794081162/.test-fileid1_100.log.1} exists. Appending to existing file
25418 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25441 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25450 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25450 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25473 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25475 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25477 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25478 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1} has a corrupted block at 7874
25479 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1} starts at 7874
25480 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25481 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1} has a corrupted block at 7886
25483 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1} starts at 7886
25483 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25494 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25494 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1} has a corrupted block at 8121
25494 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1} starts at 8121
25494 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit2765028083794081162/.test-fileid1_100.log.1
25498 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
25521 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
25643 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25652 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25661 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25661 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
25661 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25661 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5887439020895415642
25669 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5887439020895415642 as 1
25670 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5887439020895415642/.test-fileid1_100.log.1
25680 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5887439020895415642/.test-fileid1_100.log.1} does not exist. Create a new file
25832 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25833 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5887439020895415642
25841 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5887439020895415642 as 1
25841 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5887439020895415642/.test-fileid1_100.log.1
25849 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5887439020895415642/.test-fileid1_100.log.1} exists. Appending to existing file
25973 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4078 has reached threshold 2038. Rolling over to the next version
26025 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5887439020895415642/.test-fileid1_100.log.2} does not exist. Create a new file
26068 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
26204 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26249 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26262 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26262 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
26263 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26263 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8011704398028971770
26280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8011704398028971770 as 1
26281 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8011704398028971770/.test-fileid1_100.log.1
26296 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8011704398028971770/.test-fileid1_100.log.1} does not exist. Create a new file
26422 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 7405 has reached threshold 500. Rolling over to the next version
26465 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8011704398028971770/.test-fileid1_100.log.2} does not exist. Create a new file
26549 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 7405 has reached threshold 500. Rolling over to the next version
26561 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8011704398028971770/.test-fileid1_100.log.3} does not exist. Create a new file
26567 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26571 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26575 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26575 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.1
26581 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.1
26581 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.1
26582 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.2
26589 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.2
26589 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.2
26590 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit8011704398028971770/.test-fileid1_100.log.3
26598 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
26614 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26619 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26625 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26625 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
26625 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26625 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit626161397640205354
26627 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit626161397640205354 as 1
26627 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit626161397640205354/.test-fileid1_100.log.1
26628 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit626161397640205354/.test-fileid1_100.log.1} does not exist. Create a new file
26657 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26662 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26667 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26668 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26679 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26681 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26683 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26684 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26685 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:37195/tmp/junit626161397640205354/.test-fileid1_100.log.1
26686 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
26691 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
26719 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26724 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26728 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26728 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
26729 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26729 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1509210363572403540
26731 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1509210363572403540 as 1
26731 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1509210363572403540/.test-fileid1_100.log.1
26732 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1509210363572403540/.test-fileid1_100.log.1} does not exist. Create a new file
26760 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26766 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26770 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26770 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26783 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26785 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26787 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26787 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26793 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26798 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26802 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26802 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26813 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26815 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26817 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks in hdfs://localhost:37195/tmp/junit1509210363572403540/.test-fileid1_100.log.1
26822 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
26846 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26851 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26855 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26855 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
26855 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26855 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4297012201748688517
26857 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4297012201748688517 as 1
26857 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4297012201748688517/.test-fileid1_100.log.1
26861 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4297012201748688517/.test-fileid1_100.log.1} does not exist. Create a new file
26866 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741849_1042] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741849_1042
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
26867 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
26867 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741856_1050] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741856_1050
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
26867 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741845_1033] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741845_1033
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
26866 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741847_1035] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741847_1035
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
26866 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741826_1002] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741826_1002
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
26867 [ResponseProcessor for block BP-892770822-172.17.0.1-1518663706480:blk_1073741858_1052] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-892770822-172.17.0.1-1518663706480:blk_1073741858_1052
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27072 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42693 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741858_1052]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42693 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59793 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27053 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42677 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741849_1041]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42677 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 58404 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27072 [DataNode: [[[DISK]file:/tmp/1518663702566-0/dfs/data/data1/, [DISK]file:/tmp/1518663702566-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:37195] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-892770822-172.17.0.1-1518663706480 (Datanode Uuid dcdd6d80-9723-4283-a5bd-08759f888552) service to localhost/127.0.0.1:37195 interrupted
27053 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42689 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741856_1050]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42689 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59665 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27053 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42627 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741826_1002]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42627 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 46939 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27074 [DataNode: [[[DISK]file:/tmp/1518663702566-0/dfs/data/data1/, [DISK]file:/tmp/1518663702566-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:37195] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-892770822-172.17.0.1-1518663706480 (Datanode Uuid dcdd6d80-9723-4283-a5bd-08759f888552) service to localhost/127.0.0.1:37195
27053 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42668 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741847_1035]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42668 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 56332 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27053 [DataXceiver for client DFSClient_NONMAPREDUCE_1646901710_1 at /127.0.0.1:42664 [Receiving block BP-892770822-172.17.0.1-1518663706480:blk_1073741845_1033]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42664 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55722 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
27201 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@656fcf12] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.097 sec - in com.uber.hoodie.common.table.log.HoodieLogFormatTest
Running com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
27482 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7658041473278007342 as hoodie dataset /tmp/junit7658041473278007342
27525 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7658041473278007342
27634 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7086382570578214573 as hoodie dataset /tmp/junit7086382570578214573
27689 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7086382570578214573
27734 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7584913250951278141 as hoodie dataset /tmp/junit7584913250951278141
27769 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7584913250951278141
27812 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit5962311958346762073 as hoodie dataset /tmp/junit5962311958346762073
27857 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit5962311958346762073
27898 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8378461175268262469 as hoodie dataset /tmp/junit8378461175268262469
27949 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8378461175268262469
27980 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7494755547209900892 as hoodie dataset /tmp/junit7494755547209900892
27997 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7494755547209900892
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.557 sec <<< FAILURE! - in com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
streamLatestVersionsBefore(com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest)  Time elapsed: 0.118 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit7658041473278007342 is valid dataset
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.io.EOFException: End of File Exception between local host is: "spirals-librepair/172.17.0.1"; destination host is: "localhost":37195; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.io.EOFException

testGetLatestDataFilesForFileId(com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest)  Time elapsed: 0.131 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit7086382570578214573 is valid dataset
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)

testStreamLatestVersionInPartition(com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest)  Time elapsed: 0.076 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit7584913250951278141 is valid dataset
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)

streamLatestVersionInRange(com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest)  Time elapsed: 0.088 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit5962311958346762073 is valid dataset
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)

testStreamEveryVersionInPartition(com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest)  Time elapsed: 0.095 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit8378461175268262469 is valid dataset
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)

streamLatestVersions(com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest)  Time elapsed: 0.048 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit7494755547209900892 is valid dataset
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest.init(HoodieTableFileSystemViewTest.java:60)

Running com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
28079 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1300564929527171943 as hoodie dataset /tmp/junit1300564929527171943
28141 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1300564929527171943
28194 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit669321941633576537 as hoodie dataset /tmp/junit669321941633576537
28209 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit669321941633576537
28244 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8759484906312246575 as hoodie dataset /tmp/junit8759484906312246575
28265 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8759484906312246575
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.107 sec <<< FAILURE! - in com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
testLoadingInstantsFromFiles(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)  Time elapsed: 0 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit1300564929527171943 is valid dataset
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)

testLoadingInstantsFromFiles(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NullPointerException
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.tearDown(HoodieActiveTimelineTest.java:52)

testTimelineOperations(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)  Time elapsed: 0.052 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit669321941633576537 is valid dataset
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)

testTimelineOperations(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)  Time elapsed: 0.052 sec  <<< ERROR!
java.lang.NullPointerException
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.tearDown(HoodieActiveTimelineTest.java:52)

testTimelineOperationsBasic(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)  Time elapsed: 0.054 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit8759484906312246575 is valid dataset
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.setUp(HoodieActiveTimelineTest.java:47)

testTimelineOperationsBasic(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)  Time elapsed: 0.055 sec  <<< ERROR!
java.lang.NullPointerException
	at com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.tearDown(HoodieActiveTimelineTest.java:52)

Running com.uber.hoodie.common.table.HoodieTableMetaClientTest
28296 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7465049799006515716 as hoodie dataset /tmp/junit7465049799006515716
28304 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7465049799006515716
28329 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2282125433855791064 as hoodie dataset /tmp/junit2282125433855791064
28337 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2282125433855791064
28363 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3906387193144026841 as hoodie dataset /tmp/junit3906387193144026841
28374 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3906387193144026841
28400 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1638164351563195499 as hoodie dataset /tmp/junit1638164351563195499
28409 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1638164351563195499
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 0.138 sec <<< FAILURE! - in com.uber.hoodie.common.table.HoodieTableMetaClientTest
checkSerDe(com.uber.hoodie.common.table.HoodieTableMetaClientTest)  Time elapsed: 0.019 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit7465049799006515716 is valid dataset
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)

checkCommitTimeline(com.uber.hoodie.common.table.HoodieTableMetaClientTest)  Time elapsed: 0.034 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit2282125433855791064 is valid dataset
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)

checkArchiveCommitTimeline(com.uber.hoodie.common.table.HoodieTableMetaClientTest)  Time elapsed: 0.038 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit3906387193144026841 is valid dataset
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)

checkMetadata(com.uber.hoodie.common.table.HoodieTableMetaClientTest)  Time elapsed: 0.037 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Could not check if dataset /tmp/junit1638164351563195499 is valid dataset
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.table.HoodieTableMetaClientTest.init(HoodieTableMetaClientTest.java:47)

Running com.uber.hoodie.common.TestBloomFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec - in com.uber.hoodie.common.TestBloomFilter
Running com.uber.hoodie.common.model.TestHoodieWriteStat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in com.uber.hoodie.common.model.TestHoodieWriteStat
Running com.uber.hoodie.common.util.TestNumericUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in com.uber.hoodie.common.util.TestNumericUtils
Running com.uber.hoodie.common.util.TestFSUtils
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.031 sec - in com.uber.hoodie.common.util.TestFSUtils
Running com.uber.hoodie.common.util.TestParquetUtils
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.996 sec <<< FAILURE! - in com.uber.hoodie.common.util.TestParquetUtils
testHoodieWriteSupport(com.uber.hoodie.common.util.TestParquetUtils)  Time elapsed: 0.996 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieIOException: Failed to read row keys from Parquet /tmp/junit6345388829796712470/test.parquet
	at com.uber.hoodie.common.util.TestParquetUtils.testHoodieWriteSupport(TestParquetUtils.java:84)
Caused by: java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at com.uber.hoodie.common.util.TestParquetUtils.testHoodieWriteSupport(TestParquetUtils.java:84)
Caused by: java.net.ConnectException: Connection refused
	at com.uber.hoodie.common.util.TestParquetUtils.testHoodieWriteSupport(TestParquetUtils.java:84)

Feb 15, 2018 4:02:11 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 44,000
Feb 15, 2018 4:02:11 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 22,431B for [_hoodie_record_key] BINARY: 1,000 values, 40,007B raw, 22,329B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
29509 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16420
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-c5e2da15-94e3-44b4-b370-53e4fed1e3fe,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
29524 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16438
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-c5e2da15-94e3-44b4-b370-53e4fed1e3fe,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
29537 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16391
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-ab0af316-c1f4-4a97-b4db-dac77d2f5bbd,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
29540 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16423
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-ab0af316-c1f4-4a97-b4db-dac77d2f5bbd,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
29540 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16441
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-c5e2da15-94e3-44b4-b370-53e4fed1e3fe,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
29578 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16426
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-ab0af316-c1f4-4a97-b4db-dac77d2f5bbd,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
29594 [Thread-39] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16444
java.net.ConnectException: Call From spirals-librepair/172.17.0.1 to localhost:37195 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy23.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy24.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 19 more

Results :

Tests in error: 
  HoodieTableMetaClientTest.init:47  HoodieIO Could not check if dataset /tmp/j...
  HoodieTableMetaClientTest.init:47  HoodieIO Could not check if dataset /tmp/j...
  HoodieTableMetaClientTest.init:47  HoodieIO Could not check if dataset /tmp/j...
  HoodieTableMetaClientTest.init:47  HoodieIO Could not check if dataset /tmp/j...
com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.testLoadingInstantsFromFiles(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)
  Run 1: HoodieActiveTimelineTest.setUp:47  HoodieIO Could not check if dataset /tmp/j...
  Run 2: HoodieActiveTimelineTest.tearDown:52 NullPointer

com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.testTimelineOperations(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)
  Run 1: HoodieActiveTimelineTest.setUp:47  HoodieIO Could not check if dataset /tmp/j...
  Run 2: HoodieActiveTimelineTest.tearDown:52 NullPointer

com.uber.hoodie.common.table.string.HoodieActiveTimelineTest.testTimelineOperationsBasic(com.uber.hoodie.common.table.string.HoodieActiveTimelineTest)
  Run 1: HoodieActiveTimelineTest.setUp:47  HoodieIO Could not check if dataset /tmp/j...
  Run 2: HoodieActiveTimelineTest.tearDown:52 NullPointer

  HoodieTableFileSystemViewTest.init:60  HoodieIO Could not check if dataset /t...
  HoodieTableFileSystemViewTest.init:60  HoodieIO Could not check if dataset /t...
  HoodieTableFileSystemViewTest.init:60  HoodieIO Could not check if dataset /t...
  HoodieTableFileSystemViewTest.init:60  HoodieIO Could not check if dataset /t...
  HoodieTableFileSystemViewTest.init:60  HoodieIO Could not check if dataset /t...
  HoodieTableFileSystemViewTest.init:60  HoodieIO Could not check if dataset /t...
  TestParquetUtils.testHoodieWriteSupport:84  HoodieIO Failed to read row keys ...

Tests run: 42, Failures: 0, Errors: 14, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hoodie ............................................. SUCCESS [  0.232 s]
[INFO] hoodie-common ...................................... FAILURE [ 40.742 s]
[INFO] hoodie-hadoop-mr ................................... SKIPPED
[INFO] hoodie-client ...................................... SKIPPED
[INFO] hoodie-spark ....................................... SKIPPED
[INFO] hoodie-hive ........................................ SKIPPED
[INFO] hoodie-utilities ................................... SKIPPED
[INFO] hoodie-cli ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 41.511 s
[INFO] Finished at: 2018-02-15T04:02:12+01:00
[INFO] Final Memory: 24M/399M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hoodie-common: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/uber/hudi/341719691/hoodie-common/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hoodie-common
