[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-common:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/349988061/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-client:jar:0.4.1-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hbase:hbase-client:jar -> version (?) vs 1.2.3 @ com.uber.hoodie:hoodie-client:[unknown-version], /root/workspace/uber/hudi/349988061/hoodie-client/pom.xml, line 195, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/349988061/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-cli:jar:0.4.1-SNAPSHOT
[WARNING] 'dependencies.dependency.systemPath' for dnl.utils:textutils:jar should not point at files within the project directory, ${basedir}/lib/dnl/utils/textutils/0.3.3/textutils-0.3.3.jar will be unresolvable by dependent projects @ com.uber.hoodie:hoodie-cli:[unknown-version], /root/workspace/uber/hudi/349988061/hoodie-cli/pom.xml, line 164, column 19
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/349988061/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hadoop-mr:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/349988061/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-hive:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie:0.4.1-SNAPSHOT, /root/workspace/uber/hudi/349988061/pom.xml, line 156, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie-utilities:jar:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ com.uber.hoodie:hoodie-utilities:[unknown-version], /root/workspace/uber/hudi/349988061/hoodie-utilities/pom.xml, line 37, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for com.uber.hoodie:hoodie:pom:0.4.1-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-compiler-plugin is missing. @ line 156, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Hoodie
[INFO] hoodie-common
[INFO] hoodie-hadoop-mr
[INFO] hoodie-client
[INFO] hoodie-spark
[INFO] hoodie-hive
[INFO] hoodie-utilities
[INFO] hoodie-cli
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hoodie 0.4.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-common 0.4.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-common ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/349988061/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/349988061/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- avro-maven-plugin:1.7.6:schema (default) @ hoodie-common ---
[INFO] Importing File: /root/workspace/uber/hudi/349988061/hoodie-common/src/main/avro/HoodieCommitMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/349988061/hoodie-common/src/main/avro/HoodieSavePointMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/349988061/hoodie-common/src/main/avro/HoodieCompactionMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/349988061/hoodie-common/src/main/avro/HoodieCleanMetadata.avsc
[INFO] Importing File: /root/workspace/uber/hudi/349988061/hoodie-common/src/main/avro/HoodieRollbackMetadata.avsc
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/349988061/hoodie-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-common ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-common ---
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.pom (4 KB at 10.7 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.19.1/surefire-providers-2.19.1.pom (3 KB at 125.1 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.19.1/surefire-junit4-2.19.1.jar (74 KB at 1636.7 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.common.model.TestHoodieWriteStat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.161 sec - in com.uber.hoodie.common.model.TestHoodieWriteStat
Running com.uber.hoodie.common.util.TestParquetUtils
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.903 sec - in com.uber.hoodie.common.util.TestParquetUtils
Running com.uber.hoodie.common.util.collection.TestExternalSpillableMap
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.489 sec - in com.uber.hoodie.common.util.collection.TestExternalSpillableMap
Running com.uber.hoodie.common.util.collection.TestDiskBasedMap
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 sec - in com.uber.hoodie.common.util.collection.TestDiskBasedMap
Running com.uber.hoodie.common.util.TestNumericUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.uber.hoodie.common.util.TestNumericUtils
Running com.uber.hoodie.common.util.TestFSUtils
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.029 sec - in com.uber.hoodie.common.util.TestFSUtils
Running com.uber.hoodie.common.table.HoodieTableMetaClientTest
1553 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1768200581537274338 as hoodie dataset /tmp/junit1768200581537274338
1567 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1768200581537274338
1569 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1768200581537274338/.hoodie/hoodie.properties
1570 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1768200581537274338
1570 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1768200581537274338
1756 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
1757 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>1__commit]
1764 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit1768200581537274338/.hoodie/1.inflight
1764 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
1770 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
1771 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
1798 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6976073377650942131 as hoodie dataset /tmp/junit6976073377650942131
1804 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6976073377650942131
1805 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6976073377650942131/.hoodie/hoodie.properties
1806 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6976073377650942131
1806 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6976073377650942131
1806 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
1813 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>1__commit]
1818 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit6976073377650942131/.hoodie/1.inflight
1818 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
1823 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
1824 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
1841 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit5613679294061457010 as hoodie dataset /tmp/junit5613679294061457010
1847 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit5613679294061457010
1847 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit5613679294061457010/.hoodie/hoodie.properties
1848 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5613679294061457010
1848 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit5613679294061457010
1903 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit317440943543569412 as hoodie dataset /tmp/junit317440943543569412
1909 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit317440943543569412
1909 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit317440943543569412/.hoodie/hoodie.properties
1910 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit317440943543569412
1910 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit317440943543569412
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.385 sec - in com.uber.hoodie.common.table.HoodieTableMetaClientTest
Running com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
1927 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2864647543907306722 as hoodie dataset /tmp/junit2864647543907306722
1932 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2864647543907306722
1933 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2864647543907306722/.hoodie/hoodie.properties
1933 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2864647543907306722
1933 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2864647543907306722
1936 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
1939 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2864647543907306722
1940 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2864647543907306722/.hoodie/hoodie.properties
1940 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2864647543907306722
1940 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit2864647543907306722
1941 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
1979 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7291262548527124837 as hoodie dataset /tmp/junit7291262548527124837
1985 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7291262548527124837
1985 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7291262548527124837/.hoodie/hoodie.properties
1986 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7291262548527124837
1986 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7291262548527124837
1987 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
1990 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7291262548527124837
1991 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7291262548527124837/.hoodie/hoodie.properties
1992 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7291262548527124837
1992 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit7291262548527124837
1992 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
1994 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
1999 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7291262548527124837/.hoodie/1.inflight
2000 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
2000 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7291262548527124837
2000 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7291262548527124837/.hoodie/hoodie.properties
2001 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7291262548527124837
2001 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit7291262548527124837
2002 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
2003 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7291262548527124837
2004 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7291262548527124837/.hoodie/hoodie.properties
2004 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7291262548527124837
2004 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit7291262548527124837
2005 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit]]
2006 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>2__commit]
2011 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit7291262548527124837/.hoodie/2.inflight
2011 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>2__commit]
2011 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7291262548527124837
2012 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7291262548527124837/.hoodie/hoodie.properties
2012 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7291262548527124837
2012 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit7291262548527124837
2013 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit]]
2031 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6910429733340339079 as hoodie dataset /tmp/junit6910429733340339079
2037 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6910429733340339079
2037 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6910429733340339079/.hoodie/hoodie.properties
2038 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6910429733340339079
2038 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6910429733340339079
2038 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
2041 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6910429733340339079
2042 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6910429733340339079/.hoodie/hoodie.properties
2042 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6910429733340339079
2042 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit6910429733340339079
2043 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
2070 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3901362177139731850 as hoodie dataset /tmp/junit3901362177139731850
2075 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3901362177139731850
2076 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3901362177139731850/.hoodie/hoodie.properties
2076 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3901362177139731850
2076 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3901362177139731850
2077 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
2080 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3901362177139731850
2080 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3901362177139731850/.hoodie/hoodie.properties
2081 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3901362177139731850
2081 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit3901362177139731850
2081 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
2101 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit5779836277134989571 as hoodie dataset /tmp/junit5779836277134989571
2106 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit5779836277134989571
2106 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit5779836277134989571/.hoodie/hoodie.properties
2106 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5779836277134989571
2106 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit5779836277134989571
2107 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
2109 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit5779836277134989571
2109 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit5779836277134989571/.hoodie/hoodie.properties
2109 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5779836277134989571
2109 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit5779836277134989571
2110 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
2128 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit627511238206709923 as hoodie dataset /tmp/junit627511238206709923
2132 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit627511238206709923
2133 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit627511238206709923/.hoodie/hoodie.properties
2133 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit627511238206709923
2133 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit627511238206709923
2133 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
2136 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit627511238206709923
2136 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit627511238206709923/.hoodie/hoodie.properties
2136 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit627511238206709923
2136 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading Active commit timeline for /tmp/junit627511238206709923
2137 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [2__commit], [3__commit], [4__commit]]
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.229 sec - in com.uber.hoodie.common.table.view.HoodieTableFileSystemViewTest
Running com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
2158 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit9020356815131624513 as hoodie dataset /tmp/junit9020356815131624513
2163 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit9020356815131624513
2163 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit9020356815131624513/.hoodie/hoodie.properties
2164 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit9020356815131624513
2164 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit9020356815131624513
2164 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
2164 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>1__commit]
2168 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit9020356815131624513/.hoodie/1.inflight
2168 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>1__commit]
2168 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>3__commit]
2181 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit9020356815131624513/.hoodie/3.inflight
2181 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>3__commit]
2181 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>5__commit]
2186 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit9020356815131624513/.hoodie/5.inflight
2186 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>5__commit]
2186 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Marking instant complete [==>8__commit]
2190 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit9020356815131624513/.hoodie/8.inflight
2190 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Completed [==>8__commit]
2190 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Creating a new in-flight instant [==>9__commit]
2194 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Created a new file in meta path: /tmp/junit9020356815131624513/.hoodie/9.inflight
2195 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[1__commit], [3__commit], [5__commit], [8__commit], [==>9__commit]]
2215 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2155973955049735999 as hoodie dataset /tmp/junit2155973955049735999
2220 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2155973955049735999
2220 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2155973955049735999/.hoodie/hoodie.properties
2221 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2155973955049735999
2221 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2155973955049735999
2245 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2673159209622853705 as hoodie dataset /tmp/junit2673159209622853705
2249 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2673159209622853705
2250 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2673159209622853705/.hoodie/hoodie.properties
2250 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2673159209622853705
2250 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2673159209622853705
2250 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants []
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.109 sec - in com.uber.hoodie.common.table.string.HoodieActiveTimelineTest
Running com.uber.hoodie.common.table.log.HoodieLogFormatTest
2265 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - Cleaning HDFS cluster data at: /tmp/1520366816027-0/dfs and starting fresh.
2272 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS force binding to ip: 127.0.0.1
Formatting using clusterid: testClusterID
5581 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
8639 [main] INFO  com.uber.hoodie.common.minicluster.HdfsTestService  - HDFS Minicluster service started.
8686 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper force binding to: 127.0.0.1
8738 [main] INFO  com.uber.hoodie.common.minicluster.ZookeeperTestService  - Zookeeper Minicluster service started on client port: 2828
8780 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
9469 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
9478 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
9545 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
9545 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
9548 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
9548 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4984072605169042443
9561 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4984072605169042443 as 1
9561 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4984072605169042443/.test-fileid1_100.log.1
9564 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4984072605169042443/.test-fileid1_100.log.1} does not exist. Create a new file
9623 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
9629 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
9633 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
9634 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9715 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9718 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9720 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit4984072605169042443/.test-fileid1_100.log.1
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
9723 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
9739 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
10166 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
10172 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
10179 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
10179 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
10179 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
10179 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit877359477835815678
10183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit877359477835815678 as 1
10183 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit877359477835815678/.test-fileid1_100.log.1
10185 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit877359477835815678/.test-fileid1_100.log.1} does not exist. Create a new file
10634 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
11062 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
11070 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
11077 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
11077 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
11077 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
11077 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4477226615462729587
11080 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4477226615462729587 as 1
11080 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4477226615462729587/.test-fileid1_100.log.1
11081 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4477226615462729587/.test-fileid1_100.log.1} does not exist. Create a new file
11522 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
11548 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
11555 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
11560 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
11560 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
11560 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
11560 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6218135655990276273
11562 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6218135655990276273 as 1
11562 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6218135655990276273/.test-fileid1_100.log.1
11563 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6218135655990276273/.test-fileid1_100.log.1} does not exist. Create a new file
11585 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
11587 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6218135655990276273
11589 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6218135655990276273 as 1
11589 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6218135655990276273/.test-fileid1_100.log.1
11591 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6218135655990276273/.test-fileid1_100.log.1} exists. Appending to existing file
11595 [IPC Server handler 3 on 36663] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit6218135655990276273/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1422260101_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
11596 [IPC Server handler 3 on 36663] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit6218135655990276273/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1422260101_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
11602 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit6218135655990276273/.test-fileid1_100.log.1
11607 [IPC Server handler 4 on 36663] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit6218135655990276273/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1009 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-1e68b8f1-ef05-446d-9cd5-793f0c5e6e73:NORMAL:127.0.0.1:50010|RBW]]}
12614 [IPC Server handler 6 on 36663] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit6218135655990276273/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1010 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-1e68b8f1-ef05-446d-9cd5-793f0c5e6e73:NORMAL:127.0.0.1:50010|RBW]]}
13617 [IPC Server handler 7 on 36663] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit6218135655990276273/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1011 for block blk_1073741832_1008{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-1e68b8f1-ef05-446d-9cd5-793f0c5e6e73:NORMAL:127.0.0.1:50010|RBW]]}
14590 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38790 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741832_1008]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38790 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:38790]. 56997 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
14590 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741832_1008] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741832_1008
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
14620 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit6218135655990276273/.test-fileid1_100.log.1
15178 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
15604 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15611 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15616 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15616 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
15617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2894145141203587003
15619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2894145141203587003 as 1
15619 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2894145141203587003/.test-fileid1_100.log.1
15620 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2894145141203587003/.test-fileid1_100.log.1} does not exist. Create a new file
16079 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16079 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2894145141203587003
16081 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2894145141203587003 as 1
16081 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2894145141203587003/.test-fileid1_100.log.1
16083 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2894145141203587003/.test-fileid1_100.log.1} exists. Appending to existing file
16528 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16535 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16539 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16540 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16548 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16549 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1} has a corrupted block at 8917
16590 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1} starts at 8948
16592 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16594 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16594 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16594 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Invalid or extra rollback command block in hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16594 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
16596 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16599 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit2894145141203587003/.test-fileid1_100.log.1
16605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
16605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 96
16605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8256
16605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 104
16605 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 15078
16618 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16636 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16642 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16646 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16646 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16672 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16672 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit2230577909728248200/append_test
16673 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit2230577909728248200/append_test as 1
16673 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit2230577909728248200/append_test/.commits.archive_.archive.1
16673 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit2230577909728248200/append_test/.commits.archive_.archive.1} does not exist. Create a new file
16683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit2230577909728248200/append_test
16684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit2230577909728248200/append_test as 1
16684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit2230577909728248200/append_test/.commits.archive_.archive.1
16684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit2230577909728248200/append_test/.commits.archive_.archive.1} exists. Appending to existing file
16684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
16699 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
17118 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
17124 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
17130 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
17130 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
17130 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
17131 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit870319957920264161
17132 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit870319957920264161 as 1
17132 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit870319957920264161/.test-fileid1_100.log.1
17133 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit870319957920264161/.test-fileid1_100.log.1} does not exist. Create a new file
17575 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
17582 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
17586 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
17587 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17594 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17596 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17599 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17599 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17599 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
17602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit870319957920264161/.test-fileid1_100.log.1
17609 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
17609 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 107
17609 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8239
17609 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 93
17609 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 13487
17621 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18042 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18049 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18054 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18054 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18054 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18054 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6635256145908429054
18056 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6635256145908429054 as 1
18056 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6635256145908429054/.test-fileid1_100.log.1
18057 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6635256145908429054/.test-fileid1_100.log.1} does not exist. Create a new file
18480 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18480 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6635256145908429054
18482 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6635256145908429054 as 1
18482 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6635256145908429054/.test-fileid1_100.log.1
18483 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6635256145908429054/.test-fileid1_100.log.1} exists. Appending to existing file
18924 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18924 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6635256145908429054
18926 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6635256145908429054 as 1
18927 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6635256145908429054/.test-fileid1_100.log.1
18930 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6635256145908429054/.test-fileid1_100.log.1} exists. Appending to existing file
19377 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19800 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19806 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19812 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19812 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19812 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19812 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9146923390266202204
19813 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9146923390266202204 as 1
19813 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9146923390266202204/.test-fileid1_100.log.1
19815 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9146923390266202204/.test-fileid1_100.log.1} does not exist. Create a new file
20237 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20237 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9146923390266202204
20241 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9146923390266202204 as 1
20241 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9146923390266202204/.test-fileid1_100.log.1
20243 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9146923390266202204/.test-fileid1_100.log.1} exists. Appending to existing file
20970 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
20970 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9146923390266202204
20972 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9146923390266202204 as 1
20972 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9146923390266202204/.test-fileid1_100.log.1
20973 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9146923390266202204/.test-fileid1_100.log.1} exists. Appending to existing file
21424 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
21843 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
21848 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
21853 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
21853 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
21853 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21853 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4703358201653266208
21854 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4703358201653266208 as 1
21855 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4703358201653266208/.test-fileid1_100.log.1
21856 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} does not exist. Create a new file
22710 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} has a corrupted block at 2051
22749 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} starts at 2082
23178 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23178 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4703358201653266208
23180 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4703358201653266208 as 1
23180 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4703358201653266208/.test-fileid1_100.log.1
23181 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} exists. Appending to existing file
23631 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} has a corrupted block at 2051
23670 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} starts at 2082
23671 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} has a corrupted block at 2094
23707 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {/tmp/junit4703358201653266208/.test-fileid1_100.log.1} starts at 2130
23717 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
24136 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24142 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24147 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24147 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
24147 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
24147 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8711125316762918828
24149 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8711125316762918828 as 1
24149 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8711125316762918828/.test-fileid1_100.log.1
24150 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8711125316762918828/.test-fileid1_100.log.1} does not exist. Create a new file
24173 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24178 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24183 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24184 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24197 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24198 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24200 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24200 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24200 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24200 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
24202 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit8711125316762918828/.test-fileid1_100.log.1
24202 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
24202 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
24202 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
24203 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
24203 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
24203 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
24209 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
24633 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24640 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24645 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24645 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
24646 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
24647 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1464247537896866936
24648 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1464247537896866936 as 1
24648 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1464247537896866936/.test-fileid1_100.log.1
24649 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1464247537896866936/.test-fileid1_100.log.1} does not exist. Create a new file
24670 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
24674 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
24678 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
24679 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit1464247537896866936/.test-fileid1_100.log.1
24690 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit1464247537896866936/.test-fileid1_100.log.1
24691 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit1464247537896866936/.test-fileid1_100.log.1
24692 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Invalid or extra rollback command block in hdfs://localhost:36663/tmp/junit1464247537896866936/.test-fileid1_100.log.1
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit1464247537896866936/.test-fileid1_100.log.1
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 99
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8217
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 1
24692 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 138
24698 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
25115 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
25120 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
25124 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
25124 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
25125 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
25125 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8508084388022011047
25126 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8508084388022011047 as 1
25126 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8508084388022011047/.test-fileid1_100.log.1
25127 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8508084388022011047/.test-fileid1_100.log.1} does not exist. Create a new file
26404 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26404 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8508084388022011047
26406 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8508084388022011047 as 1
26406 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8508084388022011047/.test-fileid1_100.log.1
26407 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8508084388022011047/.test-fileid1_100.log.1} exists. Appending to existing file
27262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27262 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8508084388022011047
27264 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8508084388022011047 as 1
27264 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8508084388022011047/.test-fileid1_100.log.1
27265 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8508084388022011047/.test-fileid1_100.log.1} exists. Appending to existing file
27289 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27294 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27297 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27298 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27308 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27310 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27311 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27311 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1} has a corrupted block at 9374
27312 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1} starts at 9374
27313 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27314 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1} has a corrupted block at 9386
27315 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1} starts at 9386
27315 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27317 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27317 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Log HoodieLogFile {hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1} has a corrupted block at 9621
27317 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatReader  - Next available block in HoodieLogFile {hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1} starts at 9621
27317 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit8508084388022011047/.test-fileid1_100.log.1
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
27320 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
27321 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
27321 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
27326 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27742 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27747 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27751 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27751 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27751 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27751 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2041630913874523876
27752 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2041630913874523876 as 1
27752 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2041630913874523876/.test-fileid1_100.log.1
27753 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2041630913874523876/.test-fileid1_100.log.1} does not exist. Create a new file
28170 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
28170 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2041630913874523876
28171 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2041630913874523876 as 1
28171 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2041630913874523876/.test-fileid1_100.log.1
28172 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2041630913874523876/.test-fileid1_100.log.1} exists. Appending to existing file
28198 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4078 has reached threshold 2038. Rolling over to the next version
28205 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2041630913874523876/.test-fileid1_100.log.2} does not exist. Create a new file
28213 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
28626 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
28631 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
28635 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
28635 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
28636 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
28636 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1924602696004983038
28637 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1924602696004983038 as 1
28637 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1924602696004983038/.test-fileid1_100.log.1
28638 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1924602696004983038/.test-fileid1_100.log.1} does not exist. Create a new file
28652 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 8905 has reached threshold 500. Rolling over to the next version
28658 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1924602696004983038/.test-fileid1_100.log.2} does not exist. Create a new file
28672 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 8905 has reached threshold 500. Rolling over to the next version
29079 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1924602696004983038/.test-fileid1_100.log.3} does not exist. Create a new file
29085 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29089 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29093 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29094 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.1
29099 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.1
29099 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.1
29099 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.2
29105 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.2
29105 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.2
29112 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit1924602696004983038/.test-fileid1_100.log.3
29114 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29114 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 104
29114 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8216
29114 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 96
29114 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 13902
29127 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
29541 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29546 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29550 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29550 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
29550 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
29550 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit333308168934741892
29551 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit333308168934741892 as 1
29551 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit333308168934741892/.test-fileid1_100.log.1
29552 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit333308168934741892/.test-fileid1_100.log.1} does not exist. Create a new file
29571 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29574 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29577 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29578 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29585 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29586 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29589 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29590 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:36663/tmp/junit333308168934741892/.test-fileid1_100.log.1
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29591 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
29592 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
29592 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
29592 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
29596 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
29609 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29612 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29615 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29615 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
29616 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
29616 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2124753011325526555
29617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2124753011325526555 as 1
29617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2124753011325526555/.test-fileid1_100.log.1
29617 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2124753011325526555/.test-fileid1_100.log.1} does not exist. Create a new file
29642 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29646 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29650 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29651 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29658 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29660 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29661 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29667 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 47
29669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 4042
29669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 103
29669 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 15064
29677 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
29680 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
29683 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
29685 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29695 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29697 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29699 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in hdfs://localhost:36663/tmp/junit2124753011325526555/.test-fileid1_100.log.1
29705 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
29705 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 96
29705 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8256
29705 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 104
29705 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 15064
29714 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
30130 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
30135 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
30139 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
30139 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
30139 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
30139 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8408179643890047699
30141 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8408179643890047699 as 1
30141 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8408179643890047699/.test-fileid1_100.log.1
30142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8408179643890047699/.test-fileid1_100.log.1} does not exist. Create a new file
30146 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
30147 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741856_1051] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741856_1051
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
30147 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741826_1002] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741826_1002
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
30147 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741847_1036] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741847_1036
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
30147 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741845_1034] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741845_1034
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
30147 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741858_1053] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741858_1053
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
30147 [ResponseProcessor for block BP-801675566-172.17.0.3-1520366817018:blk_1073741849_1043] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-801675566-172.17.0.3-1520366817018:blk_1073741849_1043
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
30287 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38916 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741847_1036]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38916 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 54390 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
30290 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38908 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741845_1034]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38908 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 53893 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
30289 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38982 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741858_1053]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38982 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59399 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
30289 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38974 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741856_1051]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38974 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59292 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
30288 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38772 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741826_1002]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38772 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 39340 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
30288 [DataNode: [[[DISK]file:/tmp/1520366816027-0/dfs/data/data1/, [DISK]file:/tmp/1520366816027-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36663] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-801675566-172.17.0.3-1520366817018 (Datanode Uuid 50385469-ecc0-4270-9bb8-797a4600afa5) service to localhost/127.0.0.1:36663 interrupted
30291 [DataXceiver for client DFSClient_NONMAPREDUCE_1422260101_1 at /127.0.0.1:38946 [Receiving block BP-801675566-172.17.0.3-1520366817018:blk_1073741849_1042]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38946 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 57008 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:745)
30295 [DataNode: [[[DISK]file:/tmp/1520366816027-0/dfs/data/data1/, [DISK]file:/tmp/1520366816027-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36663] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-801675566-172.17.0.3-1520366817018 (Datanode Uuid 50385469-ecc0-4270-9bb8-797a4600afa5) service to localhost/127.0.0.1:36663
30312 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@7c2a5a0d] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.306 sec - in com.uber.hoodie.common.table.log.HoodieLogFormatTest
Running com.uber.hoodie.common.TestBloomFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in com.uber.hoodie.common.TestBloomFilter
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 44,000
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 22,407B for [_hoodie_record_key] BINARY: 1,000 values, 40,007B raw, 22,305B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:06:54 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 16 ms. row count = 1000
30580 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16420
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-1e68b8f1-ef05-446d-9cd5-793f0c5e6e73,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
30582 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16438
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-1e68b8f1-ef05-446d-9cd5-793f0c5e6e73,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
30582 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16391
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-1e68b8f1-ef05-446d-9cd5-793f0c5e6e73,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
30583 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16423
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-708a98e8-0c99-44fb-980d-0cbebbd55692,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
30584 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16441
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-708a98e8-0c99-44fb-980d-0cbebbd55692,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
30585 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16426
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-708a98e8-0c99-44fb-980d-0cbebbd55692,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
30589 [Thread-4] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16444
java.io.EOFException: End of File Exception between local host is: "spirals-vortex.lille.inria.fr/172.17.0.3"; destination host is: "localhost":36663; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy28.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1080)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:975)

Results :

Tests run: 51, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:report (post-unit-test) @ hoodie-common ---
[INFO] Loading execution data file /root/workspace/uber/hudi/349988061/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] Analyzed bundle 'hoodie-common' with 103 classes
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-hadoop-mr 0.4.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /root/workspace/uber/hudi/349988061/hoodie-hadoop-mr/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-hadoop-mr ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.hadoop.HoodieInputFormatTest
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
171  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
173  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7448168364343180228 as hoodie dataset /tmp/junit7448168364343180228
197  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7448168364343180228
198  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
199  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7448168364343180228/.hoodie/hoodie.properties
204  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7448168364343180228
204  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7448168364343180228
328  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit7448168364343180228
328  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit7448168364343180228
329  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
329  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit7448168364343180228/.hoodie/hoodie.properties
330  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit7448168364343180228
330  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
335  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
362  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
387  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
387  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
387  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
395  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid0_1_200.parquet
395  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid0_1_200.parquet
396  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid2_1_200.parquet
396  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid2_1_200.parquet
396  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid1_1_200.parquet
396  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid1_1_200.parquet
397  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid4_1_200.parquet
397  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid4_1_200.parquet
397  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid3_1_200.parquet
397  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid3_1_200.parquet
397  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
472  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit7448168364343180228
472  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit7448168364343180228
473  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
474  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit7448168364343180228/.hoodie/hoodie.properties
474  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit7448168364343180228
474  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
475  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
477  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
479  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
480  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 3
480  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
481  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid0_1_400.parquet
481  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid0_1_400.parquet
481  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid2_1_400.parquet
481  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid2_1_400.parquet
482  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid1_1_400.parquet
482  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid1_1_400.parquet
482  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid4_1_200.parquet
482  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid4_1_200.parquet
482  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid3_1_300.parquet
482  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid3_1_300.parquet
483  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
570  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit7448168364343180228
570  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit7448168364343180228
570  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
571  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit7448168364343180228/.hoodie/hoodie.properties
571  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit7448168364343180228
571  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
572  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
573  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
576  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
577  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 2147483647
577  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
578  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid0_1_600.parquet
578  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid0_1_600.parquet
578  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid2_1_400.parquet
578  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid2_1_400.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid1_1_500.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid1_1_500.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid4_1_200.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid4_1_200.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit7448168364343180228/2016/05/01/fileid3_1_300.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit7448168364343180228/2016/05/01/fileid3_1_300.parquet
579  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
629  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
630  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit376422999358762164 as hoodie dataset /tmp/junit376422999358762164
636  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit376422999358762164
636  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
637  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit376422999358762164/.hoodie/hoodie.properties
637  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit376422999358762164
637  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit376422999358762164
677  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit376422999358762164
678  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit376422999358762164
678  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
678  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit376422999358762164/.hoodie/hoodie.properties
679  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit376422999358762164
679  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
680  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
680  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
683  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
683  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid0_1_100.parquet
683  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid2_1_100.parquet
684  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid1_1_100.parquet
684  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid4_1_100.parquet
684  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid3_1_100.parquet
684  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid6_1_100.parquet
684  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid5_1_100.parquet
685  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid8_1_100.parquet
685  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid7_1_100.parquet
685  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid9_1_100.parquet
726  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit376422999358762164
726  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit376422999358762164
726  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
726  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit376422999358762164/.hoodie/hoodie.properties
727  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit376422999358762164
727  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
727  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
728  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
729  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
729  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid0_1_100.parquet
730  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid2_1_100.parquet
730  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid1_1_100.parquet
730  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid4_1_100.parquet
730  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid3_1_100.parquet
730  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid6_1_100.parquet
730  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid5_1_100.parquet
731  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid8_1_100.parquet
731  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid7_1_100.parquet
731  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit376422999358762164/2016/05/01/fileid9_1_100.parquet
768  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
768  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit7973293361089734590 as hoodie dataset /tmp/junit7973293361089734590
775  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit7973293361089734590
775  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
775  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit7973293361089734590/.hoodie/hoodie.properties
776  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7973293361089734590
776  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7973293361089734590
812  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit7973293361089734590
812  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit7973293361089734590
812  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
812  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit7973293361089734590/.hoodie/hoodie.properties
813  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit7973293361089734590
813  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
814  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
814  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
816  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
817  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
817  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
817  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 0
849  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
849  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1398190234261762428 as hoodie dataset /tmp/junit1398190234261762428
855  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1398190234261762428
855  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
855  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1398190234261762428/.hoodie/hoodie.properties
856  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1398190234261762428
856  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1398190234261762428
889  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit1398190234261762428
889  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit1398190234261762428
889  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
889  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit1398190234261762428/.hoodie/hoodie.properties
890  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit1398190234261762428
890  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
891  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
891  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid0_1_100.parquet
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid2_1_100.parquet
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid1_1_100.parquet
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid4_1_100.parquet
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid3_1_100.parquet
893  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid6_1_100.parquet
894  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid5_1_100.parquet
894  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid8_1_100.parquet
894  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid7_1_100.parquet
894  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid9_1_100.parquet
940  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit1398190234261762428
940  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit1398190234261762428
940  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
940  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit1398190234261762428/.hoodie/hoodie.properties
941  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit1398190234261762428
941  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
941  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
942  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
943  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
943  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid0_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid2_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid1_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid4_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid3_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid6_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid5_1_100.parquet
944  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid8_1_100.parquet
945  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid7_1_100.parquet
945  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid9_1_100.parquet
989  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit1398190234261762428
989  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit1398190234261762428
989  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
989  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit1398190234261762428/.hoodie/hoodie.properties
990  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit1398190234261762428
990  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
990  [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
991  [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit]]
992  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
992  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid0_1_200.parquet
992  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid2_1_200.parquet
992  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid1_1_100.parquet
992  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid4_1_100.parquet
992  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid3_1_100.parquet
993  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid6_1_200.parquet
993  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid5_1_200.parquet
993  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid8_1_200.parquet
993  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid7_1_100.parquet
993  [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit1398190234261762428/2016/05/01/fileid9_1_100.parquet
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.489 sec - in com.uber.hoodie.hadoop.HoodieInputFormatTest
Running com.uber.hoodie.hadoop.AnnotationTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec - in com.uber.hoodie.hadoop.AnnotationTest
Running com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
1015 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1015 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6149313694638140300 as hoodie dataset /tmp/junit6149313694638140300
1020 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6149313694638140300
1020 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1020 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6149313694638140300/.hoodie/hoodie.properties
1021 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6149313694638140300
1021 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6149313694638140300
1052 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit6149313694638140300
1052 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1052 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit6149313694638140300/.hoodie/hoodie.properties
1052 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit6149313694638140300
1053 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[001__commit], [002__commit], [==>003__commit]]
1054 [main] INFO  com.uber.hoodie.hadoop.HoodieROTablePathFilter  - Based on hoodie metadata from base path: file:/tmp/junit6149313694638140300, caching 3 files under file:/tmp/junit6149313694638140300/2017/01/01
1141 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/
1141 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@1a245833]
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.142 sec - in com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
Running com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
1172 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1318 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit45308027113369764 as hoodie dataset /tmp/junit45308027113369764
1324 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit45308027113369764
1324 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1324 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit45308027113369764/.hoodie/hoodie.properties
1324 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit45308027113369764
1324 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit45308027113369764
1341 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1341 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6758922742909045669 as hoodie dataset /tmp/junit6758922742909045669
1346 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6758922742909045669
1347 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
1347 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6758922742909045669/.hoodie/hoodie.properties
1347 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6758922742909045669
1347 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6758922742909045669
2000 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
2000 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit6758922742909045669/2016/05/01
2003 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit6758922742909045669/2016/05/01 as 1
2004 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6758922742909045669/2016/05/01/.fileid0_100.log.1
2004 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6758922742909045669/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
2558 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favoriteIntNumber,favoriteNumber,favoriteFloatNumber,favoriteDoubleNumber,tags,testNestedRecord,stringArray
2565 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit6758922742909045669/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit6758922742909045669/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favoriteIntNumber, favoriteNumber, favoriteFloatNumber, favoriteDoubleNumber, tags, testNestedRecord, stringArray]
2566 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
2566 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6758922742909045669
2566 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
2566 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6758922742909045669/.hoodie/hoodie.properties
2567 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6758922742909045669
2570 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file /tmp/junit6758922742909045669/2016/05/01/.fileid0_100.log.1
2598 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit6758922742909045669/2016/05/01/.fileid0_100.log.1
2598 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in /tmp/junit6758922742909045669/2016/05/01/.fileid0_100.log.1
2718 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 194
2735 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
2736 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 4
2736 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 776
2736 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 46
2736 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 10744
2792 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
2793 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3980255725812610708 as hoodie dataset /tmp/junit3980255725812610708
2800 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3980255725812610708
2800 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
2800 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3980255725812610708/.hoodie/hoodie.properties
2801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit3980255725812610708
2801 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit3980255725812610708
2815 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
2815 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3017705560572982912 as hoodie dataset /tmp/junit3017705560572982912
2821 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3017705560572982912
2821 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
2822 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3017705560572982912/.hoodie/hoodie.properties
2822 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3017705560572982912
2822 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3017705560572982912
2974 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
2974 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit3017705560572982912/2016/05/01
2974 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit3017705560572982912/2016/05/01 as 1
2975 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3017705560572982912/2016/05/01/.fileid0_100.log.1
2975 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3017705560572982912/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
3094 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favorite_number,favorite_color,favorite_movie
3097 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit3017705560572982912/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit3017705560572982912/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favorite_number, favorite_color, favorite_movie]
3097 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
3097 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3017705560572982912
3097 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@707194ba]
3097 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3017705560572982912/.hoodie/hoodie.properties
3098 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3017705560572982912
3098 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file /tmp/junit3017705560572982912/2016/05/01/.fileid0_100.log.1
3102 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit3017705560572982912/2016/05/01/.fileid0_100.log.1
3102 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final blocks in /tmp/junit3017705560572982912/2016/05/01/.fileid0_100.log.1
3103 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 66
3111 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
3111 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 10
3111 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 660
3111 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 90
3111 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 8670
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.979 sec - in com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 88,387
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteIntNumber] INT32: 100 values, 407B raw, 407B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteNumber] INT64: 100 values, 807B raw, 807B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteFloatNumber] FLOAT: 100 values, 407B raw, 407B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteDoubleNumber] DOUBLE: 100 values, 807B raw, 807B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 108B for [tags, map, key] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 2 entries, 24B raw, 2B comp}
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,156B for [tags, map, value, item1] BINARY: 200 values, 2,117B raw, 2,117B comp, 1 pages, encodings: [PLAIN, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,963B for [tags, map, value, item2] BINARY: 200 values, 2,917B raw, 2,917B comp, 1 pages, encodings: [PLAIN, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 48B for [testNestedRecord, isAdmin] BOOLEAN: 100 values, 20B raw, 20B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,647B for [testNestedRecord, userId] BINARY: 100 values, 1,597B raw, 1,597B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:28 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [stringArray, array] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 2 entries, 40B raw, 2B comp}
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8,589
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_number] INT64: 100 values, 7B raw, 7B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_color] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_movie] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
Mar 6, 2018 9:07:29 PM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 6, 2018 9:07:29 PM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 6, 2018 9:07:29 PM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:29 PM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
Mar 6, 2018 9:07:29 PM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 6, 2018 9:07:29 PM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 6, 2018 9:07:29 PM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:29 PM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 100

Results :

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-client 0.4.1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-client ---
[INFO] surefireArgLine set to -javaagent:/root/./workspace/uber/hudi/349988061/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/root/workspace/uber/hudi/349988061/hoodie-client/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-client ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.metrics.TestHoodieMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.467 sec - in com.uber.hoodie.metrics.TestHoodieMetrics
Running com.uber.hoodie.io.TestHoodieCommitArchiveLog
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.946 sec - in com.uber.hoodie.io.TestHoodieCommitArchiveLog
Running com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec - in com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Running com.uber.hoodie.io.TestHoodieCompactor
4284 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 100
5843 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=29, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=38, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=33, numUpdates=0}}}
5860 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
5860 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
5860 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 29, totalInsertBuckets => 1, recordsPerBucket => 500000
5862 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
5862 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
5862 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 38, totalInsertBuckets => 1, recordsPerBucket => 500000
5863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
5863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
5863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 33, totalInsertBuckets => 1, recordsPerBucket => 500000
5863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
5863 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
5930 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 100
5930 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 100
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,553
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 137B for [_hoodie_commit_seqno] BINARY: 29 values, 345B raw, 96B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 763B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 664B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 757B for [_row_key] BINARY: 29 values, 1,160B raw, 658B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,254
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_hoodie_commit_seqno] BINARY: 38 values, 462B raw, 112B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 960B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 861B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 955B for [_row_key] BINARY: 38 values, 1,520B raw, 856B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,315
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 150B for [_hoodie_commit_seqno] BINARY: 33 values, 403B raw, 106B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 852B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 753B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 844B for [_row_key] BINARY: 33 values, 1,320B raw, 745B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED7325 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
7325 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
7402 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
7402 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
7809 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
7825 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 100 as complete
7826 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 100
8267 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 101
8822 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
8822 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
9138 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit6872922971126255019/2015/03/17/2704cdc9-deae-4e82-a558-fd6bf0091f43_2_100.parquet
9238 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 33 row keys from /tmp/junit6872922971126255019/2015/03/17/2704cdc9-deae-4e82-a558-fd6bf0091f43_2_100.parquet
9238 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit6872922971126255019/2015/03/17/2704cdc9-deae-4e82-a558-fd6bf0091f43_2_100.parquet => [061ae5c4-a38f-4caf-9f3a-01d370eeff9a, 1e8ffa72-fef4-4162-b497-9c429f3b9bea, 280f7b2f-2890-4489-ac32-e2931c6cb09e, 3214a783-a443-440d-903e-a3a41b35d497, 379cd2d4-4b2f-4a4f-ac4c-010c6adbb279, 42d35514-5cdb-4b36-b7b8-208d4530bfe2, 43d7ce25-4104-4a67-84e7-a98cbd90f425, 71488e82-8a5b-411b-b0ed-c43b6e71ec65, 71615767-7b04-44f6-9a4d-fba69a8b10a2, 7482af3d-110c-4f33-8511-4f0a57fd1cd2, 7bad5db3-d33d-4fca-af60-fdcb879f0ec9, 8262269f-5ad0-464f-b2c9-ef5f58c01892, 897bc460-a5c3-4e20-bdb8-4bb423782ddd, 8be31f1d-23d5-4e2a-a411-a0de01c57d17, 8e7fa70c-a69a-4a09-90af-d057a1db5d36, 9367f77f-d2fb-49b0-afe9-65647b12107c, 9fac897e-3c7b-4db0-8bf7-03dd4e5e5282, a0d5d583-55cf-43d3-8c8c-ee9e0cbb03dd, a3aae68d-cfab-41e5-a5dc-e71cc85cb793, a8cb7ff9-f465-4d71-93e0-c927e180267a, ae3e8702-0ef5-4b9a-8783-d1a8cffa7412, b2deea88-7c21-4e5a-a112-33836f641220, b89e0e79-f9b7-4f0e-9bcb-1dd6baa737cd, cfbc5d1e-326f-4e69-8aae-a3a77bd55e00, d027e221-2b3c-405f-b74c-e571154c68b9, d1747fea-cdcd-4b0f-ab42-dd1ca6906211, d3c60ec7-6099-4dba-9bb7-26e98212c8e3, d81cdd5a-5a45-4d93-a621-8c578df12b8c, d9dd1478-00e5-4ee3-9388-08a00b22943d, e1c670f0-2a94-4b5a-8a70-d24a95e649c0, efb3a7c0-fae1-4514-bf70-8881f4686bdb, f36caad6-1f83-4e9f-ab6e-28bb416e6947, f40f1224-7b15-46a5-856f-2fe3f22ed04d]
9263 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit6872922971126255019/2016/03/15/6446b3fb-503c-4e33-8de2-43e0c652cc1e_0_100.parquet
9282 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 29 row keys from /tmp/junit6872922971126255019/2016/03/15/6446b3fb-503c-4e33-8de2-43e0c652cc1e_0_100.parquet
9282 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit6872922971126255019/2016/03/15/6446b3fb-503c-4e33-8de2-43e0c652cc1e_0_100.parquet => [006818cd-fe35-48a1-a13b-2041af32fd3d, 077bd9a3-07cf-4eb2-8478-1cf9acaef564, 08c427e8-9a8e-4a7e-90c8-7c27b8806753, 0d0e24bf-5333-44ef-8a33-6372f74dfd01, 11a9605b-f470-4f29-bbf0-d7582c0fe948, 13f11f5d-e43b-47c7-a88b-28a1c8799583, 1a5e04a6-0391-46c9-97d1-5e6be95a5af0, 3ac52b69-f842-47fb-8ab4-54fe70722355, 4f3e2de1-db3a-463b-9612-69cc7533d5cb, 5e8e3c39-294c-46c9-9f75-b59e428bcc79, 62a96348-c1b1-4508-ba5e-cfc78a5149dd, 63c7716f-e085-4e18-be04-4e18c6ee0594, 68913934-3234-4166-9390-23922fba4d8d, 722c6787-cf25-4963-9ad7-b725bf7fa76d, 7bb161f1-8142-4e4f-bd59-77e56982058c, 9019405b-0532-4a09-947e-45b47d2696dc, 9c78e9e0-68a0-4629-a2e9-a055be484296, b261c086-559c-4769-a2ce-765c3ec01a51, b6408c8e-715a-4d43-a7d5-17154bfbc226, b864f285-4f3d-4c4f-9e1d-373b0039444c, b9789324-2081-4d47-9c52-324c84377551, bf16e223-b5ee-40ef-b113-6e6c637e0932, c5128a0c-4181-4a5a-993e-287f51d6f3cd, c9a76cbb-fe6b-42aa-a708-961c380cb8fc, ce45c17a-c512-4614-8e3d-0a63099d0479, e0129e65-d7b9-4e18-b168-9b4a975a13fb, e4dc2f08-75d0-4d91-88d7-05c116edb2ea, e6bbf62f-7be8-4595-b638-11703dc10516, eaa5dbee-269a-4f48-9377-48db6e5e4b69]
9299 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit6872922971126255019/2015/03/16/894a2f5e-5570-4c91-a42e-791c627b319e_1_100.parquet
9316 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 38 row keys from /tmp/junit6872922971126255019/2015/03/16/894a2f5e-5570-4c91-a42e-791c627b319e_1_100.parquet
9316 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit6872922971126255019/2015/03/16/894a2f5e-5570-4c91-a42e-791c627b319e_1_100.parquet => [0a433c16-8c83-443c-ac53-ee5c46e7f22b, 0c8d53cd-34ad-4b64-8580-deacedbce273, 150a0631-1cd5-4b16-be11-9d5b62f71899, 1b21ece0-bd40-45a6-9fd5-b41aa7b53326, 254409ab-f385-42e2-bee7-bdeb01f7af84, 31c19abc-1c46-4011-b244-656fd1661019, 38025068-21c3-4415-b8df-e735e962538e, 389b375b-87bb-40a0-bcb8-4599547e38f3, 4589f497-6b9a-4a1f-a4a8-24f0d92b9b26, 4d7a8bf0-3013-4bd4-9eca-c01c1e2b6bd2, 58496041-9e9a-4b2a-b715-f2cf1b54e4b5, 5bb810e8-4674-4796-9ee4-216a4c8ef231, 5d55dd28-e0e9-4de0-9b59-55db47a9b43b, 5d9ddb58-bd61-483b-a913-a080f5844e91, 64a5d8a0-55e3-48be-9640-bb3337d4462f, 6acf787d-a252-4da7-bbc4-bc14cf17b08a, 76862157-42ac-4a0a-94fa-28580a38f40e, 797206e0-9a9b-46d8-91f0-333fd6abba1f, 7d276070-a7d5-4fa4-a166-f5ad195e9bf8, 7f97d686-0f0f-4b02-a12c-192dfe90e6fb, 7fcadab9-195f-424a-8d5b-b83c6d2476dc, 8a7dbee3-e65f-4510-abc8-bfebdb303b90, 8f0dbb7f-f804-4a13-8a68-3e8177fa0a9c, 91c847e5-9878-4639-aa38-67226ec7e879, 949cb967-6536-4e26-b7cb-1322412f6d6c, 968b4503-52d1-4e26-a9a4-a900968fc1a4, b1f0a6fc-d71e-44f7-8dd1-67ef93853eba, bc453f3d-27de-40a8-9767-065bc44dd699, c0d0182e-ff4e-4771-91d4-2e3d095f2735, c5c986fa-131e-4b4f-86f0-4b6143229ca4, dc649d4d-5b6d-461a-8388-38cdb54947b4, dda772ca-6e3a-403d-812f-c5a6d989ac67, e77c4c6f-1eb5-43db-8c77-262f07863a64, ec773338-3a88-4b16-b252-0de80aa9e2cb, ef379076-2017-43d3-ada8-442a278e651b, f06b9db7-10f7-423b-ac3d-016b13cb940f, fd6921b9-868b-4e08-880f-2b8ddbf87ce4, fe13377c-cc49-4dbf-b91f-6fe8be250bae]
], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:39 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 33
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:41 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 38
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 38 records.
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 38
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,732
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 195B for [_hoodie_commit_seqno] BINARY: 38 values, 918B raw, 128B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 960B for [_hoodie_record_key] BINARY: 38 values, 1,526B raw, 861B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 38 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 955B for [_row_key] BINARY: 38 values, 1,520B raw, 856B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 38 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [begin_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lat] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [end_lon] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 370B for [fare] DOUBLE: 38 values, 304B raw, 327B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,732
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 10545 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306210742
10655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=33, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=30, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=37, numUpdates=0}}}
10664 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
10664 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
10664 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 33, totalInsertBuckets => 1, recordsPerBucket => 500000
10664 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
10664 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
10664 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 30, totalInsertBuckets => 1, recordsPerBucket => 500000
10665 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
10665 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
10665 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
10665 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
10665 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
10706 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180306210742
10706 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306210742
1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 188B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 121B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 852B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 753B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 844B for [_row_key] BINARY: 33 values, 1,320B raw, 745B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 180B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 113B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 763B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 664B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 757B for [_row_key] BINARY: 29 values, 1,160B raw, 658B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:42 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,754
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 186B for [_hoodie_commit_seqno] BINARY: 33 values, 798B raw, 119B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 852B for [_hoodie_record_key] BINARY: 33 values, 1,326B raw, 753B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 33 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic {11252 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
11253 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
11313 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
11313 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
11540 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
11547 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180306210742 as complete
11547 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180306210742
11835 [main] WARN  com.uber.hoodie.io.compact.HoodieRealtimeTableCompactor  - After filtering, Nothing to compact for /tmp/junit6082277730819509980
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.39 sec - in com.uber.hoodie.io.TestHoodieCompactor
Running com.uber.hoodie.config.HoodieWriteConfigTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.config.HoodieWriteConfigTest
Running com.uber.hoodie.index.TestHbaseIndex
Formatting using clusterid: testClusterID
15387 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
15554 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
16642 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
21433 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
21817 [spirals-vortex:32788.activeMasterManager] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
22348 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
24280 [spirals-vortex:32788.activeMasterManager] WARN  org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore  - Log directory not found: File hdfs://localhost:41841/user/root/test-data/b23ac210-24bb-4367-9348-6147b3858d03/MasterProcWALs does not exist.
24461 [RS:0;spirals-vortex:36231] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
30214 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306210802
30924 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=56, numUpdates=0}}}
31021 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
31343 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
31343 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
31343 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
31343 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
31344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
31409 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 845B for [_row_key] BINARY: 33 values, 1,320B raw, 746B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 33 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [begin_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lat] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [end_lon] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 330B for [fare] DOUBLE: 33 values, 264B raw, 287B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,154
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 182B for [_hoodie_commit_seqno] BINARY: 30 values, 726B raw, 115B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 788B for [_hoodie_record_key] BINARY: 30 values, 1,206B raw, 689B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 30 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 30 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 781B for [_row_key] BINARY: 30 values, 1,200B raw, 682B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 30 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 30 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [begin_lat] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [begin_lon] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [end_lat] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [end_lon] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 306B for [fare] DOUBLE: 30 values, 240B raw, 263B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,554
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 197B for [_hoodie_commit_seqno] BINARY: 37 values, 894B raw, 130B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 936B for [_hoodie_record_key] BINARY: 37 values, 1,486B raw, 837B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 929B for [_row_key] BINARY: 37 values, 1,480B raw, 830B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:07:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [fare] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,718
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 219B for [_hoodie_commit_seqno] BINARY: 72 values, 943B raw, 173B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,717B for [_hoodie_record_key] BINARY: 72 values, 2,887B raw, 1,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,706B for [_row_key] BINARY: 72 values, 2,880B raw, 1,606B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:04 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [fare] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,718
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 217B for [_hoodie_commit_seqno] BINARY: 72 values, 943B raw, 171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,714B for [_hoodie_record_key] BINARY: 72 values, 2,887B raw, 1,614B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 72 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,705B for [_row_key] BINARY: 72 values, 2,880B raw, 1,605B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 72 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [begin_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lat] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [end_lon] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:05 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 643B for [fare] DOUBLE: 72 values, 576B raw, 599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10,694
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 191B for [_hoodie_commit_seqno] BINARY: 56 values, 734B raw, 146B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,357B for [_hoodie_record_key] BINARY: 56 values, 2,246B raw, 1,258B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 56 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,348B for [_row_key] BINARY: 56 values, 2,240B raw, 1,249B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 56 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B r34954 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
35569 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
35569 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
35605 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
35605 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
35951 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
35967 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
35967 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
37122 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306210809
37453 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=70, numUpdates=0}}}
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 70, totalInsertBuckets => 1, recordsPerBucket => 500000
37872 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
37873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
37930 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180306210809
aw, 1B comp}
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [begin_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lat] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [end_lon] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:06 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 514B for [fare] DOUBLE: 56 values, 448B raw, 471B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,154
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 247B for [_hoodie_commit_seqno] BINARY: 65 values, 1,567B raw, 179B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,562B for [_hoodie_record_key] BINARY: 65 values, 2,607B raw, 1,462B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,552B for [_row_key] BINARY: 65 values, 2,600B raw, 1,452B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [fare] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,154
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 257B for [_hoodie_commit_seqno] BINARY: 65 values, 1,567B raw, 189B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,565B for [_hoodie_record_key] BINARY: 65 values, 2,607B raw, 1,465B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 65 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,558B for [_row_key] BINARY: 65 values, 2,600B raw, 1,458B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 65 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [begin_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lat] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [end_lon] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:11 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 587B for [fare] DOUBLE: 65 values, 520B raw, 543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 14,154
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 70 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 258B for [_hoodie_commit_seqno] BINARY: 70 values, 1,687B raw, 190B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,673B for [_hoodie_record_key] BINARY: 70 values, 2,807B raw, 1,573B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_p41024 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306210809
41654 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
41655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
41687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
41687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
42000 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
42420 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20180306210809 as complete
42420 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180306210809
43478 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20180306210809]
43478 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20180306210809]
43591 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
43596 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41841/tmp/junit794118931459240011/2015/03/16/91e004d4-4cb8-4de5-975b-fb0ebdc7fcea_1_20180306210809.parquet	true
43600 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
43604 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41841/tmp/junit794118931459240011/2015/03/17/fa04146a-2d12-473f-b391-a90e32f5bd79_2_20180306210809.parquet	true
43604 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
43607 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41841/tmp/junit794118931459240011/2016/03/15/2c1dd9da-0fbf-4aea-b49d-c3f82396472d_0_20180306210809.parquet	true
43613 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20180306210809]
43614 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20180306210809]
44036 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20180306210809] rollback is complete
44036 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
44883 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306210817
45154 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
45572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
45572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
45598 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180306210817
46084 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306210818
46333 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
46750 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
46750 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
46813 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20180306210818
46813 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306210818
47330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
47330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
47344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
47344 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
47344 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20180306210818
53289 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
55424 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:59372] WARN  org.apache.zookeeper.server.NIOServerCnxn  - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x161fced5e280000, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
	at java.lang.Thread.run(Thread.java:745)
55434 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
55560 [DataNode: [[[DISK]file:/root/workspace/uber/hudi/349988061/hoodie-client/target/test-data/be2a5ac1-f24c-422a-8fab-b31fee93ec0d/dfscluster_1ba4b857-cd09-4bba-998a-32e8a258b50d/dfs/data/data1/, [DISK]file:/root/workspace/uber/hudi/349988061/hoodie-client/target/test-data/be2a5ac1-f24c-422a-8fab-b31fee93ec0d/dfscluster_1ba4b857-cd09-4bba-998a-32e8a258b50d/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41841] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1719698396-172.17.0.3-1520366865101 (Datanode Uuid 955ac895-eba4-42ea-900b-3bd0ede5dabb) service to localhost/127.0.0.1:41841 interrupted
55560 [DataNode: [[[DISK]file:/root/workspace/uber/hudi/349988061/hoodie-client/target/test-data/be2a5ac1-f24c-422a-8fab-b31fee93ec0d/dfscluster_1ba4b857-cd09-4bba-998a-32e8a258b50d/dfs/data/data1/, [DISK]file:/root/workspace/uber/hudi/349988061/hoodie-client/target/test-data/be2a5ac1-f24c-422a-8fab-b31fee93ec0d/dfscluster_1ba4b857-cd09-4bba-998a-32e8a258b50d/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41841] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1719698396-172.17.0.3-1520366865101 (Datanode Uuid 955ac895-eba4-42ea-900b-3bd0ede5dabb) service to localhost/127.0.0.1:41841
55578 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@3d020cf8] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 43.97 sec - in com.uber.hoodie.index.TestHbaseIndex
Running com.uber.hoodie.index.TestHoodieIndex
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.index.TestHoodieIndex
Running com.uber.hoodie.index.bloom.TestHoodieBloomIndex
56107 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
56108 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${0}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
56831 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
57520 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
57668 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 1
57668 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
57807 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8384097886704407953/2016/01/31/a587831d-42ab-4ce8-b346-282af2700940_1_20180306210829.parquet
57820 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8384097886704407953/2016/01/31/a587831d-42ab-4ce8-b346-282af2700940_1_20180306210829.parquet
57820 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8384097886704407953/2016/01/31/a587831d-42ab-4ce8-b346-282af2700940_1_20180306210829.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
58254 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
58254 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
58667 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
58727 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
60312 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
60743 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
ath] BINARY: 70 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 70 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 70 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,664B for [_row_key] BINARY: 70 values, 2,800B raw, 1,564B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [rider] BINARY: 70 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 91B for [driver] BINARY: 70 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [begin_lat] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [begin_lon] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [end_lat] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [end_lon] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:12 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 627B for [fare] DOUBLE: 70 values, 560B raw, 583B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:29 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:30 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:30 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:30 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:30 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:30 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:30 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:31 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 232B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 83B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:33 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memor61896 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
62127 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
62127 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
62294 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6865393489744285916/2016/01/31/72048437-0037-4c27-8891-40cc818a08fd_1_20180306210833.parquet
62303 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6865393489744285916/2016/01/31/72048437-0037-4c27-8891-40cc818a08fd_1_20180306210833.parquet
62303 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6865393489744285916/2016/01/31/72048437-0037-4c27-8891-40cc818a08fd_1_20180306210833.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
62309 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6865393489744285916/2016/01/31/844a3635-d4a1-492f-bdef-2b0aef74f542_1_20180306210831.parquet
62318 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6865393489744285916/2016/01/31/844a3635-d4a1-492f-bdef-2b0aef74f542_1_20180306210831.parquet
62318 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6865393489744285916/2016/01/31/844a3635-d4a1-492f-bdef-2b0aef74f542_1_20180306210831.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
62322 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6865393489744285916/2015/01/31/99d4fddc-984a-45f4-86e9-e571d8ec9bef_1_20180306210834.parquet
62329 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6865393489744285916/2015/01/31/99d4fddc-984a-45f4-86e9-e571d8ec9bef_1_20180306210834.parquet
62329 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6865393489744285916/2015/01/31/99d4fddc-984a-45f4-86e9-e571d8ec9bef_1_20180306210834.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
62458 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
62717 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
62717 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
63611 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
64356 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
64778 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
y: 246
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:34 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:36 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 160B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 63B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 233B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 84B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 155B for [_row_key] BINARY: 1 values, 40B raw, 58B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:37 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 246
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BIN66017 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
66221 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
66222 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
66340 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5738728014217385018/2016/01/31/35c137e8-2285-423d-81d5-74c46f6e740b_1_20180306210836.parquet
66351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5738728014217385018/2016/01/31/35c137e8-2285-423d-81d5-74c46f6e740b_1_20180306210836.parquet
66351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5738728014217385018/2016/01/31/35c137e8-2285-423d-81d5-74c46f6e740b_1_20180306210836.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
66357 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5738728014217385018/2015/01/31/81d112ce-9521-4030-83a0-cd7cdd9f1d1e_1_20180306210838.parquet
66364 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5738728014217385018/2015/01/31/81d112ce-9521-4030-83a0-cd7cdd9f1d1e_1_20180306210838.parquet
66364 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5738728014217385018/2015/01/31/81d112ce-9521-4030-83a0-cd7cdd9f1d1e_1_20180306210838.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
66367 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5738728014217385018/2016/01/31/fa35282f-efe7-4f7a-b243-30034713a50f_1_20180306210837.parquet
66375 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5738728014217385018/2016/01/31/fa35282f-efe7-4f7a-b243-30034713a50f_1_20180306210837.parquet
66375 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5738728014217385018/2016/01/31/fa35282f-efe7-4f7a-b243-30034713a50f_1_20180306210837.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
66711 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
67809 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 2 row keys from /tmp/junit1088976742807972787/2016/01/31/7ca9cf1b-7058-44bf-b523-f13eb0e16a54_1_20180306210840.parquet
67810 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit1088976742807972787/2016/01/31/7ca9cf1b-7058-44bf-b523-f13eb0e16a54_1_20180306210840.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0, 2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
67818 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
ARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:38 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 395
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 2 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_seqno] BINARY: 2 values, 16B raw, 32B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_record_key] BINARY: 2 values, 86B raw, 76B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 2 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 2 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 65B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 171B for [_row_key] BINARY: 2 values, 80B raw, 72B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 128B for [time] BINARY: 2 values, 56B raw, 55B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [number] INT32: 2 values, 14B raw, 31B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 2 records.
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 2
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 145
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 93B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 40B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 56B for [_hoodie_commit_seqno] BINARY: 1 values, 11B raw, 29B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_record_key] BINARY: 1 values, 13B raw, 31B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [_hoodie_file_name] BINARY: 1 values, 36B raw, 52B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 58B for [_row_key] BINARY: 1 values, 7B raw, 27B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 247
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 18B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 62B for [_hoodie_commit_seqno] BINARY: 3 values, 21B raw, 35B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [_hoodie_record_key] BINARY: 3 values, 27B raw, 37B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie68474 [Executor task launch worker-0] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit7368741270766587253/2016/04/01/2_0_20160401010101.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1520366920000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
68487 [Executor task launch worker-0] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit7368741270766587253/2015/03/12/1_0_20150312101010.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1520366920000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
68494 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.706 sec - in com.uber.hoodie.index.bloom.TestHoodieBloomIndex
Running com.uber.hoodie.table.TestMergeOnReadTable
Formatting using clusterid: testClusterID
68943 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
70087 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
70617 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
71329 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
72106 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
72206 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
72283 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
73032 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
73222 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
73551 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
73551 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
73664 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
73886 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=73, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=69, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=58, numUpdates=0}}}
74322 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 58, totalInsertBuckets => 1, recordsPerBucket => 500000
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
74325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
74347 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
74347 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
74767 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
75307 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
76746 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 105B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 30B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [_row_key] BINARY: 3 values, 21B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 3 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 6, 2018 9:08:40 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 3 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,907
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 221B for [_hoodie_commit_seqno] BINARY: 73 values, 956B raw, 175B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,731B for [_hoodie_record_key] BINARY: 73 values, 2,927B raw, 1,631B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,723B for [_row_key] BINARY: 73 values, 2,920B raw, 1,623B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [fare] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,151
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 69 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 212B for [_hoodie_commit_seqno] BINARY: 69 values, 904B raw, 166B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,653B for [_hoodie_record_key] BINARY: 69 values, 2,767B raw, 1,553B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 69 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 69 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 69 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,644B for [_row_key] BINARY: 69 values, 2,760B raw, 1,544B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 69 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 69 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [begin_lat] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [begin_lon] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [end_lat] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [end_lon] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [fare] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 11,072
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 58 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 7B raw, 1B comp}
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 194B for [_hoodie_commit_seqno] BINARY: 58 values, 760B raw, 149B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,402B for [_hoodie_record_key] BINARY: 58 values, 2,326B raw, 1,303B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.77401 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
77919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
77919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
77934 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
77935 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
78070 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
78172 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
78584 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
78584 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
78763 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
79009 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
79224 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 83
79224 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
79440 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit6439035818665425197/2015/03/16/519a6190-e38a-4910-82d6-af24b9f9b502_1_001.parquet
79469 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 69 row keys from /tmp/junit6439035818665425197/2015/03/16/519a6190-e38a-4910-82d6-af24b9f9b502_1_001.parquet
79469 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit6439035818665425197/2015/03/16/519a6190-e38a-4910-82d6-af24b9f9b502_1_001.parquet => [05b77ce7-cfc2-46e8-875e-aea89953a8cc, 09aadd8e-38ff-4862-bbb8-a84c9e1148ab, 16989aff-8b31-4aa0-b9f9-bc289641fd67, 19ae24f2-71a0-480a-959c-18fe6f1a30a4, 291a37d4-378b-47ae-9eb2-182f4de1dc7d, 2964df6f-c915-4763-a448-2820890b4f8e, 2fc2f707-80f3-4456-8a50-e802eca1c31b, 3fce116e-7df4-4caa-82fb-214159e7b841, 44b3011d-d77f-4e81-b7ff-1ed6582c455b, 559d4578-de65-4727-ac51-a53c0c27cdf1, 5a167d2e-3eab-45f1-bf67-83fb203cef34, 5e2de57e-0a9a-4de9-a30e-8c1fbe2ba348, 652c57f7-4666-4a9c-9237-65a662677904, 68243695-4541-4812-908f-be046e5db556, 75757466-7b0e-4efe-b033-5a2c9d390f78, 77f55382-c13f-4041-8bdf-293984d1e9b8, 7e875398-8801-4409-940a-6e95db252de7, 7f2f233c-dca8-49a8-ba1b-a76db7082b18, 87cd6535-f5b4-4955-85e0-c8a59fff6b0c, bbd42dac-4715-4aa6-b987-f4259084a9b5, c1275469-6f47-43f9-9e55-28614cf2b1ba, edf89a01-f28c-494f-8e39-f94720393e3e, f918f159-1b03-4de9-900e-8461c885690a, fb965eb1-9e81-4871-91a5-659bae7af296, fbe5ca01-170d-40b0-9608-17a76b4c9f4c]
79506 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit6439035818665425197/2016/03/15/660ba48d-905b-4d91-9fcd-4fc25c347b8f_0_001.parquet
79534 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 73 row keys from /tmp/junit6439035818665425197/2016/03/15/660ba48d-905b-4d91-9fcd-4fc25c347b8f_0_001.parquet
79534 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit6439035818665425197/2016/03/15/660ba48d-905b-4d91-9fcd-4fc25c347b8f_0_001.parquet => [0d05b371-0b2d-402a-9f20-5ba9f750ea99, 0d4a430b-b005-4be0-9530-ba763f471287, 0ff9e055-9049-41f9-8720-fc239c793aff, 13e3f3b5-092b-4c9a-95f8-830125e14ea5, 1e8122cc-7d44-416d-9b6e-8a2617ca4646, 225ede87-7219-4e39-a164-e9201be78e15, 2537c224-0c57-4cf9-97ab-66753f1cda2e, 3aaf028f-94d6-400a-be43-cfa9bd93c0b4, 423d66fa-de6e-4e16-b5f2-75f4bc2abab2, 4ee3d710-ab8f-43ec-ab40-59edebed4d36, 508636ba-7f2b-405c-955d-6c9bc7c1acde, 52ecd842-7636-4fac-923a-aa34e7b1b8d1, 58305c97-f30a-492c-988d-cde96712ebaf, 62abab4e-1432-43fa-ae0a-fe697ba424b7, 6b9598a7-2c74-43bb-a66b-a65ccfed1b9b, 71e44671-b83a-41dd-8ac7-4b95a47daca1, 754ee19e-b993-411c-ac1c-cec033e9fc58]
79587 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 15 for /tmp/junit6439035818665425197/2016/03/15/660ba48d-905b-4d91-9fcd-4fc25c347b8f_0_001.parquet
79615 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 73 row keys from /tmp/junit6439035818665425197/2016/03/15/660ba48d-905b-4d91-9fcd-4fc25c347b8f_0_001.parquet
79615 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 15 results, for file /tmp/junit6439035818665425197/2016/03/15/660ba48d-905b-4d91-9fcd-4fc25c347b8f_0_001.parquet => [83f3448e-65d6-4eab-b90a-a7e3431fc5b1, 933abade-966f-4284-9816-d9670621441c, 9800665d-5489-4b37-9e4c-8e9899b9262b, 9cbf66df-a840-4ac5-a31e-649d5b69e8ca, a08e5c7e-5f6d-4c9a-b048-459f40f57f62, aabb1ba5-da7a-4cbb-8015-34bc68c6efc8, c02a980f-7a49-409c-8581-b269cd13630a, c073d91e-df45-46d7-9f48-ec2bae50a4b7, d0d0116c-3c23-4182-a784-44fb6210d600, d13808d1-20f9-48d4-ac68-0d04041c3b7d, d54501be-ac9b-45a9-87f9-6ba3793e7a97, dbd4513a-b3d3-49a6-b8cc-1d39655d7b2b, dcc605ee-ea50-4fcb-92b2-961ad959dcb5, effd1e75-0a63-41a9-b8db-848bb47c9b25, f0847cf8-dc8b-4220-8960-fd312603155c]
79640 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit6439035818665425197/2015/03/17/cd80abc2-b569-4749-b386-bb72a6cd7d67_2_001.parquet
79664 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit6439035818665425197/2015/03/17/cd80abc2-b569-4749-b386-bb72a6cd7d67_2_001.parquet
79664 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit6439035818665425197/2015/03/17/cd80abc2-b569-4749-b386-bb72a6cd7d67_2_001.parquet => [228b394c-ac15-4063-8550-46e3db44f9db, 2e8e29d6-77d1-4f7c-afc1-506f85f496c3, 43a625f2-b16e-4bdf-a6cc-21f2f3ef2ac5, 4bdd74ff-b088-4427-a437-e2d1b3ba300d, 54a63e24-25cd-485b-b3f2-5df1b20c0ca1, 6a3195b3-8b97-4770-9dc3-e4b9de88eaf7, 70a7e164-d93d-4635-80ab-f35f19751aaa, 7cfdf4fc-e160-45a5-b29f-75f8f4b7c0ff, 8afc63c1-1089-4b69-84d6-6f6071cd1302, 8c801e4d-93fd-411a-8635-1e379b679120, 93427fb8-3c8c-4545-b7c0-af6d891dc447, 973326ed-d06f-4137-86fd-416e1fa6075b, aa13f3b5-3052-47cc-9f13-4927615d8b51, aa9ba8fc-b863-4cda-9d8c-88b440cddd8d, b6da813b-0217-4b69-9c08-24ac83a9faab, b7fa5cc0-36ea-4e53-853b-ff06a7cb513c, b90db4c3-b987-4fdb-83ed-6aeba41e0803, b943a418-e227-4fb1-ae3a-882aa4a29c94, c16d6212-c512-4d80-bdbf-6f3de4d3e694, d2196943-fe11-40de-999e-5912a7b7bb06, dc1db06d-ea42-4d4f-9fd0-298b070c641a, e1a383b1-4081-408d-8fb5-271cfa94e815, e2f53229-0249-4ddb-8f41-06475120bd60, e5d9515b-63df-4229-860e-de8ece2c0a17, f81c76ac-f241-44b3-9142-044f08bdd60d, f8dfe0a7-3868-45d8-8d96-4a79df049863]
79740 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=83}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=32}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=26}}}
80071 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
80160 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
80161 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=519a6190-e38a-4910-82d6-af24b9f9b502}, 1=BucketInfo {bucketType=UPDATE, fileLoc=660ba48d-905b-4d91-9fcd-4fc25c347b8f}, 2=BucketInfo {bucketType=UPDATE, fileLoc=cd80abc2-b569-4749-b386-bb72a6cd7d67}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{519a6190-e38a-4910-82d6-af24b9f9b502=0, 660ba48d-905b-4d91-9fcd-4fc25c347b8f=1, cd80abc2-b569-4749-b386-bb72a6cd7d67=2}
80187 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
80188 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
80419 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 519a6190-e38a-4910-82d6-af24b9f9b502
80902 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 660ba48d-905b-4d91-9fcd-4fc25c347b8f
81068 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
81384 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file cd80abc2-b569-4749-b386-bb72a6cd7d67
81756 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
82285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
82285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
82302 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
82302 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
82388 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
82532 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
82945 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
82947 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 58 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 58 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 54B raw, 1B comp}
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 58 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,395B for [_row_key] BINARY: 58 values, 2,320B raw, 1,296B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 58 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 58 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [begin_lat] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [begin_lon] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [end_lat] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [end_lon] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [fare] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 69 records.
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 69
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 73 records.
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 73
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 73 records.
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 73
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 58 records.
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:51 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 58
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 73 records.
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 73
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 14,369
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_commit_time] BINARY: 73 values, 19B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 25B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 281B for [_hoodie_commit_seqno] BINARY: 73 values, 1,308B raw, 224B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,731B for [_hoodie_record_key] BINARY: 73 values, 2,927B raw, 1,631B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 177B for [_hoodie_file_name] BINARY: 73 values, 19B raw, 40B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 119B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,723B for [_row_key] BINARY: 73 values, 2,920B raw, 1,623B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 76B for [rider] BINARY: 73 values, 12B raw, 32B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 26B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [driver] BINARY: 73 values, 12B raw, 32B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 28B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pag83489 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
83826 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
es, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [fare] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 58 records.
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 58
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 11,468
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_commit_time] BINARY: 58 values, 16B raw, 36B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 25B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 250B for [_hoodie_commit_seqno] BINARY: 58 values, 1,046B raw, 194B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,402B for [_hoodie_record_key] BINARY: 58 values, 2,326B raw, 1,303B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 58 values, 8B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 172B for [_hoodie_file_name] BINARY: 58 values, 16B raw, 36B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 119B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 58 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,395B for [_row_key] BINARY: 58 values, 2,320B raw, 1,296B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [rider] BINARY: 58 values, 10B raw, 30B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 26B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 75B for [driver] BINARY: 58 values, 10B raw, 30B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 28B raw, 2B comp}
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [begin_lat] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [begin_lon] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [end_lat] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [end_lon] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:55 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 530B for [fare] DOUBLE: 58 values, 464B raw, 487B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 69 records.
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 69
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,536
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 69 values, 18B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 25B raw, 2B comp}
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 267B for [_hoodie_commit_seqno] BINARY: 69 values, 1,179B raw, 210B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,653B for [_hoodie_record_key] BINARY: 69 values, 2,767B raw, 1,553B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 69 values, 10B raw, 28B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 69 values, 18B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 119B raw, 2B comp}
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 69 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,644B for [_row_key] BINARY: 69 values, 2,760B raw, 1,544B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 75B for [rider] BINARY: 69 values, 11B raw, 31B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 26B raw, 2B comp}
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 77B for [driver] BINARY: 69 values, 11B raw, 31B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 28B raw, 2B comp}
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [begin_lat] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [begin_lon] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 619B for [end_lat] DOUBLE: 69 values, 552B raw, 575B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 6, 2018 9:08:56 PM INFO: org.apache.parquet.hadoop84965 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
85160 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
86594 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
87032 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
87715 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
88704 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
89705 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
90751 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
91110 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
91272 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
91694 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
91694 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
92029 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=68, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=60, numUpdates=0}}}
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 68, totalInsertBuckets => 1, recordsPerBucket => 500000
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 60, totalInsertBuckets => 1, recordsPerBucket => 500000
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
92045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
92107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
92107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
92113 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
92892 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
93874 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
94573 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
95244 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
95511 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
95511 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
95525 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
95525 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
95693 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
95715 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
96106 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
96107 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
96213 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
96861 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
96862 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
97095 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit7133371808668470744/2015/03/17/200922fb-33f8-458f-8b95-6959feff1db7_2_001.parquet
97120 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
97121 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit7133371808668470744/2015/03/17/200922fb-33f8-458f-8b95-6959feff1db7_2_001.parquet
97121 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit7133371808668470744/2015/03/17/200922fb-33f8-458f-8b95-6959feff1db7_2_001.parquet => [0114d5bc-1ac2-4b51-914e-ac04db63462f, 053d8583-0d9c-47f5-b0f0-9baee938f53a, 06c53ad7-93ce-4255-ab8f-6ffc06c169b4, 08e3c167-49a9-4c14-be93-2d1e79e3326c, 0a20f555-a71d-4b44-b551-f2c0dec047ec, 0d24aa04-6b37-46a0-b854-2e98e181aea0, 100cf713-e3c1-442f-bca2-5eeec0bb5cb8, 1623e273-013d-4b67-953e-6d0a52b2724b, 17c7e60c-c110-48bb-930b-f88deb305182, 189ea145-7135-4623-80fa-992f166bb0fe, 1d9fde1b-d853-418a-af39-2dca8dc4964e, 1e71f645-7edb-4ee0-a4fd-460baa95e8cf, 2091480b-c0eb-414e-abc0-ba84f4a9e1c0, 22bb8059-3021-4925-ba39-9c6ffa131066, 23126a22-c8ce-451b-888a-088186d7e35a, 23abe133-aaf1-490d-b82b-df934836feab, 24b80948-f006-41e8-b93c-ce89d4ba5939, 24fefe20-9184-4f11-ac30-e96c18a4325a, 312862cf-9665-429a-86f4-a085c09b6213, 33ec5f42-598d-46f7-a33c-9c894fe7d14b, 34cec2c9-a824-41a7-86bb-87f6a55604f9, 380b6ce0-6cd5-4bc4-ba6a-ea0ed4aab259, 41baa7a0-08c5-4ae5-b48c-cc0c7b11f066, 46e3becb-11e1-4c81-8318-e8d5cd102465, 4ab8b589-ccf2-4008-9091-dfcbad6c27e0, 4ae48a91-9b52-4401-82b7-af0cc2b02095, 4fcb0c15-ab5d-4ed1-984c-8b3d6eda06fb, 5440c715-4ab0-4c88-977f-a48545b2d02e, 5987a543-d76c-45cd-b824-7ac7cebaec31, 634bdec1-4b22-44d2-bab2-7e1e0e092520, 6387236c-b09e-4c04-b91d-c19b80e96984, 6d9fa525-7835-493c-adb0-cc5080043f93, 6e7e29c4-89fc-409e-9b01-968ec9147df8, 71452d81-e8a8-49da-8ba5-a0338d76ee6a, 7a770993-2e87-41dd-b7b3-3095c6616ea7, 7f3a8956-41a4-4036-abf7-f67f8aa8797b, 8ea5b192-9a61-4db0-a0f6-8ab585a7f280, 92b3f5a5-19f9-4541-94d2-d332df1aa284, 98ca60ad-5d81-4a28-bce1-5b3067adc615, 9da13a1e-8672-47bb-9849-b0516e5dd044, 9f16c288-b6d1-4800-9ae0-16fa720d7103, a375c3d6-1d98-49e8-a935-f6cab11cffe5, a3a09ba1-f534-4781-893f-ccfc1ed2cc9c, a9db487e-725d-4737-8cdf-cfd3d87925f7, af86a3c4-38b8-4227-a558-b1a6551aada3, b057184b-04e0-4acf-a864-e537ec2ab7b1, b228be11-cc5c-40e3-816f-e0af7093b83b, b4d98632-04af-4067-bf7f-8f34242e48cf, baa56d45-c8f8-408e-819b-dcb6da003e54, bcfb7d7f-5bd0-4659-b130-a24ab95b1513, c136448f-ca44-4e00-a63e-84afa129cbd4, c15b733c-81e0-4895-9bcc-dfc5c13e58de, c81f919d-f83b-4463-b091-fdc052f4e368, cd828096-c602-433a-8515-19abadb79daf, ea33c7c7-2901-4cf6-a9a8-c6af6ff881d3, eb79f139-33c4-4b94-bbf9-ac62b1841a4e, f2ebdd0d-75fa-4ac6-9f9f-d2f754a00a9f, f3e2ba98-0a9f-435f-8b7a-4b0bc2c9d1ce, fb680721-55e1-4414-ac41-33c7630ab40e, fe8464d1-b26f-4ae8-99bb-664836f5f9f4]
97145 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 43 for /tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_1_001.parquet
97168 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 68 row keys from /tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_1_001.parquet
97168 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 43 results, for file /tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_1_001.parquet => [04f00583-35d9-4ea8-83ac-a26dbcd64727, 0b45e8e4-045b-45b2-b94a-0a32bab85b2b, 12eef05d-2862-45c3-b0ad-3f2b17dd75a2, 143c5d04-2b89-478e-9e50-b84b461315a5, 15005477-ad5e-401b-b794-ce1cfadb2401, 15b79e75-734a-4745-a72a-c5e27c30a22b, 17ad09e1-9e61-43ec-88e2-886ef52c6df9, 1a3891b6-5dc2-4adf-9b62-b5dda4a75162, 1d15d504-48da-47ab-8fb5-71eb3bde4660, 1fe3e5d5-26f8-461d-932c-1eb58ff3cee0, 21000863-0dfd-4fdd-8afe-52b9f833ddd1, 2223b59a-b251-4bfa-a896-d504615a9ec8, 267ef0b0-a908-4a9f-928d-1f869cff9f68, 26a0b761-22e1-4f16-bfa5-361a9be2688e, 2bbde597-5907-492e-b7a3-310b2479f65d, 2cd3ed45-5da6-4ab2-84bc-e3f670c25f17, 313274db-c38a-472a-b50c-1f2b8c1fdb17, 326c36d7-094a-4359-85e8-594f63d7eef0, 33126d6b-f8fb-43c1-b483-c79193cb40b3, 381734e2-1434-4705-8c46-2dec86df565d, 398f1f07-4be9-4cd6-b54c-5215fb6248d2, 399aca2e-3181-4a64-8491-4437ba922910, 408822bb-e910-4aa8-8910-a2e11920252f, 40c7c59f-b881-4930-a5dc-a109f7478590, 471a7784-5bed-4547-ac42-25009005ac46, 4a86f096-28ae-48dd-b362-f91a004d6cfd, 4dc4a2cd-7b65-4a51-802f-99a856fe2321, 50211e5d-d484-44e3-9b2f-442e99f6c619, 51a85c04-7fab-4b65-9ac3-b41c6e9edf53, 55b8a9c8-4f3b-4f8e-af10-45146f8fbbd9, 57687eb6-3028-44fb-8603-1149dc97a0f3, 593aa3b0-99d0-46aa-b531-010e2f3ead55, 5d194ba1-8bdc-44a9-b488-e5f2af3ab185, 5ea47704-63cb-4cfb-9b2a-38cd5fefedad, 61e0141e-3ea4-4b6a-81be-7c8b8a2dbc9b, 62726e4f-4ebf-479c-a696-73222fc6bba8, 64b06a74-a5ff-4b9d-832b-d1c2fc6dae47, 6ea02ae0-5344-4dfc-80d4-9d30c04eb7de, 73a53a60-fcc3-4090-a079-8138e5516935, 73c0837c-0d94-475b-a219-e6e4195b0f1a, 777f3125-5cab-4ddf-99cc-9ccf8ade2050, 7d169d58-61a1-4fe1-85c6-224acdb6a717, 984258cc-a50c-4070-8b3b-33d46ea2a987]
97211 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_1_001.parquet
97234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 68 row keys from /tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_1_001.parquet
97234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_1_001.parquet => [a098a5be-9b7c-410c-a4fc-6aa6df739548, a66a3aee-2054-4abc-be44-b88a4ef636fd, ac23c462-c7e8-4bb4-84bd-b6cd0771ef14, b14296d4-b20a-4932-b725-3e1c4c0b463e, b39a3fd8-f59e-4b72-9358-71f3d2fa4c1b, b9137c65-b31d-4a98-92f7-dcf8cd8a8775, ba4ab249-9b49-45ce-bafc-64b2b6b77ad0, bd135f36-1b3a-45dc-b090-db29a1a9ae81, befd278f-90a5-4ab2-a86c-8da8c1fa3ee2, c0201968-6e1d-41c1-a67c-4b105a14066d, c20943ac-a468-40a4-b377-e95bdc16208a, c57bafe2-d0c4-459b-b558-a145387d5571, ced0878b-dd9b-448b-80fc-e6ce526214cc, d2b5e446-76a2-4ab2-b011-7b74a9da97b6, d68d8fde-7092-4754-a9ae-f12d0b6466bd, df7454ab-d554-4330-96cc-62b308c028ab, e7e518f9-3bca-41ac-8864-bd9d8edfe171, e9388e38-0182-4243-8873-a46012342a22, ea774ba5-3888-45a5-8b9d-0abed5138f9b, ebff1392-746d-41c8-92f7-0839d47824c4, eea9b720-03b4-4843-800a-91ab23721918, f0292257-55a2-4d7e-bdd1-7942d62c9e82, f064bd64-c054-43d3-89c3-1e9626e67233, f62571b3-4cf4-47f9-9f85-814d3eb25073, f87a5080-9544-4b29-8edd-3f4aad86121c]
97266 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 72 for /tmp/junit7133371808668470744/2016/03/15/e31af8f8-b9d9-423e-8232-9953ba038a17_0_001.parquet
97293 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit7133371808668470744/2016/03/15/e31af8f8-b9d9-423e-8232-9953ba038a17_0_001.parquet
97293 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 72 results, for file /tmp/junit7133371808668470744/2016/03/15/e31af8f8-b9d9-423e-8232-9953ba038a17_0_001.parquet => [0137927b-afc1-46cf-8de3-720e406f8a9a, 0692b02d-69dd-4ddf-9817-90a414f77362, 0cb1d867-895b-48b3-b7c5-9d26d0bf3363, 10e573fb-289f-492b-977f-e1d7960f5016, 155a1ab8-3713-4d34-94e0-5afe239f1c18, 1e0a5d33-3cfa-438e-9489-73373c189a8e, 1ecf09ef-f76d-43fe-a8e4-a83ac00494e5, 1f514936-dc99-40b4-9ed0-b2d5a51649a2, 24c8d44e-4d57-4e1a-b374-28b3ef5c61f8, 26180555-30ab-41a5-9504-8462b80c89d7, 26784d2b-d06f-4373-bddc-220bc3716115, 26b9b7b7-0b41-43c6-96e7-87ece10fff74, 2a45069a-3678-4caf-8655-d8a899993133, 2de01766-170a-4d23-b9e6-adddfbe6e7f6, 3064edac-6732-4c9d-bb23-2521e90e177f, 341a9550-e28a-4c10-b0b9-0ee4549f2f24, 357aeadc-812c-4bf3-a1ab-2d10dda996ce, 3a3b667d-cb8b-4394-a440-45d6ff768411, 3fd411b1-f6df-4242-813e-7f9698abb98e, 430c769f-aa8d-462e-b698-7e415d8912c5, 4468d232-3199-43b7-aa71-1c473fba44fb, 46442ea4-491d-4198-8c44-4d8b8769c74c, 4745f6f9-f7a4-4ebe-98c7-35e8160e8576, 5254f779-850a-49eb-9343-6c05f4a20d3a, 55f772c5-2baf-4a8f-b239-35c8778cfea6, 5674c9dc-7c65-4d27-9e39-fe8e8747f0de, 5d088fee-2dbb-46b3-bf6a-3ba45b84fd09, 5d2cce56-6a2f-4c21-af77-fa176f5b72b7, 6591d2f6-fdf7-4858-b94a-e63d851d5823, 66c07f6d-749b-4794-b7c9-02672aedf2cc, 6d70890c-7d79-4046-a213-f442a784a571, 6dc987bb-a4f9-4024-b79a-6681612bc416, 7740ee09-4933-46a4-accb-fe6401bf13b0, 7cae13c0-b0f9-46bf-89cc-e942e4de0895, 7e009e4c-d2b8-41ce-97da-0eacc3f2d459, 82544e24-81d3-4ff1-9962-abad969b8f67, 82ee02da-cf79-423c-b434-f5c2ac9da446, 8df8aaf9-363e-4c6a-a9e6-680727c6465f, 8e082978-da0e-40cc-99fc-d7b4fdc1bd76, 914fda5b-1184-4e10-9f32-0a7ac24b3a1a, 9221e44d-d262-4c7d-bbcf-59f1743c44de, 9c0e2da1-3231-4d29-b9fa-03dcf80b43c6, 9c27940a-35c2-4e7b-ab39-476de8cf6d05, a6924c4c-0dd7-4d99-9ec0-200f3d34ba35, a7690bd7-0b4a-4ef9-acd1-5380c162cc26, a7c7cb5f-de1b-4c37-b7da-4c4a452e711c, ae1258d1-438c-4fff-bc43-30ed8a3d4353, ae5e3964-14b5-4cae-8e2c-0adb8d079146, b01cbb39-8f28-49db-886f-e5e26edb341c, b2d45a93-9021-441f-8e55-34486d231c71, b76bd17d-1d8a-4e8a-a2c1-324a65d15256, ba66cf01-764e-49b5-92b3-0da393453144, c2858a8c-dfab-4b11-8abe-700e2c7e3de8, c6635aca-c5bc-4934-9a1e-0af42f13bd61, c911e79a-3b83-4a3b-8ea2-7e6f95583037, c9c3777e-ef4a-4ae5-b204-7f4a29714a29, cdfb45f4-ab1f-4527-a2e2-1174f26daef7, d5d3133a-f821-462b-a20e-a5f332be6f96, d8093edc-e59b-475b-8b80-60f4ba023591, deca1b45-4c53-4008-91ee-78733946ed5d, e2a64ecb-39d7-4eda-962b-a17f15809a86, e5370b6e-e946-4d51-833d-f688563407bd, e7cc421b-cea8-4be1-b945-328e11a664a5, e8f649f3-c820-4b7f-804e-38ba24fb46a8, eaa7e19f-b499-4321-9b41-82eb4c9aef24, ee09d65c-298f-41f5-85a9-72004d1bdd43, ef32be22-ae9b-4703-8985-364d70dd6de0, f033d6d6-0768-4c1c-979d-008f58bb3577, f040d9f8-d2a0-473f-bc7f-ff0233d4eadd, f59a67ef-50a4-4148-94c8-9eda08d652e6, f701a98f-ed0d-4bb7-a322-9f8e8dcca22a, f9399c44-bb77-40c9-99b7-5414490f07de]
97421 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=72}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=68}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=60}}}
97769 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
97849 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
97849 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=2b132064-95d3-43da-8f66-423012625b5d}, 1=BucketInfo {bucketType=UPDATE, fileLoc=200922fb-33f8-458f-8b95-6959feff1db7}, 2=BucketInfo {bucketType=UPDATE, fileLoc=e31af8f8-b9d9-423e-8232-9953ba038a17}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{2b132064-95d3-43da-8f66-423012625b5d=0, 200922fb-33f8-458f-8b95-6959feff1db7=1, e31af8f8-b9d9-423e-8232-9953ba038a17=2}
97907 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
97907 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
98812 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
99693 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
100071 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
100071 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
100086 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
100086 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
100251 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
100337 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
100758 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
100758 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
100950 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
101357 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
101428 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
101510 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
101514 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41540/tmp/junit7133371808668470744/2015/03/16/2b132064-95d3-43da-8f66-423012625b5d_0_002.parquet	true
101514 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
101518 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41540/tmp/junit7133371808668470744/2015/03/17/200922fb-33f8-458f-8b95-6959feff1db7_1_002.parquet	true
101518 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
101520 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41540/tmp/junit7133371808668470744/2016/03/15/e31af8f8-b9d9-423e-8232-9953ba038a17_2_002.parquet	true
101528 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
101941 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
101941 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
102447 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
102500 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
102945 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
102945 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
103239 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=12, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=3, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=5, numUpdates=0}}}
103485 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
103652 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
103652 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
103652 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 12, totalInsertBuckets => 1, recordsPerBucket => 500000
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 3, totalInsertBuckets => 1, recordsPerBucket => 500000
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 5, totalInsertBuckets => 1, recordsPerBucket => 500000
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
103653 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
103707 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
103707 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
104221 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
105407 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
105477 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
107022 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
107022 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
107045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
107045 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
107197 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
107288 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
107522 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
107608 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
107608 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
107706 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
108235 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
108235 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
108439 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet
108462 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet
108462 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet => [44964fb8-0570-4c57-bd7a-960facf514b3, 8106fbc1-6c1c-4401-9385-9ae47e27da0c, 84a5e542-3512-44e4-874d-7773795bc30d, 90252d89-94ce-448f-9d32-1d82156cbd18, aefd43a3-b55c-481c-8717-b28afa2649ec]
108484 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet
108509 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 3 row keys from /tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet
108509 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet => [552ad05b-0772-48c0-adb4-f5f92a962d38, a0166fec-810b-46f6-a87e-67874fb6d386, be6dc38d-58de-4432-8cf8-4984789a5ce9]
108528 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 2 for /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
108546 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 12 row keys from /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
108546 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet => [0879fc96-72a2-4caa-884e-3d638b667359, 1681e5a2-8cda-4fd1-adbc-23f4a9fbb1d6]
108587 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
108605 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 12 row keys from /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
108605 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet => [19cb7043-5e98-4a63-8519-d50bcf92b749, 22748ba2-7974-41f6-a37d-1de6e5d610e2, 301f8929-7042-4baf-8c0e-a39edb3dd6da, 3957ba59-baf2-4b3f-8eb1-bce804b101ad, 3c7459e1-8b3f-4b05-bd04-f355a63e71c6, 4e11891f-29a2-4a4a-a3f2-cf985f016a3c, 4e187923-f753-495c-aaa9-8ae6da2a06b0, 56c64343-0558-4e4a-960e-9b48c5e47493, c68b093f-d678-49d8-b51f-40d21d7b19e9, ee433256-d3d5-44bf-bcb9-a60720ccc563]
108683 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=12}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=3}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=5}}}
108697 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
109094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
109094 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=1d0dc375-7943-4a25-975f-c22d3e32cef5}, 1=BucketInfo {bucketType=UPDATE, fileLoc=8e70ea33-c655-49c8-9879-af0dd7e6a1be}, 2=BucketInfo {bucketType=UPDATE, fileLoc=410a8c52-7fbd-4586-8ded-b8218fa07bf9}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{1d0dc375-7943-4a25-975f-c22d3e32cef5=0, 8e70ea33-c655-49c8-9879-af0dd7e6a1be=1, 410a8c52-7fbd-4586-8ded-b8218fa07bf9=2}
109134 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
109134 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
109302 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 1d0dc375-7943-4a25-975f-c22d3e32cef5
109360 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
109737 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 8e70ea33-c655-49c8-9879-af0dd7e6a1be
109842 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
110179 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 410a8c52-7fbd-4586-8ded-b8218fa07bf9
111035 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
111035 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
111048 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
111048 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
111143 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
111275 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
111688 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
111688 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
111862 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
111877 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
112344 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
112546 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
112546 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
112728 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet
112753 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet
112753 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet => [44964fb8-0570-4c57-bd7a-960facf514b3, 8106fbc1-6c1c-4401-9385-9ae47e27da0c, 84a5e542-3512-44e4-874d-7773795bc30d, 90252d89-94ce-448f-9d32-1d82156cbd18, aefd43a3-b55c-481c-8717-b28afa2649ec]
112779 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet
112804 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 3 row keys from /tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet
112804 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet => [552ad05b-0772-48c0-adb4-f5f92a962d38, a0166fec-810b-46f6-a87e-67874fb6d386, be6dc38d-58de-4432-8cf8-4984789a5ce9]
112825 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 2 for /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
112845 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 12 row keys from /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
112845 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet => [0879fc96-72a2-4caa-884e-3d638b667359, 1681e5a2-8cda-4fd1-adbc-23f4a9fbb1d6]
112882 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
112901 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 12 row keys from /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet
112901 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet => [19cb7043-5e98-4a63-8519-d50bcf92b749, 22748ba2-7974-41f6-a37d-1de6e5d610e2, 301f8929-7042-4baf-8c0e-a39edb3dd6da, 3957ba59-baf2-4b3f-8eb1-bce804b101ad, 3c7459e1-8b3f-4b05-bd04-f355a63e71c6, 4e11891f-29a2-4a4a-a3f2-cf985f016a3c, 4e187923-f753-495c-aaa9-8ae6da2a06b0, 56c64343-0558-4e4a-960e-9b48c5e47493, c68b093f-d678-49d8-b51f-40d21d7b19e9, ee433256-d3d5-44bf-bcb9-a60720ccc563]
112973 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=12}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=3}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=5}}}
113391 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
113391 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=1d0dc375-7943-4a25-975f-c22d3e32cef5}, 1=BucketInfo {bucketType=UPDATE, fileLoc=8e70ea33-c655-49c8-9879-af0dd7e6a1be}, 2=BucketInfo {bucketType=UPDATE, fileLoc=410a8c52-7fbd-4586-8ded-b8218fa07bf9}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{1d0dc375-7943-4a25-975f-c22d3e32cef5=0, 8e70ea33-c655-49c8-9879-af0dd7e6a1be=1, 410a8c52-7fbd-4586-8ded-b8218fa07bf9=2}
113412 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
113412 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
113520 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 1d0dc375-7943-4a25-975f-c22d3e32cef5
113548 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
113963 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
114009 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 8e70ea33-c655-49c8-9879-af0dd7e6a1be
114466 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 410a8c52-7fbd-4586-8ded-b8218fa07bf9
115337 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
115337 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
115351 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
115351 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
115369 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
115502 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
115514 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
115925 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
115926 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
116127 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6219660614366757653
116132 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
116136 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
116137 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
116145 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6219660614366757653
116155 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
116155 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
116741 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
117003 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
117025 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6219660614366757653/2015/03/17/.1d0dc375-7943-4a25-975f-c22d3e32cef5_001.log.1] for base split hdfs://localhost:41540/tmp/junit6219660614366757653/2015/03/17/1d0dc375-7943-4a25-975f-c22d3e32cef5_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
117027 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
117027 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
117067 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6219660614366757653
117073 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
117073 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
117075 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
117080 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6219660614366757653
117088 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
117088 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
117100 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
117121 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
117133 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6219660614366757653/2016/03/15/.8e70ea33-c655-49c8-9879-af0dd7e6a1be_001.log.1] for base split hdfs://localhost:41540/tmp/junit6219660614366757653/2016/03/15/8e70ea33-c655-49c8-9879-af0dd7e6a1be_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
117156 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6219660614366757653
117163 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
117163 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
117165 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
117169 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6219660614366757653
117177 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
117177 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
117185 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
117199 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
117209 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6219660614366757653/2015/03/16/.410a8c52-7fbd-4586-8ded-b8218fa07bf9_001.log.1] for base split hdfs://localhost:41540/tmp/junit6219660614366757653/2015/03/16/410a8c52-7fbd-4586-8ded-b8218fa07bf9_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
117751 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
118073 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
118073 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
118277 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=8, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=7, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=5, numUpdates=0}}}
118415 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
118694 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 8, totalInsertBuckets => 1, recordsPerBucket => 500000
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 7, totalInsertBuckets => 1, recordsPerBucket => 500000
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
118695 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 5, totalInsertBuckets => 1, recordsPerBucket => 500000
118696 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
118696 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
118708 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
118708 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
118852 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
119794 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
120586 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
121705 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
121901 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
121901 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
121913 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
121913 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
122525 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
122616 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
122966 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
123030 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
123030 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
123118 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
123447 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 40
123448 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
123617 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit4014274618882608484/2015/03/17/772dedcb-3e61-4449-b17b-616e43f774e6_2_001.parquet
123645 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit4014274618882608484/2015/03/17/772dedcb-3e61-4449-b17b-616e43f774e6_2_001.parquet
123645 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit4014274618882608484/2015/03/17/772dedcb-3e61-4449-b17b-616e43f774e6_2_001.parquet => [069fa483-b041-4d1e-8f0c-880b2f3cd81c, 3d0cbbb9-704e-4110-8223-b959a8a33829, 69830890-f718-49b8-8a9b-2ce439693a4d, a7797da2-a1fa-401b-b3f1-165d8767d524, e4d646d3-0457-4cda-ad80-3515d05eda2f]
123668 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 4 for /tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_1_001.parquet
123687 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 7 row keys from /tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_1_001.parquet
123687 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 4 results, for file /tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_1_001.parquet => [2f1c40df-0035-4eb7-abfa-6c902f42fa3e, 4269a34e-1d4c-4109-bced-4640af0d2e3c, 630e8f38-3ddb-49e8-bd05-6f9c7f4f75cf, 7af1229c-9bf6-448c-8737-f6e4b1b9061d]
123738 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_1_001.parquet
123767 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 7 row keys from /tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_1_001.parquet
123767 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_1_001.parquet => [c5b7c660-b17c-4297-b219-0724b534c865, c7bf7f71-cf0c-45ea-939a-c7ec06f2e65b, dbd77783-93cb-417c-ab48-20eaba1aff9a]
123789 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 8 for /tmp/junit4014274618882608484/2016/03/15/f4c551a8-336c-4eed-a5ac-24a1707243b1_0_001.parquet
123813 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit4014274618882608484/2016/03/15/f4c551a8-336c-4eed-a5ac-24a1707243b1_0_001.parquet
123813 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 8 results, for file /tmp/junit4014274618882608484/2016/03/15/f4c551a8-336c-4eed-a5ac-24a1707243b1_0_001.parquet => [197c90ff-3642-4d51-9ebf-557fe928914e, 340f2495-c322-4157-89b3-4ee809e8e834, 4a1abe37-c8dd-4f7d-afe2-298c98bcf448, 70fa92b9-f70c-4aba-83a6-157328f27ae4, 79360e62-0795-4a71-81fb-b93866414bc3, c297f21b-d577-4786-8857-5a6a437db8d3, d8f17d95-d402-4fe9-a5e0-3f5e4deff3e9, ec7a4e42-40bf-4a7e-99b9-bcb75887c850]
123901 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=4, numUpdates=8}, 2015/03/16=WorkloadStat {numInserts=7, numUpdates=7}, 2015/03/17=WorkloadStat {numInserts=9, numUpdates=5}}}
124102 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
124318 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
124323 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=f4c551a8-336c-4eed-a5ac-24a1707243b1}, sizeBytes=435861}]
124323 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 4 inserts to existing update bucket 1
124323 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=1, weight=1.0}]
124325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=b4b577d9-69c6-42ad-bc20-89af670e3ab2}, sizeBytes=435770}]
124325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 7 inserts to existing update bucket 2
124325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=2, weight=1.0}]
124326 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=772dedcb-3e61-4449-b17b-616e43f774e6}, sizeBytes=435598}]
124326 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 9 inserts to existing update bucket 0
124326 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=0, weight=1.0}]
124326 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=772dedcb-3e61-4449-b17b-616e43f774e6}, 1=BucketInfo {bucketType=UPDATE, fileLoc=f4c551a8-336c-4eed-a5ac-24a1707243b1}, 2=BucketInfo {bucketType=UPDATE, fileLoc=b4b577d9-69c6-42ad-bc20-89af670e3ab2}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=2, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{772dedcb-3e61-4449-b17b-616e43f774e6=0, f4c551a8-336c-4eed-a5ac-24a1707243b1=1, b4b577d9-69c6-42ad-bc20-89af670e3ab2=2}
124340 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
124341 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
124425 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
124487 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 772dedcb-3e61-4449-b17b-616e43f774e6
124487 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file 772dedcb-3e61-4449-b17b-616e43f774e6
124997 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file f4c551a8-336c-4eed-a5ac-24a1707243b1
124997 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file f4c551a8-336c-4eed-a5ac-24a1707243b1
125204 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
125479 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file b4b577d9-69c6-42ad-bc20-89af670e3ab2
125479 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file b4b577d9-69c6-42ad-bc20-89af670e3ab2
125787 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
125968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
125968 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
125979 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
125979 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
126114 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
126525 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
126526 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
126648 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
126684 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit4014274618882608484
126689 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
126690 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
126692 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
126698 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit4014274618882608484
126706 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
126706 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
126715 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
126728 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
126736 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:41540/tmp/junit4014274618882608484/2015/03/17/772dedcb-3e61-4449-b17b-616e43f774e6_0_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
126750 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit4014274618882608484
126756 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
126756 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
126758 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
126763 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit4014274618882608484
126772 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
126773 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
126783 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
126797 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
126811 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:41540/tmp/junit4014274618882608484/2016/03/15/f4c551a8-336c-4eed-a5ac-24a1707243b1_1_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
126829 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit4014274618882608484
126838 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
126838 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
126843 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
126848 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit4014274618882608484
126857 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
126857 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
126864 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
126877 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
126886 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:41540/tmp/junit4014274618882608484/2015/03/16/b4b577d9-69c6-42ad-bc20-89af670e3ab2_2_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
127503 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306210939
127611 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
127866 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
127866 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
128012 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
128118 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=70, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=65, numUpdates=0}}}
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 70, totalInsertBuckets => 1, recordsPerBucket => 500000
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
128529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
128541 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
129389 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
129479 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
130878 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
131134 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
131182 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
131504 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
131504 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
131779 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=55, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=80, numUpdates=0}}}
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 55, totalInsertBuckets => 1, recordsPerBucket => 500000
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 80, totalInsertBuckets => 1, recordsPerBucket => 500000
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
132199 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
132212 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
132212 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
132586 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
132651 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
134578 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
134585 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
135541 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
135541 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
135550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
135550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
135740 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
135941 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
136155 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
136156 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
136295 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
136412 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
136915 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
136915 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
137226 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 80 for /tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet
137253 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 80 row keys from /tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet
137253 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 80 results, for file /tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet => [022a4aa1-03c8-41c7-b636-cba99fcc91da, 02e34f07-1d8e-426f-98dc-b2dbae53345a, 056749ba-2d1b-4e25-8e8e-9b3f2ec19cf4, 0a981c2e-525a-443c-bfca-d8aa9207ea3b, 0fbe0a65-3dac-437a-a8ae-840decb83365, 139acc25-9393-49d8-974e-416c6af2c66d, 1532d5c7-210f-4790-a3e3-42340f8e8343, 15bd610a-0297-4c18-badb-1264c4819fd4, 16020995-cfb2-4dcc-ac2b-a4da050cd177, 1d6ac1e7-3c34-49b1-aa4d-19d57b965bed, 1f0679a7-e95b-4984-a7dc-88dc9b0e20bf, 25ded5de-b8fb-48ad-a7c1-1f9a5e139ba5, 2b9017b7-499a-4a4e-a90a-69b161bc2dd4, 2bb133f8-b4e0-4ab4-989a-c612432a7b89, 2e0001bc-2f4c-4f02-b377-8c9db24e3e8a, 3216dbf7-af77-4a3f-8092-441a0c32ebb0, 35139853-7470-4445-8bbc-40c7ba0e4610, 35ad1a5b-4c79-492f-bf5a-8fe042389ff8, 36cf48f5-d4e7-4565-a1a3-ef9d6675ec3c, 38b8a62a-82ed-469e-8b27-472b335fff15, 3b49aa51-b90d-4148-a1b3-064adf25328d, 3ed169f2-ec21-4c9a-83fb-e796d73b278c, 3f406033-bade-4595-9312-0703f08516d0, 4012f340-ef87-4a6a-a060-71b0f7af9381, 4282f79d-a5e7-47b5-819f-b22855978832, 490f8a18-ae59-4815-8178-eccd636a1b61, 52042a1b-80b7-4537-91e2-970286b20773, 53f4dd63-5931-404d-9fc6-05d00eeb9cbd, 541d8c68-3ab6-4e8d-b727-7d2f0f4a4947, 557ed8d2-5bf0-4b58-a4e7-ecd8fa3022f0, 56374594-45d4-4ebe-a9d6-c1caf937ede4, 5d58b866-040f-4d47-9611-e89bb4d5e61b, 6198895e-e190-4759-a5b0-09a318ca3d97, 62b415b1-db92-4b59-bac4-5be17ddd510b, 6844a72c-0070-4826-b26e-04283fe27ea0, 68e15c55-85ea-45d7-ae0d-b047e8e194d7, 6b96d527-e73c-4712-83f4-f5f0c8c72efc, 70f0481d-ec0c-4282-870d-de25a37bac83, 718cfda0-a085-4315-bd9b-b41559d1e2dd, 73b5c1ef-f1d1-42cd-b51e-6329880eab19, 74bdd2d1-d041-4172-9f5d-c90c1705ebb2, 7a74e166-d809-4250-a16b-837752802fea, 7b11a3b8-565e-406a-8153-535c6826a0d9, 7c77e6ef-7fc1-4076-bde4-0ab9e7ac9ff5, 7d9fa72b-5d11-417d-9ca0-2341357a77b1, 8269bf7d-a73d-446a-a3be-fb6096734d0f, 84e2a531-ad70-4a85-bd35-603d6f9b1e21, 8e6d0e53-ed34-4f0f-9a52-7f54d9be5df4, 8fa20cb7-77c8-4f1e-a42f-3b7a14e817ae, 93205b00-3b6b-49f9-aa57-2b85966db025, 93274b3c-898e-4872-af67-1379cf8706f2, 98f9de91-e0ba-4dfb-a302-1eb00b8bd9a0, 9909900a-0bcf-4e25-9f4c-c28ef4f1e21b, 994ac461-be73-4d60-ba1f-36e8ae136e23, ad75d2f9-3a73-4dc9-a375-728e908b0c4c, ae708a52-10bd-4085-b166-b43842f02c5a, b530cc39-5c91-4a6f-8a23-9c71a94a886f, b8309f2b-8a8a-4da7-9113-cb89a6e84e55, bd975884-dea1-4449-a9ac-80b4ecf41a10, c083e298-b1f9-4961-8a0f-4d5a0c89e42b, c0ed865d-489f-4ed7-ae35-fcc593f57b92, c1f44bb8-b4a5-4d01-9a6e-7254ea1de9f6, c23c803c-bdd7-437a-be3d-55a66044e589, ca830581-f3f0-4fde-980e-f34471edb2c0, cdc5f426-267d-435b-890c-0c37c0ee5fc7, d5286fb2-00c6-44f3-a0a8-1105f28f1b03, d9da3364-e15c-4a51-acb6-8e8e765b09f2, dbbd4d67-74e5-46e2-850f-f88b1a138d05, dde87995-89af-41aa-b627-36278647e970, de605820-e4e4-4652-b504-7c648214d424, e02cfdac-a29d-4ed7-91c7-7ebef427ce05, e07627a4-c8b1-4ad1-90cd-ddd125233b52, e2423c1c-eca7-4ed2-8527-c07a02b699be, e8fa702e-3ecf-42d7-af4c-3c84c41568d0, e8fdf33a-6d79-49b1-87ff-1e9ac2b446cb, ee82189b-01ec-4b49-bdd7-90f1be93b177, ef9b1dfe-ba61-47c9-9923-56b07159b245, f5669f09-64ce-41cf-ba0b-c0b6a4a8ca70, fa0d2691-4d00-4da3-9be5-6eef72aa0895, fc83754d-43b6-4fb7-810d-f262f4339009]
137279 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 21 for /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
137298 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
137298 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 21 results, for file /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet => [08d78433-ce90-4c45-a923-38d6b4250b53, 0a025df1-b9f7-412c-bcdd-da65121c3549, 0bb24e0d-c887-4551-8210-224d07ef82bf, 0d6f9aed-a892-4a34-bedb-e6f54235c92e, 102206a4-f0fd-4961-b30f-54de82533be0, 11dcc4c1-aaa0-4c0e-99d9-e796c7d2c7fb, 13fbb35d-fb3e-4c99-8fc0-f46c776d831c, 19a3b377-55a1-483b-a5e4-09cde9c7bc0f, 21fa3dfd-c84a-495d-920a-d7236afd3ec4, 2584891e-36dd-4660-b42f-47cb46d03979, 2b40e020-c349-491d-935f-4fc09bbcf775, 318b5be3-5e61-4075-b622-f0820f947481, 33fd0c7f-6044-4b5f-8701-23114c800065, 40587765-79f3-4629-ac2d-3cc21b9bc44b, 4c6b1971-b499-48b3-aa84-dbe090afdd40, 5067fe22-adbc-45a2-9ff0-fd98dcee9838, 50b316d2-d4d2-4cf4-b110-b7c25260746c, 549f964e-a558-4222-ae53-5e69ce37823a, 55bc7ca2-2f7f-4b93-9b54-88289f2e7f5c, 596fd908-3a5a-417a-8450-9dbd7ecb9ae5, 600f2afc-191a-4ba4-8853-e3fcdb7ba937]
137349 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 44 for /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
137367 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
137367 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 44 results, for file /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet => [6045ed62-18c5-4c7b-a550-e88e0431dc01, 630417e5-7ba1-4ef9-a4cc-0220d483bf65, 6c1dd006-1750-40b5-b53f-bd59a902be09, 712930c0-ca02-4773-8f2e-5b089a723955, 71a8de04-5358-46dc-840a-cfa1b96159db, 7adc2289-d388-4500-a281-88b929a98abe, 7b2b7fea-f2a0-43eb-baa5-5d582b149612, 7ba36d02-2b67-41a6-ab83-9127ac37a83b, 812b6868-bfb9-4401-870f-0c7c0039e82f, 81d61f96-154c-4281-8c6b-e35f02ad9f25, 84fb38d3-57eb-42ee-adae-d3c243acc804, 852a332d-4963-4bf8-ae30-6666f45992ae, 86781fb8-176f-4c8c-8c92-e0242534a6ec, 8b4ea8f0-f9eb-4fab-9556-d46f56643e35, 8d180cf2-f5c1-43b2-9ee3-4c6693452eb2, 8e080dee-f692-49e1-84bc-d1fb7d9e4145, 92b30134-6181-49c6-91c7-9b676b4ad7d8, 93479d60-5cf1-49bc-9c20-e95ae2e20799, a22ab64e-8637-4592-a9d5-709dd955e957, a39f6db7-52c4-41e1-981f-5ad24f7110d2, a47c034f-bffb-4d6c-b0ac-62631b897aff, a490fe8b-5ab6-4d29-8da7-035ba7530af4, a51075ec-ef34-4836-8c7d-176a5b00a13f, a59c747b-7743-4436-ad8f-b8030060a2c8, a6d61725-42ab-4329-8903-5cd4068d29c8, aab65d58-4602-4e49-849e-8ddf861bed7e, aaf61ba5-e40a-44b8-a657-d2efe6d8d50e, abd83554-0283-4492-8834-505700bd521b, af61d529-aaa8-44f7-821a-e0e561fa6c5a, b3acbf22-8c8a-47ff-93d7-0e8ae32aa9cd, b880e4a9-f61f-4729-9e08-fc5bb420b37a, c2cb221c-67da-49a6-ad2c-c5832e1b770d, c60dd11f-5bfa-410e-9c3d-6aa4b85d9cea, cb1f13e3-d76b-4bf6-98a4-0236a10133e3, cbb60897-8e15-4fac-a38f-900f60be07d3, ccec7491-d1a6-49e6-b083-dc52e431d8a6, d9c8e632-9f19-41ee-890c-2d777c2e8057, e1534fa6-ad74-4d16-84b4-27959032adad, e774b0ad-cf1a-402e-8fc1-eec006ef6178, e95bcd98-1a2e-40eb-99c0-584700c0f5bb, ed2547d6-ded7-40f9-823e-f075f664555c, f15bb679-d0ce-4117-91de-ad0fa117847a, fe43227e-232f-4ed7-87d0-f2866a440581, ffeb292b-bd86-4222-b3bd-4010232b41a1]
137388 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 55 for /tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet
137404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 55 row keys from /tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet
137404 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 55 results, for file /tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet => [0545ee5d-a2c1-4455-816e-37057d667291, 073a60c6-7db2-4fa1-89b3-c5b8ae5d6756, 0a5e2be5-b8fc-4c4f-a537-99cce73ae9ee, 0d499d2d-ddad-4148-85c1-ca1a0265cc1d, 15ed375e-6a45-4b5e-bc50-389fcaa72606, 184b62d7-4703-45e7-a9c0-d976572f362f, 18fec8a7-5085-4f7f-9456-4ff98d929cf8, 1eb24d5d-698a-4c04-b419-4bfa103dce95, 2799526e-b83d-4f4a-bd05-24f1b32bc364, 2956e384-8ad8-4c66-acda-4c3e0ab8cd61, 299d0af3-2872-4888-93f1-93052e776d65, 2a18e071-adcb-4782-8bce-f041ef23f293, 2d4461fb-592b-4dce-a3ad-2cbd9a9f8f3a, 3033c17d-a5d7-4d12-81d2-5a6df6a0b1ac, 33edfb9d-158d-48e9-8201-0820a4b0aa5a, 3e2da166-2880-473b-8569-43787e41afdd, 3edc7567-f910-4bc8-aa99-35e8748baa97, 420c7fa2-b275-482a-abeb-11ef5510b277, 461663e6-5c81-4f3c-a198-f2bc88a3ba88, 4c06b5d4-a410-4492-b000-2d4fdb16a443, 4c957653-2937-4aba-8238-5f5826942e26, 4dbda08c-6930-4e5f-a496-2bde80d0e556, 4e448e4d-207d-44c4-bf0c-39d58695ebda, 502b3951-36d0-4c54-9730-8d40ef7cd4cc, 5395e412-9d2e-42d9-9611-d35e2df6ecfc, 596ceef4-744d-463c-a04b-0f0b8bf3dca6, 5a098bf4-d250-4104-829b-32fb0846a692, 5eadebc0-6b82-44a5-b7b5-81b716687c48, 61623dd9-3e90-4f89-aab5-009772250441, 61da7793-7f41-4ff5-b2ba-3fd47967c0c5, 717a070a-d7f7-425d-a8cc-3a00d3a368df, 85a6bcda-3529-4aa7-8ded-2edd02527276, 86fba176-f698-4226-97e7-6b5765cfac2a, 8d804b71-d142-4905-ad09-b93f47986e21, 927b25c2-a5ef-4092-8e67-5e46ad212acc, 96b2af3a-71b1-4710-b228-d62112f929c6, 99c49634-78f0-4850-a070-b23b21b794e7, 9a6b5f6f-6ee4-4370-a83f-d47120a5daf2, 9b411cff-90cd-44f2-873b-e4f1c2fd5f66, a34a224b-a809-4fab-be8f-3bb7c6b55bdc, b1469aff-0b34-401b-ba9b-3078ce7d4f1f, b1fe9154-c4fd-40b6-a59b-0db644e93c84, cd571e0c-e307-4ea3-a8fa-0941bcd27ddc, d001a099-d3c5-4442-b705-a033574a7e7f, d03ea1f0-2ca4-4a53-8391-9e58cad5af0b, d075f997-be33-4edd-a1b5-39bc195e84d8, d28ce086-801b-4fe8-8e06-9a3954449b30, df41dfd3-6c61-4d6f-a0cd-4e82957e9d29, ea9e274a-f333-4fc1-b73f-2d1ac7f8fd60, ec47dc0f-c6b7-4fd8-aceb-6ac1610c04b2, ee5187aa-8236-4697-be36-e1901a4c460f, f16b4caa-f19e-43ef-a992-7e2950025829, fa6438ed-a6bf-4982-96b1-c5df7e852848, fb5d35a3-66ad-4617-b82d-633e20843746, ffdce8aa-473a-4d60-83cc-8e844d9c2d97]
137498 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=55}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=80}}}
137735 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
137913 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
137913 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=50704d99-419c-4c20-98bb-73b6d4aa204d}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b978d5ef-6e1a-471f-a15a-fa0e0ee948da}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4b53250c-5c4b-4c54-a571-76dabc6e4c7b}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{50704d99-419c-4c20-98bb-73b6d4aa204d=0, b978d5ef-6e1a-471f-a15a-fa0e0ee948da=1, 4b53250c-5c4b-4c54-a571-76dabc6e4c7b=2}
137934 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
137957 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
137959 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
138214 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 50704d99-419c-4c20-98bb-73b6d4aa204d
138692 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file b978d5ef-6e1a-471f-a15a-fa0e0ee948da
138934 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
139173 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 4b53250c-5c4b-4c54-a571-76dabc6e4c7b
139775 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
140097 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
140097 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
140110 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
140110 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
140194 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
140369 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
140780 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
140781 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
140955 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
140960 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
140960 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
140961 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
140965 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
140973 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
140973 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
140983 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
140999 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
141007 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6281856182150015276/2016/03/15/.50704d99-419c-4c20-98bb-73b6d4aa204d_001.log.1] for base split hdfs://localhost:41540/tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
141060 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
141067 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
141067 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
141069 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
141074 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
141079 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
141079 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
141089 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
141102 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
141113 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/16/.b978d5ef-6e1a-471f-a15a-fa0e0ee948da_001.log.1] for base split hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
141145 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
141149 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
141154 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
141155 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
141158 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
141165 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
141165 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
141170 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
141179 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
141187 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/17/.4b53250c-5c4b-4c54-a571-76dabc6e4c7b_001.log.1] for base split hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
141217 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
141380 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
141381 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
141620 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
141819 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
142254 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
142687 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
142691 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
142749 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
143105 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
143105 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
143187 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
143193 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
143193 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
143195 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
143198 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
143205 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
143205 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
143216 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
143230 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
143239 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6281856182150015276/2016/03/15/.50704d99-419c-4c20-98bb-73b6d4aa204d_001.log.1] for base split hdfs://localhost:41540/tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
143253 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
143259 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
143260 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
143261 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
143266 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
143273 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
143273 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
143283 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
143295 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
143305 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/16/.b978d5ef-6e1a-471f-a15a-fa0e0ee948da_001.log.1] for base split hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
143318 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
143324 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
143325 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
143326 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
143330 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41540/tmp/junit6281856182150015276
143337 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
143337 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
143344 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
143356 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
143366 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/17/.4b53250c-5c4b-4c54-a571-76dabc6e4c7b_001.log.1] for base split hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
143377 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
143470 [dispatcher-event-loop-22] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 61 contains a task of very large size (117 KB). The maximum recommended task size is 100 KB.
144039 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
144104 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 171
144104 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
144326 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
144388 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 66 for /tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet
144410 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 80 row keys from /tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet
144410 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 66 results, for file /tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_2_001.parquet => [02e34f07-1d8e-426f-98dc-b2dbae53345a, 056749ba-2d1b-4e25-8e8e-9b3f2ec19cf4, 0a981c2e-525a-443c-bfca-d8aa9207ea3b, 139acc25-9393-49d8-974e-416c6af2c66d, 1532d5c7-210f-4790-a3e3-42340f8e8343, 15bd610a-0297-4c18-badb-1264c4819fd4, 16020995-cfb2-4dcc-ac2b-a4da050cd177, 1d6ac1e7-3c34-49b1-aa4d-19d57b965bed, 1f0679a7-e95b-4984-a7dc-88dc9b0e20bf, 25ded5de-b8fb-48ad-a7c1-1f9a5e139ba5, 2b9017b7-499a-4a4e-a90a-69b161bc2dd4, 2bb133f8-b4e0-4ab4-989a-c612432a7b89, 2e0001bc-2f4c-4f02-b377-8c9db24e3e8a, 3216dbf7-af77-4a3f-8092-441a0c32ebb0, 35ad1a5b-4c79-492f-bf5a-8fe042389ff8, 36cf48f5-d4e7-4565-a1a3-ef9d6675ec3c, 38b8a62a-82ed-469e-8b27-472b335fff15, 3ed169f2-ec21-4c9a-83fb-e796d73b278c, 3f406033-bade-4595-9312-0703f08516d0, 4282f79d-a5e7-47b5-819f-b22855978832, 52042a1b-80b7-4537-91e2-970286b20773, 53f4dd63-5931-404d-9fc6-05d00eeb9cbd, 541d8c68-3ab6-4e8d-b727-7d2f0f4a4947, 557ed8d2-5bf0-4b58-a4e7-ecd8fa3022f0, 56374594-45d4-4ebe-a9d6-c1caf937ede4, 5d58b866-040f-4d47-9611-e89bb4d5e61b, 6198895e-e190-4759-a5b0-09a318ca3d97, 62b415b1-db92-4b59-bac4-5be17ddd510b, 6844a72c-0070-4826-b26e-04283fe27ea0, 68e15c55-85ea-45d7-ae0d-b047e8e194d7, 6b96d527-e73c-4712-83f4-f5f0c8c72efc, 70f0481d-ec0c-4282-870d-de25a37bac83, 718cfda0-a085-4315-bd9b-b41559d1e2dd, 73b5c1ef-f1d1-42cd-b51e-6329880eab19, 74bdd2d1-d041-4172-9f5d-c90c1705ebb2, 7a74e166-d809-4250-a16b-837752802fea, 7c77e6ef-7fc1-4076-bde4-0ab9e7ac9ff5, 7d9fa72b-5d11-417d-9ca0-2341357a77b1, 8269bf7d-a73d-446a-a3be-fb6096734d0f, 84e2a531-ad70-4a85-bd35-603d6f9b1e21, 8e6d0e53-ed34-4f0f-9a52-7f54d9be5df4, 93205b00-3b6b-49f9-aa57-2b85966db025, 93274b3c-898e-4872-af67-1379cf8706f2, 98f9de91-e0ba-4dfb-a302-1eb00b8bd9a0, 9909900a-0bcf-4e25-9f4c-c28ef4f1e21b, 994ac461-be73-4d60-ba1f-36e8ae136e23, ad75d2f9-3a73-4dc9-a375-728e908b0c4c, ae708a52-10bd-4085-b166-b43842f02c5a, b530cc39-5c91-4a6f-8a23-9c71a94a886f, b8309f2b-8a8a-4da7-9113-cb89a6e84e55, bd975884-dea1-4449-a9ac-80b4ecf41a10, c23c803c-bdd7-437a-be3d-55a66044e589, ca830581-f3f0-4fde-980e-f34471edb2c0, d5286fb2-00c6-44f3-a0a8-1105f28f1b03, d9da3364-e15c-4a51-acb6-8e8e765b09f2, dbbd4d67-74e5-46e2-850f-f88b1a138d05, dde87995-89af-41aa-b627-36278647e970, de605820-e4e4-4652-b504-7c648214d424, e02cfdac-a29d-4ed7-91c7-7ebef427ce05, e07627a4-c8b1-4ad1-90cd-ddd125233b52, e8fa702e-3ecf-42d7-af4c-3c84c41568d0, e8fdf33a-6d79-49b1-87ff-1e9ac2b446cb, ee82189b-01ec-4b49-bdd7-90f1be93b177, f5669f09-64ce-41cf-ba0b-c0b6a4a8ca70, fa0d2691-4d00-4da3-9be5-6eef72aa0895, fc83754d-43b6-4fb7-810d-f262f4339009]
144446 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 13 for /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
144469 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
144469 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 13 results, for file /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet => [08d78433-ce90-4c45-a923-38d6b4250b53, 0a025df1-b9f7-412c-bcdd-da65121c3549, 0bb24e0d-c887-4551-8210-224d07ef82bf, 0d6f9aed-a892-4a34-bedb-e6f54235c92e, 102206a4-f0fd-4961-b30f-54de82533be0, 13fbb35d-fb3e-4c99-8fc0-f46c776d831c, 19a3b377-55a1-483b-a5e4-09cde9c7bc0f, 21fa3dfd-c84a-495d-920a-d7236afd3ec4, 2584891e-36dd-4660-b42f-47cb46d03979, 2b40e020-c349-491d-935f-4fc09bbcf775, 318b5be3-5e61-4075-b622-f0820f947481, 33fd0c7f-6044-4b5f-8701-23114c800065, 40587765-79f3-4629-ac2d-3cc21b9bc44b]
144531 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 44 for /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
144556 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet
144556 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 44 results, for file /tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_0_001.parquet => [4c6b1971-b499-48b3-aa84-dbe090afdd40, 5067fe22-adbc-45a2-9ff0-fd98dcee9838, 50b316d2-d4d2-4cf4-b110-b7c25260746c, 55bc7ca2-2f7f-4b93-9b54-88289f2e7f5c, 596fd908-3a5a-417a-8450-9dbd7ecb9ae5, 600f2afc-191a-4ba4-8853-e3fcdb7ba937, 6045ed62-18c5-4c7b-a550-e88e0431dc01, 630417e5-7ba1-4ef9-a4cc-0220d483bf65, 6c1dd006-1750-40b5-b53f-bd59a902be09, 712930c0-ca02-4773-8f2e-5b089a723955, 71a8de04-5358-46dc-840a-cfa1b96159db, 7adc2289-d388-4500-a281-88b929a98abe, 7b2b7fea-f2a0-43eb-baa5-5d582b149612, 7ba36d02-2b67-41a6-ab83-9127ac37a83b, 812b6868-bfb9-4401-870f-0c7c0039e82f, 81d61f96-154c-4281-8c6b-e35f02ad9f25, 84fb38d3-57eb-42ee-adae-d3c243acc804, 852a332d-4963-4bf8-ae30-6666f45992ae, 8b4ea8f0-f9eb-4fab-9556-d46f56643e35, 8d180cf2-f5c1-43b2-9ee3-4c6693452eb2, 8e080dee-f692-49e1-84bc-d1fb7d9e4145, 92b30134-6181-49c6-91c7-9b676b4ad7d8, 93479d60-5cf1-49bc-9c20-e95ae2e20799, a22ab64e-8637-4592-a9d5-709dd955e957, a39f6db7-52c4-41e1-981f-5ad24f7110d2, a47c034f-bffb-4d6c-b0ac-62631b897aff, a490fe8b-5ab6-4d29-8da7-035ba7530af4, a51075ec-ef34-4836-8c7d-176a5b00a13f, a59c747b-7743-4436-ad8f-b8030060a2c8, a6d61725-42ab-4329-8903-5cd4068d29c8, aab65d58-4602-4e49-849e-8ddf861bed7e, abd83554-0283-4492-8834-505700bd521b, af61d529-aaa8-44f7-821a-e0e561fa6c5a, c2cb221c-67da-49a6-ad2c-c5832e1b770d, c60dd11f-5bfa-410e-9c3d-6aa4b85d9cea, cb1f13e3-d76b-4bf6-98a4-0236a10133e3, cbb60897-8e15-4fac-a38f-900f60be07d3, ccec7491-d1a6-49e6-b083-dc52e431d8a6, d9c8e632-9f19-41ee-890c-2d777c2e8057, e1534fa6-ad74-4d16-84b4-27959032adad, e95bcd98-1a2e-40eb-99c0-584700c0f5bb, ed2547d6-ded7-40f9-823e-f075f664555c, f15bb679-d0ce-4117-91de-ad0fa117847a, fe43227e-232f-4ed7-87d0-f2866a440581]
144583 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 48 for /tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet
144601 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 55 row keys from /tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet
144601 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 48 results, for file /tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_1_001.parquet => [0545ee5d-a2c1-4455-816e-37057d667291, 073a60c6-7db2-4fa1-89b3-c5b8ae5d6756, 0a5e2be5-b8fc-4c4f-a537-99cce73ae9ee, 0d499d2d-ddad-4148-85c1-ca1a0265cc1d, 18fec8a7-5085-4f7f-9456-4ff98d929cf8, 1eb24d5d-698a-4c04-b419-4bfa103dce95, 2799526e-b83d-4f4a-bd05-24f1b32bc364, 2956e384-8ad8-4c66-acda-4c3e0ab8cd61, 299d0af3-2872-4888-93f1-93052e776d65, 2a18e071-adcb-4782-8bce-f041ef23f293, 2d4461fb-592b-4dce-a3ad-2cbd9a9f8f3a, 3033c17d-a5d7-4d12-81d2-5a6df6a0b1ac, 33edfb9d-158d-48e9-8201-0820a4b0aa5a, 3e2da166-2880-473b-8569-43787e41afdd, 3edc7567-f910-4bc8-aa99-35e8748baa97, 420c7fa2-b275-482a-abeb-11ef5510b277, 461663e6-5c81-4f3c-a198-f2bc88a3ba88, 4c06b5d4-a410-4492-b000-2d4fdb16a443, 4dbda08c-6930-4e5f-a496-2bde80d0e556, 4e448e4d-207d-44c4-bf0c-39d58695ebda, 502b3951-36d0-4c54-9730-8d40ef7cd4cc, 5395e412-9d2e-42d9-9611-d35e2df6ecfc, 596ceef4-744d-463c-a04b-0f0b8bf3dca6, 61623dd9-3e90-4f89-aab5-009772250441, 61da7793-7f41-4ff5-b2ba-3fd47967c0c5, 717a070a-d7f7-425d-a8cc-3a00d3a368df, 85a6bcda-3529-4aa7-8ded-2edd02527276, 86fba176-f698-4226-97e7-6b5765cfac2a, 8d804b71-d142-4905-ad09-b93f47986e21, 96b2af3a-71b1-4710-b228-d62112f929c6, 99c49634-78f0-4850-a070-b23b21b794e7, 9a6b5f6f-6ee4-4370-a83f-d47120a5daf2, 9b411cff-90cd-44f2-873b-e4f1c2fd5f66, a34a224b-a809-4fab-be8f-3bb7c6b55bdc, b1fe9154-c4fd-40b6-a59b-0db644e93c84, cd571e0c-e307-4ea3-a8fa-0941bcd27ddc, d001a099-d3c5-4442-b705-a033574a7e7f, d03ea1f0-2ca4-4a53-8391-9e58cad5af0b, d075f997-be33-4edd-a1b5-39bc195e84d8, d28ce086-801b-4fe8-8e06-9a3954449b30, df41dfd3-6c61-4d6f-a0cd-4e82957e9d29, ea9e274a-f333-4fc1-b73f-2d1ac7f8fd60, ec47dc0f-c6b7-4fd8-aceb-6ac1610c04b2, ee5187aa-8236-4697-be36-e1901a4c460f, f16b4caa-f19e-43ef-a992-7e2950025829, fa6438ed-a6bf-4982-96b1-c5df7e852848, fb5d35a3-66ad-4617-b82d-633e20843746, ffdce8aa-473a-4d60-83cc-8e844d9c2d97]
144686 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=171}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=57}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=48}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=66}}}
145101 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
145101 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=50704d99-419c-4c20-98bb-73b6d4aa204d}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b978d5ef-6e1a-471f-a15a-fa0e0ee948da}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4b53250c-5c4b-4c54-a571-76dabc6e4c7b}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{50704d99-419c-4c20-98bb-73b6d4aa204d=0, b978d5ef-6e1a-471f-a15a-fa0e0ee948da=1, 4b53250c-5c4b-4c54-a571-76dabc6e4c7b=2}
145159 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
145159 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
145385 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 50704d99-419c-4c20-98bb-73b6d4aa204d
145470 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
145517 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
145862 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file b978d5ef-6e1a-471f-a15a-fa0e0ee948da
146341 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 4b53250c-5c4b-4c54-a571-76dabc6e4c7b
146846 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
146846 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
146857 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
146857 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
147087 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
147150 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
147564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
147564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
147605 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
148899 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
149687 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
150159 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [20180306211000]
150353 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
150356 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/16/b978d5ef-6e1a-471f-a15a-fa0e0ee948da_2_20180306211000.parquet	true
150356 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
150358 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41540/tmp/junit6281856182150015276/2015/03/17/4b53250c-5c4b-4c54-a571-76dabc6e4c7b_0_20180306211000.parquet	true
150358 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
150360 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41540/tmp/junit6281856182150015276/2016/03/15/50704d99-419c-4c20-98bb-73b6d4aa204d_1_20180306211000.parquet	true
150366 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20180306211000]
150415 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
150779 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20180306211000] rollback is complete
150779 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
150821 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
150839 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
150954 [DataNode: [[[DISK]file:/tmp/1520366920884-0/dfs/data/data1/, [DISK]file:/tmp/1520366920884-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41540] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1381836472-172.17.0.3-1520366921158 (Datanode Uuid 5044da1f-336e-44ab-8a8b-0a5e4961ced6) service to localhost/127.0.0.1:41540 interrupted
150955 [DataNode: [[[DISK]file:/tmp/1520366920884-0/dfs/data/data1/, [DISK]file:/tmp/1520366920884-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41540] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1381836472-172.17.0.3-1520366921158 (Datanode Uuid 5044da1f-336e-44ab-8a8b-0a5e4961ced6) service to localhost/127.0.0.1:41540
150978 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@33782942] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 82.659 sec - in com.uber.hoodie.table.TestMergeOnReadTable
Running com.uber.hoodie.table.TestCopyOnWriteTable
151368 [dispatcher-event-loop-13] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (152 KB). The maximum recommended task size is 100 KB.
151408 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
151408 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
151408 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
151409 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
151409 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]
151409 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]}, 
UpdateLocations mapped to buckets =>{file1=0}
151756 [dispatcher-event-loop-21] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 2 contains a task of very large size (752 KB). The maximum recommended task size is 100 KB.
151814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
151815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
151815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
151815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 2200, totalInsertBuckets => 2, recordsPerBucket => 1000
151815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]
151815 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]}, 
UpdateLocations mapped to buckets =>{file1=0}
152131 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
152312 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
32bf8a16-d7e7-49cc-b57c-8f117576779e_0_20180306211004.parquet-478542
dd0b1ed8-4397-4fd2-94bb-b435777fcca8_0_20180306211004.parquet-478478
f12b1397-1d04-4bca-9100-dc44cfa0d739_0_20180306211004.parquet-451889
153567 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
153975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
153975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
153975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
153975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]
153975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]}, 
UpdateLocations mapped to buckets =>{file1=0}
154205 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
154869 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.194 sec - in com.uber.hoodie.table.TestCopyOnWriteTable
Running com.uber.hoodie.func.TestUpdateMapFunction
155438 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
43fc7986-74f3-4ad7-a986-2e188f62265a
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.183 sec - in com.uber.hoodie.func.TestUpdateMapFunction
Running com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
155627 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
155698 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
155708 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
155708 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
155708 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
155708 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
155708 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
155720 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
155720 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
156022 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306211008
156232 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=177, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=146, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=177, numUpdates=0}}}
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 177, totalInsertBuckets => 1, recordsPerBucket => 500000
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 146, totalInsertBuckets => 1, recordsPerBucket => 500000
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 177, totalInsertBuckets => 1, recordsPerBucket => 500000
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
156243 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
156259 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180306211008
156259 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306211008
156807 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
156916 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306211009
157255 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
157255 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
157278 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
157529 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180306211009
157529 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306211009
158541 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
158657 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
159781 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
159838 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161282 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161527 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
161873 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
161956 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
161956 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
162721 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
163071 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
164138 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
164515 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
165713 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
165830 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
167299 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
167797 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
169019 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
169269 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
170260 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
170945 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
172124 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
172619 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
172707 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
172953 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
172953 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
173218 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=56, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=73, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=71, numUpdates=0}}}
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 71, totalInsertBuckets => 1, recordsPerBucket => 500000
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
173230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
173247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
173247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
173306 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
173725 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01], with policy KEEP_LATEST_FILE_VERSIONS
173725 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
173956 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
174099 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
174187 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=74, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=71, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=55, numUpdates=0}}}
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 74, totalInsertBuckets => 1, recordsPerBucket => 500000
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 71, totalInsertBuckets => 1, recordsPerBucket => 500000
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 55, totalInsertBuckets => 1, recordsPerBucket => 500000
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
174195 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
174209 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
174209 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
174618 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
174618 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
174769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
174769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
174915 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
174915 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
175055 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
175055 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
175275 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
175275 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
175275 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
175335 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
175619 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
175654 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
175654 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
175878 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
175889 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
175889 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
175889 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
175889 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
175889 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
175909 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
175909 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
176196 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
176270 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=59, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=69, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=72, numUpdates=0}}}
176279 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
176279 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
176279 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 59, totalInsertBuckets => 1, recordsPerBucket => 500000
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
176280 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
176294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
176294 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
177277 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
177280 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit33174917230470403/.hoodie/.temp/2016-03-15_badb639e-d260-44ad-ac98-2350795d445f_0_001_3_3.parquet to /tmp/junit33174917230470403/2016/03/15/badb639e-d260-44ad-ac98-2350795d445f_0_001.parquet
177567 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
177834 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit33174917230470403/.hoodie/.temp/2015-03-16_c1765fbb-2d91-4c8c-a4b0-4425e4556f58_1_001_3_4.parquet to /tmp/junit33174917230470403/2015/03/16/c1765fbb-2d91-4c8c-a4b0-4425e4556f58_1_001.parquet
178381 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit33174917230470403/.hoodie/.temp/2015-03-17_097e92c0-0daa-49c4-b776-6ec1eb49a4ae_2_001_3_5.parquet to /tmp/junit33174917230470403/2015/03/17/097e92c0-0daa-49c4-b776-6ec1eb49a4ae_2_001.parquet
178593 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
178689 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
179759 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
179836 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
180258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02, 2016/06/02], with policy KEEP_LATEST_COMMITS
180258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
180561 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306211032
180757 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=160, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=158, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=182, numUpdates=0}}}
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 160, totalInsertBuckets => 1, recordsPerBucket => 500000
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 158, totalInsertBuckets => 1, recordsPerBucket => 500000
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 182, totalInsertBuckets => 1, recordsPerBucket => 500000
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
180766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
180785 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180306211032
180785 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306211032
180919 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
181443 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
181490 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
181490 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
181701 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
181702 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/01/id31_1_20160506030611.parquet	true
181702 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
181703 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/02/id32_1_20160506030611.parquet	true
181703 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
181704 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/06/id33_1_20160506030611.parquet	true
181710 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
181710 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
181721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
181723 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
181723 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
181921 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
181922 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
181922 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
181928 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
181928 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
181941 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
181942 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
181942 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
182150 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
182151 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/01/id21_1_20160502020601.parquet	true
182151 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
182152 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/02/id22_1_20160502020601.parquet	true
182152 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
182152 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/06/id23_1_20160502020601.parquet	true
182158 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
182158 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
182167 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
182169 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
182169 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
182348 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
182349 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/01/id21_1_20160502020601.parquet	true
182349 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
182350 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/02/id22_1_20160502020601.parquet	true
182350 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
182350 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/06/id23_1_20160502020601.parquet	true
182357 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
182357 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
182365 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
182367 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160501010101]
182367 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160501010101]
182522 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
182523 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/01/id11_1_20160501010101.parquet	true
182523 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
182524 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/02/id12_1_20160501010101.parquet	true
182524 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
182524 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5685517685508841759/2016/05/06/id13_1_20160501010101.parquet	true
182527 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160501010101]
182527 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160501010101]
182537 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160501010101] rollback is complete
182603 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
182740 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
182989 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
183087 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
183087 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
183395 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=300, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=94, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=100, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=106, numUpdates=0}}}
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 94, totalInsertBuckets => 1, recordsPerBucket => 500000
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 500000
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 106, totalInsertBuckets => 1, recordsPerBucket => 500000
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
183407 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
183425 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
183426 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
183947 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
183947 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
184142 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
184142 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
184301 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
184353 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
184353 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
184598 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
184598 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
184824 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
184839 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
184839 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
185180 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
185594 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
185594 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
185656 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
185890 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=68, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=60, numUpdates=0}}}
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 68, totalInsertBuckets => 1, recordsPerBucket => 500000
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 60, totalInsertBuckets => 1, recordsPerBucket => 500000
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
185903 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
185919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
185919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
186261 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
186449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
186449 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
186627 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
186628 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5521018362883388100/2016/05/01/id31_1_20160506030611.parquet	true
186628 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
186629 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5521018362883388100/2016/05/02/id32_1_20160506030611.parquet	true
186629 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
186629 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5521018362883388100/2016/05/06/id33_1_20160506030611.parquet	true
186634 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
186634 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
186644 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
186645 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
186645 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
186837 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
186838 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5521018362883388100/2016/05/01/id21_1_20160502020601.parquet	true
186838 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
186839 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5521018362883388100/2016/05/02/id22_1_20160502020601.parquet	true
186839 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
186839 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit5521018362883388100/2016/05/06/id23_1_20160502020601.parquet	true
186846 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
186846 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
186857 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
187362 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
187368 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
187368 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
187541 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
187543 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
187545 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
187545 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
187880 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/d6b0899f-fef2-4acd-b7cb-0e5bf65ac9fb_1_000.parquet	true
188041 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/b1f79dc2-3842-4a94-bf86-c6ce86ade7d6_1_000.parquet	true
188217 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/1b3f0f56-977f-4a9e-b55f-65fb5c27dd5f_1_000.parquet	true
188401 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/018f4a77-fdb9-4978-8de1-a4cfc4d85bef_1_000.parquet	true
188550 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/144a9467-8079-431f-b567-57c28a709e91_1_000.parquet	true
188690 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/0ba62908-06ce-460d-abe5-e8b68c5eb686_1_000.parquet	true
188839 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/aeadf536-89f9-48c7-a564-95ee5ba551c9_1_000.parquet	true
188852 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
188999 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/72682957-8063-443b-8010-7d8c86d0cbd8_1_000.parquet	true
189170 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/797ad3a3-23c0-4043-ba0b-681ccac0a80d_1_000.parquet	true
189330 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit396620305812597595/.hoodie/.temp/31d194bf-443c-47c7-a0e0-2e8b99aea1ea_1_000.parquet	true
189333 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
Tests run: 19, Failures: 0, Errors: 11, Skipped: 0, Time elapsed: 33.765 sec <<< FAILURE! - in com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
testSmallInsertHandlingForInserts(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 0.378 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieInsertException: Failed to insert for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForInserts(TestHoodieClientOnCopyOnWriteStorage.java:1108)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForInserts(TestHoodieClientOnCopyOnWriteStorage.java:1108)

testInsertAndCleanByCommits(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 0.9 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieInsertException: Failed to insert prepared records for commit time 20180306211008
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByCommits(TestHoodieClientOnCopyOnWriteStorage.java:732)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByCommits(TestHoodieClientOnCopyOnWriteStorage.java:732)

testFilterExist(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 4.944 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieInsertException: Failed to bulk insert for commit time 20180306211009
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testFilterExist(TestHoodieClientOnCopyOnWriteStorage.java:171)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testFilterExist(TestHoodieClientOnCopyOnWriteStorage.java:171)

testAutoCommit(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 10.732 sec  <<< ERROR!
java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testAutoCommit(TestHoodieClientOnCopyOnWriteStorage.java:199)

testCreateSavepoint(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 1.082 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testCreateSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:421)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testCreateSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:421)

testUpserts(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 0.66 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert prepared records for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:233)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpserts(TestHoodieClientOnCopyOnWriteStorage.java:218)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:233)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpserts(TestHoodieClientOnCopyOnWriteStorage.java:218)

testSmallInsertHandlingForUpserts(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 0.862 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForUpserts(TestHoodieClientOnCopyOnWriteStorage.java:995)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForUpserts(TestHoodieClientOnCopyOnWriteStorage.java:995)

testUpsertsWithFinalizeWrite(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 4.014 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert prepared records for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:233)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsWithFinalizeWrite(TestHoodieClientOnCopyOnWriteStorage.java:313)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsInternal(TestHoodieClientOnCopyOnWriteStorage.java:233)
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testUpsertsWithFinalizeWrite(TestHoodieClientOnCopyOnWriteStorage.java:313)

testInsertAndCleanByVersions(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 0.892 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieInsertException: Failed to insert for commit time 20180306211032
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByVersions(TestHoodieClientOnCopyOnWriteStorage.java:631)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByVersions(TestHoodieClientOnCopyOnWriteStorage.java:631)

testDeletes(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 1.328 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testDeletes(TestHoodieClientOnCopyOnWriteStorage.java:338)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testDeletes(TestHoodieClientOnCopyOnWriteStorage.java:338)

testRollbackToSavepoint(com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage)  Time elapsed: 1.272 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 001
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testRollbackToSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:521)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage.testRollbackToSavepoint(TestHoodieClientOnCopyOnWriteStorage.java:521)

Running com.uber.hoodie.TestMultiFS
Formatting using clusterid: testClusterID
189371 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
190565 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
190692 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
191809 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
192140 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
192160 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
192238 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
192922 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20180306211045
192933 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting commit 20180306211045
193231 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
193232 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
193264 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
193453 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=28, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=35, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=37, numUpdates=0}}}
193611 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 28, totalInsertBuckets => 1, recordsPerBucket => 500000
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 35, totalInsertBuckets => 1, recordsPerBucket => 500000
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
193873 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
193874 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
193893 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20180306211045
193893 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20180306211045
194585 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
195221 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
195861 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
197110 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
197171 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
197231 [DataNode: [[[DISK]file:/tmp/1520367041677-0/dfs/data/data1/, [DISK]file:/tmp/1520367041677-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:39028] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-597151968-172.17.0.3-1520367041766 (Datanode Uuid 0c1965c1-2a0d-4e79-974d-b24a48981626) service to localhost/127.0.0.1:39028 interrupted
197231 [DataNode: [[[DISK]file:/tmp/1520367041677-0/dfs/data/data1/, [DISK]file:/tmp/1520367041677-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:39028] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-597151968-172.17.0.3-1520367041766 (Datanode Uuid 0c1965c1-2a0d-4e79-974d-b24a48981626) service to localhost/127.0.0.1:39028
197242 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@73203c49] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
197303 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.982 sec <<< FAILURE! - in com.uber.hoodie.TestMultiFS
readLocalWriteHDFS(com.uber.hoodie.TestMultiFS)  Time elapsed: 4.61 sec  <<< ERROR!
com.uber.hoodie.exception.HoodieUpsertException: Failed to upsert for commit time 20180306211045
	at com.uber.hoodie.TestMultiFS.readLocalWriteHDFS(TestMultiFS.java:124)
Caused by: java.lang.ClassCastException: com.uber.hoodie.table.HoodieCopyOnWriteTable cannot be cast to com.uber.hoodie.table.HoodieMergeOnReadTable
	at com.uber.hoodie.TestMultiFS.readLocalWriteHDFS(TestMultiFS.java:124)

198273 [Executor task launch worker-0-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
198443 [spirals-vortex:32788.activeMasterManager-SendThread(localhost:59372)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x161fced5e280005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)

Results :

Tests in error: 
  TestHoodieClientOnCopyOnWriteStorage.testAutoCommit:199 » ClassCast com.uber.h...
  TestHoodieClientOnCopyOnWriteStorage.testCreateSavepoint:421 » HoodieUpsert Fa...
  TestHoodieClientOnCopyOnWriteStorage.testDeletes:338 » HoodieUpsert Failed to ...
  TestHoodieClientOnCopyOnWriteStorage.testFilterExist:171 » HoodieInsert Failed...
  TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByCommits:732 » HoodieInsert
  TestHoodieClientOnCopyOnWriteStorage.testInsertAndCleanByVersions:631 » HoodieInsert
  TestHoodieClientOnCopyOnWriteStorage.testRollbackToSavepoint:521 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForInserts:1108 » HoodieInsert
  TestHoodieClientOnCopyOnWriteStorage.testSmallInsertHandlingForUpserts:995 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testUpserts:218->testUpsertsInternal:233 » HoodieUpsert
  TestHoodieClientOnCopyOnWriteStorage.testUpsertsWithFinalizeWrite:313->testUpsertsInternal:233 » HoodieUpsert
  TestMultiFS.readLocalWriteHDFS:124 » HoodieUpsert Failed to upsert for commit ...

Tests run: 62, Failures: 0, Errors: 12, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hoodie ............................................. SUCCESS [  0.058 s]
[INFO] hoodie-common ...................................... SUCCESS [ 36.057 s]
[INFO] hoodie-hadoop-mr ................................... SUCCESS [  4.569 s]
[INFO] hoodie-client ...................................... FAILURE [03:21 min]
[INFO] hoodie-spark ....................................... SKIPPED
[INFO] hoodie-hive ........................................ SKIPPED
[INFO] hoodie-utilities ................................... SKIPPED
[INFO] hoodie-cli ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:02 min
[INFO] Finished at: 2018-03-06T21:10:51+01:00
[INFO] Final Memory: 45M/1126M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hoodie-client: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/uber/hudi/349988061/hoodie-client/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hoodie-client
