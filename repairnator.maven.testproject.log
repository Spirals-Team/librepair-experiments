[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Apache SAMOA
[INFO] samoa-instances
[INFO] samoa-api
[INFO] samoa-test
[INFO] samoa-local
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache SAMOA 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa ---
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-api
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-instances
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-local
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-storm
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-flink
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-apex
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-samza
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-test
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/samoa-threads
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/250285295/bin
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[INFO] Scan 383 files header done in 899.49ms.
[INFO] 
 * uptodate header on 373 files.
 * fail header on 10 files.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa ---
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-instances 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-instances ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-instances ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250285295/samoa-instances/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250285295/samoa-instances/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-instances ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250285295/samoa-instances/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom (3 KB at 6.5 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom (3 KB at 108.0 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar (67 KB at 1327.9 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.instances.ArffLoaderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec - in org.apache.samoa.instances.ArffLoaderTest

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-api 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-api ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250285295/samoa-api/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-api ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
2017-07-05 16:42:32,941 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
2017-07-05 16:42:33,649 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
2017-07-05 16:42:33,847 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:33,886 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:33,945 INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1049)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2017-07-05 16:42:33,946 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:33,947 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:33,951 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:33,955 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:33
2017-07-05 16:42:33,958 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:33,959 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:33,963 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:33,964 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:34,015 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:34,016 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:34,017 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:34,020 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:34,021 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:34,021 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:34,022 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:34,022 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:34,023 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:34,041 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:34,041 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:34,042 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:34,043 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:34,047 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:34,170 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:34,170 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:34,171 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:34,171 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:34,178 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:34,189 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:34,189 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:34,190 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:34,190 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:34,194 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:34,194 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:34,194 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:34,196 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:34,197 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:34,200 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:34,200 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:34,201 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:34,201 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:34,209 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:34,209 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:34,209 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:34,307 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:34,379 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 16:42:34,412 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 16:42:34,630 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 16:42:34,662 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 16:42:34,693 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 16:42:34,784 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 16:42:34,784 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 16:42:34,786 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 16:42:34,825 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 16:42:34,896 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-07-05 16:42:34,902 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 16:42:34,917 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:34,922 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 16:42:34,922 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:34,952 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 16:42:34,955 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:34,983 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 45862
2017-07-05 16:42:34,983 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:35,054 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_45862_hdfs____mbl1fh/webapp
2017-07-05 16:42:35,527 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45862
2017-07-05 16:42:35,541 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:35,541 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:35,550 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:35,550 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:35,551 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:35,551 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:35
2017-07-05 16:42:35,551 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:35,552 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:35,552 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:35,552 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:35,563 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:35,563 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:35,563 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:35,564 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:35,564 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:35,564 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:35,564 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:35,565 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:35,565 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:35,565 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:35,565 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:35,565 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:35,566 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:35,566 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:35,567 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:35,567 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:35,567 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:35,568 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:35,573 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:35,573 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:35,574 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:35,574 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:35,574 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:35,576 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:35,577 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:35,577 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:35,577 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:35,577 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:35,578 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:35,578 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:35,578 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:35,578 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:35,581 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:35,581 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:35,581 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:35,599 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:35,614 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:35,620 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 16:42:35,621 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 16:42:35,621 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 16:42:35,648 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 16:42:35,669 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 16:42:35,670 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 16:42:35,676 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 16:42:35,677 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 16:42:35,751 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 16:42:35,751 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 169 msecs
2017-07-05 16:42:36,174 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 16:42:36,194 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:36,248 INFO  [Socket Reader #1 for port 38030] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 38030
2017-07-05 16:42:36,391 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:38030 to access this namenode/service.
2017-07-05 16:42:36,403 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 16:42:36,482 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:36,482 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:36,482 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 16:42:36,484 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 16:42:36,484 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 16:42:36,485 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 16:42:36,518 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 16:42:36,518 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 16:42:36,518 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 16:42:36,518 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 16:42:36,518 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 16:42:36,518 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 34 msec
2017-07-05 16:42:36,622 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:36,642 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:38030
2017-07-05 16:42:36,643 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 16:42:36,653 INFO  [IPC Server listener on 38030] ipc.Server (Server.java:run(674)) - IPC Server listener on 38030: starting
2017-07-05 16:42:36,680 INFO  [CacheReplicationMonitor(738486921)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 16:42:36,681 INFO  [CacheReplicationMonitor(738486921)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2427045528 milliseconds
2017-07-05 16:42:36,683 INFO  [CacheReplicationMonitor(738486921)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 16:42:36,688 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 16:42:36,688 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 16:42:36,794 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 16:42:36,796 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 16:42:36,810 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 16:42:36,819 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:42568
2017-07-05 16:42:36,823 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 16:42:36,823 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 16:42:36,827 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 16:42:36,828 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:36,828 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 16:42:36,829 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:36,834 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:36,834 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 41597
2017-07-05 16:42:36,835 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:36,847 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_41597_datanode____k53djx/webapp
2017-07-05 16:42:37,001 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41597
2017-07-05 16:42:37,026 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 16:42:37,026 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 16:42:37,050 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:37,051 INFO  [Socket Reader #1 for port 48398] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 48398
2017-07-05 16:42:37,063 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:48398
2017-07-05 16:42:37,085 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 16:42:37,088 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 16:42:37,129 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:37,129 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38030 starting to offer service
2017-07-05 16:42:37,150 INFO  [IPC Server listener on 48398] ipc.Server (Server.java:run(674)) - IPC Server listener on 48398: starting
2017-07-05 16:42:38,015 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 16:42:38,060 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:38,061 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:38,061 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:38,092 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:38,105 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:38,105 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:38,393 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:38,393 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:38,394 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1656393774-172.17.0.6-1499265754227 is not formatted.
2017-07-05 16:42:38,394 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:38,395 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1656393774-172.17.0.6-1499265754227 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1656393774-172.17.0.6-1499265754227/current
2017-07-05 16:42:38,428 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:38,429 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1656393774-172.17.0.6-1499265754227 is not formatted.
2017-07-05 16:42:38,429 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:38,429 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1656393774-172.17.0.6-1499265754227 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1656393774-172.17.0.6-1499265754227/current
2017-07-05 16:42:38,443 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:38,444 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:38,484 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=896389854;bpid=BP-1656393774-172.17.0.6-1499265754227;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=896389854;c=0;bpid=BP-1656393774-172.17.0.6-1499265754227;dnuuid=null
2017-07-05 16:42:38,522 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 8ba2bbbb-3103-491f-80e7-534f10ca65c2
2017-07-05 16:42:38,522 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:38,539 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:38,658 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:38,658 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:38,691 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 16:42:38,692 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 16:42:38,701 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 16:42:38,701 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 16:42:38,747 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 16:42:38,760 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499270769760 with interval 21600000
2017-07-05 16:42:38,762 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:38,763 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:38,768 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:38,802 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1656393774-172.17.0.6-1499265754227 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:38,792 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1656393774-172.17.0.6-1499265754227 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:38,891 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:38,894 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:38,912 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1656393774-172.17.0.6-1499265754227 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 108ms
2017-07-05 16:42:38,916 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1656393774-172.17.0.6-1499265754227 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 114ms
2017-07-05 16:42:38,920 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1656393774-172.17.0.6-1499265754227: 152ms
2017-07-05 16:42:38,925 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1656393774-172.17.0.6-1499265754227 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:38,925 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1656393774-172.17.0.6-1499265754227 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 16:42:38,948 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1656393774-172.17.0.6-1499265754227 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:38,949 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1656393774-172.17.0.6-1499265754227 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 16:42:38,956 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 36ms
2017-07-05 16:42:38,962 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid null) service to localhost/127.0.0.1:38030 beginning handshake with NN
2017-07-05 16:42:38,997 INFO  [IPC Server handler 2 on 38030] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=8ba2bbbb-3103-491f-80e7-534f10ca65c2, infoPort=41597, ipcPort=48398, storageInfo=lv=-56;cid=testClusterID;nsid=896389854;c=0) storage 8ba2bbbb-3103-491f-80e7-534f10ca65c2
2017-07-05 16:42:39,003 INFO  [IPC Server handler 2 on 38030] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:39,004 INFO  [IPC Server handler 2 on 38030] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:42568
2017-07-05 16:42:39,017 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2237)) - !dn.datanode.isDatanodeFullyStarted()
2017-07-05 16:42:39,017 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:39,061 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid null) service to localhost/127.0.0.1:38030 successfully registered with NN
2017-07-05 16:42:39,061 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:38030 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 16:42:39,088 INFO  [IPC Server handler 9 on 38030] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:39,101 INFO  [IPC Server handler 9 on 38030] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a for DN 127.0.0.1:42568
2017-07-05 16:42:39,101 INFO  [IPC Server handler 9 on 38030] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc for DN 127.0.0.1:42568
2017-07-05 16:42:39,129 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:39,143 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid 8ba2bbbb-3103-491f-80e7-534f10ca65c2) service to localhost/127.0.0.1:38030 trying to claim ACTIVE state with txid=1
2017-07-05 16:42:39,148 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid 8ba2bbbb-3103-491f-80e7-534f10ca65c2) service to localhost/127.0.0.1:38030
2017-07-05 16:42:39,184 INFO  [IPC Server handler 8 on 38030] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:39,190 INFO  [IPC Server handler 8 on 38030] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc node DatanodeRegistration(127.0.0.1, datanodeUuid=8ba2bbbb-3103-491f-80e7-534f10ca65c2, infoPort=41597, ipcPort=48398, storageInfo=lv=-56;cid=testClusterID;nsid=896389854;c=0), blocks: 0, hasStaleStorages: true, processing time: 9 msecs
2017-07-05 16:42:39,191 INFO  [IPC Server handler 8 on 38030] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:39,191 INFO  [IPC Server handler 8 on 38030] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a node DatanodeRegistration(127.0.0.1, datanodeUuid=8ba2bbbb-3103-491f-80e7-534f10ca65c2, infoPort=41597, ipcPort=48398, storageInfo=lv=-56;cid=testClusterID;nsid=896389854;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 16:42:39,198 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:39,294 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 123 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@71e1a6a
2017-07-05 16:42:39,295 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:39,327 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 16:42:39,328 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:39,328 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 16:42:39,328 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 16:42:39,331 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:39,338 INFO  [IPC Server handler 7 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 16:42:39,345 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1656393774-172.17.0.6-1499265754227 to blockPoolScannerMap, new size=1
2017-07-05 16:42:39,420 INFO  [IPC Server handler 5 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:39,560 INFO  [IPC Server handler 0 on 38030] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-1656393774-172.17.0.6-1499265754227 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc:NORMAL:127.0.0.1:42568|RBW]]}
2017-07-05 16:42:40,598 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-320582036_1 at /127.0.0.1:45627 [Receiving block BP-1656393774-172.17.0.6-1499265754227:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1656393774-172.17.0.6-1499265754227:blk_1073741825_1001 src: /127.0.0.1:45627 dest: /127.0.0.1:42568
2017-07-05 16:42:40,746 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45627, dest: /127.0.0.1:42568, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320582036_1, offset: 0, srvID: 8ba2bbbb-3103-491f-80e7-534f10ca65c2, blockid: BP-1656393774-172.17.0.6-1499265754227:blk_1073741825_1001, duration: 88257687
2017-07-05 16:42:40,746 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:40,768 INFO  [IPC Server handler 0 on 38030] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc:NORMAL:127.0.0.1:42568|RBW]]} has not reached minimal replication 1
2017-07-05 16:42:40,789 INFO  [IPC Server handler 5 on 38030] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42568 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc:NORMAL:127.0.0.1:42568|RBW]]} size 1
2017-07-05 16:42:41,175 INFO  [IPC Server handler 7 on 38030] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_-320582036_1
2017-07-05 16:42:41,184 INFO  [IPC Server handler 8 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:41,196 INFO  [IPC Server handler 6 on 38030] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-1656393774-172.17.0.6-1499265754227 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a:NORMAL:127.0.0.1:42568|RBW]]}
2017-07-05 16:42:41,205 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-320582036_1 at /127.0.0.1:45628 [Receiving block BP-1656393774-172.17.0.6-1499265754227:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1656393774-172.17.0.6-1499265754227:blk_1073741826_1002 src: /127.0.0.1:45628 dest: /127.0.0.1:42568
2017-07-05 16:42:41,261 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45628, dest: /127.0.0.1:42568, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320582036_1, offset: 0, srvID: 8ba2bbbb-3103-491f-80e7-534f10ca65c2, blockid: BP-1656393774-172.17.0.6-1499265754227:blk_1073741826_1002, duration: 31024063
2017-07-05 16:42:41,262 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:41,274 INFO  [IPC Server handler 9 on 38030] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42568 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc:NORMAL:127.0.0.1:42568|FINALIZED]]} size 0
2017-07-05 16:42:41,283 INFO  [IPC Server handler 2 on 38030] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_-320582036_1
2017-07-05 16:42:41,291 INFO  [IPC Server handler 3 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:41,297 INFO  [IPC Server handler 4 on 38030] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3. BP-1656393774-172.17.0.6-1499265754227 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a:NORMAL:127.0.0.1:42568|RBW]]}
2017-07-05 16:42:41,313 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-320582036_1 at /127.0.0.1:45629 [Receiving block BP-1656393774-172.17.0.6-1499265754227:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1656393774-172.17.0.6-1499265754227:blk_1073741827_1003 src: /127.0.0.1:45629 dest: /127.0.0.1:42568
2017-07-05 16:42:41,357 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45629, dest: /127.0.0.1:42568, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320582036_1, offset: 0, srvID: 8ba2bbbb-3103-491f-80e7-534f10ca65c2, blockid: BP-1656393774-172.17.0.6-1499265754227:blk_1073741827_1003, duration: 39813560
2017-07-05 16:42:41,358 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:41,358 INFO  [IPC Server handler 1 on 38030] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42568 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a:NORMAL:127.0.0.1:42568|RBW]]} size 0
2017-07-05 16:42:41,362 INFO  [IPC Server handler 0 on 38030] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3 is closed by DFSClient_NONMAPREDUCE_-320582036_1
2017-07-05 16:42:41,366 INFO  [IPC Server handler 5 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:41,386 INFO  [IPC Server handler 7 on 38030] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4. BP-1656393774-172.17.0.6-1499265754227 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-faceb46f-a078-4f59-84d4-a11e8a8d1e9a:NORMAL:127.0.0.1:42568|RBW]]}
2017-07-05 16:42:41,393 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-320582036_1 at /127.0.0.1:45630 [Receiving block BP-1656393774-172.17.0.6-1499265754227:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1656393774-172.17.0.6-1499265754227:blk_1073741828_1004 src: /127.0.0.1:45630 dest: /127.0.0.1:42568
2017-07-05 16:42:41,453 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45630, dest: /127.0.0.1:42568, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320582036_1, offset: 0, srvID: 8ba2bbbb-3103-491f-80e7-534f10ca65c2, blockid: BP-1656393774-172.17.0.6-1499265754227:blk_1073741828_1004, duration: 37604007
2017-07-05 16:42:41,454 INFO  [PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1656393774-172.17.0.6-1499265754227:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:41,457 INFO  [IPC Server handler 8 on 38030] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42568 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-71bfa03b-f031-4aca-bbbb-cff46b1e8efc:NORMAL:127.0.0.1:42568|FINALIZED]]} size 0
2017-07-05 16:42:41,458 INFO  [IPC Server handler 6 on 38030] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4 is closed by DFSClient_NONMAPREDUCE_-320582036_1
2017-07-05 16:42:41,469 INFO  [IPC Server handler 9 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 16:42:41,492 INFO  [IPC Server handler 2 on 38030] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 16:42:41,498 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 16:42:41,499 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 16:42:41,499 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37eeec90] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 16:42:41,499 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 16:42:41,523 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:41,629 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 16:42:41,629 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 48398
2017-07-05 16:42:41,636 INFO  [IPC Server listener on 48398] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 48398
2017-07-05 16:42:41,636 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid 8ba2bbbb-3103-491f-80e7-534f10ca65c2) service to localhost/127.0.0.1:38030 interrupted
2017-07-05 16:42:41,636 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid 8ba2bbbb-3103-491f-80e7-534f10ca65c2) service to localhost/127.0.0.1:38030
2017-07-05 16:42:41,637 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:41,637 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1656393774-172.17.0.6-1499265754227 (Datanode Uuid 8ba2bbbb-3103-491f-80e7-534f10ca65c2)
2017-07-05 16:42:41,638 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1656393774-172.17.0.6-1499265754227 from blockPoolScannerMap
2017-07-05 16:42:41,638 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38030] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1656393774-172.17.0.6-1499265754227
2017-07-05 16:42:41,646 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@648368e6] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 16:42:41,646 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 16:42:41,647 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 16:42:41,647 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 16:42:41,647 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 16:42:41,647 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 16:42:41,648 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:41,648 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 16:42:41,650 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 128 Number of transactions batched in Syncs: 1 Number of syncs: 16 SyncTimes(ms): 17 5 
2017-07-05 16:42:41,650 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@37f21974] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 16:42:41,652 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@6f80fafe] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 16:42:41,658 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-05 16:42:41,659 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-05 16:42:41,670 INFO  [CacheReplicationMonitor(738486921)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 16:42:41,672 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 38030
2017-07-05 16:42:41,679 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@13518f37] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 16:42:41,683 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:41,684 INFO  [IPC Server listener on 38030] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 38030
2017-07-05 16:42:41,686 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@165b8a71] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 16:42:41,723 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:41,724 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 16:42:41,727 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:41,728 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 16:42:41,729 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 16:42:41,730 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 16:42:41,774 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 16:42:41,779 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:41,780 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:41,782 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:41,782 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:41,783 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:41,783 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:41
2017-07-05 16:42:41,783 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:41,783 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:41,784 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:41,784 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:41,806 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:41,807 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:41,807 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:41,807 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:41,807 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:41,808 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:41,808 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:41,808 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:41,808 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:41,808 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:41,808 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:41,809 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:41,809 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:41,809 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:41,810 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:41,810 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:41,810 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:41,810 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:41,818 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:41,819 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:41,819 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:41,819 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:41,820 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:41,822 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:41,822 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:41,822 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:41,822 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:41,822 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:41,823 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:41,823 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:41,823 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:41,823 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:41,825 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:41,825 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:41,826 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:41,827 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:41,875 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 16:42:41,908 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 16:42:41,972 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 16:42:41,974 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 16:42:41,975 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 16:42:41,976 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 16:42:41,977 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 16:42:41,977 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 16:42:41,982 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 16:42:41,983 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 16:42:41,983 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:41,984 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 16:42:41,984 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:41,985 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 16:42:41,986 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:41,987 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 49951
2017-07-05 16:42:41,988 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:41,996 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_49951_hdfs____ya8vx6/webapp
2017-07-05 16:42:42,127 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49951
2017-07-05 16:42:42,136 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:42,136 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:42,137 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:42,137 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:42,138 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:42,138 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:42
2017-07-05 16:42:42,138 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:42,139 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:42,139 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:42,139 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:42,237 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:42,237 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:42,238 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:42,238 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:42,238 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:42,238 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:42,238 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:42,238 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:42,240 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:42,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:42,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:42,244 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:42,244 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:42,244 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:42,245 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:42,245 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:42,245 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:42,245 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:42,251 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:42,251 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:42,251 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:42,252 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:42,252 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:42,254 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:42,254 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:42,254 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:42,255 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:42,255 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:42,255 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:42,255 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:42,256 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:42,256 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:42,257 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:42,258 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:42,258 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:42,272 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:42,284 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:42,291 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 16:42:42,292 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 16:42:42,292 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 16:42:42,293 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 16:42:42,294 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 16:42:42,294 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 16:42:42,295 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 16:42:42,295 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 16:42:42,336 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 16:42:42,337 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 79 msecs
2017-07-05 16:42:42,337 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 16:42:42,338 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:42,343 INFO  [Socket Reader #1 for port 49214] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 49214
2017-07-05 16:42:42,363 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:49214 to access this namenode/service.
2017-07-05 16:42:42,366 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 16:42:42,407 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:42,407 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:42,407 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 16:42:42,408 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 16:42:42,408 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 16:42:42,408 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 16:42:42,452 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 16:42:42,453 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 16:42:42,453 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 16:42:42,453 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 16:42:42,453 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 16:42:42,453 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 44 msec
2017-07-05 16:42:42,464 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:42,468 INFO  [IPC Server listener on 49214] ipc.Server (Server.java:run(674)) - IPC Server listener on 49214: starting
2017-07-05 16:42:42,469 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:49214
2017-07-05 16:42:42,473 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 16:42:42,492 INFO  [CacheReplicationMonitor(13306457)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 16:42:42,492 INFO  [CacheReplicationMonitor(13306457)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2427051340 milliseconds
2017-07-05 16:42:42,495 INFO  [CacheReplicationMonitor(13306457)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 16:42:42,498 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 16:42:42,498 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 16:42:42,548 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 16:42:42,548 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 16:42:42,549 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 16:42:42,549 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:59652
2017-07-05 16:42:42,550 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 16:42:42,550 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 16:42:42,551 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 16:42:42,551 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:42,552 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 16:42:42,554 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:42,556 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:42,557 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 57836
2017-07-05 16:42:42,557 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:42,572 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_57836_datanode____.kdtmg0/webapp
2017-07-05 16:42:42,694 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57836
2017-07-05 16:42:42,697 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 16:42:42,697 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 16:42:42,697 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:42,708 INFO  [Socket Reader #1 for port 38408] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 38408
2017-07-05 16:42:42,715 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:38408
2017-07-05 16:42:42,730 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 16:42:42,730 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 16:42:42,732 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:49214 starting to offer service
2017-07-05 16:42:42,737 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:42,740 INFO  [IPC Server listener on 38408] ipc.Server (Server.java:run(674)) - IPC Server listener on 38408: starting
2017-07-05 16:42:42,807 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:42,807 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:42,825 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 16:42:42,840 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:42,841 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:42,841 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:42,870 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:42,871 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:42,871 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:42,909 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:42,910 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:42,998 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:42,998 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:42,998 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1198605905-172.17.0.6-1499265761827 is not formatted.
2017-07-05 16:42:42,999 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:42,999 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1198605905-172.17.0.6-1499265761827 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1198605905-172.17.0.6-1499265761827/current
2017-07-05 16:42:43,008 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:43,009 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1198605905-172.17.0.6-1499265761827 is not formatted.
2017-07-05 16:42:43,009 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:43,009 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1198605905-172.17.0.6-1499265761827 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1198605905-172.17.0.6-1499265761827/current
2017-07-05 16:42:43,020 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:43,025 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:43,023 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:43,025 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:43,054 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1488296562;bpid=BP-1198605905-172.17.0.6-1499265761827;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1488296562;c=0;bpid=BP-1198605905-172.17.0.6-1499265761827;dnuuid=null
2017-07-05 16:42:43,071 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 3634aa68-19b9-42d1-a8dc-9ee98a497acf
2017-07-05 16:42:43,076 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 16:42:43,077 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 16:42:43,077 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 16:42:43,077 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 16:42:43,085 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 16:42:43,091 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499282412091 with interval 21600000
2017-07-05 16:42:43,108 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:43,110 INFO  [Thread-154] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1198605905-172.17.0.6-1499265761827 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:43,120 INFO  [Thread-155] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1198605905-172.17.0.6-1499265761827 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:43,131 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:43,131 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:43,221 INFO  [Thread-155] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1198605905-172.17.0.6-1499265761827 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 96ms
2017-07-05 16:42:43,222 INFO  [Thread-154] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1198605905-172.17.0.6-1499265761827 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 102ms
2017-07-05 16:42:43,230 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1198605905-172.17.0.6-1499265761827: 121ms
2017-07-05 16:42:43,233 INFO  [Thread-158] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1198605905-172.17.0.6-1499265761827 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:43,233 INFO  [Thread-158] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1198605905-172.17.0.6-1499265761827 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 16:42:43,245 INFO  [Thread-159] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1198605905-172.17.0.6-1499265761827 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:43,245 INFO  [Thread-159] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1198605905-172.17.0.6-1499265761827 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 16:42:43,246 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 17ms
2017-07-05 16:42:43,250 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:43,250 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:43,256 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid null) service to localhost/127.0.0.1:49214 beginning handshake with NN
2017-07-05 16:42:43,269 INFO  [IPC Server handler 6 on 49214] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=3634aa68-19b9-42d1-a8dc-9ee98a497acf, infoPort=57836, ipcPort=38408, storageInfo=lv=-56;cid=testClusterID;nsid=1488296562;c=0) storage 3634aa68-19b9-42d1-a8dc-9ee98a497acf
2017-07-05 16:42:43,269 INFO  [IPC Server handler 6 on 49214] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:43,269 INFO  [IPC Server handler 6 on 49214] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:59652
2017-07-05 16:42:43,271 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid null) service to localhost/127.0.0.1:49214 successfully registered with NN
2017-07-05 16:42:43,272 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:49214 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 16:42:43,278 INFO  [IPC Server handler 7 on 49214] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:43,293 INFO  [IPC Server handler 7 on 49214] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-f856f313-a2dd-48f2-a072-28b0e2acb5fa for DN 127.0.0.1:59652
2017-07-05 16:42:43,293 INFO  [IPC Server handler 7 on 49214] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-97e6d2e5-86e4-47b9-9d68-dd049ad23ee6 for DN 127.0.0.1:59652
2017-07-05 16:42:43,297 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid 3634aa68-19b9-42d1-a8dc-9ee98a497acf) service to localhost/127.0.0.1:49214 trying to claim ACTIVE state with txid=1
2017-07-05 16:42:43,301 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid 3634aa68-19b9-42d1-a8dc-9ee98a497acf) service to localhost/127.0.0.1:49214
2017-07-05 16:42:43,306 INFO  [IPC Server handler 8 on 49214] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-97e6d2e5-86e4-47b9-9d68-dd049ad23ee6,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:43,314 INFO  [IPC Server handler 8 on 49214] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-97e6d2e5-86e4-47b9-9d68-dd049ad23ee6 node DatanodeRegistration(127.0.0.1, datanodeUuid=3634aa68-19b9-42d1-a8dc-9ee98a497acf, infoPort=57836, ipcPort=38408, storageInfo=lv=-56;cid=testClusterID;nsid=1488296562;c=0), blocks: 0, hasStaleStorages: true, processing time: 6 msecs
2017-07-05 16:42:43,315 INFO  [IPC Server handler 8 on 49214] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-f856f313-a2dd-48f2-a072-28b0e2acb5fa,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:43,315 INFO  [IPC Server handler 8 on 49214] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-f856f313-a2dd-48f2-a072-28b0e2acb5fa node DatanodeRegistration(127.0.0.1, datanodeUuid=3634aa68-19b9-42d1-a8dc-9ee98a497acf, infoPort=57836, ipcPort=38408, storageInfo=lv=-56;cid=testClusterID;nsid=1488296562;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 16:42:43,316 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 15 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@43c86f7f
2017-07-05 16:42:43,317 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:43,321 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 16:42:43,322 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:43,322 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 16:42:43,322 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 16:42:43,329 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:43,330 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1198605905-172.17.0.6-1499265761827 to blockPoolScannerMap, new size=1
2017-07-05 16:42:43,353 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:43,366 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:43,374 INFO  [IPC Server handler 1 on 49214] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 16:42:43,378 INFO  [IPC Server handler 2 on 49214] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:43,410 INFO  [IPC Server handler 3 on 49214] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1198605905-172.17.0.6-1499265761827 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-97e6d2e5-86e4-47b9-9d68-dd049ad23ee6:NORMAL:127.0.0.1:59652|RBW]]}
2017-07-05 16:42:43,425 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1933979692_1 at /127.0.0.1:39788 [Receiving block BP-1198605905-172.17.0.6-1499265761827:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1198605905-172.17.0.6-1499265761827:blk_1073741825_1001 src: /127.0.0.1:39788 dest: /127.0.0.1:59652
2017-07-05 16:42:43,478 INFO  [PacketResponder: BP-1198605905-172.17.0.6-1499265761827:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:39788, dest: /127.0.0.1:59652, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1933979692_1, offset: 0, srvID: 3634aa68-19b9-42d1-a8dc-9ee98a497acf, blockid: BP-1198605905-172.17.0.6-1499265761827:blk_1073741825_1001, duration: 39898172
2017-07-05 16:42:43,479 INFO  [PacketResponder: BP-1198605905-172.17.0.6-1499265761827:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1198605905-172.17.0.6-1499265761827:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:43,479 INFO  [IPC Server handler 4 on 49214] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59652 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f856f313-a2dd-48f2-a072-28b0e2acb5fa:NORMAL:127.0.0.1:59652|FINALIZED]]} size 0
2017-07-05 16:42:43,483 INFO  [IPC Server handler 6 on 49214] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-1933979692_1
2017-07-05 16:42:43,486 INFO  [IPC Server handler 7 on 49214] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 16:42:43,488 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 16:42:43,488 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 16:42:43,488 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 16:42:43,488 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@383790cf] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 16:42:43,506 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:43,510 WARN  [88335763@qtp-320850799-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57836] http.HttpServer2 (HttpServer2.java:isRunning(460)) - HttpServer Acceptor: isRunning is false. Rechecking.
2017-07-05 16:42:43,510 WARN  [88335763@qtp-320850799-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57836] http.HttpServer2 (HttpServer2.java:isRunning(469)) - HttpServer Acceptor: isRunning is false
2017-07-05 16:42:43,607 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 16:42:43,608 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 38408
2017-07-05 16:42:43,609 INFO  [IPC Server listener on 38408] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 38408
2017-07-05 16:42:43,610 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:43,611 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid 3634aa68-19b9-42d1-a8dc-9ee98a497acf) service to localhost/127.0.0.1:49214 interrupted
2017-07-05 16:42:43,611 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid 3634aa68-19b9-42d1-a8dc-9ee98a497acf) service to localhost/127.0.0.1:49214
2017-07-05 16:42:43,611 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1198605905-172.17.0.6-1499265761827 (Datanode Uuid 3634aa68-19b9-42d1-a8dc-9ee98a497acf)
2017-07-05 16:42:43,612 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1198605905-172.17.0.6-1499265761827 from blockPoolScannerMap
2017-07-05 16:42:43,612 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:49214] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1198605905-172.17.0.6-1499265761827
2017-07-05 16:42:43,613 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@6a5d9b6] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 16:42:43,613 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 16:42:43,614 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 16:42:43,620 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 16:42:43,621 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 16:42:43,621 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 16:42:43,621 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:43,622 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@460b6d54] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 16:42:43,622 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@1bbae752] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 16:42:43,622 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 16:42:43,623 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 11 0 
2017-07-05 16:42:43,624 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-05 16:42:43,625 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-05 16:42:43,625 INFO  [CacheReplicationMonitor(13306457)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 16:42:43,629 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 49214
2017-07-05 16:42:43,633 INFO  [IPC Server listener on 49214] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 49214
2017-07-05 16:42:43,637 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:43,637 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@d176a31] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 16:42:43,638 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@6d3c232f] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 16:42:43,651 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:43,651 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 16:42:43,731 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:43,733 WARN  [1562249660@qtp-1402333753-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49951] http.HttpServer2 (HttpServer2.java:isRunning(460)) - HttpServer Acceptor: isRunning is false. Rechecking.
2017-07-05 16:42:43,733 WARN  [1562249660@qtp-1402333753-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49951] http.HttpServer2 (HttpServer2.java:isRunning(469)) - HttpServer Acceptor: isRunning is false
2017-07-05 16:42:43,832 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 16:42:43,833 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 16:42:43,834 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 16:42:43,899 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 16:42:43,911 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:43,912 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:43,917 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:43,917 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:43,917 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:43,918 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:43
2017-07-05 16:42:43,918 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:43,918 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:43,919 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:43,919 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:43,946 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:43,946 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:43,947 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:43,947 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:43,947 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:43,947 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:43,947 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:43,947 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:43,948 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:43,948 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:43,948 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:43,948 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:43,949 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:43,949 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:43,949 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:43,950 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:43,950 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:43,950 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:43,963 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:43,963 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:43,963 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:43,964 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:43,964 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:43,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:43,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:43,973 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:43,973 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:43,973 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:43,973 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:43,973 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:43,974 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:43,974 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:43,975 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:43,976 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:43,976 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:43,985 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:44,017 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 16:42:44,062 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 16:42:44,153 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 16:42:44,154 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 16:42:44,156 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 16:42:44,157 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 16:42:44,157 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 16:42:44,161 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 16:42:44,167 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 16:42:44,168 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 16:42:44,170 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:44,171 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 16:42:44,171 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:44,172 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 16:42:44,172 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:44,173 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 33984
2017-07-05 16:42:44,173 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:44,199 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_33984_hdfs____.22eitd/webapp
2017-07-05 16:42:44,353 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33984
2017-07-05 16:42:44,355 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:44,356 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:44,361 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:44,361 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:44,362 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:44,362 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:44
2017-07-05 16:42:44,362 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:44,363 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:44,363 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:44,363 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:44,492 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:44,492 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:44,493 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:44,494 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:44,494 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:44,494 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:44,494 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:44,494 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:44,495 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:44,495 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:44,496 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:44,496 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:44,501 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:44,502 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:44,502 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:44,502 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:44,503 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:44,504 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:44,508 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:44,509 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:44,509 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:44,509 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:44,509 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:44,509 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:44,510 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:44,510 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:44,511 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:44,511 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:44,511 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:44,532 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:44,548 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:44,551 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 16:42:44,551 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 16:42:44,552 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 16:42:44,556 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 16:42:44,557 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 16:42:44,557 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 16:42:44,558 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 16:42:44,558 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 16:42:44,591 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 16:42:44,591 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 79 msecs
2017-07-05 16:42:44,592 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 16:42:44,592 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:44,596 INFO  [Socket Reader #1 for port 47755] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 47755
2017-07-05 16:42:44,605 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:47755 to access this namenode/service.
2017-07-05 16:42:44,608 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 16:42:44,647 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:44,647 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:44,648 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 16:42:44,650 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 16:42:44,651 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 16:42:44,651 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 16:42:44,681 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 16:42:44,681 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 16:42:44,681 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 16:42:44,681 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 16:42:44,681 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 16:42:44,682 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 31 msec
2017-07-05 16:42:44,682 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:44,682 INFO  [IPC Server listener on 47755] ipc.Server (Server.java:run(674)) - IPC Server listener on 47755: starting
2017-07-05 16:42:44,684 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:47755
2017-07-05 16:42:44,700 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 16:42:44,713 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 16:42:44,714 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 16:42:44,726 INFO  [CacheReplicationMonitor(1699374435)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 16:42:44,726 INFO  [CacheReplicationMonitor(1699374435)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2427053573 milliseconds
2017-07-05 16:42:44,735 INFO  [CacheReplicationMonitor(1699374435)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 10 millisecond(s).
2017-07-05 16:42:44,756 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 16:42:44,757 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 16:42:44,757 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 16:42:44,758 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:56797
2017-07-05 16:42:44,758 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 16:42:44,758 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 16:42:44,759 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 16:42:44,760 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:44,760 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 16:42:44,760 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:44,761 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:44,762 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 42234
2017-07-05 16:42:44,762 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:44,774 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_42234_datanode____1mhuf0/webapp
2017-07-05 16:42:44,941 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42234
2017-07-05 16:42:44,953 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 16:42:44,954 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 16:42:44,954 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:44,961 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:34971
2017-07-05 16:42:44,963 INFO  [Socket Reader #1 for port 34971] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 34971
2017-07-05 16:42:44,974 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 16:42:44,975 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 16:42:44,976 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:47755 starting to offer service
2017-07-05 16:42:44,976 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:44,977 INFO  [IPC Server listener on 34971] ipc.Server (Server.java:run(674)) - IPC Server listener on 34971: starting
2017-07-05 16:42:45,040 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 16:42:45,046 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:45,046 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:45,060 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:45,061 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,061 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:45,096 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:45,097 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,097 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:45,160 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:45,161 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:45,176 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,177 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:45,177 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-372421122-172.17.0.6-1499265763985 is not formatted.
2017-07-05 16:42:45,177 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:45,177 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-372421122-172.17.0.6-1499265763985 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-372421122-172.17.0.6-1499265763985/current
2017-07-05 16:42:45,189 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:45,190 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-372421122-172.17.0.6-1499265763985 is not formatted.
2017-07-05 16:42:45,190 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:45,190 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-372421122-172.17.0.6-1499265763985 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-372421122-172.17.0.6-1499265763985/current
2017-07-05 16:42:45,208 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:45,209 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:45,252 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=434038813;bpid=BP-372421122-172.17.0.6-1499265763985;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=434038813;c=0;bpid=BP-372421122-172.17.0.6-1499265763985;dnuuid=null
2017-07-05 16:42:45,262 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:45,263 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:45,290 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 579f4300-31d5-4385-b987-1f6a547624a4
2017-07-05 16:42:45,292 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 16:42:45,292 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 16:42:45,292 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 16:42:45,292 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 16:42:45,297 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 16:42:45,298 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499286014298 with interval 21600000
2017-07-05 16:42:45,298 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,299 INFO  [Thread-228] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-372421122-172.17.0.6-1499265763985 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:45,299 INFO  [Thread-229] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-372421122-172.17.0.6-1499265763985 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:45,306 INFO  [Thread-228] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-372421122-172.17.0.6-1499265763985 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 8ms
2017-07-05 16:42:45,307 INFO  [Thread-229] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-372421122-172.17.0.6-1499265763985 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 7ms
2017-07-05 16:42:45,308 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-372421122-172.17.0.6-1499265763985: 9ms
2017-07-05 16:42:45,308 INFO  [Thread-232] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-372421122-172.17.0.6-1499265763985 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:45,308 INFO  [Thread-232] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-372421122-172.17.0.6-1499265763985 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 16:42:45,309 INFO  [Thread-233] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-372421122-172.17.0.6-1499265763985 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:45,309 INFO  [Thread-233] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-372421122-172.17.0.6-1499265763985 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 1ms
2017-07-05 16:42:45,310 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-05 16:42:45,313 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid null) service to localhost/127.0.0.1:47755 beginning handshake with NN
2017-07-05 16:42:45,315 INFO  [IPC Server handler 4 on 47755] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=579f4300-31d5-4385-b987-1f6a547624a4, infoPort=42234, ipcPort=34971, storageInfo=lv=-56;cid=testClusterID;nsid=434038813;c=0) storage 579f4300-31d5-4385-b987-1f6a547624a4
2017-07-05 16:42:45,315 INFO  [IPC Server handler 4 on 47755] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:45,315 INFO  [IPC Server handler 4 on 47755] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:56797
2017-07-05 16:42:45,320 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid null) service to localhost/127.0.0.1:47755 successfully registered with NN
2017-07-05 16:42:45,321 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:47755 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 16:42:45,322 INFO  [IPC Server handler 5 on 47755] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:45,323 INFO  [IPC Server handler 5 on 47755] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-62af974b-7bbb-4650-b946-e37077618f48 for DN 127.0.0.1:56797
2017-07-05 16:42:45,323 INFO  [IPC Server handler 5 on 47755] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-f7a7d930-bfcd-460f-becd-3c3762bc7d9d for DN 127.0.0.1:56797
2017-07-05 16:42:45,324 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid 579f4300-31d5-4385-b987-1f6a547624a4) service to localhost/127.0.0.1:47755 trying to claim ACTIVE state with txid=1
2017-07-05 16:42:45,326 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid 579f4300-31d5-4385-b987-1f6a547624a4) service to localhost/127.0.0.1:47755
2017-07-05 16:42:45,327 INFO  [IPC Server handler 6 on 47755] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-f7a7d930-bfcd-460f-becd-3c3762bc7d9d,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:45,327 INFO  [IPC Server handler 6 on 47755] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-f7a7d930-bfcd-460f-becd-3c3762bc7d9d node DatanodeRegistration(127.0.0.1, datanodeUuid=579f4300-31d5-4385-b987-1f6a547624a4, infoPort=42234, ipcPort=34971, storageInfo=lv=-56;cid=testClusterID;nsid=434038813;c=0), blocks: 0, hasStaleStorages: true, processing time: 0 msecs
2017-07-05 16:42:45,328 INFO  [IPC Server handler 6 on 47755] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-62af974b-7bbb-4650-b946-e37077618f48,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:45,328 INFO  [IPC Server handler 6 on 47755] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-62af974b-7bbb-4650-b946-e37077618f48 node DatanodeRegistration(127.0.0.1, datanodeUuid=579f4300-31d5-4385-b987-1f6a547624a4, infoPort=42234, ipcPort=34971, storageInfo=lv=-56;cid=testClusterID;nsid=434038813;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2017-07-05 16:42:45,328 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@583d3115
2017-07-05 16:42:45,329 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,329 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 16:42:45,329 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:45,329 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 16:42:45,330 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 16:42:45,333 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,334 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-372421122-172.17.0.6-1499265763985 to blockPoolScannerMap, new size=1
2017-07-05 16:42:45,366 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:45,375 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:45,380 INFO  [IPC Server handler 9 on 47755] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 16:42:45,384 INFO  [IPC Server handler 0 on 47755] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:45,391 INFO  [IPC Server handler 1 on 47755] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-372421122-172.17.0.6-1499265763985 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-62af974b-7bbb-4650-b946-e37077618f48:NORMAL:127.0.0.1:56797|RBW]]}
2017-07-05 16:42:45,401 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-819191676_1 at /127.0.0.1:55804 [Receiving block BP-372421122-172.17.0.6-1499265763985:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-372421122-172.17.0.6-1499265763985:blk_1073741825_1001 src: /127.0.0.1:55804 dest: /127.0.0.1:56797
2017-07-05 16:42:45,415 INFO  [PacketResponder: BP-372421122-172.17.0.6-1499265763985:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:55804, dest: /127.0.0.1:56797, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-819191676_1, offset: 0, srvID: 579f4300-31d5-4385-b987-1f6a547624a4, blockid: BP-372421122-172.17.0.6-1499265763985:blk_1073741825_1001, duration: 7895825
2017-07-05 16:42:45,415 INFO  [PacketResponder: BP-372421122-172.17.0.6-1499265763985:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-372421122-172.17.0.6-1499265763985:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:45,419 INFO  [IPC Server handler 3 on 47755] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-62af974b-7bbb-4650-b946-e37077618f48:NORMAL:127.0.0.1:56797|RBW]]} has not reached minimal replication 1
2017-07-05 16:42:45,419 INFO  [IPC Server handler 3 on 47755] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 16:42:45,419 INFO  [IPC Server handler 3 on 47755] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 16:42:45,420 INFO  [IPC Server handler 3 on 47755] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:56797 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-62af974b-7bbb-4650-b946-e37077618f48:NORMAL:127.0.0.1:56797|RBW]]} size 1
2017-07-05 16:42:45,822 INFO  [IPC Server handler 5 on 47755] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-819191676_1
2017-07-05 16:42:45,824 INFO  [IPC Server handler 6 on 47755] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 16:42:45,825 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 16:42:45,825 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 16:42:45,826 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2764c546] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 16:42:45,826 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 16:42:45,850 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:45,851 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 16:42:45,851 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 34971
2017-07-05 16:42:45,852 INFO  [IPC Server listener on 34971] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 34971
2017-07-05 16:42:45,852 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:45,853 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid 579f4300-31d5-4385-b987-1f6a547624a4) service to localhost/127.0.0.1:47755 interrupted
2017-07-05 16:42:45,853 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid 579f4300-31d5-4385-b987-1f6a547624a4) service to localhost/127.0.0.1:47755
2017-07-05 16:42:45,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-372421122-172.17.0.6-1499265763985 (Datanode Uuid 579f4300-31d5-4385-b987-1f6a547624a4)
2017-07-05 16:42:45,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-372421122-172.17.0.6-1499265763985 from blockPoolScannerMap
2017-07-05 16:42:45,855 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:47755] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-372421122-172.17.0.6-1499265763985
2017-07-05 16:42:45,863 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@4bf93c68] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 16:42:45,866 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 16:42:45,866 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 16:42:45,867 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 16:42:45,867 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 16:42:45,867 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 16:42:45,867 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:45,868 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 16:42:45,868 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@5411dd90] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 16:42:45,868 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@50194e8d] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 16:42:45,868 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 1 1 
2017-07-05 16:42:45,870 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-05 16:42:45,871 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-05 16:42:45,871 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 47755
2017-07-05 16:42:45,872 INFO  [CacheReplicationMonitor(1699374435)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 16:42:45,873 INFO  [IPC Server listener on 47755] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 47755
2017-07-05 16:42:45,873 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:45,873 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@289fdb08] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 16:42:45,881 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@17b64941] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 16:42:45,893 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:45,894 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 16:42:45,919 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:46,020 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 16:42:46,022 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 16:42:46,023 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 16:42:46,050 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 16:42:46,057 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:46,058 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:46,059 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:46,059 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:46,060 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:46,060 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:46
2017-07-05 16:42:46,061 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:46,061 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,062 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:46,062 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:46,073 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:46,074 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:46,074 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:46,074 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:46,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:46,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:46,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:46,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:46,076 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:46,076 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:46,077 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:46,077 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:46,077 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:46,078 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:46,078 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:46,079 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,079 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:46,079 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:46,085 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:46,086 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:46,086 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,087 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:46,087 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:46,089 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:46,089 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:46,090 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:46,090 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:46,090 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:46,091 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:46,091 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,092 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:46,092 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:46,093 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:46,094 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:46,094 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:46,096 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:46,134 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 16:42:46,167 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 16:42:46,226 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 16:42:46,227 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 16:42:46,229 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 16:42:46,230 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 16:42:46,230 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 16:42:46,231 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 16:42:46,234 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 16:42:46,235 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 16:42:46,235 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:46,236 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 16:42:46,236 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:46,237 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 16:42:46,237 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:46,238 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 41827
2017-07-05 16:42:46,238 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:46,242 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_41827_hdfs____jdu556/webapp
2017-07-05 16:42:46,363 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41827
2017-07-05 16:42:46,364 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:46,369 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:46,370 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:46,370 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:46,370 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:46,371 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:46
2017-07-05 16:42:46,371 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:46,371 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,371 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:46,372 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:46,403 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:46,403 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:46,403 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:46,404 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:46,404 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:46,404 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:46,404 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:46,404 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:46,404 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:46,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:46,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:46,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:46,405 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:46,406 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:46,406 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:46,406 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,407 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:46,407 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:46,599 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:46,600 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:46,600 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,600 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:46,611 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:46,617 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:46,617 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:46,617 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:46,617 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:46,618 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:46,618 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:46,618 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:46,618 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:46,619 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:46,620 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:46,620 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:46,620 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:46,637 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:46,660 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:46,663 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 16:42:46,663 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 16:42:46,664 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 16:42:46,665 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 16:42:46,666 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 16:42:46,669 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 16:42:46,669 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 16:42:46,670 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 16:42:46,743 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 16:42:46,743 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 123 msecs
2017-07-05 16:42:46,744 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 16:42:46,745 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:46,753 INFO  [Socket Reader #1 for port 46179] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 46179
2017-07-05 16:42:46,758 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:46179 to access this namenode/service.
2017-07-05 16:42:46,761 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 16:42:46,781 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:46,781 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:46,781 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 16:42:46,782 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 16:42:46,782 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 16:42:46,782 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 16:42:46,814 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 16:42:46,814 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 16:42:46,814 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 16:42:46,814 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 16:42:46,814 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 16:42:46,814 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 32 msec
2017-07-05 16:42:46,821 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:46,821 INFO  [IPC Server listener on 46179] ipc.Server (Server.java:run(674)) - IPC Server listener on 46179: starting
2017-07-05 16:42:46,824 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:46179
2017-07-05 16:42:46,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 16:42:46,831 INFO  [CacheReplicationMonitor(505521407)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 16:42:46,831 INFO  [CacheReplicationMonitor(505521407)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2427055678 milliseconds
2017-07-05 16:42:46,833 INFO  [CacheReplicationMonitor(505521407)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 16:42:46,837 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 16:42:46,837 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 16:42:46,900 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 16:42:46,901 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 16:42:46,901 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 16:42:46,902 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:59283
2017-07-05 16:42:46,902 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 16:42:46,902 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 16:42:46,903 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 16:42:46,904 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:46,904 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 16:42:46,904 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:46,905 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:46,905 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 36315
2017-07-05 16:42:46,906 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:46,927 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_36315_datanode____.hq76it/webapp
2017-07-05 16:42:47,061 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36315
2017-07-05 16:42:47,062 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 16:42:47,062 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 16:42:47,064 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:47,065 INFO  [Socket Reader #1 for port 34751] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 34751
2017-07-05 16:42:47,075 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:34751
2017-07-05 16:42:47,079 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 16:42:47,079 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 16:42:47,080 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46179 starting to offer service
2017-07-05 16:42:47,081 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:47,094 INFO  [IPC Server listener on 34751] ipc.Server (Server.java:run(674)) - IPC Server listener on 34751: starting
2017-07-05 16:42:47,119 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 16:42:47,121 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:47,121 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:47,136 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:47,137 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:47,137 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:47,159 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:47,159 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:47,159 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:47,228 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:47,228 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:47,246 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:47,247 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:47,248 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-205437870-172.17.0.6-1499265766096 is not formatted.
2017-07-05 16:42:47,248 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:47,248 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-205437870-172.17.0.6-1499265766096 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-205437870-172.17.0.6-1499265766096/current
2017-07-05 16:42:47,259 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:47,260 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-205437870-172.17.0.6-1499265766096 is not formatted.
2017-07-05 16:42:47,260 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:47,260 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-205437870-172.17.0.6-1499265766096 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-205437870-172.17.0.6-1499265766096/current
2017-07-05 16:42:47,274 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:47,275 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:47,306 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1124059488;bpid=BP-205437870-172.17.0.6-1499265766096;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1124059488;c=0;bpid=BP-205437870-172.17.0.6-1499265766096;dnuuid=null
2017-07-05 16:42:47,330 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:47,330 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:47,342 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 0361538a-7735-4297-8331-db3e76456292
2017-07-05 16:42:47,343 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 16:42:47,344 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 16:42:47,344 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 16:42:47,345 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 16:42:47,347 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 16:42:47,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499270929350 with interval 21600000
2017-07-05 16:42:47,351 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:47,351 INFO  [Thread-302] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-205437870-172.17.0.6-1499265766096 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:47,352 INFO  [Thread-303] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-205437870-172.17.0.6-1499265766096 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:47,361 INFO  [Thread-302] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-205437870-172.17.0.6-1499265766096 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 9ms
2017-07-05 16:42:47,365 INFO  [Thread-303] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-205437870-172.17.0.6-1499265766096 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 13ms
2017-07-05 16:42:47,365 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-205437870-172.17.0.6-1499265766096: 15ms
2017-07-05 16:42:47,367 INFO  [Thread-306] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-205437870-172.17.0.6-1499265766096 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:47,368 INFO  [Thread-307] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-205437870-172.17.0.6-1499265766096 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:47,368 INFO  [Thread-306] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-205437870-172.17.0.6-1499265766096 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 16:42:47,368 INFO  [Thread-307] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-205437870-172.17.0.6-1499265766096 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 16:42:47,368 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 3ms
2017-07-05 16:42:47,369 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid null) service to localhost/127.0.0.1:46179 beginning handshake with NN
2017-07-05 16:42:47,371 INFO  [IPC Server handler 4 on 46179] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=0361538a-7735-4297-8331-db3e76456292, infoPort=36315, ipcPort=34751, storageInfo=lv=-56;cid=testClusterID;nsid=1124059488;c=0) storage 0361538a-7735-4297-8331-db3e76456292
2017-07-05 16:42:47,371 INFO  [IPC Server handler 4 on 46179] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:47,371 INFO  [IPC Server handler 4 on 46179] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:59283
2017-07-05 16:42:47,373 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid null) service to localhost/127.0.0.1:46179 successfully registered with NN
2017-07-05 16:42:47,374 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:46179 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 16:42:47,376 INFO  [IPC Server handler 5 on 46179] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:47,376 INFO  [IPC Server handler 5 on 46179] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-19c88e55-c469-459d-8b76-51d186b4d61f for DN 127.0.0.1:59283
2017-07-05 16:42:47,376 INFO  [IPC Server handler 5 on 46179] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-11353261-1dab-441a-9379-6db2061939bf for DN 127.0.0.1:59283
2017-07-05 16:42:47,377 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid 0361538a-7735-4297-8331-db3e76456292) service to localhost/127.0.0.1:46179 trying to claim ACTIVE state with txid=1
2017-07-05 16:42:47,377 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid 0361538a-7735-4297-8331-db3e76456292) service to localhost/127.0.0.1:46179
2017-07-05 16:42:47,379 INFO  [IPC Server handler 6 on 46179] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-11353261-1dab-441a-9379-6db2061939bf,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:47,379 INFO  [IPC Server handler 6 on 46179] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-11353261-1dab-441a-9379-6db2061939bf node DatanodeRegistration(127.0.0.1, datanodeUuid=0361538a-7735-4297-8331-db3e76456292, infoPort=36315, ipcPort=34751, storageInfo=lv=-56;cid=testClusterID;nsid=1124059488;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 16:42:47,379 INFO  [IPC Server handler 6 on 46179] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-19c88e55-c469-459d-8b76-51d186b4d61f,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:47,379 INFO  [IPC Server handler 6 on 46179] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-19c88e55-c469-459d-8b76-51d186b4d61f node DatanodeRegistration(127.0.0.1, datanodeUuid=0361538a-7735-4297-8331-db3e76456292, infoPort=36315, ipcPort=34751, storageInfo=lv=-56;cid=testClusterID;nsid=1124059488;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 16:42:47,380 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@2ecf3582
2017-07-05 16:42:47,380 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:47,381 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 16:42:47,381 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:47,381 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 16:42:47,381 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 16:42:47,384 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:47,385 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-205437870-172.17.0.6-1499265766096 to blockPoolScannerMap, new size=1
2017-07-05 16:42:47,433 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:47,443 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:47,452 INFO  [IPC Server handler 9 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 16:42:47,456 INFO  [IPC Server handler 0 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:47,465 INFO  [IPC Server handler 1 on 46179] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-205437870-172.17.0.6-1499265766096 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]}
2017-07-05 16:42:47,469 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1383329452_1 at /127.0.0.1:46870 [Receiving block BP-205437870-172.17.0.6-1499265766096:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-205437870-172.17.0.6-1499265766096:blk_1073741825_1001 src: /127.0.0.1:46870 dest: /127.0.0.1:59283
2017-07-05 16:42:47,476 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:46870, dest: /127.0.0.1:59283, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1383329452_1, offset: 0, srvID: 0361538a-7735-4297-8331-db3e76456292, blockid: BP-205437870-172.17.0.6-1499265766096:blk_1073741825_1001, duration: 2252436
2017-07-05 16:42:47,477 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:47,478 INFO  [IPC Server handler 3 on 46179] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]} has not reached minimal replication 1
2017-07-05 16:42:47,478 INFO  [IPC Server handler 3 on 46179] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 16:42:47,478 INFO  [IPC Server handler 3 on 46179] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 16:42:47,480 INFO  [IPC Server handler 3 on 46179] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59283 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]} size 1
2017-07-05 16:42:47,882 INFO  [IPC Server handler 5 on 46179] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_1383329452_1
2017-07-05 16:42:47,886 INFO  [IPC Server handler 6 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:47,894 INFO  [IPC Server handler 7 on 46179] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-205437870-172.17.0.6-1499265766096 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-19c88e55-c469-459d-8b76-51d186b4d61f:NORMAL:127.0.0.1:59283|RBW]]}
2017-07-05 16:42:47,901 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1383329452_1 at /127.0.0.1:46872 [Receiving block BP-205437870-172.17.0.6-1499265766096:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-205437870-172.17.0.6-1499265766096:blk_1073741826_1002 src: /127.0.0.1:46872 dest: /127.0.0.1:59283
2017-07-05 16:42:47,949 INFO  [IPC Server handler 8 on 46179] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59283 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|FINALIZED]]} size 0
2017-07-05 16:42:47,955 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:46872, dest: /127.0.0.1:59283, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1383329452_1, offset: 0, srvID: 0361538a-7735-4297-8331-db3e76456292, blockid: BP-205437870-172.17.0.6-1499265766096:blk_1073741826_1002, duration: 9375390
2017-07-05 16:42:47,956 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:47,958 INFO  [IPC Server handler 9 on 46179] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_1383329452_1
2017-07-05 16:42:47,961 INFO  [IPC Server handler 0 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:47,966 INFO  [IPC Server handler 1 on 46179] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-205437870-172.17.0.6-1499265766096 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]}
2017-07-05 16:42:47,969 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1383329452_1 at /127.0.0.1:46874 [Receiving block BP-205437870-172.17.0.6-1499265766096:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-205437870-172.17.0.6-1499265766096:blk_1073741827_1003 src: /127.0.0.1:46874 dest: /127.0.0.1:59283
2017-07-05 16:42:47,980 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:46874, dest: /127.0.0.1:59283, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1383329452_1, offset: 0, srvID: 0361538a-7735-4297-8331-db3e76456292, blockid: BP-205437870-172.17.0.6-1499265766096:blk_1073741827_1003, duration: 5956956
2017-07-05 16:42:47,980 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:47,981 INFO  [IPC Server handler 2 on 46179] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59283 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-19c88e55-c469-459d-8b76-51d186b4d61f:NORMAL:127.0.0.1:59283|FINALIZED]]} size 0
2017-07-05 16:42:47,985 INFO  [IPC Server handler 4 on 46179] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_1383329452_1
2017-07-05 16:42:47,995 INFO  [IPC Server handler 3 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:48,005 INFO  [IPC Server handler 5 on 46179] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-205437870-172.17.0.6-1499265766096 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-19c88e55-c469-459d-8b76-51d186b4d61f:NORMAL:127.0.0.1:59283|RBW]]}
2017-07-05 16:42:48,013 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1383329452_1 at /127.0.0.1:46875 [Receiving block BP-205437870-172.17.0.6-1499265766096:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-205437870-172.17.0.6-1499265766096:blk_1073741828_1004 src: /127.0.0.1:46875 dest: /127.0.0.1:59283
2017-07-05 16:42:48,045 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:46875, dest: /127.0.0.1:59283, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1383329452_1, offset: 0, srvID: 0361538a-7735-4297-8331-db3e76456292, blockid: BP-205437870-172.17.0.6-1499265766096:blk_1073741828_1004, duration: 20042554
2017-07-05 16:42:48,046 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:48,049 INFO  [IPC Server handler 6 on 46179] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59283 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|FINALIZED]]} size 0
2017-07-05 16:42:48,050 INFO  [IPC Server handler 7 on 46179] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_1383329452_1
2017-07-05 16:42:48,061 INFO  [IPC Server handler 8 on 46179] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 16:42:48,061 INFO  [IPC Server handler 8 on 46179] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-05 16:42:48,061 INFO  [IPC Server handler 8 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 16:42:48,064 INFO  [IPC Server handler 9 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:48,072 INFO  [IPC Server handler 0 on 46179] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-205437870-172.17.0.6-1499265766096 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-19c88e55-c469-459d-8b76-51d186b4d61f:NORMAL:127.0.0.1:59283|RBW]]}
2017-07-05 16:42:48,077 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1383329452_1 at /127.0.0.1:46876 [Receiving block BP-205437870-172.17.0.6-1499265766096:blk_1073741829_1005]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-205437870-172.17.0.6-1499265766096:blk_1073741829_1005 src: /127.0.0.1:46876 dest: /127.0.0.1:59283
2017-07-05 16:42:48,114 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:46876, dest: /127.0.0.1:59283, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1383329452_1, offset: 0, srvID: 0361538a-7735-4297-8331-db3e76456292, blockid: BP-205437870-172.17.0.6-1499265766096:blk_1073741829_1005, duration: 33216794
2017-07-05 16:42:48,115 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:48,118 INFO  [IPC Server handler 1 on 46179] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59283 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-19c88e55-c469-459d-8b76-51d186b4d61f:NORMAL:127.0.0.1:59283|RBW]]} size 0
2017-07-05 16:42:48,125 INFO  [IPC Server handler 2 on 46179] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_1383329452_1
2017-07-05 16:42:48,129 INFO  [IPC Server handler 4 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:48,138 INFO  [IPC Server handler 3 on 46179] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-205437870-172.17.0.6-1499265766096 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]}
2017-07-05 16:42:48,149 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1383329452_1 at /127.0.0.1:46877 [Receiving block BP-205437870-172.17.0.6-1499265766096:blk_1073741830_1006]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-205437870-172.17.0.6-1499265766096:blk_1073741830_1006 src: /127.0.0.1:46877 dest: /127.0.0.1:59283
2017-07-05 16:42:48,187 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:46877, dest: /127.0.0.1:59283, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1383329452_1, offset: 0, srvID: 0361538a-7735-4297-8331-db3e76456292, blockid: BP-205437870-172.17.0.6-1499265766096:blk_1073741830_1006, duration: 36789802
2017-07-05 16:42:48,188 INFO  [PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-205437870-172.17.0.6-1499265766096:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:48,190 INFO  [IPC Server handler 6 on 46179] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741830_1006{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]} has not reached minimal replication 1
2017-07-05 16:42:48,191 INFO  [IPC Server handler 5 on 46179] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:59283 is added to blk_1073741830_1006{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-11353261-1dab-441a-9379-6db2061939bf:NORMAL:127.0.0.1:59283|RBW]]} size 1
2017-07-05 16:42:48,593 INFO  [IPC Server handler 7 on 46179] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_1383329452_1
2017-07-05 16:42:48,595 INFO  [IPC Server handler 8 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 16:42:48,596 INFO  [IPC Server handler 9 on 46179] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 16:42:48,598 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 16:42:48,598 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 16:42:48,599 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@7c891ba7] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 16:42:48,599 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 16:42:48,619 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:48,620 WARN  [1592601990@qtp-1022830989-0 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36315] http.HttpServer2 (HttpServer2.java:isRunning(460)) - HttpServer Acceptor: isRunning is false. Rechecking.
2017-07-05 16:42:48,620 WARN  [1592601990@qtp-1022830989-0 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36315] http.HttpServer2 (HttpServer2.java:isRunning(469)) - HttpServer Acceptor: isRunning is false
2017-07-05 16:42:48,620 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 16:42:48,621 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 34751
2017-07-05 16:42:48,629 INFO  [IPC Server listener on 34751] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 34751
2017-07-05 16:42:48,629 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:48,629 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid 0361538a-7735-4297-8331-db3e76456292) service to localhost/127.0.0.1:46179 interrupted
2017-07-05 16:42:48,630 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid 0361538a-7735-4297-8331-db3e76456292) service to localhost/127.0.0.1:46179
2017-07-05 16:42:48,630 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-205437870-172.17.0.6-1499265766096 (Datanode Uuid 0361538a-7735-4297-8331-db3e76456292)
2017-07-05 16:42:48,630 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-205437870-172.17.0.6-1499265766096 from blockPoolScannerMap
2017-07-05 16:42:48,630 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46179] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-205437870-172.17.0.6-1499265766096
2017-07-05 16:42:48,635 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@49102800] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 16:42:48,635 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 16:42:48,635 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 16:42:48,636 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 16:42:48,636 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 16:42:48,636 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 16:42:48,636 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:48,637 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@736309a9] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 16:42:48,637 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 16:42:48,637 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@4e93dcb9] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 16:42:48,639 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 33 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 1 Number of syncs: 24 SyncTimes(ms): 1 3 
2017-07-05 16:42:48,640 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000033
2017-07-05 16:42:48,640 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000033
2017-07-05 16:42:48,641 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 46179
2017-07-05 16:42:48,642 INFO  [CacheReplicationMonitor(505521407)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 16:42:48,643 INFO  [IPC Server listener on 46179] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 46179
2017-07-05 16:42:48,648 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:48,648 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@4726927c] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 16:42:48,648 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@571a9686] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 16:42:48,661 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:48,661 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 16:42:48,694 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:48,696 WARN  [942804532@qtp-501650218-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41827] http.HttpServer2 (HttpServer2.java:isRunning(460)) - HttpServer Acceptor: isRunning is false. Rechecking.
2017-07-05 16:42:48,704 WARN  [942804532@qtp-501650218-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41827] http.HttpServer2 (HttpServer2.java:isRunning(469)) - HttpServer Acceptor: isRunning is false
2017-07-05 16:42:48,795 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 16:42:48,796 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 16:42:48,796 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-05 16:42:48,833 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-05 16:42:48,838 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:48,838 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:48,839 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:48,839 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:48,840 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:48,840 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:48
2017-07-05 16:42:48,840 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:48,840 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:48,841 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:48,841 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:48,858 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:48,865 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:48,866 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:48,866 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:48,866 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:48,866 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:48,866 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:48,867 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:48,867 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:48,867 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:48,867 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:48,874 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:48,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:48,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:48,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:48,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:48,879 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:48,880 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:48,880 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:48,880 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:48,880 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:48,880 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:48,880 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:48,881 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:48,881 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:48,882 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:48,882 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:48,883 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:48,885 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:48,922 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-05 16:42:48,960 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-05 16:42:49,048 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-05 16:42:49,049 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-05 16:42:49,050 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-05 16:42:49,065 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-05 16:42:49,065 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-05 16:42:49,066 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-05 16:42:49,069 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-05 16:42:49,070 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-05 16:42:49,070 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:49,071 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-05 16:42:49,071 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:49,072 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-05 16:42:49,072 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:49,073 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 39296
2017-07-05 16:42:49,073 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:49,088 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_39296_hdfs____do9tub/webapp
2017-07-05 16:42:49,255 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39296
2017-07-05 16:42:49,256 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-05 16:42:49,256 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-05 16:42:49,257 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-05 16:42:49,257 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-05 16:42:49,257 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-05 16:42:49,258 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 05 16:42:49
2017-07-05 16:42:49,258 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-05 16:42:49,258 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:49,258 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-05 16:42:49,258 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-05 16:42:49,270 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-05 16:42:49,270 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-05 16:42:49,270 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-05 16:42:49,271 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-05 16:42:49,271 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-05 16:42:49,271 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-05 16:42:49,271 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-05 16:42:49,271 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-05 16:42:49,271 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-05 16:42:49,271 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-05 16:42:49,272 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-05 16:42:49,272 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-05 16:42:49,272 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-05 16:42:49,272 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-05 16:42:49,273 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-05 16:42:49,273 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:49,273 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-05 16:42:49,273 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-05 16:42:49,279 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-05 16:42:49,279 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-05 16:42:49,279 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:49,279 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-05 16:42:49,280 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-05 16:42:49,283 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-05 16:42:49,284 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-05 16:42:49,284 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-05 16:42:49,284 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-05 16:42:49,284 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-05 16:42:49,284 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-05 16:42:49,285 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:49,285 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-05 16:42:49,285 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-05 16:42:49,286 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-05 16:42:49,286 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-05 16:42:49,287 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-05 16:42:49,301 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:49,322 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:49,324 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current
2017-07-05 16:42:49,324 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current
2017-07-05 16:42:49,325 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-05 16:42:49,326 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-05 16:42:49,327 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-05 16:42:49,327 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-05 16:42:49,327 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-05 16:42:49,327 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-05 16:42:49,354 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-05 16:42:49,355 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 68 msecs
2017-07-05 16:42:49,356 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-05 16:42:49,356 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:49,358 INFO  [Socket Reader #1 for port 53752] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 53752
2017-07-05 16:42:49,363 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:53752 to access this namenode/service.
2017-07-05 16:42:49,363 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-05 16:42:49,376 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:49,377 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-05 16:42:49,377 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-05 16:42:49,378 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-05 16:42:49,378 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-05 16:42:49,378 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-05 16:42:49,395 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-05 16:42:49,396 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-05 16:42:49,396 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-05 16:42:49,396 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-05 16:42:49,396 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-05 16:42:49,396 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 18 msec
2017-07-05 16:42:49,396 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:49,397 INFO  [IPC Server listener on 53752] ipc.Server (Server.java:run(674)) - IPC Server listener on 53752: starting
2017-07-05 16:42:49,398 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:53752
2017-07-05 16:42:49,398 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-05 16:42:49,415 INFO  [CacheReplicationMonitor(1649934110)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-05 16:42:49,416 INFO  [CacheReplicationMonitor(1649934110)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 2427058263 milliseconds
2017-07-05 16:42:49,418 INFO  [CacheReplicationMonitor(1649934110)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-05 16:42:49,421 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2
2017-07-05 16:42:49,421 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-05 16:42:49,434 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-05 16:42:49,434 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-05 16:42:49,435 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-05 16:42:49,435 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:58214
2017-07-05 16:42:49,436 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-05 16:42:49,436 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-05 16:42:49,437 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-05 16:42:49,437 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-05 16:42:49,438 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-05 16:42:49,438 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-05 16:42:49,439 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-05 16:42:49,439 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 51184
2017-07-05 16:42:49,439 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-05 16:42:49,444 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_51184_datanode____.yacyiw/webapp
2017-07-05 16:42:49,552 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51184
2017-07-05 16:42:49,556 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-05 16:42:49,557 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-05 16:42:49,558 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-05 16:42:49,559 INFO  [Socket Reader #1 for port 59085] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 59085
2017-07-05 16:42:49,561 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:59085
2017-07-05 16:42:49,563 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-05 16:42:49,563 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-05 16:42:49,565 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:53752 starting to offer service
2017-07-05 16:42:49,565 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-05 16:42:49,575 INFO  [IPC Server listener on 59085] ipc.Server (Server.java:run(674)) - IPC Server listener on 59085: starting
2017-07-05 16:42:49,595 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-05 16:42:49,596 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:49,596 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:49,617 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:49,618 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:49,618 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:49,650 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 260@spirals-librepair
2017-07-05 16:42:49,650 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:49,651 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-05 16:42:49,702 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:49,704 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:49,728 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:49,729 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:49,729 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-574329914-172.17.0.6-1499265768885 is not formatted.
2017-07-05 16:42:49,729 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:49,729 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-574329914-172.17.0.6-1499265768885 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current/BP-574329914-172.17.0.6-1499265768885/current
2017-07-05 16:42:49,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-05 16:42:49,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-574329914-172.17.0.6-1499265768885 is not formatted.
2017-07-05 16:42:49,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-05 16:42:49,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-574329914-172.17.0.6-1499265768885 directory /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current/BP-574329914-172.17.0.6-1499265768885/current
2017-07-05 16:42:49,755 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:49,755 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-05 16:42:49,786 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=463693328;bpid=BP-574329914-172.17.0.6-1499265768885;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=463693328;c=0;bpid=BP-574329914-172.17.0.6-1499265768885;dnuuid=null
2017-07-05 16:42:49,806 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-05 16:42:49,806 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-05 16:42:49,820 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3
2017-07-05 16:42:49,823 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-05 16:42:49,823 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-05 16:42:49,826 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-05 16:42:49,826 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-05 16:42:49,827 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-05 16:42:49,827 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499286556827 with interval 21600000
2017-07-05 16:42:49,828 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:49,828 INFO  [Thread-396] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-574329914-172.17.0.6-1499265768885 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:49,830 INFO  [Thread-397] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-574329914-172.17.0.6-1499265768885 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:49,834 INFO  [Thread-396] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-574329914-172.17.0.6-1499265768885 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 5ms
2017-07-05 16:42:49,835 INFO  [Thread-397] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-574329914-172.17.0.6-1499265768885 on /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 4ms
2017-07-05 16:42:49,835 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-574329914-172.17.0.6-1499265768885: 7ms
2017-07-05 16:42:49,836 INFO  [Thread-400] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-574329914-172.17.0.6-1499265768885 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-05 16:42:49,836 INFO  [Thread-400] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-574329914-172.17.0.6-1499265768885 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-05 16:42:49,842 INFO  [Thread-401] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-574329914-172.17.0.6-1499265768885 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-05 16:42:49,842 INFO  [Thread-401] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-574329914-172.17.0.6-1499265768885 on volume /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-05 16:42:49,842 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 6ms
2017-07-05 16:42:49,843 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid null) service to localhost/127.0.0.1:53752 beginning handshake with NN
2017-07-05 16:42:49,845 INFO  [IPC Server handler 6 on 53752] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, infoPort=51184, ipcPort=59085, storageInfo=lv=-56;cid=testClusterID;nsid=463693328;c=0) storage 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3
2017-07-05 16:42:49,845 INFO  [IPC Server handler 6 on 53752] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:49,846 INFO  [IPC Server handler 6 on 53752] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:58214
2017-07-05 16:42:49,847 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid null) service to localhost/127.0.0.1:53752 successfully registered with NN
2017-07-05 16:42:49,847 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:53752 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-05 16:42:49,853 INFO  [IPC Server handler 0 on 53752] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-05 16:42:49,853 INFO  [IPC Server handler 0 on 53752] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14 for DN 127.0.0.1:58214
2017-07-05 16:42:49,853 INFO  [IPC Server handler 0 on 53752] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-9747a72e-2876-433a-ab8b-c9f81f63f2a8 for DN 127.0.0.1:58214
2017-07-05 16:42:49,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3) service to localhost/127.0.0.1:53752 trying to claim ACTIVE state with txid=1
2017-07-05 16:42:49,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3) service to localhost/127.0.0.1:53752
2017-07-05 16:42:49,855 INFO  [IPC Server handler 5 on 53752] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-9747a72e-2876-433a-ab8b-c9f81f63f2a8,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:49,856 INFO  [IPC Server handler 5 on 53752] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-9747a72e-2876-433a-ab8b-c9f81f63f2a8 node DatanodeRegistration(127.0.0.1, datanodeUuid=3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, infoPort=51184, ipcPort=59085, storageInfo=lv=-56;cid=testClusterID;nsid=463693328;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-05 16:42:49,856 INFO  [IPC Server handler 5 on 53752] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-05 16:42:49,856 INFO  [IPC Server handler 5 on 53752] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14 node DatanodeRegistration(127.0.0.1, datanodeUuid=3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, infoPort=51184, ipcPort=59085, storageInfo=lv=-56;cid=testClusterID;nsid=463693328;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-05 16:42:49,857 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@51eeb653
2017-07-05 16:42:49,858 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:49,859 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-05 16:42:49,859 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-05 16:42:49,859 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-05 16:42:49,859 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-05 16:42:49,866 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:49,867 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-574329914-172.17.0.6-1499265768885 to blockPoolScannerMap, new size=1
2017-07-05 16:42:49,909 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:49,918 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-05 16:42:49,931 INFO  [IPC Server handler 7 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-05 16:42:49,945 INFO  [IPC Server handler 3 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:49,949 INFO  [IPC Server handler 1 on 53752] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-574329914-172.17.0.6-1499265768885 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14:NORMAL:127.0.0.1:58214|RBW]]}
2017-07-05 16:42:49,953 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_748289503_1 at /127.0.0.1:51206 [Receiving block BP-574329914-172.17.0.6-1499265768885:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-574329914-172.17.0.6-1499265768885:blk_1073741825_1001 src: /127.0.0.1:51206 dest: /127.0.0.1:58214
2017-07-05 16:42:49,973 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:51206, dest: /127.0.0.1:58214, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_748289503_1, offset: 0, srvID: 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, blockid: BP-574329914-172.17.0.6-1499265768885:blk_1073741825_1001, duration: 3311487
2017-07-05 16:42:49,973 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:49,976 INFO  [IPC Server handler 9 on 53752] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58214 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14:NORMAL:127.0.0.1:58214|RBW]]} size 0
2017-07-05 16:42:49,980 INFO  [IPC Server handler 6 on 53752] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_748289503_1
2017-07-05 16:42:49,984 INFO  [IPC Server handler 0 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:49,990 INFO  [IPC Server handler 5 on 53752] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-574329914-172.17.0.6-1499265768885 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14:NORMAL:127.0.0.1:58214|RBW]]}
2017-07-05 16:42:49,994 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_748289503_1 at /127.0.0.1:51207 [Receiving block BP-574329914-172.17.0.6-1499265768885:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-574329914-172.17.0.6-1499265768885:blk_1073741826_1002 src: /127.0.0.1:51207 dest: /127.0.0.1:58214
2017-07-05 16:42:50,004 INFO  [IPC Server handler 2 on 53752] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58214 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9747a72e-2876-433a-ab8b-c9f81f63f2a8:NORMAL:127.0.0.1:58214|FINALIZED]]} size 0
2017-07-05 16:42:50,005 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:51207, dest: /127.0.0.1:58214, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_748289503_1, offset: 0, srvID: 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, blockid: BP-574329914-172.17.0.6-1499265768885:blk_1073741826_1002, duration: 2847446
2017-07-05 16:42:50,005 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:50,008 INFO  [IPC Server handler 8 on 53752] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_748289503_1
2017-07-05 16:42:50,011 INFO  [IPC Server handler 7 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:50,015 INFO  [IPC Server handler 3 on 53752] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-574329914-172.17.0.6-1499265768885 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14:NORMAL:127.0.0.1:58214|RBW]]}
2017-07-05 16:42:50,021 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_748289503_1 at /127.0.0.1:51208 [Receiving block BP-574329914-172.17.0.6-1499265768885:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-574329914-172.17.0.6-1499265768885:blk_1073741827_1003 src: /127.0.0.1:51208 dest: /127.0.0.1:58214
2017-07-05 16:42:50,033 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:51208, dest: /127.0.0.1:58214, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_748289503_1, offset: 0, srvID: 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, blockid: BP-574329914-172.17.0.6-1499265768885:blk_1073741827_1003, duration: 2831277
2017-07-05 16:42:50,033 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:50,037 INFO  [IPC Server handler 1 on 53752] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58214 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14:NORMAL:127.0.0.1:58214|RBW]]} size 0
2017-07-05 16:42:50,040 INFO  [IPC Server handler 4 on 53752] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_748289503_1
2017-07-05 16:42:50,043 INFO  [IPC Server handler 9 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-05 16:42:50,051 INFO  [IPC Server handler 6 on 53752] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-574329914-172.17.0.6-1499265768885 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-32db96c1-6f50-4dc2-8f8f-f6df7388bb14:NORMAL:127.0.0.1:58214|RBW]]}
2017-07-05 16:42:50,055 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_748289503_1 at /127.0.0.1:51209 [Receiving block BP-574329914-172.17.0.6-1499265768885:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-574329914-172.17.0.6-1499265768885:blk_1073741828_1004 src: /127.0.0.1:51209 dest: /127.0.0.1:58214
2017-07-05 16:42:50,090 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:51209, dest: /127.0.0.1:58214, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_748289503_1, offset: 0, srvID: 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3, blockid: BP-574329914-172.17.0.6-1499265768885:blk_1073741828_1004, duration: 12452700
2017-07-05 16:42:50,090 INFO  [PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-574329914-172.17.0.6-1499265768885:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-05 16:42:50,091 INFO  [IPC Server handler 0 on 53752] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58214 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9747a72e-2876-433a-ab8b-c9f81f63f2a8:NORMAL:127.0.0.1:58214|FINALIZED]]} size 0
2017-07-05 16:42:50,093 INFO  [IPC Server handler 5 on 53752] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_748289503_1
2017-07-05 16:42:50,095 INFO  [IPC Server handler 2 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 16:42:50,097 INFO  [IPC Server handler 8 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-05 16:42:50,113 INFO  [IPC Server handler 7 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-05 16:42:50,151 INFO  [IPC Server handler 3 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/2.txt	dst=null	perm=null	proto=rpc
2017-07-05 16:42:50,164 INFO  [IPC Server handler 1 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/3.txt	dst=null	perm=null	proto=rpc
2017-07-05 16:42:50,169 INFO  [IPC Server handler 4 on 53752] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/4.txt	dst=null	perm=null	proto=rpc
2017-07-05 16:42:50,176 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-05 16:42:50,176 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-05 16:42:50,177 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-05 16:42:50,177 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2123064f] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-05 16:42:50,196 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:50,197 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 1
2017-07-05 16:42:50,199 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-05 16:42:50,199 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 59085
2017-07-05 16:42:50,200 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:50,201 INFO  [IPC Server listener on 59085] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 59085
2017-07-05 16:42:50,205 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3) service to localhost/127.0.0.1:53752 interrupted
2017-07-05 16:42:50,205 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3) service to localhost/127.0.0.1:53752
2017-07-05 16:42:50,205 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-574329914-172.17.0.6-1499265768885 (Datanode Uuid 3b472ea6-d4c2-47ea-9bae-8cd1b759dbc3)
2017-07-05 16:42:50,206 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-574329914-172.17.0.6-1499265768885 from blockPoolScannerMap
2017-07-05 16:42:50,206 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:53752] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-574329914-172.17.0.6-1499265768885
2017-07-05 16:42:50,213 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-05 16:42:50,213 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-05 16:42:50,214 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-05 16:42:50,214 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@30aa8b0d] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-05 16:42:50,214 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-05 16:42:50,214 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-05 16:42:50,215 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:50,216 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-05 16:42:50,217 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@5a237731] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-05 16:42:50,217 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@6f1a80fb] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-05 16:42:50,217 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 1 2 
2017-07-05 16:42:50,218 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-05 16:42:50,219 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/250285295/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-05 16:42:50,219 INFO  [CacheReplicationMonitor(1649934110)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-05 16:42:50,220 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 53752
2017-07-05 16:42:50,221 INFO  [IPC Server listener on 53752] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 53752
2017-07-05 16:42:50,222 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-05 16:42:50,222 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@64fe9da7] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-05 16:42:50,222 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@38792286] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-05 16:42:50,242 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-05 16:42:50,243 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-05 16:42:50,298 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-05 16:42:50,401 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-05 16:42:50,403 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-05 16:42:50,403 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.971 sec - in org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
Running org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec - in org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Running org.apache.samoa.streams.kafka.KafkaUtilsTest
2017-07-05 16:42:50,559 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-05 16:42:50,559 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:host.name=spirals-librepair
2017-07-05 16:42:50,559 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.version=1.8.0_121
2017-07-05 16:42:50,560 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.vendor=Oracle Corporation
2017-07-05 16:42:50,560 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-05 16:42:50,560 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.class.path=/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/250285295/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.io.tmpdir=/tmp
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.compiler=<NA>
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.name=Linux
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.arch=amd64
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.version=3.16.0-4-amd64
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.name=root
2017-07-05 16:42:50,561 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.home=/root
2017-07-05 16:42:50,562 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.dir=/root/workspace/apache/incubator-samoa/250285295/samoa-api
2017-07-05 16:42:50,584 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-1319050681618040297/version-2 snapdir /tmp/kafka-1243386590009652272/version-2
2017-07-05 16:42:50,592 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 16:42:50,655 INFO  [ZkClient-EventThread-482-127.0.0.1:42276] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 16:42:50,661 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-05 16:42:50,661 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:host.name=spirals-librepair
2017-07-05 16:42:50,662 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.version=1.8.0_121
2017-07-05 16:42:50,662 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.vendor=Oracle Corporation
2017-07-05 16:42:50,662 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-05 16:42:50,662 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.class.path=/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/250285295/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/250285295/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/250285295/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.io.tmpdir=/tmp
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.compiler=<NA>
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.name=Linux
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.arch=amd64
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.version=3.16.0-4-amd64
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.name=root
2017-07-05 16:42:50,663 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.home=/root
2017-07-05 16:42:50,664 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.dir=/root/workspace/apache/incubator-samoa/250285295/samoa-api
2017-07-05 16:42:50,665 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:42276 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@5793b87
2017-07-05 16:42:50,701 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 16:42:50,706 INFO  [main-SendThread(127.0.0.1:42276)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:42276. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 16:42:50,706 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:48939
2017-07-05 16:42:50,710 INFO  [main-SendThread(127.0.0.1:42276)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:42276, initiating session
2017-07-05 16:42:50,721 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:48939
2017-07-05 16:42:50,727 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 16:42:50,761 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d1334246b0000 with negotiated timeout 10000 for client /127.0.0.1:48939
2017-07-05 16:42:50,764 INFO  [main-SendThread(127.0.0.1:42276)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:42276, sessionid = 0x15d1334246b0000, negotiated timeout = 10000
2017-07-05 16:42:50,767 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 16:42:51,138 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafkaUtils-4567311385325562484
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:42276
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 16:42:51,255 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 16:42:51,258 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:42276
2017-07-05 16:42:51,262 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:42276 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5cbd94b2
2017-07-05 16:42:51,407 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 16:42:51,407 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 16:42:51,408 INFO  [main-SendThread(127.0.0.1:42276)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:42276. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 16:42:51,413 INFO  [main-SendThread(127.0.0.1:42276)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:42276, initiating session
2017-07-05 16:42:51,414 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:48942
2017-07-05 16:42:51,414 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:48942
2017-07-05 16:42:51,416 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d1334246b0001 with negotiated timeout 6000 for client /127.0.0.1:48942
2017-07-05 16:42:51,416 INFO  [main-SendThread(127.0.0.1:42276)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:42276, sessionid = 0x15d1334246b0001, negotiated timeout = 6000
2017-07-05 16:42:51,417 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 16:42:51,446 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 16:42:51,459 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 16:42:51,471 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 16:42:51,535 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x1b zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 16:42:51,543 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = pE1sLmG-Q9-3n6ltIsAWrg
2017-07-05 16:42:51,552 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-4567311385325562484/meta.properties
2017-07-05 16:42:51,587 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 16:42:51,592 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 16:42:51,646 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 16:42:51,657 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 16:42:51,707 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 16:42:51,712 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 16:42:51,715 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 16:42:51,721 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 16:42:51,778 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 16:42:51,787 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 16:42:51,816 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:42:51,818 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:42:51,859 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 16:42:51,872 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 16:42:51,880 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 16:42:51,881 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 16:42:51,882 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 16:42:51,891 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:setData cxid:0x25 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 16:42:51,896 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 16:42:51,928 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 16:42:51,929 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 16:42:51,930 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 16:42:51,936 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 16:42:51,937 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 16:42:51,939 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 16:42:51,944 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 16:42:51,945 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 16:42:51,950 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 16:42:51,950 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 16:42:51,951 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 16:42:51,987 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 16:42:51,991 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 16:42:51,992 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 16:42:51,994 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 16:42:51,996 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 16:42:52,000 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:delete cxid:0x36 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 16:42:52,003 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 16:42:52,004 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 16:42:52,013 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:42:52,018 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:42:52,034 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:42:52,038 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 16:42:52,040 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 16:42:52,056 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 15 milliseconds.
2017-07-05 16:42:52,085 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 16:42:52,125 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 16:42:52,127 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 16:42:52,128 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 16:42:52,132 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 16:42:52,134 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 16:42:52,135 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-4567311385325562484/meta.properties
2017-07-05 16:42:52,179 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:52,179 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:52,180 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 16:42:52,216 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 16:42:52,233 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 16:42:52,347 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 16:42:52,354 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-r Error:KeeperErrorCode = NoNode for /config/topics/test-r
2017-07-05 16:42:52,365 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:42:52,374 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:42:52,433 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0000 type:setData cxid:0xb zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 16:42:52,435 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0000 type:create cxid:0xc zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:42:52,439 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:42:52,457 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 16:42:52,456 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 16:42:52,492 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 16:42:52,524 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:52,529 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:52,561 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-r, test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0), [test-r,0] -> List(0))]
2017-07-05 16:42:52,562 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0],[test-r,0]
2017-07-05 16:42:52,578 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0],[test-r,0]
2017-07-05 16:42:52,581 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0],[test-r,0]
2017-07-05 16:42:52,600 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=test-r,Partition=0,Replica=0]
2017-07-05 16:42:52,617 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0],[test-r,0]
2017-07-05 16:42:52,623 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x4f zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 16:42:52,625 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x50 zxid:0x25 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 16:42:52,682 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x54 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions/0
2017-07-05 16:42:52,693 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x55 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions
2017-07-05 16:42:52,764 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:52,764 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:52,857 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=test-r,Partition=0,Replica=0]
2017-07-05 16:42:52,979 INFO  [kafka-request-handler-2] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-r-0,test-s-0
2017-07-05 16:42:53,038 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-05 16:42:53,068 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log test-r-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,084 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-r,0] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,085 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [test-r,0] on broker 0: No checkpointed highwatermark is found for partition test-r-0
2017-07-05 16:42:53,093 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:setData cxid:0x61 zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 16:42:53,109 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x62 zxid:0x2f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:42:53,127 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 16:42:53,134 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 16:42:53,141 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,143 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,150 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 16:42:53,198 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 4 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-05 16:42:53,250 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 16:42:53,253 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:42:53,265 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:42:53,267 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:42:53,273 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 16:42:53,358 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:42:53,359 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xa5 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 16:42:53,364 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xa6 zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 16:42:53,379 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xaa zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 16:42:53,384 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xad zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 16:42:53,390 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xb0 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 16:42:53,395 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xb3 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 16:42:53,400 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xb6 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 16:42:53,411 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xb9 zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 16:42:53,430 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xbe zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 16:42:53,455 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xc2 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 16:42:53,462 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xc5 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 16:42:53,470 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xc8 zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 16:42:53,475 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xcb zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 16:42:53,480 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xce zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 16:42:53,508 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xd1 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 16:42:53,518 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xd5 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 16:42:53,538 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xda zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 16:42:53,546 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xdd zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 16:42:53,561 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xe0 zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 16:42:53,578 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xe3 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 16:42:53,589 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xe6 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 16:42:53,594 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xe9 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 16:42:53,600 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xec zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 16:42:53,605 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xef zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 16:42:53,610 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xf2 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 16:42:53,615 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xf5 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 16:42:53,620 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xfa zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 16:42:53,626 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0xfe zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 16:42:53,633 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x101 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 16:42:53,637 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x104 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 16:42:53,655 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x107 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 16:42:53,660 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x10a zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 16:42:53,665 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x10d zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 16:42:53,669 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x110 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 16:42:53,673 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x113 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 16:42:53,677 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x116 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 16:42:53,684 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x119 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 16:42:53,691 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x11c zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 16:42:53,695 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x11f zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 16:42:53,700 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x122 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 16:42:53,737 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x127 zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 16:42:53,755 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x12b zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 16:42:53,761 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x12e zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 16:42:53,765 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x131 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 16:42:53,778 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x134 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 16:42:53,782 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x137 zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 16:42:53,787 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x13a zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 16:42:53,791 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x13d zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 16:42:53,795 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x140 zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 16:42:53,799 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x143 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 16:42:53,812 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334246b0001 type:create cxid:0x146 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 16:42:53,827 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 16:42:53,891 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 16:42:53,898 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,900 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,907 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 16:42:53,917 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,919 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,920 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 16:42:53,933 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,939 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,940 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 16:42:53,945 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,948 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,948 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 16:42:53,957 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,959 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,960 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 16:42:53,975 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,976 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,981 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 16:42:53,988 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,990 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,991 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 16:42:53,996 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:53,998 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:53,999 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 16:42:54,004 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,007 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,008 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 16:42:54,014 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,017 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,017 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 16:42:54,032 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,034 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,034 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 16:42:54,041 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,044 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,045 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 16:42:54,050 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,052 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,052 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 16:42:54,057 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,065 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,066 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 16:42:54,071 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,073 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,074 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 16:42:54,079 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,081 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,081 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 16:42:54,086 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,088 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,089 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 16:42:54,094 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,096 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,096 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 16:42:54,101 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,103 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,103 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 16:42:54,108 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,110 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,111 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 16:42:54,115 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,117 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,118 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 16:42:54,122 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,124 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,125 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 16:42:54,131 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,133 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,133 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 16:42:54,141 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,142 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,143 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 16:42:54,151 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,153 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,154 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 16:42:54,158 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,160 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,160 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 16:42:54,165 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,167 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,168 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 16:42:54,174 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,175 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,176 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 16:42:54,181 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,183 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,183 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 16:42:54,189 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,190 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,191 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 16:42:54,196 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,198 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,198 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 16:42:54,204 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,206 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,207 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 16:42:54,212 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,213 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,214 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 16:42:54,220 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,221 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,222 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 16:42:54,237 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,239 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,240 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 16:42:54,245 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,247 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,247 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 16:42:54,252 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,254 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,255 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 16:42:54,263 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,265 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,265 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 16:42:54,315 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,316 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,317 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 16:42:54,332 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,360 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,362 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 16:42:54,372 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,374 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,377 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 16:42:54,384 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,386 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,386 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 16:42:54,404 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,415 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,416 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 16:42:54,427 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,428 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,429 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 16:42:54,439 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,441 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,441 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 16:42:54,458 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,461 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,465 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 16:42:54,474 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,476 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,477 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 16:42:54,482 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,486 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,487 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 16:42:54,492 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,494 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,495 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 16:42:54,505 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:42:54,507 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafkaUtils-4567311385325562484 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:42:54,507 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 16:42:54,516 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 16:42:54,544 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 25 milliseconds.
2017-07-05 16:42:54,551 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 16:42:54,553 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 16:42:54,553 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:42:54,553 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 16:42:54,554 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 16:42:54,554 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 16:42:54,556 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 0 milliseconds.
2017-07-05 16:42:54,556 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 16:42:54,557 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 16:42:54,557 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 16:42:54,558 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 16:42:54,558 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 16:42:54,559 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:42:54,559 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 16:42:54,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 2 milliseconds.
2017-07-05 16:42:54,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 16:42:54,565 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 3 milliseconds.
2017-07-05 16:42:54,565 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 16:42:54,567 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 2 milliseconds.
2017-07-05 16:42:54,567 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 16:42:54,569 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 16:42:54,569 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 16:42:54,572 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 3 milliseconds.
2017-07-05 16:42:54,572 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 16:42:54,574 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 2 milliseconds.
2017-07-05 16:42:54,574 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 16:42:54,575 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 1 milliseconds.
2017-07-05 16:42:54,576 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 16:42:54,577 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-05 16:42:54,577 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 16:42:54,579 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:42:54,579 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 2 milliseconds.
2017-07-05 16:42:54,579 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 16:42:54,582 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 3 milliseconds.
2017-07-05 16:42:54,582 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 16:42:54,585 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 3 milliseconds.
2017-07-05 16:42:54,585 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 16:42:54,588 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 3 milliseconds.
2017-07-05 16:42:54,588 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 16:42:54,590 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 2 milliseconds.
2017-07-05 16:42:54,590 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 16:42:54,591 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-05 16:42:54,591 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 16:42:54,592 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-05 16:42:54,592 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 16:42:54,594 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 2 milliseconds.
2017-07-05 16:42:54,594 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 16:42:54,595 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 16:42:54,595 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 16:42:54,596 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 16:42:54,597 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 16:42:54,598 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 16:42:54,598 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 16:42:54,599 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 16:42:54,599 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 16:42:54,600 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 0 milliseconds.
2017-07-05 16:42:54,601 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 16:42:54,602 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 16:42:54,602 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 16:42:54,603 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-05 16:42:54,603 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 16:42:54,604 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 16:42:54,604 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 16:42:54,606 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 0 milliseconds.
2017-07-05 16:42:54,606 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 16:42:54,607 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 16:42:54,607 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 16:42:55,056 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 448 milliseconds.
2017-07-05 16:42:55,056 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 16:42:55,058 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 16:42:55,058 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 16:42:55,059 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:55,059 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 16:42:55,059 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 16:42:55,059 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:55,061 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-05 16:42:55,061 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 16:42:55,062 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 16:42:55,062 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 16:42:55,063 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-05 16:42:55,064 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 16:42:55,065 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-05 16:42:55,065 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 16:42:55,066 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-05 16:42:55,068 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:42:55,070 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:42:55,071 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 16:42:55,072 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 1 milliseconds.
2017-07-05 16:42:55,072 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 16:42:55,066 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:55,088 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:55,093 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 21 milliseconds.
2017-07-05 16:42:55,093 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 16:42:55,095 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 2 milliseconds.
2017-07-05 16:42:55,095 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 16:42:55,096 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 1 milliseconds.
2017-07-05 16:42:55,097 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 16:42:55,098 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:42:55,098 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 0 milliseconds.
2017-07-05 16:42:55,099 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 16:42:55,101 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 0 milliseconds.
2017-07-05 16:42:55,102 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 16:42:55,103 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-05 16:42:55,103 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 16:42:55,104 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 1 milliseconds.
2017-07-05 16:42:55,104 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 16:42:55,106 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 2 milliseconds.
2017-07-05 16:42:55,106 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 16:42:55,111 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 5 milliseconds.
2017-07-05 16:42:55,105 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:42:55,121 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 16:42:55,125 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:42:55,143 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 16:42:55,155 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 16:42:55,180 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 16:42:55,201 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:42:55,201 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:42:55,229 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:42:55,229 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-05 16:42:55,236 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 2
2017-07-05 16:42:55,239 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 2
2017-07-05 16:42:55,275 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 2
2017-07-05 16:42:55,275 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [] for group test
2017-07-05 16:42:55,289 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 2
2017-07-05 16:42:55,292 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 16:42:55,352 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 2
2017-07-05 16:42:56,062 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:56,070 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:56,073 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:56,073 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:56,080 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = sendM-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 16:42:56,095 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:42:56,096 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 16:42:56,096 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:42:56,104 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 16:42:56,105 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:56,106 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:56,401 INFO  [main] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-05 16:42:56,625 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 3
2017-07-05 16:42:56,627 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 3
2017-07-05 16:42:56,629 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 3
2017-07-05 16:42:56,630 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-05 16:42:57,942 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = rcv-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 16:42:57,946 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 16:42:57,946 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:57,946 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:57,947 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:57,947 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-4
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:42:57,950 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:42:57,950 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:42:58,213 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 3
2017-07-05 16:42:58,214 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 4 is now empty
2017-07-05 16:43:01,153 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:01,154 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 16:43:01,155 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:01,158 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 4
2017-07-05 16:43:01,158 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 5
2017-07-05 16:43:01,160 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 5
2017-07-05 16:43:01,162 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 5
2017-07-05 16:43:01,170 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-s-0] for group test
2017-07-05 16:43:01,198 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 5
2017-07-05 16:43:01,198 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 6 is now empty
2017-07-05 16:43:01,228 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 16:43:01,230 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 16:43:01,243 INFO  [kafka-request-handler-2] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 16:43:01,260 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 16:43:01,262 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 16:43:01,273 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 16:43:01,274 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 16:43:01,277 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 16:43:01,281 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 16:43:01,594 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 16:43:01,594 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 16:43:01,594 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 16:43:01,600 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 16:43:01,600 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 16:43:01,601 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 16:43:01,605 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 16:43:01,606 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 16:43:01,608 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 16:43:01,608 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:01,673 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:01,673 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:01,674 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:01,684 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:01,684 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:01,709 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 16:43:01,710 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:01,888 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:01,888 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:01,892 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 16:43:01,893 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:02,085 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:02,085 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:02,086 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:02,284 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:02,284 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:02,286 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 16:43:02,288 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 16:43:02,293 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 16:43:02,294 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 16:43:02,294 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 16:43:02,295 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 16:43:02,571 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 16:43:02,579 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 16:43:02,581 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 16:43:02,602 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 16:43:02,602 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 16:43:02,602 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 16:43:02,612 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 16:43:02,614 INFO  [ZkClient-EventThread-486-127.0.0.1:42276] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 16:43:02,620 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d1334246b0001
2017-07-05 16:43:02,625 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d1334246b0001 closed
2017-07-05 16:43:02,628 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 16:43:02,634 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 16:43:02,635 INFO  [ZkClient-EventThread-482-127.0.0.1:42276] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 16:43:02,646 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:48942 which had sessionid 0x15d1334246b0001
2017-07-05 16:43:02,654 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d1334246b0000
2017-07-05 16:43:02,657 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d1334246b0000 closed
2017-07-05 16:43:02,660 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 16:43:02,660 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 16:43:02,673 WARN  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:doIO(357)) - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x15d1334246b0000, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
	at java.lang.Thread.run(Thread.java:745)
2017-07-05 16:43:02,673 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 16:43:02,685 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:48939 which had sessionid 0x15d1334246b0000
2017-07-05 16:43:02,685 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 16:43:02,686 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 16:43:02,692 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 16:43:02,693 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 16:43:02,694 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 16:43:02,706 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 16:43:02,710 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 16:43:02,710 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 16:43:02,710 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 16:43:02,713 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 16:43:02,713 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.291 sec - in org.apache.samoa.streams.kafka.KafkaUtilsTest
Running org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
2017-07-05 16:43:02,726 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-7752439346912346878/version-2 snapdir /tmp/kafka-8063359241660545945/version-2
2017-07-05 16:43:02,727 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 16:43:02,793 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:36000 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@3d2f3dcb
2017-07-05 16:43:02,794 INFO  [ZkClient-EventThread-542-127.0.0.1:36000] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 16:43:02,797 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 16:43:02,812 INFO  [main-SendThread(127.0.0.1:36000)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:36000. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 16:43:02,813 INFO  [main-SendThread(127.0.0.1:36000)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:36000, initiating session
2017-07-05 16:43:02,815 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:54657
2017-07-05 16:43:02,815 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:54657
2017-07-05 16:43:02,817 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 16:43:02,827 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d133453d10000 with negotiated timeout 10000 for client /127.0.0.1:54657
2017-07-05 16:43:02,828 INFO  [main-SendThread(127.0.0.1:36000)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:36000, sessionid = 0x15d133453d10000, negotiated timeout = 10000
2017-07-05 16:43:02,836 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 16:43:02,838 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-6023055269381228249
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:36000
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 16:43:02,841 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 16:43:02,841 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:36000
2017-07-05 16:43:02,842 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:36000 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@59845579
2017-07-05 16:43:02,843 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 16:43:02,859 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 16:43:02,859 INFO  [main-SendThread(127.0.0.1:36000)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:36000. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 16:43:02,860 INFO  [main-SendThread(127.0.0.1:36000)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:36000, initiating session
2017-07-05 16:43:02,860 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:54658
2017-07-05 16:43:02,861 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:54658
2017-07-05 16:43:02,864 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d133453d10001 with negotiated timeout 6000 for client /127.0.0.1:54658
2017-07-05 16:43:02,864 INFO  [main-SendThread(127.0.0.1:36000)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:36000, sessionid = 0x15d133453d10001, negotiated timeout = 6000
2017-07-05 16:43:02,865 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 16:43:02,886 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 16:43:02,895 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 16:43:02,910 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 16:43:02,924 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 16:43:02,934 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = 76emd_m3SJej5aCcWx0VWA
2017-07-05 16:43:02,935 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-6023055269381228249/meta.properties
2017-07-05 16:43:02,952 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 16:43:02,971 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 16:43:02,952 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 16:43:02,968 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 16:43:03,001 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-05 16:43:03,013 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 16:43:03,029 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 16:43:03,031 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 16:43:03,040 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 16:43:03,057 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 16:43:03,061 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 16:43:03,078 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:03,091 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:03,091 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 16:43:03,095 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 16:43:03,098 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 16:43:03,098 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 16:43:03,098 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 16:43:03,100 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 16:43:03,104 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 16:43:03,116 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 16:43:03,116 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 16:43:03,117 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 16:43:03,119 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 16:43:03,120 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 16:43:03,120 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 16:43:03,121 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 16:43:03,122 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 16:43:03,122 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 16:43:03,123 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 16:43:03,123 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 16:43:03,124 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 16:43:03,124 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 16:43:03,125 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 16:43:03,125 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 16:43:03,125 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 16:43:03,126 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 16:43:03,129 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 16:43:03,130 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 16:43:03,132 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 16:43:03,134 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:03,136 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:03,137 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:03,138 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 16:43:03,138 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 16:43:03,138 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-05 16:43:03,140 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 16:43:03,143 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 54 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-05 16:43:03,149 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 16:43:03,150 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 16:43:03,150 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 16:43:03,153 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 16:43:03,153 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 16:43:03,154 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 16:43:03,154 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-6023055269381228249/meta.properties
2017-07-05 16:43:03,158 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 16:43:03,160 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 16:43:03,161 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 16:43:03,162 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 16:43:03,226 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:43:03,226 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:43:03,227 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 16:43:03,238 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10000 type:setData cxid:0x5 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-05 16:43:03,263 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10000 type:create cxid:0x6 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:03,270 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:43:03,271 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:setData cxid:0x4a zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 16:43:03,275 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x4c zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:03,282 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:43:03,282 INFO  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 16:43:03,286 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-5
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:43:03,293 INFO  [kafka-request-handler-3] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:43:03,294 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:43:03,294 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:43:03,295 WARN  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 16:43:03,295 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:43:03,295 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:43:03,295 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos)], deleted topics: [Set()], new partition replica assignment [Map([samoa_test-oos,0] -> List(0))]
2017-07-05 16:43:03,295 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [samoa_test-oos,0]
2017-07-05 16:43:03,301 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [samoa_test-oos,0]
2017-07-05 16:43:03,301 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [samoa_test-oos,0]
2017-07-05 16:43:03,301 INFO  [kafka-request-handler-3] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-05 16:43:03,302 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 16:43:03,302 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 55 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:03,303 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [samoa_test-oos,0]
2017-07-05 16:43:03,304 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x55 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-05 16:43:03,307 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x56 zxid:0x25 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-05 16:43:03,314 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:03,318 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 16:43:03,320 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:setData cxid:0x60 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 16:43:03,321 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0
2017-07-05 16:43:03,322 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x63 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:03,324 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:03,325 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-05 16:43:03,325 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-05 16:43:03,325 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:03,326 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-05 16:43:03,375 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-05 16:43:03,375 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-05 16:43:03,375 INFO  [kafka-request-handler-6] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 16:43:03,376 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 16:43:03,382 INFO  [kafka-request-handler-6] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 16:43:03,382 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-05 16:43:03,383 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x69 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 16:43:03,385 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x6a zxid:0x2e txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 16:43:03,392 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-05 16:43:03,393 INFO  [kafka-request-handler-5] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-05 16:43:03,421 INFO  [kafka-request-handler-5] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:03,424 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 16:43:03,424 INFO  [kafka-request-handler-5] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:03,425 INFO  [kafka-request-handler-5] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 16:43:03,428 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:03,432 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:03,434 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:03,440 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 16:43:03,472 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:03,473 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xa8 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 16:43:03,497 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xa9 zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 16:43:03,509 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xad zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 16:43:03,512 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xb0 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 16:43:03,519 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xb3 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 16:43:03,525 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xb6 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 16:43:03,531 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xb9 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 16:43:03,537 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xbe zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 16:43:03,540 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xc1 zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 16:43:03,583 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xc5 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 16:43:03,591 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xc8 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 16:43:03,595 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xcb zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 16:43:03,600 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xce zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 16:43:03,606 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xd1 zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 16:43:03,612 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xd4 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 16:43:03,616 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xd7 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 16:43:03,620 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xda zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 16:43:03,624 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xdd zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 16:43:03,629 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xe0 zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 16:43:03,686 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xe6 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 16:43:03,694 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xe9 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 16:43:03,710 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xec zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 16:43:03,714 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xef zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 16:43:03,719 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xf2 zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 16:43:03,723 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xf5 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 16:43:03,728 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xf8 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 16:43:03,733 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xfb zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 16:43:03,737 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0xfe zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 16:43:03,742 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x102 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 16:43:03,785 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x106 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 16:43:03,790 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x109 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 16:43:03,794 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x10c zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 16:43:03,799 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x110 zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 16:43:03,803 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x113 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 16:43:03,807 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x116 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 16:43:03,812 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x119 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 16:43:03,825 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x11c zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 16:43:03,831 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x11f zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 16:43:03,837 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x122 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 16:43:03,842 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x125 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 16:43:03,881 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x12b zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 16:43:03,885 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x12e zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 16:43:03,889 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x131 zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 16:43:03,905 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x134 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 16:43:03,922 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x137 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 16:43:03,928 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x13a zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 16:43:03,937 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x13d zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 16:43:03,943 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x140 zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 16:43:03,947 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x143 zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 16:43:03,952 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x146 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 16:43:03,998 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d133453d10001 type:create cxid:0x14b zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 16:43:04,127 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 16:43:04,181 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 16:43:04,186 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,187 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,189 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 16:43:04,193 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,196 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,198 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 16:43:04,203 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,206 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,207 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 16:43:04,216 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,217 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,218 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 16:43:04,224 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,226 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,227 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 16:43:04,240 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,241 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,242 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 16:43:04,269 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,278 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,280 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 16:43:04,291 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,295 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,296 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 16:43:04,305 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,315 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,316 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 16:43:04,326 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,334 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,335 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 16:43:04,339 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,350 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,350 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 16:43:04,359 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,360 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,361 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 16:43:04,374 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,375 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,376 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 16:43:04,387 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,395 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,396 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 16:43:04,418 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,419 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,420 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 16:43:04,434 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,435 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,436 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 16:43:04,454 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,456 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,457 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 16:43:04,467 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,469 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,470 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 16:43:04,484 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,490 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,491 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 16:43:04,514 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,515 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,515 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 16:43:04,530 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,531 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,531 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 16:43:04,556 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,562 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,563 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 16:43:04,575 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,588 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,589 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 16:43:04,593 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,606 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,607 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 16:43:04,613 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,621 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,621 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 16:43:04,635 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,636 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,637 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 16:43:04,643 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,651 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,652 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 16:43:04,667 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,678 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,685 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 16:43:04,693 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,699 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,699 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 16:43:04,708 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,721 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,722 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 16:43:04,733 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,737 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,745 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 16:43:04,764 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,778 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,781 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 16:43:04,799 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,813 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,814 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 16:43:04,819 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,820 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,829 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 16:43:04,839 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,840 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,841 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 16:43:04,857 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,858 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,859 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 16:43:04,871 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,872 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,873 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 16:43:04,880 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,886 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,887 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 16:43:04,893 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,894 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,894 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 16:43:04,903 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,904 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,905 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 16:43:04,916 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,918 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,919 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 16:43:04,930 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,934 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,935 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 16:43:04,943 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,945 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,945 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 16:43:04,955 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,956 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,956 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 16:43:04,966 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,967 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,968 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 16:43:04,975 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,976 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,978 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 16:43:04,989 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:04,992 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:04,995 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 16:43:05,003 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:05,004 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:05,004 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 16:43:05,010 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:05,011 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:05,011 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 16:43:05,016 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:05,017 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-6023055269381228249 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:05,017 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 16:43:05,022 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 16:43:05,024 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-05 16:43:05,024 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 16:43:05,025 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 16:43:05,025 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 16:43:05,027 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 16:43:05,027 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 16:43:05,028 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-05 16:43:05,028 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 16:43:05,029 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-05 16:43:05,030 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 16:43:05,031 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 16:43:05,031 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 16:43:05,032 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-05 16:43:05,040 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 16:43:05,043 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 16:43:05,044 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 16:43:05,045 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-05 16:43:05,045 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 16:43:05,047 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 16:43:05,047 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 16:43:05,048 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-05 16:43:05,049 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 16:43:05,051 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 2 milliseconds.
2017-07-05 16:43:05,052 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 16:43:05,053 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 1 milliseconds.
2017-07-05 16:43:05,055 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 16:43:05,056 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-05 16:43:05,060 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 16:43:05,060 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:05,063 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 3 milliseconds.
2017-07-05 16:43:05,063 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 16:43:05,064 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 16:43:05,065 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:05,065 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 1 milliseconds.
2017-07-05 16:43:05,065 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 16:43:05,067 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-05 16:43:05,068 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 16:43:05,071 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:05,086 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 18 milliseconds.
2017-07-05 16:43:05,087 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 16:43:05,089 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 2 milliseconds.
2017-07-05 16:43:05,094 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 16:43:05,095 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-05 16:43:05,096 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 16:43:05,102 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 6 milliseconds.
2017-07-05 16:43:05,103 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 16:43:05,104 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-05 16:43:05,104 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 16:43:05,106 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-05 16:43:05,107 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 16:43:05,108 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-05 16:43:05,108 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 16:43:05,110 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-05 16:43:05,110 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 16:43:05,111 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-05 16:43:05,112 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 16:43:05,113 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-05 16:43:05,114 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 16:43:05,115 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-05 16:43:05,116 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 16:43:05,117 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-05 16:43:05,117 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 16:43:05,119 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 0 milliseconds.
2017-07-05 16:43:05,119 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 16:43:05,120 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-05 16:43:05,121 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 16:43:05,122 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 16:43:05,123 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 16:43:05,124 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-05 16:43:05,124 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 16:43:05,126 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-05 16:43:05,126 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 16:43:05,128 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-05 16:43:05,128 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 16:43:05,129 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 0 milliseconds.
2017-07-05 16:43:05,130 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 16:43:05,131 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-05 16:43:05,131 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 16:43:05,133 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-05 16:43:05,134 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 16:43:05,137 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 2 milliseconds.
2017-07-05 16:43:05,137 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 16:43:05,140 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 2 milliseconds.
2017-07-05 16:43:05,141 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 16:43:05,145 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 4 milliseconds.
2017-07-05 16:43:05,146 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 16:43:05,149 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 3 milliseconds.
2017-07-05 16:43:05,150 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 16:43:05,156 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 3 milliseconds.
2017-07-05 16:43:05,157 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 16:43:05,171 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 14 milliseconds.
2017-07-05 16:43:05,171 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 16:43:05,173 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:05,176 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:05,181 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 10 milliseconds.
2017-07-05 16:43:05,182 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 16:43:05,184 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:05,184 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 2 milliseconds.
2017-07-05 16:43:05,192 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 16:43:05,195 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 2 milliseconds.
2017-07-05 16:43:05,195 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 16:43:05,223 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 28 milliseconds.
2017-07-05 16:43:05,223 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 16:43:05,227 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 3 milliseconds.
2017-07-05 16:43:05,227 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 16:43:05,230 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 3 milliseconds.
2017-07-05 16:43:05,286 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:05,286 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:05,292 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 16:43:05,292 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 16:43:05,305 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 16:43:05,308 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 16:43:05,309 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-05 16:43:36,381 INFO  [Thread-434] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-05 16:43:36,382 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 16:43:36,382 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 16:43:36,445 INFO  [kafka-request-handler-3] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 16:43:36,446 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 16:43:36,447 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 16:43:36,450 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:36,457 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 16:43:36,457 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 16:43:36,460 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 16:43:36,462 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 16:43:37,024 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 16:43:37,025 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 16:43:37,025 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 16:43:37,032 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 16:43:37,033 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 16:43:37,033 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 16:43:37,033 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 16:43:37,033 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 16:43:37,033 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 16:43:37,033 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:37,082 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:37,083 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:37,083 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:37,137 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:37,137 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:37,148 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 16:43:37,149 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:37,289 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:37,289 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:37,290 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 16:43:37,290 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:37,373 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:37,373 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:37,373 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:37,497 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:37,497 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:37,497 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 16:43:37,498 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 16:43:37,498 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 16:43:37,498 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 16:43:37,498 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 16:43:37,498 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 16:43:37,520 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 16:43:37,524 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 16:43:37,525 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 16:43:37,533 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 16:43:37,533 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 16:43:37,544 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 16:43:37,545 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 16:43:37,545 INFO  [ZkClient-EventThread-545-127.0.0.1:36000] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 16:43:37,563 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d133453d10001
2017-07-05 16:43:37,573 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:54658 which had sessionid 0x15d133453d10001
2017-07-05 16:43:37,576 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d133453d10001 closed
2017-07-05 16:43:37,577 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 16:43:37,585 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 16:43:37,585 INFO  [ZkClient-EventThread-542-127.0.0.1:36000] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 16:43:37,593 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d133453d10000
2017-07-05 16:43:37,594 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:54657 which had sessionid 0x15d133453d10000
2017-07-05 16:43:37,594 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 16:43:37,595 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d133453d10000 closed
2017-07-05 16:43:37,595 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 16:43:37,595 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 16:43:37,595 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 16:43:37,596 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 16:43:37,596 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 16:43:37,596 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 16:43:37,596 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 16:43:37,597 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 16:43:37,598 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 16:43:37,598 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 16:43:37,598 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 16:43:37,599 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 16:43:37,599 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.875 sec - in org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
Running org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
2017-07-05 16:43:37,602 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-2281020545850122786/version-2 snapdir /tmp/kafka-6118254407330000274/version-2
2017-07-05 16:43:37,602 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-05 16:43:37,615 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:43450 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@54ffa561
2017-07-05 16:43:37,615 INFO  [ZkClient-EventThread-594-127.0.0.1:43450] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 16:43:37,617 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 16:43:37,627 INFO  [main-SendThread(127.0.0.1:43450)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:43450. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 16:43:37,628 INFO  [main-SendThread(127.0.0.1:43450)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:43450, initiating session
2017-07-05 16:43:37,628 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:57057
2017-07-05 16:43:37,628 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:57057
2017-07-05 16:43:37,634 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-05 16:43:37,645 INFO  [main-SendThread(127.0.0.1:43450)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:43450, sessionid = 0x15d1334dc020000, negotiated timeout = 10000
2017-07-05 16:43:37,645 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 16:43:37,645 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d1334dc020000 with negotiated timeout 10000 for client /127.0.0.1:57057
2017-07-05 16:43:37,647 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-7497199622475375554
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:43450
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-05 16:43:37,648 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-05 16:43:37,648 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:43450
2017-07-05 16:43:37,652 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:43450 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@49c099b
2017-07-05 16:43:37,653 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-05 16:43:37,657 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-05 16:43:37,662 INFO  [main-SendThread(127.0.0.1:43450)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:43450. Will not attempt to authenticate using SASL (unknown error)
2017-07-05 16:43:37,663 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:57060
2017-07-05 16:43:37,663 INFO  [main-SendThread(127.0.0.1:43450)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:43450, initiating session
2017-07-05 16:43:37,664 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:57060
2017-07-05 16:43:37,666 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d1334dc020001 with negotiated timeout 6000 for client /127.0.0.1:57060
2017-07-05 16:43:37,666 INFO  [main-SendThread(127.0.0.1:43450)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:43450, sessionid = 0x15d1334dc020001, negotiated timeout = 6000
2017-07-05 16:43:37,666 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-05 16:43:37,688 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-05 16:43:37,697 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-05 16:43:37,706 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-05 16:43:37,727 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-05 16:43:37,731 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = nyHbpGRbSam3D6lce9ZOBA
2017-07-05 16:43:37,732 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-7497199622475375554/meta.properties
2017-07-05 16:43:37,744 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-05 16:43:37,745 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-05 16:43:37,747 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-05 16:43:37,748 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-05 16:43:37,769 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-05 16:43:37,770 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-05 16:43:37,771 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-05 16:43:37,775 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-05 16:43:37,781 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-05 16:43:37,796 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-05 16:43:37,818 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:37,818 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:37,845 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-05 16:43:37,858 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-05 16:43:37,872 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 16:43:37,873 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-05 16:43:37,873 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-05 16:43:37,875 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-05 16:43:37,885 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-05 16:43:37,904 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-05 16:43:37,904 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-05 16:43:37,905 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-05 16:43:37,905 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-05 16:43:37,905 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-05 16:43:37,906 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-05 16:43:37,906 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-05 16:43:37,906 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-05 16:43:37,907 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-05 16:43:37,907 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-05 16:43:37,907 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-05 16:43:37,907 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-05 16:43:37,908 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-05 16:43:37,908 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-05 16:43:37,908 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-05 16:43:37,908 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-05 16:43:37,915 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-05 16:43:37,917 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-05 16:43:37,918 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-05 16:43:37,930 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:37,930 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:37,940 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-05 16:43:37,939 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-05 16:43:37,945 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-05 16:43:37,947 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-05 16:43:37,948 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-05 16:43:37,957 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 8059 : {samoa_test-oos=INVALID_REPLICATION_FACTOR}
2017-07-05 16:43:37,959 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 57 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-05 16:43:37,965 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-05 16:43:37,969 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-05 16:43:37,971 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x42 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-05 16:43:37,971 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-05 16:43:37,984 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-05 16:43:37,984 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-05 16:43:37,985 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-7497199622475375554/meta.properties
2017-07-05 16:43:37,995 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:43:37,995 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:43:37,995 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-05 16:43:37,996 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-05 16:43:38,000 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-05 16:43:38,004 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-05 16:43:38,025 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-05 16:43:38,030 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-kdp Error:KeeperErrorCode = NoNode for /config/topics/test-kdp
2017-07-05 16:43:38,032 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-05 16:43:38,033 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-05 16:43:38,038 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:38,052 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:43:38,058 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-05 16:43:38,064 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-05 16:43:38,065 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2017-07-05 16:43:38,065 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:43:38,065 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:43:38,073 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:43:38,073 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-6
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-05 16:43:38,085 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-kdp)], deleted topics: [Set()], new partition replica assignment [Map([test-kdp,0] -> List(0))]
2017-07-05 16:43:38,086 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-kdp,0]
2017-07-05 16:43:38,090 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-05 16:43:38,090 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-05 16:43:38,105 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-kdp,0]
2017-07-05 16:43:38,105 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-kdp,0]
2017-07-05 16:43:38,105 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-05 16:43:38,109 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-kdp,0]
2017-07-05 16:43:38,110 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x53 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions/0
2017-07-05 16:43:38,114 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:setData cxid:0x54 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-05 16:43:38,114 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x57 zxid:0x22 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions
2017-07-05 16:43:38,129 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:setData cxid:0x5a zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-05 16:43:38,129 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x5b zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:38,137 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x60 zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:38,144 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:43:38,146 WARN  [Thread-439] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:38,146 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-05 16:43:38,150 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-kdp-0
2017-07-05 16:43:38,155 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-05 16:43:38,156 INFO  [kafka-request-handler-2] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-05 16:43:38,156 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 58 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:38,158 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log test-kdp-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:38,159 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-kdp,0] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:38,159 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [test-kdp,0] on broker 0: No checkpointed highwatermark is found for partition test-kdp-0
2017-07-05 16:43:38,181 INFO  [kafka-request-handler-2] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic samoa_test-oos with 1 partitions and replication factor 1 is successful
2017-07-05 16:43:38,181 WARN  [kafka-producer-network-thread | test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 1 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:38,182 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 8060 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:38,200 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:setData cxid:0x72 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-05 16:43:38,202 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos, test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0), [samoa_test-oos,0] -> List(0))]
2017-07-05 16:43:38,203 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0],[samoa_test-oos,0]
2017-07-05 16:43:38,209 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x76 zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-05 16:43:38,219 INFO  [kafka-request-handler-7] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 16:43:38,220 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0],[samoa_test-oos,0]
2017-07-05 16:43:38,221 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0],[samoa_test-oos,0]
2017-07-05 16:43:38,221 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 16:43:38,227 INFO  [kafka-request-handler-5] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-05 16:43:38,229 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x7c zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets
2017-07-05 16:43:38,229 INFO  [kafka-request-handler-7] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-05 16:43:38,244 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0],[samoa_test-oos,0]
2017-07-05 16:43:38,248 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x7e zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-05 16:43:38,264 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x7f zxid:0x34 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-05 16:43:38,307 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x88 zxid:0x38 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-05 16:43:38,323 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 8062 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-05 16:43:38,331 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x8a zxid:0x39 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-05 16:43:38,349 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-05 16:43:38,354 INFO  [kafka-request-handler-7] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0,test-s-0
2017-07-05 16:43:38,387 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-05 16:43:38,388 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:38,389 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:38,389 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:38,391 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:38,392 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:38,394 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-05 16:43:38,391 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 16:43:38,409 INFO  [kafka-request-handler-7] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:38,410 INFO  [kafka-request-handler-7] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:38,411 INFO  [kafka-request-handler-7] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-05 16:43:38,503 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-05 16:43:38,506 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xd2 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-05 16:43:38,516 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xd3 zxid:0x3e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-05 16:43:38,526 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xd7 zxid:0x42 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-05 16:43:38,530 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xda zxid:0x45 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-05 16:43:38,533 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xdd zxid:0x48 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-05 16:43:38,536 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xe0 zxid:0x4b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-05 16:43:38,603 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xe7 zxid:0x4e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-05 16:43:38,610 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xec zxid:0x51 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-05 16:43:38,615 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xef zxid:0x54 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-05 16:43:38,635 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xf2 zxid:0x57 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-05 16:43:38,644 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xf5 zxid:0x5a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-05 16:43:38,658 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xfb zxid:0x5d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-05 16:43:38,668 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0xfe zxid:0x60 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-05 16:43:38,672 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x101 zxid:0x63 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-05 16:43:38,676 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x104 zxid:0x66 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-05 16:43:38,680 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x107 zxid:0x69 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-05 16:43:38,692 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x10b zxid:0x6c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-05 16:43:38,701 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x10f zxid:0x6f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-05 16:43:38,718 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x113 zxid:0x72 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-05 16:43:38,732 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x116 zxid:0x75 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-05 16:43:38,736 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x119 zxid:0x78 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-05 16:43:38,739 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x11c zxid:0x7b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-05 16:43:38,743 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x11f zxid:0x7e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-05 16:43:38,746 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x122 zxid:0x81 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-05 16:43:38,754 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x127 zxid:0x84 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-05 16:43:38,767 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x12b zxid:0x87 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-05 16:43:38,774 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x12e zxid:0x8a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-05 16:43:38,790 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x131 zxid:0x8d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-05 16:43:38,795 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x134 zxid:0x90 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-05 16:43:38,800 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x138 zxid:0x93 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-05 16:43:38,804 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x13c zxid:0x96 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-05 16:43:38,808 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x140 zxid:0x99 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-05 16:43:38,819 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x143 zxid:0x9c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-05 16:43:38,823 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x146 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-05 16:43:38,827 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x149 zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-05 16:43:38,830 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x14c zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-05 16:43:38,833 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x14f zxid:0xa8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-05 16:43:38,849 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x153 zxid:0xab txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-05 16:43:38,860 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x158 zxid:0xae txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-05 16:43:38,866 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x15b zxid:0xb1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-05 16:43:38,870 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x15e zxid:0xb4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-05 16:43:38,874 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x161 zxid:0xb7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-05 16:43:38,881 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x164 zxid:0xba txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-05 16:43:38,884 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x167 zxid:0xbd txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-05 16:43:38,888 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x16a zxid:0xc0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-05 16:43:38,892 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x16d zxid:0xc3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-05 16:43:38,912 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x173 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-05 16:43:38,924 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x176 zxid:0xc9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-05 16:43:38,932 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x179 zxid:0xcc txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-05 16:43:38,935 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x17c zxid:0xcf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-05 16:43:38,944 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d1334dc020001 type:create cxid:0x17f zxid:0xd2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-05 16:43:38,950 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-05 16:43:38,987 INFO  [kafka-request-handler-2] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-05 16:43:38,999 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,003 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,004 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-05 16:43:39,009 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,010 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,010 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-05 16:43:39,015 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,021 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,022 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-05 16:43:39,029 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,030 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,030 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-05 16:43:39,038 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,039 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,039 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-05 16:43:39,047 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,048 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,049 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-05 16:43:39,055 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,056 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,056 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-05 16:43:39,066 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,067 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,068 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-05 16:43:39,087 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,088 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,088 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-05 16:43:39,096 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,097 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,098 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-05 16:43:39,101 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,102 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,102 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-05 16:43:39,106 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,106 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,107 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-05 16:43:39,126 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,126 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,127 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-05 16:43:39,139 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,140 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,140 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-05 16:43:39,154 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,155 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,155 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-05 16:43:39,164 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,165 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,165 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-05 16:43:39,172 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,173 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,174 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-05 16:43:39,192 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,193 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,193 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-05 16:43:39,207 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,208 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,209 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-05 16:43:39,229 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,230 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,230 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-05 16:43:39,242 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,245 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,246 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-05 16:43:39,251 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,254 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,254 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-05 16:43:39,260 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,261 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,261 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-05 16:43:39,296 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,297 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,297 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-05 16:43:39,315 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,315 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,316 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-05 16:43:39,327 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,328 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,328 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-05 16:43:39,337 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,338 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,338 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-05 16:43:39,345 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,346 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,346 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-05 16:43:39,355 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,356 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,356 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-05 16:43:39,360 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,360 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,361 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-05 16:43:39,371 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,372 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,372 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-05 16:43:39,377 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,377 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,378 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-05 16:43:39,387 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,388 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,388 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-05 16:43:39,396 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,397 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,397 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-05 16:43:39,409 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,410 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,410 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-05 16:43:39,423 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,424 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,433 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-05 16:43:39,439 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,440 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,440 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-05 16:43:39,443 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,444 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,445 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-05 16:43:39,448 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,448 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,449 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-05 16:43:39,452 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,453 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,453 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-05 16:43:39,457 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,458 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,458 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-05 16:43:39,461 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,462 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,462 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-05 16:43:39,466 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,466 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,467 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-05 16:43:39,472 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,472 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,473 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-05 16:43:39,483 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,484 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,485 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-05 16:43:39,492 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,493 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,494 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-05 16:43:39,498 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,501 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,510 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-05 16:43:39,520 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,520 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,521 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-05 16:43:39,531 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,532 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,532 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-05 16:43:39,539 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-05 16:43:39,539 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-7497199622475375554 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-05 16:43:39,540 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-05 16:43:39,560 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-05 16:43:39,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-05 16:43:39,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-05 16:43:39,563 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-05 16:43:39,564 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-05 16:43:39,565 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-05 16:43:39,565 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-05 16:43:39,566 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-05 16:43:39,566 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-05 16:43:39,567 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 0 milliseconds.
2017-07-05 16:43:39,568 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-05 16:43:39,569 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-05 16:43:39,569 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-05 16:43:39,570 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-05 16:43:39,570 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-05 16:43:39,571 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-05 16:43:39,571 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-05 16:43:39,572 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-05 16:43:39,573 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-05 16:43:39,574 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-05 16:43:39,574 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-05 16:43:39,575 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-05 16:43:39,575 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-05 16:43:39,581 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 6 milliseconds.
2017-07-05 16:43:39,581 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-05 16:43:39,584 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 2 milliseconds.
2017-07-05 16:43:39,584 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-05 16:43:39,595 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:39,601 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 17 milliseconds.
2017-07-05 16:43:39,602 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-05 16:43:39,608 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 6 milliseconds.
2017-07-05 16:43:39,608 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-05 16:43:39,608 INFO  [Thread-433] internals.Fetcher (Fetcher.java:parseCompletedFetch(820)) - Fetch offset 11111 is out of range for partition samoa_test-oos-0, resetting offset
2017-07-05 16:43:39,609 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:39,610 WARN  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onComplete(622)) - Auto-commit of offsets {samoa_test-oos-0=OffsetAndMetadata{offset=11111, metadata=''}} failed for group test: Offset commit failed with a retriable exception. You should retry committing offsets.
2017-07-05 16:43:39,611 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:39,611 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 3 milliseconds.
2017-07-05 16:43:39,612 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-05 16:43:39,612 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-05 16:43:39,612 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:39,624 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 12 milliseconds.
2017-07-05 16:43:39,625 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-05 16:43:39,627 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 2 milliseconds.
2017-07-05 16:43:39,628 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-05 16:43:39,628 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:39,631 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 3 milliseconds.
2017-07-05 16:43:39,631 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-05 16:43:39,638 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 7 milliseconds.
2017-07-05 16:43:39,646 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-05 16:43:39,649 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 3 milliseconds.
2017-07-05 16:43:39,649 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-05 16:43:39,651 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 2 milliseconds.
2017-07-05 16:43:39,652 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-05 16:43:39,662 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 10 milliseconds.
2017-07-05 16:43:39,663 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-05 16:43:39,665 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 2 milliseconds.
2017-07-05 16:43:39,665 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-05 16:43:39,675 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 10 milliseconds.
2017-07-05 16:43:39,675 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-05 16:43:39,682 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 7 milliseconds.
2017-07-05 16:43:39,683 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-05 16:43:39,685 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 2 milliseconds.
2017-07-05 16:43:39,686 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-05 16:43:39,693 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 7 milliseconds.
2017-07-05 16:43:39,694 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-05 16:43:39,700 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 2 milliseconds.
2017-07-05 16:43:39,701 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-05 16:43:39,702 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-05 16:43:39,702 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-05 16:43:39,703 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-05 16:43:39,704 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-05 16:43:39,705 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-05 16:43:39,705 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-05 16:43:39,710 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 5 milliseconds.
2017-07-05 16:43:39,710 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-05 16:43:39,713 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 2 milliseconds.
2017-07-05 16:43:39,713 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-05 16:43:39,715 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 2 milliseconds.
2017-07-05 16:43:39,716 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-05 16:43:39,725 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 9 milliseconds.
2017-07-05 16:43:39,726 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-05 16:43:39,728 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 2 milliseconds.
2017-07-05 16:43:39,728 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-05 16:43:39,730 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:39,730 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:39,731 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 2 milliseconds.
2017-07-05 16:43:39,731 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-05 16:43:39,732 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:39,734 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 3 milliseconds.
2017-07-05 16:43:39,734 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-05 16:43:39,738 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 4 milliseconds.
2017-07-05 16:43:39,738 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-05 16:43:39,741 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 2 milliseconds.
2017-07-05 16:43:39,741 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-05 16:43:39,743 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 2 milliseconds.
2017-07-05 16:43:39,743 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-05 16:43:39,746 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 2 milliseconds.
2017-07-05 16:43:39,746 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-05 16:43:39,749 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 3 milliseconds.
2017-07-05 16:43:39,749 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-05 16:43:39,751 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 2 milliseconds.
2017-07-05 16:43:39,752 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-05 16:43:39,754 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 2 milliseconds.
2017-07-05 16:43:39,754 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-05 16:43:39,757 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 2 milliseconds.
2017-07-05 16:43:39,757 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-05 16:43:39,760 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 3 milliseconds.
2017-07-05 16:43:39,760 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-05 16:43:40,022 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 262 milliseconds.
2017-07-05 16:43:40,023 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-05 16:43:40,024 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-05 16:43:40,024 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:40,024 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:40,025 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-05 16:43:40,025 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-05 16:43:40,027 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-05 16:43:40,028 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-05 16:43:40,028 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-05 16:43:40,131 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-05 16:43:40,141 WARN  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onComplete(622)) - Auto-commit of offsets {samoa_test-oos-0=OffsetAndMetadata{offset=0, metadata=''}} failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
2017-07-05 16:43:40,141 WARN  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:maybeAutoCommitOffsetsSync(648)) - Auto-commit of offsets {samoa_test-oos-0=OffsetAndMetadata{offset=0, metadata=''}} failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
2017-07-05 16:43:40,141 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [samoa_test-oos-0] for group test
2017-07-05 16:43:40,141 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:40,147 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-05 16:43:43,081 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [test-kdp-0] for group test
2017-07-05 16:43:43,081 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:43,083 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 2
2017-07-05 16:43:43,090 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 2
2017-07-05 16:43:43,092 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 2
2017-07-05 16:43:43,092 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 2
2017-07-05 16:43:43,093 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-05 16:43:43,093 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-05 16:43:54,007 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 2
2017-07-05 16:43:55,101 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [samoa_test-oos-0] for group test
2017-07-05 16:43:55,101 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-05 16:43:55,103 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 3
2017-07-05 16:43:55,104 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 3
2017-07-05 16:43:55,106 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 3
2017-07-05 16:43:55,106 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-05 16:43:56,006 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-05 16:43:56,006 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-05 16:43:56,014 INFO  [kafka-request-handler-7] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-05 16:43:56,016 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-05 16:43:56,017 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-05 16:43:56,038 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-05 16:43:56,038 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-05 16:43:56,043 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-05 16:43:56,043 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-05 16:43:56,052 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-05 16:43:56,748 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-05 16:43:56,749 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-05 16:43:56,750 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-05 16:43:57,749 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-05 16:43:57,749 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-05 16:43:57,750 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-05 16:43:57,750 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-05 16:43:57,751 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-05 16:43:57,751 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-05 16:43:57,751 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:57,802 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:57,802 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:57,803 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:57,852 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:57,852 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:57,882 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-05 16:43:57,882 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:58,052 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:58,052 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:58,053 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-05 16:43:58,053 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:58,167 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:58,167 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:58,168 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-05 16:43:58,357 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-05 16:43:58,357 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-05 16:43:58,357 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-05 16:43:58,357 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-05 16:43:58,358 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-05 16:43:58,358 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-05 16:43:58,358 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-05 16:43:58,358 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-05 16:43:58,622 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-05 16:43:58,623 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-05 16:43:58,623 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-05 16:43:58,625 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-05 16:43:58,626 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-05 16:43:58,626 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-05 16:43:58,626 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-05 16:43:58,626 INFO  [ZkClient-EventThread-597-127.0.0.1:43450] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 16:43:58,635 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d1334dc020001
2017-07-05 16:43:58,637 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d1334dc020001 closed
2017-07-05 16:43:58,638 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 16:43:58,638 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-05 16:43:58,638 INFO  [ZkClient-EventThread-594-127.0.0.1:43450] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-05 16:43:58,639 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:57060 which had sessionid 0x15d1334dc020001
2017-07-05 16:43:58,640 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d1334dc020000
2017-07-05 16:43:58,641 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:57057 which had sessionid 0x15d1334dc020000
2017-07-05 16:43:58,641 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d1334dc020000 closed
2017-07-05 16:43:58,641 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-05 16:43:58,641 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 16:43:58,642 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 16:43:58,642 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 16:43:58,642 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-05 16:43:58,642 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 16:43:58,642 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-05 16:43:58,643 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-05 16:43:58,644 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-05 16:43:58,644 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-05 16:43:58,644 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-05 16:43:58,644 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-05 16:43:58,644 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-05 16:43:58,645 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.042 sec - in org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
Running org.apache.samoa.core.DoubleVectorTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.019 sec - in org.apache.samoa.core.DoubleVectorTest

Results :

Tests run: 22, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-test 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-test ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-test ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250285295/samoa-test/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250285295/samoa-test/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-test ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250285295/samoa-test/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-local 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-local ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-local ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/250285295/samoa-local/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18:test (default-test) @ samoa-local ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/250285295/samoa-local/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom (3 KB at 67.6 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom (3 KB at 110.6 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar (67 KB at 1326.5 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.topology.impl.SimpleEngineTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.268 sec - in org.apache.samoa.topology.impl.SimpleEngineTest
Running org.apache.samoa.topology.impl.SimpleStreamTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.197 sec - in org.apache.samoa.topology.impl.SimpleStreamTest
Running org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 sec - in org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Running org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.428 sec <<< FAILURE! - in org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
testStartSendingEvents(org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest)  Time elapsed: 0.34 sec  <<< ERROR!
java.lang.IllegalStateException: Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest$4.<init>(SimpleEntranceProcessingItemTest.java:159)
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest.testStartSendingEvents(SimpleEntranceProcessingItemTest.java:155)

Running org.apache.samoa.topology.impl.SimpleTopologyTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 sec - in org.apache.samoa.topology.impl.SimpleTopologyTest
Running org.apache.samoa.topology.impl.SimpleProcessingItemTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.081 sec - in org.apache.samoa.topology.impl.SimpleProcessingItemTest
Running org.apache.samoa.AlgosTest
2017-07-05 16:44:06,241 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test4614189560502166001test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test4614189560502166001test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-05 16:44:06,302 [pool-1-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 16:44:07,860 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-05 16:44:07,925 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 65.645
Kappa Statistic (percent) = 27.592
Kappa Temporal Statistic (percent) = 30.173
2017-07-05 16:44:08,700 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:08,700 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 70.69
Kappa Statistic (percent) = 39.422
Kappa Temporal Statistic (percent) = 40.272
2017-07-05 16:44:09,226 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:09,232 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 73.663
Kappa Statistic (percent) = 45.786
Kappa Temporal Statistic (percent) = 46.436
2017-07-05 16:44:09,525 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:09,525 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 75.545
Kappa Statistic (percent) = 49.77
Kappa Temporal Statistic (percent) = 50.387
2017-07-05 16:44:09,910 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:09,910 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 77.083
Kappa Statistic (percent) = 53.033
Kappa Temporal Statistic (percent) = 53.669
2017-07-05 16:44:10,295 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:10,295 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 78.397
Kappa Statistic (percent) = 55.776
Kappa Temporal Statistic (percent) = 56.372
2017-07-05 16:44:10,525 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:10,526 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 79.531
Kappa Statistic (percent) = 58.117
Kappa Temporal Statistic (percent) = 58.646
2017-07-05 16:44:10,806 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:10,807 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 80.527
Kappa Statistic (percent) = 60.173
Kappa Temporal Statistic (percent) = 60.566
2017-07-05 16:44:11,181 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:11,181 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 81.387
Kappa Statistic (percent) = 61.939
Kappa Temporal Statistic (percent) = 62.332
2017-07-05 16:44:11,459 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:11,460 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 82.144
Kappa Statistic (percent) = 63.505
Kappa Temporal Statistic (percent) = 63.842
2017-07-05 16:44:11,461 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 16:44:11,461 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 16:44:11,462 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,65.645,27.592487187843105,30.172764227642272
40000.0,40000.0,70.69,39.42219140754782,40.27204646186763
60000.0,60000.0,73.66333333333334,45.78556998992169,46.43571404359175
80000.0,80000.0,75.545,49.770097068022935,50.386731925037395
100000.0,100000.0,77.083,53.033355520313066,53.66933527413876
120000.0,120000.0,78.3975,55.77588004396512,56.37180652327577
140000.0,140000.0,79.53071428571428,58.11662806596387,58.64611743654127
160000.0,160000.0,80.526875,60.17259494934151,60.56625026894989
180000.0,180000.0,81.38666666666666,61.93941888230634,62.3317780650964
200000.0,200000.0,82.1435,63.50464751569946,63.842259795484466

2017-07-05 16:44:11,462 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 5 seconds for 200000 instances
2017-07-05 16:44:16,276 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test4614189560502166001test
2017-07-05 16:44:16,295 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test3803379326920131102test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=60.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test3803379326920131102test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)
2017-07-05 16:44:16,306 [pool-4-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 16:44:18,444 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-05 16:44:18,445 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 84.665
Kappa Statistic (percent) = 68.618
Kappa Temporal Statistic (percent) = 68.417
2017-07-05 16:44:19,352 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:19,353 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 88.582
Kappa Statistic (percent) = 76.644
Kappa Temporal Statistic (percent) = 76.555
2017-07-05 16:44:20,399 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-05 16:44:20,400 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 90.413
Kappa Statistic (percent) = 80.438
Kappa Temporal Statistic (percent) = 80.399
2017-07-05 16:44:21,363 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:21,364 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 91.711
Kappa Statistic (percent) = 83.093
Kappa Temporal Statistic (percent) = 83.167
2017-07-05 16:44:22,202 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:22,203 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 92.662
Kappa Statistic (percent) = 85.037
Kappa Temporal Statistic (percent) = 85.069
2017-07-05 16:44:23,080 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:23,081 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 93.344
Kappa Statistic (percent) = 86.424
Kappa Temporal Statistic (percent) = 86.437
2017-07-05 16:44:24,029 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:24,031 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 93.916
Kappa Statistic (percent) = 87.594
Kappa Temporal Statistic (percent) = 87.595
2017-07-05 16:44:24,942 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:24,943 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 94.359
Kappa Statistic (percent) = 88.494
Kappa Temporal Statistic (percent) = 88.476
2017-07-05 16:44:25,756 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:25,756 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 94.703
Kappa Statistic (percent) = 89.195
Kappa Temporal Statistic (percent) = 89.176
2017-07-05 16:44:26,542 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:26,543 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 94.998
Kappa Statistic (percent) = 89.799
Kappa Temporal Statistic (percent) = 89.778
2017-07-05 16:44:26,543 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 16:44:26,544 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 16:44:26,544 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,84.665,68.61823521397044,68.41725877870456
40000.0,40000.0,88.5825,76.64361542718096,76.55544147843942
60000.0,60000.0,90.41333333333334,80.43844937179757,80.39937299802358
80000.0,80000.0,91.71125,83.09318081844135,83.16748743463471
100000.0,100000.0,92.662,85.03712352050859,85.06867433106115
120000.0,120000.0,93.34416666666667,86.42407531197561,86.43696508626545
140000.0,140000.0,93.91642857142857,87.59386080615302,87.59521694169736
160000.0,160000.0,94.35875,88.49406509040814,88.47622087456114
180000.0,180000.0,94.70277777777778,89.19537135102495,89.17596576267724
200000.0,200000.0,94.9975,89.79852977059544,89.77777777777777

2017-07-05 16:44:26,545 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 10 seconds for 200000 instances
2017-07-05 16:44:27,301 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test3803379326920131102test
2017-07-05 16:44:27,305 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test1915519230533456595test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=65.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test1915519230533456595test -i 200000 -f 20000 -w 0 -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)
2017-07-05 16:44:27,332 [pool-16-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-05 16:44:27,586 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:27,587 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 84.275
Kappa Statistic (percent) = 68.502
Kappa Temporal Statistic (percent) = 68.778
2017-07-05 16:44:27,753 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:27,753 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 83.803
Kappa Statistic (percent) = 67.594
Kappa Temporal Statistic (percent) = 67.819
2017-07-05 16:44:27,918 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:27,918 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 83.59
Kappa Statistic (percent) = 67.214
Kappa Temporal Statistic (percent) = 67.392
2017-07-05 16:44:28,084 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:28,085 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 83.546
Kappa Statistic (percent) = 67.127
Kappa Temporal Statistic (percent) = 67.269
2017-07-05 16:44:28,259 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:28,260 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 83.559
Kappa Statistic (percent) = 67.135
Kappa Temporal Statistic (percent) = 67.209
2017-07-05 16:44:28,423 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:28,424 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 83.531
Kappa Statistic (percent) = 67.072
Kappa Temporal Statistic (percent) = 67.151
2017-07-05 16:44:28,586 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:28,587 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 83.504
Kappa Statistic (percent) = 67.03
Kappa Temporal Statistic (percent) = 67.054
2017-07-05 16:44:28,744 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:28,745 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 83.518
Kappa Statistic (percent) = 67.05
Kappa Temporal Statistic (percent) = 67.065
2017-07-05 16:44:28,899 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:28,899 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 83.54
Kappa Statistic (percent) = 67.089
Kappa Temporal Statistic (percent) = 67.126
2017-07-05 16:44:29,058 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-05 16:44:29,058 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 83.523
Kappa Statistic (percent) = 67.057
Kappa Temporal Statistic (percent) = 67.143
2017-07-05 16:44:29,059 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-05 16:44:29,059 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-05 16:44:29,059 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,84.275,68.50233079747552,68.77792117541944
40000.0,40000.0,83.80250000000001,67.59439866955276,67.81900362588786
60000.0,60000.0,83.59,67.21382484118784,67.39195230998509
80000.0,80000.0,83.54625,67.12663795918682,67.26924607121543
100000.0,100000.0,83.55900000000001,67.13483590334214,67.20915853926087
120000.0,120000.0,83.53083333333333,67.07192304919516,67.15145269596435
140000.0,140000.0,83.50357142857143,67.02990305706534,67.05373828442632
160000.0,160000.0,83.518125,67.04964827201962,67.06465673356729
180000.0,180000.0,83.54,67.08882037897833,67.1260235670062
200000.0,200000.0,83.5235,67.05662351796806,67.142614990378

2017-07-05 16:44:29,059 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 1 seconds for 200000 instances
2017-07-05 16:44:37,306 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test1915519230533456595test
2017-07-05 16:44:37,317 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test4284413313659211402test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialCVEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialCVEvaluation -d /tmp/test4284413313659211402test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-05 16:44:37,329 [pool-18-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialCVEvaluation
2017-07-05 16:44:39,613 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:44:39,614 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 20,000
[avg] classified instances = 20,000
[err] classified instances = 0
[avg] classifications correct (percent) = 64.627
[err] classifications correct (percent) = 0.189
[avg] Kappa Statistic (percent) = 25.294
[err] Kappa Statistic (percent) = 0.352
[avg] Kappa Temporal Statistic (percent) = 28.104
[err] Kappa Temporal Statistic (percent) = 0.384
2017-07-05 16:44:42,531 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:44:42,532 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 40,000
[avg] classified instances = 40,000
[err] classified instances = 0
[avg] classifications correct (percent) = 70.032
[err] classifications correct (percent) = 0.203
[avg] Kappa Statistic (percent) = 38.074
[err] Kappa Statistic (percent) = 0.445
[avg] Kappa Temporal Statistic (percent) = 38.931
[err] Kappa Temporal Statistic (percent) = 0.414
2017-07-05 16:44:45,615 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 16:44:45,616 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 60,000
[avg] classified instances = 60,000
[err] classified instances = 0
[avg] classifications correct (percent) = 73.498
[err] classifications correct (percent) = 0.308
[avg] Kappa Statistic (percent) = 45.523
[err] Kappa Statistic (percent) = 0.671
[avg] Kappa Temporal Statistic (percent) = 46.1
[err] Kappa Temporal Statistic (percent) = 0.627
2017-07-05 16:44:48,200 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:44:48,201 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 80,000
[avg] classified instances = 80,000
[err] classified instances = 0
[avg] classifications correct (percent) = 75.64
[err] classifications correct (percent) = 0.397
[avg] Kappa Statistic (percent) = 50.074
[err] Kappa Statistic (percent) = 0.86
[avg] Kappa Temporal Statistic (percent) = 50.579
[err] Kappa Temporal Statistic (percent) = 0.806
2017-07-05 16:44:50,806 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:44:50,807 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 100,000
[avg] classified instances = 100,000
[err] classified instances = 0
[avg] classifications correct (percent) = 77.273
[err] classifications correct (percent) = 0.442
[avg] Kappa Statistic (percent) = 53.513
[err] Kappa Statistic (percent) = 0.947
[avg] Kappa Temporal Statistic (percent) = 54.053
[err] Kappa Temporal Statistic (percent) = 0.894
2017-07-05 16:44:53,599 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:44:53,600 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 120,000
[avg] classified instances = 120,000
[err] classified instances = 0
[avg] classifications correct (percent) = 78.553
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 56.166
[err] Kappa Statistic (percent) = 0.983
[avg] Kappa Temporal Statistic (percent) = 56.687
[err] Kappa Temporal Statistic (percent) = 0.938
2017-07-05 16:44:56,726 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 16:44:56,727 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 140,000
[avg] classified instances = 140,000
[err] classified instances = 0
[avg] classifications correct (percent) = 79.767
[err] classifications correct (percent) = 0.471
[avg] Kappa Statistic (percent) = 58.667
[err] Kappa Statistic (percent) = 0.991
[avg] Kappa Temporal Statistic (percent) = 59.123
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-05 16:44:58,945 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:44:58,946 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 160,000
[avg] classified instances = 160,000
[err] classified instances = 0
[avg] classifications correct (percent) = 80.8
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 60.79
[err] Kappa Statistic (percent) = 0.973
[avg] Kappa Temporal Statistic (percent) = 61.12
[err] Kappa Temporal Statistic (percent) = 0.939
2017-07-05 16:45:01,500 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-05 16:45:01,501 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 180,000
[avg] classified instances = 180,000
[err] classified instances = 0
[avg] classifications correct (percent) = 81.614
[err] classifications correct (percent) = 0.462
[avg] Kappa Statistic (percent) = 62.451
[err] Kappa Statistic (percent) = 0.969
[avg] Kappa Temporal Statistic (percent) = 62.792
[err] Kappa Temporal Statistic (percent) = 0.936
2017-07-05 16:45:05,284 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-05 16:45:05,285 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 200,000
[avg] classified instances = 200,000.9
[err] classified instances = 0.9
[avg] classifications correct (percent) = 82.333
[err] classifications correct (percent) = 0.47
[avg] Kappa Statistic (percent) = 63.93
[err] Kappa Statistic (percent) = 0.984
[avg] Kappa Temporal Statistic (percent) = 64.226
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-05 16:45:05,286 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:215) - last event is received!
2017-07-05 16:45:05,286 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:216) - total count: 200001
2017-07-05 16:45:05,286 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:219) - org.apache.samoa.evaluation.EvaluatorCVProcessorid = 0
evaluation instances,[avg] classified instances,[err] classified instances,[avg] classifications correct (percent),[err] classifications correct (percent),[avg] Kappa Statistic (percent),[err] Kappa Statistic (percent),[avg] Kappa Temporal Statistic (percent),[err] Kappa Temporal Statistic (percent)
20000.0,20000.0,0.0,64.62700000000001,0.18916071943661472,25.293690418855896,0.3519627618187893,28.103658536585368,0.38447300698498993
40000.0,40000.0,0.0,70.032,0.20324410610560478,38.07419026287571,0.44506467062749777,38.931173264048084,0.4141710858537992
60000.0,60000.0,0.0,73.49833333333333,0.3080789741624503,45.52336429068201,0.6710378342453998,46.10013219890851,0.6265800633791055
80000.0,80000.0,0.0,75.63975,0.3973700346584335,50.07382624902415,0.8597101949765279,50.57895671138387,0.8061674935377634
100000.0,100000.0,0.0,77.27289999999999,0.4422633566854323,53.51347314843381,0.9469128869060368,54.05325084910237,0.8941115896115002
120000.0,120000.0,0.0,78.5535,0.4643362247656641,56.1661763049406,0.9830956362259596,56.68686256689892,0.9377688069588297
140000.0,140000.0,0.0,79.76700000000001,0.47139196346560175,58.66745091659613,0.9910982373644812,59.12348297906114,0.9523482240960528
160000.0,160000.0,0.0,80.80025000000002,0.4637947455202826,60.789541927185965,0.9729687801934683,61.11984407234435,0.9392003554346215
180000.0,180000.0,0.0,81.61388888888888,0.46241870972111077,62.451145738939225,0.9693840059052787,62.791612794423514,0.9358071589161842
200000.0,200000.9,0.8999999999996271,82.33328358598864,0.4701473780403362,63.930181311539584,0.9842393397954078,64.2263845297155,0.9520407427484003

2017-07-05 16:45:05,287 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:223) - total evaluation time: 27 seconds for 200001 instances
2017-07-05 16:45:05,323 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test4284413313659211402test
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.586 sec - in org.apache.samoa.AlgosTest

Results :


Tests in error: 
  Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope


Tests run: 28, Failures: 0, Errors: 1, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache SAMOA ....................................... SUCCESS [  2.724 s]
[INFO] samoa-instances .................................... SUCCESS [  2.716 s]
[INFO] samoa-api .......................................... SUCCESS [01:31 min]
[INFO] samoa-test ......................................... SUCCESS [  2.291 s]
[INFO] samoa-local ........................................ FAILURE [01:04 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:43 min
[INFO] Finished at: 2017-07-05T16:45:05+02:00
[INFO] Final Memory: 38M/1027M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18:test (default-test) on project samoa-local: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/apache/incubator-samoa/250285295/samoa-local/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :samoa-local
