[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Apache SAMOA
[INFO] samoa-instances
[INFO] samoa-api
[INFO] samoa-test
[INFO] samoa-local
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache SAMOA 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa ---
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-api
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-instances
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-local
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-storm
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-flink
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-apex
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-samza
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-test
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/samoa-threads
[INFO] Will search files to update from root /root/workspace/apache/incubator-samoa/252350217/bin
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/TestUtilsForKafka.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessorTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/KafkaUtilsTest.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/java/org/apache/samoa/streams/kafka/OosTestSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDestinationProcessor.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaUtils.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaConsumerThread.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaTask.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaSerializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[WARNING] skip failed file : Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


org.codehaus.mojo.license.header.InvalideFileHeaderException: Could not extract header on file /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/main/java/org/apache/samoa/streams/kafka/KafkaDeserializer.java for reason could not find 3 sections in
SAMOA
%%
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:913)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processFile(AbstractFileHeaderMojo.java:826)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.processCommentStyle(AbstractFileHeaderMojo.java:800)
	at org.codehaus.mojo.license.AbstractFileHeaderMojo.doAction(AbstractFileHeaderMojo.java:581)
	at org.codehaus.mojo.license.AbstractLicenseMojo.execute(AbstractLicenseMojo.java:207)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[INFO] Scan 383 files header done in 888.418ms.
[INFO] 
 * uptodate header on 373 files.
 * fail header on 10 files.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa ---
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-instances 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-instances ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-instances ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/252350217/samoa-instances/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-instances ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/252350217/samoa-instances/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-instances ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-instances ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/252350217/samoa-instances/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.pom (3 KB at 3.8 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18.1/surefire-providers-2.18.1.pom (3 KB at 82.8 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18.1/surefire-junit4-2.18.1.jar (67 KB at 650.9 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.instances.ArffLoaderTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 sec - in org.apache.samoa.instances.ArffLoaderTest

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-api 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-api ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/252350217/samoa-api/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-api ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
2017-07-11 16:34:17,371 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
2017-07-11 16:34:17,998 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
2017-07-11 16:34:18,254 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:18,297 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:18,350 INFO  [main] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1049)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2017-07-11 16:34:18,350 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:18,351 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:18,353 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:18,356 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:18
2017-07-11 16:34:18,359 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:18,359 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:18,361 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:18,361 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:18,409 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:18,410 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:18,410 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:18,411 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:18,412 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:18,412 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:18,412 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:18,412 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:18,413 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:18,423 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:18,424 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:18,424 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:18,425 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:18,432 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:18,561 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:18,561 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:18,562 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:18,562 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:18,570 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:18,581 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:18,581 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:18,582 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:18,582 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:18,586 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:18,586 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:18,587 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:18,588 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:18,589 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:18,592 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:18,592 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:18,593 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:18,593 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:18,600 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:18,600 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:18,601 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:18,688 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:18,768 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-11 16:34:18,800 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-11 16:34:19,011 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-11 16:34:19,056 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-11 16:34:19,107 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-11 16:34:19,262 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-11 16:34:19,262 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-11 16:34:19,265 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-11 16:34:19,368 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-11 16:34:19,456 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-07-11 16:34:19,462 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-11 16:34:19,477 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:19,480 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-11 16:34:19,481 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:19,538 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-11 16:34:19,540 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:19,562 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 57608
2017-07-11 16:34:19,563 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:19,605 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_57608_hdfs____.7n8oze/webapp
2017-07-11 16:34:20,058 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57608
2017-07-11 16:34:20,079 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:20,084 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:20,086 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:20,087 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:20,088 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:20,089 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:20
2017-07-11 16:34:20,089 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:20,096 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:20,097 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:20,097 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:20,108 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:20,108 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:20,109 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:20,109 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:20,109 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:20,109 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:20,110 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:20,110 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:20,110 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:20,111 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:20,111 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:20,111 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:20,112 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:20,112 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:20,113 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:20,113 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:20,114 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:20,114 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:20,119 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:20,120 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:20,120 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:20,120 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:20,121 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:20,122 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:20,122 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:20,123 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:20,123 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:20,123 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:20,123 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:20,124 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:20,124 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:20,125 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:20,127 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:20,127 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:20,127 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:20,162 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:20,173 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:20,178 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current
2017-07-11 16:34:20,180 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current
2017-07-11 16:34:20,181 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-11 16:34:20,212 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-11 16:34:20,223 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-11 16:34:20,225 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-11 16:34:20,231 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-11 16:34:20,232 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-11 16:34:20,280 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-11 16:34:20,281 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 153 msecs
2017-07-11 16:34:20,824 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-11 16:34:20,836 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:20,856 INFO  [Socket Reader #1 for port 38691] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 38691
2017-07-11 16:34:20,915 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:38691 to access this namenode/service.
2017-07-11 16:34:20,919 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-11 16:34:21,126 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:21,127 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:21,127 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-11 16:34:21,128 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 1 secs
2017-07-11 16:34:21,128 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-11 16:34:21,128 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-11 16:34:21,147 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-11 16:34:21,148 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-11 16:34:21,148 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-11 16:34:21,148 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-11 16:34:21,148 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-11 16:34:21,148 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 20 msec
2017-07-11 16:34:21,191 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:21,193 INFO  [IPC Server listener on 38691] ipc.Server (Server.java:run(674)) - IPC Server listener on 38691: starting
2017-07-11 16:34:21,196 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:38691
2017-07-11 16:34:21,197 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-11 16:34:21,205 INFO  [CacheReplicationMonitor(1138299928)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-11 16:34:21,205 INFO  [CacheReplicationMonitor(1138299928)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 451211403 milliseconds
2017-07-11 16:34:21,209 INFO  [CacheReplicationMonitor(1138299928)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2017-07-11 16:34:21,212 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2
2017-07-11 16:34:21,212 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-11 16:34:21,318 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-11 16:34:21,319 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-11 16:34:21,325 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-11 16:34:21,336 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:50212
2017-07-11 16:34:21,338 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-11 16:34:21,338 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-11 16:34:21,342 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-11 16:34:21,342 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:21,344 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-11 16:34:21,345 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:21,349 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:21,350 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 44131
2017-07-11 16:34:21,350 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:21,356 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_44131_datanode____.pmxbre/webapp
2017-07-11 16:34:21,513 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44131
2017-07-11 16:34:21,518 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-11 16:34:21,519 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-11 16:34:21,545 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:21,546 INFO  [Socket Reader #1 for port 49202] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 49202
2017-07-11 16:34:21,560 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:49202
2017-07-11 16:34:21,577 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-11 16:34:21,582 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-11 16:34:21,635 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38691 starting to offer service
2017-07-11 16:34:21,647 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:21,652 INFO  [IPC Server listener on 49202] ipc.Server (Server.java:run(674)) - IPC Server listener on 49202: starting
2017-07-11 16:34:22,291 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-11 16:34:22,306 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:22,309 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:22,309 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:22,339 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:22,339 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:22,340 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:22,452 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:22,453 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:22,454 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1934182244-172.17.0.3-1499783658616 is not formatted.
2017-07-11 16:34:22,455 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:22,455 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1934182244-172.17.0.3-1499783658616 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1934182244-172.17.0.3-1499783658616/current
2017-07-11 16:34:22,464 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:22,464 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1934182244-172.17.0.3-1499783658616 is not formatted.
2017-07-11 16:34:22,464 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:22,464 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1934182244-172.17.0.3-1499783658616 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1934182244-172.17.0.3-1499783658616/current
2017-07-11 16:34:22,479 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:22,480 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:22,511 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=518564983;bpid=BP-1934182244-172.17.0.3-1499783658616;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=518564983;c=0;bpid=BP-1934182244-172.17.0.3-1499783658616;dnuuid=null
2017-07-11 16:34:22,543 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID f33c6b86-1e46-4306-a420-069cc68854d4
2017-07-11 16:34:22,561 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:22,562 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:22,593 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-11 16:34:22,594 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-11 16:34:22,595 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-11 16:34:22,595 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-11 16:34:22,608 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-11 16:34:22,623 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499798341623 with interval 21600000
2017-07-11 16:34:22,624 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:22,625 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1934182244-172.17.0.3-1499783658616 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:22,632 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1934182244-172.17.0.3-1499783658616 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:22,665 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:22,666 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:22,716 INFO  [Thread-69] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1934182244-172.17.0.3-1499783658616 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 84ms
2017-07-11 16:34:22,716 INFO  [Thread-68] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1934182244-172.17.0.3-1499783658616 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 84ms
2017-07-11 16:34:22,718 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1934182244-172.17.0.3-1499783658616: 94ms
2017-07-11 16:34:22,719 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1934182244-172.17.0.3-1499783658616 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:22,721 INFO  [Thread-73] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1934182244-172.17.0.3-1499783658616 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 1ms
2017-07-11 16:34:22,719 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1934182244-172.17.0.3-1499783658616 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:22,722 INFO  [Thread-72] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1934182244-172.17.0.3-1499783658616 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-11 16:34:22,727 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 9ms
2017-07-11 16:34:22,731 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid null) service to localhost/127.0.0.1:38691 beginning handshake with NN
2017-07-11 16:34:22,747 INFO  [IPC Server handler 7 on 38691] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=f33c6b86-1e46-4306-a420-069cc68854d4, infoPort=44131, ipcPort=49202, storageInfo=lv=-56;cid=testClusterID;nsid=518564983;c=0) storage f33c6b86-1e46-4306-a420-069cc68854d4
2017-07-11 16:34:22,754 INFO  [IPC Server handler 7 on 38691] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:22,755 INFO  [IPC Server handler 7 on 38691] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:50212
2017-07-11 16:34:22,763 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid null) service to localhost/127.0.0.1:38691 successfully registered with NN
2017-07-11 16:34:22,764 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:38691 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-11 16:34:22,775 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2246)) - dn.getCapacity() == 0
2017-07-11 16:34:22,778 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:22,791 INFO  [IPC Server handler 5 on 38691] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:22,792 INFO  [IPC Server handler 5 on 38691] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-04840ad7-e009-4a02-af70-d06ca9d03cad for DN 127.0.0.1:50212
2017-07-11 16:34:22,793 INFO  [IPC Server handler 5 on 38691] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-9265d07b-f2d2-4fac-9b45-e3aaef082cb0 for DN 127.0.0.1:50212
2017-07-11 16:34:22,812 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid f33c6b86-1e46-4306-a420-069cc68854d4) service to localhost/127.0.0.1:38691 trying to claim ACTIVE state with txid=1
2017-07-11 16:34:22,813 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid f33c6b86-1e46-4306-a420-069cc68854d4) service to localhost/127.0.0.1:38691
2017-07-11 16:34:22,829 INFO  [IPC Server handler 6 on 38691] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-9265d07b-f2d2-4fac-9b45-e3aaef082cb0,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:22,830 INFO  [IPC Server handler 6 on 38691] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-9265d07b-f2d2-4fac-9b45-e3aaef082cb0 node DatanodeRegistration(127.0.0.1, datanodeUuid=f33c6b86-1e46-4306-a420-069cc68854d4, infoPort=44131, ipcPort=49202, storageInfo=lv=-56;cid=testClusterID;nsid=518564983;c=0), blocks: 0, hasStaleStorages: true, processing time: 3 msecs
2017-07-11 16:34:22,830 INFO  [IPC Server handler 6 on 38691] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-04840ad7-e009-4a02-af70-d06ca9d03cad,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:22,831 INFO  [IPC Server handler 6 on 38691] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-04840ad7-e009-4a02-af70-d06ca9d03cad node DatanodeRegistration(127.0.0.1, datanodeUuid=f33c6b86-1e46-4306-a420-069cc68854d4, infoPort=44131, ipcPort=49202, storageInfo=lv=-56;cid=testClusterID;nsid=518564983;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2017-07-11 16:34:22,870 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 3 msec to generate and 53 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@7968fc44
2017-07-11 16:34:22,872 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:22,879 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-11 16:34:22,879 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:22,881 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-11 16:34:22,881 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-11 16:34:22,885 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:22,893 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:22,897 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1934182244-172.17.0.3-1499783658616 to blockPoolScannerMap, new size=1
2017-07-11 16:34:22,907 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:22,957 INFO  [IPC Server handler 2 on 38691] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-11 16:34:22,981 INFO  [IPC Server handler 0 on 38691] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:23,020 INFO  [IPC Server handler 8 on 38691] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-1934182244-172.17.0.3-1499783658616 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-04840ad7-e009-4a02-af70-d06ca9d03cad:NORMAL:127.0.0.1:50212|RBW]]}
2017-07-11 16:34:23,111 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_1879982179_1 at /127.0.0.1:32934 [Receiving block BP-1934182244-172.17.0.3-1499783658616:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1934182244-172.17.0.3-1499783658616:blk_1073741825_1001 src: /127.0.0.1:32934 dest: /127.0.0.1:50212
2017-07-11 16:34:23,186 INFO  [PacketResponder: BP-1934182244-172.17.0.3-1499783658616:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:32934, dest: /127.0.0.1:50212, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1879982179_1, offset: 0, srvID: f33c6b86-1e46-4306-a420-069cc68854d4, blockid: BP-1934182244-172.17.0.3-1499783658616:blk_1073741825_1001, duration: 44772109
2017-07-11 16:34:23,186 INFO  [PacketResponder: BP-1934182244-172.17.0.3-1499783658616:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1934182244-172.17.0.3-1499783658616:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:23,201 INFO  [IPC Server handler 7 on 38691] namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(3691)) - BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-04840ad7-e009-4a02-af70-d06ca9d03cad:NORMAL:127.0.0.1:50212|RBW]]} has not reached minimal replication 1
2017-07-11 16:34:23,201 INFO  [IPC Server handler 7 on 38691] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-11 16:34:23,201 INFO  [IPC Server handler 7 on 38691] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-11 16:34:23,207 INFO  [IPC Server handler 1 on 38691] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50212 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-04840ad7-e009-4a02-af70-d06ca9d03cad:NORMAL:127.0.0.1:50212|RBW]]} size 1
2017-07-11 16:34:23,609 INFO  [IPC Server handler 5 on 38691] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_1879982179_1
2017-07-11 16:34:23,622 INFO  [IPC Server handler 6 on 38691] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-11 16:34:23,625 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-11 16:34:23,625 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-11 16:34:23,626 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-11 16:34:23,627 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37eeec90] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-11 16:34:23,639 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:23,740 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-11 16:34:23,740 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 49202
2017-07-11 16:34:23,743 INFO  [IPC Server listener on 49202] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 49202
2017-07-11 16:34:23,746 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:23,747 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid f33c6b86-1e46-4306-a420-069cc68854d4) service to localhost/127.0.0.1:38691 interrupted
2017-07-11 16:34:23,747 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid f33c6b86-1e46-4306-a420-069cc68854d4) service to localhost/127.0.0.1:38691
2017-07-11 16:34:23,748 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1934182244-172.17.0.3-1499783658616 (Datanode Uuid f33c6b86-1e46-4306-a420-069cc68854d4)
2017-07-11 16:34:23,749 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1934182244-172.17.0.3-1499783658616 from blockPoolScannerMap
2017-07-11 16:34:23,749 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38691] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1934182244-172.17.0.3-1499783658616
2017-07-11 16:34:23,753 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@58308794] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-11 16:34:23,754 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-11 16:34:23,754 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-11 16:34:23,755 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-11 16:34:23,755 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-11 16:34:23,756 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-11 16:34:23,757 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:23,757 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-11 16:34:23,760 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@6f80fafe] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-11 16:34:23,761 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@37f21974] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-11 16:34:23,762 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 2 2 
2017-07-11 16:34:23,769 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-11 16:34:23,770 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-11 16:34:23,779 INFO  [CacheReplicationMonitor(1138299928)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-11 16:34:23,782 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 38691
2017-07-11 16:34:23,785 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:23,786 INFO  [IPC Server listener on 38691] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 38691
2017-07-11 16:34:23,786 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@13518f37] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-11 16:34:23,787 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@165b8a71] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-11 16:34:23,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:23,824 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-11 16:34:23,832 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:23,933 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-11 16:34:23,936 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-11 16:34:23,937 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-11 16:34:23,992 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-11 16:34:24,002 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:24,003 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:24,005 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:24,005 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:24,006 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:24,007 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:24
2017-07-11 16:34:24,007 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:24,007 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,008 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:24,008 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:24,090 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:24,090 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:24,091 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:24,091 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:24,092 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:24,092 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:24,093 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:24,093 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:24,094 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:24,095 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:24,095 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:24,096 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:24,096 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:24,097 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:24,097 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:24,098 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,098 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:24,099 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:24,107 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:24,107 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:24,108 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,108 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:24,108 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:24,111 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:24,111 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:24,111 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:24,112 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:24,112 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:24,112 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:24,112 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,113 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:24,113 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:24,115 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:24,115 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:24,115 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:24,117 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:24,147 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-11 16:34:24,186 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-11 16:34:24,262 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-11 16:34:24,265 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-11 16:34:24,268 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-11 16:34:24,270 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-11 16:34:24,271 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-11 16:34:24,272 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-11 16:34:24,280 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-11 16:34:24,281 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-11 16:34:24,282 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:24,284 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-11 16:34:24,284 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:24,287 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-11 16:34:24,287 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:24,290 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 38385
2017-07-11 16:34:24,291 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:24,298 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_38385_hdfs____eyo80l/webapp
2017-07-11 16:34:24,500 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38385
2017-07-11 16:34:24,503 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:24,503 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:24,504 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:24,505 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:24,505 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:24,506 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:24
2017-07-11 16:34:24,506 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:24,507 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,507 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:24,508 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:24,539 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:24,540 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:24,540 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:24,540 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:24,541 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:24,541 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:24,541 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:24,541 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:24,542 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:24,542 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:24,543 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:24,543 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:24,543 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:24,544 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:24,544 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:24,545 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,545 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:24,546 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:24,570 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:24,570 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:24,571 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,571 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:24,572 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:24,781 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:24,782 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:24,782 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:24,782 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:24,783 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:24,783 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:24,784 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:24,784 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:24,784 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:24,786 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:24,787 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:24,787 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:24,798 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:24,815 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:24,818 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current
2017-07-11 16:34:24,819 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current
2017-07-11 16:34:24,820 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-11 16:34:24,822 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-11 16:34:24,823 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-11 16:34:24,823 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-11 16:34:24,823 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-11 16:34:24,824 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-11 16:34:24,855 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-11 16:34:24,856 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 69 msecs
2017-07-11 16:34:24,856 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-11 16:34:24,857 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:24,858 INFO  [Socket Reader #1 for port 56329] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 56329
2017-07-11 16:34:24,863 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:56329 to access this namenode/service.
2017-07-11 16:34:24,864 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-11 16:34:24,886 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:24,886 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:24,887 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-11 16:34:24,887 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-11 16:34:24,887 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-11 16:34:24,888 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-11 16:34:24,924 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:24,924 INFO  [IPC Server listener on 56329] ipc.Server (Server.java:run(674)) - IPC Server listener on 56329: starting
2017-07-11 16:34:24,926 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-11 16:34:24,929 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-11 16:34:24,930 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-11 16:34:24,930 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-11 16:34:24,930 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-11 16:34:24,930 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 43 msec
2017-07-11 16:34:24,934 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:56329
2017-07-11 16:34:24,935 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-11 16:34:24,938 INFO  [CacheReplicationMonitor(2128884627)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-11 16:34:24,938 INFO  [CacheReplicationMonitor(2128884627)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 451215137 milliseconds
2017-07-11 16:34:24,942 INFO  [CacheReplicationMonitor(2128884627)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2017-07-11 16:34:24,948 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2
2017-07-11 16:34:24,948 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-11 16:34:24,979 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-11 16:34:24,980 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-11 16:34:24,980 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-11 16:34:24,982 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:40025
2017-07-11 16:34:24,982 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-11 16:34:24,983 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-11 16:34:24,984 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-11 16:34:24,985 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:24,986 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-11 16:34:24,987 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:24,988 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:24,989 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 57184
2017-07-11 16:34:24,989 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:24,996 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_57184_datanode____.dx00pa/webapp
2017-07-11 16:34:25,184 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57184
2017-07-11 16:34:25,186 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-11 16:34:25,186 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-11 16:34:25,187 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:25,189 INFO  [Socket Reader #1 for port 51166] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 51166
2017-07-11 16:34:25,192 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:51166
2017-07-11 16:34:25,197 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-11 16:34:25,197 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-11 16:34:25,199 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:56329 starting to offer service
2017-07-11 16:34:25,199 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:25,201 INFO  [IPC Server listener on 51166] ipc.Server (Server.java:run(674)) - IPC Server listener on 51166: starting
2017-07-11 16:34:25,221 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-11 16:34:25,223 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:25,223 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:25,234 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:25,234 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,234 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:25,268 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:25,269 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,269 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:25,331 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:25,332 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:25,383 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,384 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:25,385 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-511863389-172.17.0.3-1499783664117 is not formatted.
2017-07-11 16:34:25,385 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:25,386 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-511863389-172.17.0.3-1499783664117 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-511863389-172.17.0.3-1499783664117/current
2017-07-11 16:34:25,396 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:25,398 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-511863389-172.17.0.3-1499783664117 is not formatted.
2017-07-11 16:34:25,399 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:25,400 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-511863389-172.17.0.3-1499783664117 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-511863389-172.17.0.3-1499783664117/current
2017-07-11 16:34:25,413 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:25,413 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:25,435 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:25,436 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:25,437 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1925463789;bpid=BP-511863389-172.17.0.3-1499783664117;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1925463789;c=0;bpid=BP-511863389-172.17.0.3-1499783664117;dnuuid=null
2017-07-11 16:34:25,476 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 2e6dafe2-61b3-48e6-88d5-71040ef921d4
2017-07-11 16:34:25,477 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-11 16:34:25,477 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-11 16:34:25,478 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-11 16:34:25,478 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-11 16:34:25,484 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-11 16:34:25,485 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499787197485 with interval 21600000
2017-07-11 16:34:25,486 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,487 INFO  [Thread-142] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-511863389-172.17.0.3-1499783664117 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:25,492 INFO  [Thread-143] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-511863389-172.17.0.3-1499783664117 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:25,499 INFO  [Thread-142] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-511863389-172.17.0.3-1499783664117 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 9ms
2017-07-11 16:34:25,502 INFO  [Thread-143] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-511863389-172.17.0.3-1499783664117 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 10ms
2017-07-11 16:34:25,502 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-511863389-172.17.0.3-1499783664117: 15ms
2017-07-11 16:34:25,503 INFO  [Thread-146] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-511863389-172.17.0.3-1499783664117 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:25,503 INFO  [Thread-147] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-511863389-172.17.0.3-1499783664117 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:25,504 INFO  [Thread-146] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-511863389-172.17.0.3-1499783664117 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 1ms
2017-07-11 16:34:25,505 INFO  [Thread-147] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-511863389-172.17.0.3-1499783664117 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 1ms
2017-07-11 16:34:25,505 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 3ms
2017-07-11 16:34:25,506 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid null) service to localhost/127.0.0.1:56329 beginning handshake with NN
2017-07-11 16:34:25,508 INFO  [IPC Server handler 4 on 56329] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=2e6dafe2-61b3-48e6-88d5-71040ef921d4, infoPort=57184, ipcPort=51166, storageInfo=lv=-56;cid=testClusterID;nsid=1925463789;c=0) storage 2e6dafe2-61b3-48e6-88d5-71040ef921d4
2017-07-11 16:34:25,509 INFO  [IPC Server handler 4 on 56329] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:25,509 INFO  [IPC Server handler 4 on 56329] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:40025
2017-07-11 16:34:25,511 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid null) service to localhost/127.0.0.1:56329 successfully registered with NN
2017-07-11 16:34:25,511 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:56329 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-11 16:34:25,514 INFO  [IPC Server handler 5 on 56329] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:25,514 INFO  [IPC Server handler 5 on 56329] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-162edbc0-c18a-4843-9b95-06fa2fe50cc7 for DN 127.0.0.1:40025
2017-07-11 16:34:25,515 INFO  [IPC Server handler 5 on 56329] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-09064f6e-7595-45b8-988a-d7056080861c for DN 127.0.0.1:40025
2017-07-11 16:34:25,516 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid 2e6dafe2-61b3-48e6-88d5-71040ef921d4) service to localhost/127.0.0.1:56329 trying to claim ACTIVE state with txid=1
2017-07-11 16:34:25,517 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid 2e6dafe2-61b3-48e6-88d5-71040ef921d4) service to localhost/127.0.0.1:56329
2017-07-11 16:34:25,519 INFO  [IPC Server handler 7 on 56329] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-09064f6e-7595-45b8-988a-d7056080861c,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:25,520 INFO  [IPC Server handler 7 on 56329] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-09064f6e-7595-45b8-988a-d7056080861c node DatanodeRegistration(127.0.0.1, datanodeUuid=2e6dafe2-61b3-48e6-88d5-71040ef921d4, infoPort=57184, ipcPort=51166, storageInfo=lv=-56;cid=testClusterID;nsid=1925463789;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-11 16:34:25,520 INFO  [IPC Server handler 7 on 56329] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-162edbc0-c18a-4843-9b95-06fa2fe50cc7,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:25,520 INFO  [IPC Server handler 7 on 56329] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-162edbc0-c18a-4843-9b95-06fa2fe50cc7 node DatanodeRegistration(127.0.0.1, datanodeUuid=2e6dafe2-61b3-48e6-88d5-71040ef921d4, infoPort=57184, ipcPort=51166, storageInfo=lv=-56;cid=testClusterID;nsid=1925463789;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-11 16:34:25,522 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 5 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@44cca970
2017-07-11 16:34:25,523 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,524 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-11 16:34:25,524 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:25,525 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-11 16:34:25,525 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-11 16:34:25,529 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,530 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-511863389-172.17.0.3-1499783664117 to blockPoolScannerMap, new size=1
2017-07-11 16:34:25,540 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:25,548 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:25,556 INFO  [IPC Server handler 9 on 56329] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-11 16:34:25,562 INFO  [IPC Server handler 0 on 56329] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:25,576 INFO  [IPC Server handler 1 on 56329] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-511863389-172.17.0.3-1499783664117 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-162edbc0-c18a-4843-9b95-06fa2fe50cc7:NORMAL:127.0.0.1:40025|RBW]]}
2017-07-11 16:34:25,583 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1393440627_1 at /127.0.0.1:52077 [Receiving block BP-511863389-172.17.0.3-1499783664117:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-511863389-172.17.0.3-1499783664117:blk_1073741825_1001 src: /127.0.0.1:52077 dest: /127.0.0.1:40025
2017-07-11 16:34:25,601 INFO  [PacketResponder: BP-511863389-172.17.0.3-1499783664117:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:52077, dest: /127.0.0.1:40025, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1393440627_1, offset: 0, srvID: 2e6dafe2-61b3-48e6-88d5-71040ef921d4, blockid: BP-511863389-172.17.0.3-1499783664117:blk_1073741825_1001, duration: 6186575
2017-07-11 16:34:25,603 INFO  [PacketResponder: BP-511863389-172.17.0.3-1499783664117:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-511863389-172.17.0.3-1499783664117:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:25,607 INFO  [IPC Server handler 3 on 56329] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:40025 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-162edbc0-c18a-4843-9b95-06fa2fe50cc7:NORMAL:127.0.0.1:40025|RBW]]} size 0
2017-07-11 16:34:25,610 INFO  [IPC Server handler 4 on 56329] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-1393440627_1
2017-07-11 16:34:25,614 INFO  [IPC Server handler 5 on 56329] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-11 16:34:25,616 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-11 16:34:25,616 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-11 16:34:25,616 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-11 16:34:25,616 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@aa004a0] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-11 16:34:25,629 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:25,731 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-11 16:34:25,731 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 51166
2017-07-11 16:34:25,734 INFO  [IPC Server listener on 51166] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 51166
2017-07-11 16:34:25,736 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:25,736 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid 2e6dafe2-61b3-48e6-88d5-71040ef921d4) service to localhost/127.0.0.1:56329 interrupted
2017-07-11 16:34:25,737 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid 2e6dafe2-61b3-48e6-88d5-71040ef921d4) service to localhost/127.0.0.1:56329
2017-07-11 16:34:25,737 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-511863389-172.17.0.3-1499783664117 (Datanode Uuid 2e6dafe2-61b3-48e6-88d5-71040ef921d4)
2017-07-11 16:34:25,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-511863389-172.17.0.3-1499783664117 from blockPoolScannerMap
2017-07-11 16:34:25,740 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:56329] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-511863389-172.17.0.3-1499783664117
2017-07-11 16:34:25,743 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@66b88576] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-11 16:34:25,743 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-11 16:34:25,744 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-11 16:34:25,745 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-11 16:34:25,746 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-11 16:34:25,746 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-11 16:34:25,747 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:25,747 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@d176a31] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-11 16:34:25,748 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@3a91d146] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-11 16:34:25,748 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-11 16:34:25,749 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 8 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 2 2 
2017-07-11 16:34:25,750 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000008
2017-07-11 16:34:25,751 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000008
2017-07-11 16:34:25,752 INFO  [CacheReplicationMonitor(2128884627)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-11 16:34:25,753 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 56329
2017-07-11 16:34:25,755 INFO  [IPC Server listener on 56329] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 56329
2017-07-11 16:34:25,761 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:25,762 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@659925f4] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-11 16:34:25,762 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@1bc425e7] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-11 16:34:25,783 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:25,790 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-11 16:34:25,809 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:25,913 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-11 16:34:25,917 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-11 16:34:25,919 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-11 16:34:25,965 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-11 16:34:25,972 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:25,973 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:25,974 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:25,974 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:25,975 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:25,975 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:25
2017-07-11 16:34:25,976 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:25,976 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:25,976 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:25,977 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:25,991 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:25,992 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:25,992 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:25,992 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:25,992 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:25,993 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:25,993 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:25,993 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:25,993 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:25,994 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:25,994 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:25,994 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:25,995 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:25,995 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:25,996 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:25,996 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:25,996 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:25,997 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:26,005 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:26,006 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:26,006 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:26,007 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:26,007 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:26,009 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:26,009 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:26,010 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:26,010 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:26,010 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:26,010 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:26,011 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:26,011 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:26,011 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:26,013 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:26,013 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:26,013 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:26,015 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:26,040 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-11 16:34:26,084 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-11 16:34:26,161 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-11 16:34:26,164 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-11 16:34:26,167 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-11 16:34:26,169 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-11 16:34:26,169 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-11 16:34:26,170 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-11 16:34:26,175 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-11 16:34:26,176 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-11 16:34:26,177 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:26,178 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-11 16:34:26,179 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:26,181 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-11 16:34:26,182 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:26,185 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 56360
2017-07-11 16:34:26,185 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:26,193 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_56360_hdfs____evxhg6/webapp
2017-07-11 16:34:26,370 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56360
2017-07-11 16:34:26,373 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:26,374 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:26,375 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:26,375 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:26,376 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:26,377 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:26
2017-07-11 16:34:26,378 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:26,378 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:26,379 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:26,379 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:26,462 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:26,463 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:26,463 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:26,464 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:26,464 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:26,464 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:26,465 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:26,465 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:26,466 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:26,466 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:26,467 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:26,467 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:26,468 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:26,468 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:26,469 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:26,469 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:26,470 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:26,470 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:26,478 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:26,479 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:26,479 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:26,481 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:26,481 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:26,483 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:26,484 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:26,484 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:26,484 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:26,484 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:26,485 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:26,485 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:26,486 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:26,486 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:26,487 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:26,488 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:26,488 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:26,501 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:26,508 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:26,512 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current
2017-07-11 16:34:26,512 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current
2017-07-11 16:34:26,514 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-11 16:34:26,516 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-11 16:34:26,517 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-11 16:34:26,518 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-11 16:34:26,518 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-11 16:34:26,519 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-11 16:34:26,543 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-11 16:34:26,544 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 56 msecs
2017-07-11 16:34:26,545 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-11 16:34:26,546 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:26,548 INFO  [Socket Reader #1 for port 41747] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 41747
2017-07-11 16:34:26,553 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:41747 to access this namenode/service.
2017-07-11 16:34:26,566 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-11 16:34:26,585 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:26,586 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:26,586 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-11 16:34:26,587 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-11 16:34:26,587 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-11 16:34:26,587 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-11 16:34:26,617 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-11 16:34:26,618 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-11 16:34:26,618 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-11 16:34:26,618 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-11 16:34:26,618 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-11 16:34:26,620 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 33 msec
2017-07-11 16:34:26,624 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:26,624 INFO  [IPC Server listener on 41747] ipc.Server (Server.java:run(674)) - IPC Server listener on 41747: starting
2017-07-11 16:34:26,634 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:41747
2017-07-11 16:34:26,635 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-11 16:34:26,637 INFO  [CacheReplicationMonitor(706952232)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-11 16:34:26,637 INFO  [CacheReplicationMonitor(706952232)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 451216835 milliseconds
2017-07-11 16:34:26,641 INFO  [CacheReplicationMonitor(706952232)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2017-07-11 16:34:26,647 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2
2017-07-11 16:34:26,648 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-11 16:34:26,676 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-11 16:34:26,677 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-11 16:34:26,677 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-11 16:34:26,680 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:41009
2017-07-11 16:34:26,680 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-11 16:34:26,681 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-11 16:34:26,683 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-11 16:34:26,684 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:26,685 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-11 16:34:26,686 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:26,688 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:26,688 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 50805
2017-07-11 16:34:26,689 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:26,702 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_50805_datanode____.2djrrr/webapp
2017-07-11 16:34:26,931 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:50805
2017-07-11 16:34:26,934 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-11 16:34:26,935 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-11 16:34:26,936 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:26,937 INFO  [Socket Reader #1 for port 39858] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 39858
2017-07-11 16:34:26,945 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:39858
2017-07-11 16:34:26,949 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-11 16:34:26,949 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-11 16:34:26,951 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:41747 starting to offer service
2017-07-11 16:34:26,954 INFO  [IPC Server listener on 39858] ipc.Server (Server.java:run(674)) - IPC Server listener on 39858: starting
2017-07-11 16:34:26,954 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:26,982 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:26,982 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:26,984 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-11 16:34:27,002 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:27,003 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,004 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:27,027 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:27,029 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,029 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:27,086 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:27,089 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:27,123 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,123 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:27,124 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-365712166-172.17.0.3-1499783666015 is not formatted.
2017-07-11 16:34:27,124 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:27,124 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-365712166-172.17.0.3-1499783666015 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-365712166-172.17.0.3-1499783666015/current
2017-07-11 16:34:27,137 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:27,138 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-365712166-172.17.0.3-1499783666015 is not formatted.
2017-07-11 16:34:27,138 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:27,138 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-365712166-172.17.0.3-1499783666015 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-365712166-172.17.0.3-1499783666015/current
2017-07-11 16:34:27,145 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:27,145 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:27,175 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1885612167;bpid=BP-365712166-172.17.0.3-1499783666015;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1885612167;c=0;bpid=BP-365712166-172.17.0.3-1499783666015;dnuuid=null
2017-07-11 16:34:27,192 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:27,193 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:27,207 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 4b22dd25-0809-40ac-b4ec-5cf9714d5a87
2017-07-11 16:34:27,209 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-11 16:34:27,209 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-11 16:34:27,210 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-11 16:34:27,210 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-11 16:34:27,212 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-11 16:34:27,213 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499791351213 with interval 21600000
2017-07-11 16:34:27,213 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,214 INFO  [Thread-216] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-365712166-172.17.0.3-1499783666015 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:27,214 INFO  [Thread-217] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-365712166-172.17.0.3-1499783666015 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:27,224 INFO  [Thread-216] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-365712166-172.17.0.3-1499783666015 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 10ms
2017-07-11 16:34:27,224 INFO  [Thread-217] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-365712166-172.17.0.3-1499783666015 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 10ms
2017-07-11 16:34:27,224 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-365712166-172.17.0.3-1499783666015: 10ms
2017-07-11 16:34:27,225 INFO  [Thread-220] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-365712166-172.17.0.3-1499783666015 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:27,225 INFO  [Thread-220] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-365712166-172.17.0.3-1499783666015 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 1ms
2017-07-11 16:34:27,231 INFO  [Thread-221] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-365712166-172.17.0.3-1499783666015 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:27,231 INFO  [Thread-221] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-365712166-172.17.0.3-1499783666015 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 1ms
2017-07-11 16:34:27,232 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 7ms
2017-07-11 16:34:27,234 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid null) service to localhost/127.0.0.1:41747 beginning handshake with NN
2017-07-11 16:34:27,236 INFO  [IPC Server handler 5 on 41747] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=4b22dd25-0809-40ac-b4ec-5cf9714d5a87, infoPort=50805, ipcPort=39858, storageInfo=lv=-56;cid=testClusterID;nsid=1885612167;c=0) storage 4b22dd25-0809-40ac-b4ec-5cf9714d5a87
2017-07-11 16:34:27,236 INFO  [IPC Server handler 5 on 41747] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:27,237 INFO  [IPC Server handler 5 on 41747] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:41009
2017-07-11 16:34:27,239 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid null) service to localhost/127.0.0.1:41747 successfully registered with NN
2017-07-11 16:34:27,240 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:41747 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-11 16:34:27,242 INFO  [IPC Server handler 6 on 41747] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:27,243 INFO  [IPC Server handler 6 on 41747] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-00f7c500-e06a-45d6-a639-a05f8d6f9993 for DN 127.0.0.1:41009
2017-07-11 16:34:27,243 INFO  [IPC Server handler 6 on 41747] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-c2460802-0d46-446e-8715-14418c4392e3 for DN 127.0.0.1:41009
2017-07-11 16:34:27,244 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid 4b22dd25-0809-40ac-b4ec-5cf9714d5a87) service to localhost/127.0.0.1:41747 trying to claim ACTIVE state with txid=1
2017-07-11 16:34:27,245 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid 4b22dd25-0809-40ac-b4ec-5cf9714d5a87) service to localhost/127.0.0.1:41747
2017-07-11 16:34:27,248 INFO  [IPC Server handler 7 on 41747] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-c2460802-0d46-446e-8715-14418c4392e3,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:27,249 INFO  [IPC Server handler 7 on 41747] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-c2460802-0d46-446e-8715-14418c4392e3 node DatanodeRegistration(127.0.0.1, datanodeUuid=4b22dd25-0809-40ac-b4ec-5cf9714d5a87, infoPort=50805, ipcPort=39858, storageInfo=lv=-56;cid=testClusterID;nsid=1885612167;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-11 16:34:27,249 INFO  [IPC Server handler 7 on 41747] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-00f7c500-e06a-45d6-a639-a05f8d6f9993,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:27,249 INFO  [IPC Server handler 7 on 41747] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-00f7c500-e06a-45d6-a639-a05f8d6f9993 node DatanodeRegistration(127.0.0.1, datanodeUuid=4b22dd25-0809-40ac-b4ec-5cf9714d5a87, infoPort=50805, ipcPort=39858, storageInfo=lv=-56;cid=testClusterID;nsid=1885612167;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-11 16:34:27,251 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 5 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5f53c9a0
2017-07-11 16:34:27,251 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,252 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-11 16:34:27,252 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:27,253 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-11 16:34:27,253 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-11 16:34:27,259 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,260 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-365712166-172.17.0.3-1499783666015 to blockPoolScannerMap, new size=1
2017-07-11 16:34:27,297 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:27,304 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:27,311 INFO  [IPC Server handler 1 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-11 16:34:27,318 INFO  [IPC Server handler 2 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:27,323 INFO  [IPC Server handler 0 on 41747] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-365712166-172.17.0.3-1499783666015 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|RBW]]}
2017-07-11 16:34:27,329 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1635820394_1 at /127.0.0.1:35247 [Receiving block BP-365712166-172.17.0.3-1499783666015:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-365712166-172.17.0.3-1499783666015:blk_1073741825_1001 src: /127.0.0.1:35247 dest: /127.0.0.1:41009
2017-07-11 16:34:27,338 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:35247, dest: /127.0.0.1:41009, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1635820394_1, offset: 0, srvID: 4b22dd25-0809-40ac-b4ec-5cf9714d5a87, blockid: BP-365712166-172.17.0.3-1499783666015:blk_1073741825_1001, duration: 3092319
2017-07-11 16:34:27,338 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:27,342 INFO  [IPC Server handler 4 on 41747] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:41009 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00f7c500-e06a-45d6-a639-a05f8d6f9993:NORMAL:127.0.0.1:41009|FINALIZED]]} size 0
2017-07-11 16:34:27,345 INFO  [IPC Server handler 5 on 41747] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_-1635820394_1
2017-07-11 16:34:27,351 INFO  [IPC Server handler 6 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:27,357 INFO  [IPC Server handler 7 on 41747] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-365712166-172.17.0.3-1499783666015 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|RBW]]}
2017-07-11 16:34:27,361 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1635820394_1 at /127.0.0.1:35248 [Receiving block BP-365712166-172.17.0.3-1499783666015:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-365712166-172.17.0.3-1499783666015:blk_1073741826_1002 src: /127.0.0.1:35248 dest: /127.0.0.1:41009
2017-07-11 16:34:27,370 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:35248, dest: /127.0.0.1:41009, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1635820394_1, offset: 0, srvID: 4b22dd25-0809-40ac-b4ec-5cf9714d5a87, blockid: BP-365712166-172.17.0.3-1499783666015:blk_1073741826_1002, duration: 2845195
2017-07-11 16:34:27,370 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:27,372 INFO  [IPC Server handler 8 on 41747] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:41009 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|RBW]]} size 0
2017-07-11 16:34:27,376 INFO  [IPC Server handler 9 on 41747] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_-1635820394_1
2017-07-11 16:34:27,380 INFO  [IPC Server handler 1 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:27,384 INFO  [IPC Server handler 2 on 41747] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-365712166-172.17.0.3-1499783666015 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|RBW]]}
2017-07-11 16:34:27,389 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1635820394_1 at /127.0.0.1:35249 [Receiving block BP-365712166-172.17.0.3-1499783666015:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-365712166-172.17.0.3-1499783666015:blk_1073741827_1003 src: /127.0.0.1:35249 dest: /127.0.0.1:41009
2017-07-11 16:34:27,397 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:35249, dest: /127.0.0.1:41009, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1635820394_1, offset: 0, srvID: 4b22dd25-0809-40ac-b4ec-5cf9714d5a87, blockid: BP-365712166-172.17.0.3-1499783666015:blk_1073741827_1003, duration: 5206688
2017-07-11 16:34:27,398 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:27,400 INFO  [IPC Server handler 0 on 41747] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:41009 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00f7c500-e06a-45d6-a639-a05f8d6f9993:NORMAL:127.0.0.1:41009|FINALIZED]]} size 0
2017-07-11 16:34:27,402 INFO  [IPC Server handler 3 on 41747] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_-1635820394_1
2017-07-11 16:34:27,406 INFO  [IPC Server handler 4 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:27,409 INFO  [IPC Server handler 5 on 41747] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-365712166-172.17.0.3-1499783666015 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|RBW]]}
2017-07-11 16:34:27,418 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1635820394_1 at /127.0.0.1:35251 [Receiving block BP-365712166-172.17.0.3-1499783666015:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-365712166-172.17.0.3-1499783666015:blk_1073741828_1004 src: /127.0.0.1:35251 dest: /127.0.0.1:41009
2017-07-11 16:34:27,427 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:35251, dest: /127.0.0.1:41009, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1635820394_1, offset: 0, srvID: 4b22dd25-0809-40ac-b4ec-5cf9714d5a87, blockid: BP-365712166-172.17.0.3-1499783666015:blk_1073741828_1004, duration: 4543655
2017-07-11 16:34:27,428 INFO  [IPC Server handler 6 on 41747] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:41009 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|RBW]]} size 0
2017-07-11 16:34:27,428 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:27,432 INFO  [IPC Server handler 7 on 41747] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_-1635820394_1
2017-07-11 16:34:27,434 INFO  [IPC Server handler 8 on 41747] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-11 16:34:27,434 INFO  [IPC Server handler 8 on 41747] namenode.EditLogFileOutputStream (EditLogFileOutputStream.java:flushAndSync(200)) - Nothing to flush
2017-07-11 16:34:27,435 INFO  [IPC Server handler 8 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-11 16:34:27,439 INFO  [IPC Server handler 9 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:27,445 INFO  [IPC Server handler 1 on 41747] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-365712166-172.17.0.3-1499783666015 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00f7c500-e06a-45d6-a639-a05f8d6f9993:NORMAL:127.0.0.1:41009|RBW]]}
2017-07-11 16:34:27,456 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1635820394_1 at /127.0.0.1:35253 [Receiving block BP-365712166-172.17.0.3-1499783666015:blk_1073741829_1005]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-365712166-172.17.0.3-1499783666015:blk_1073741829_1005 src: /127.0.0.1:35253 dest: /127.0.0.1:41009
2017-07-11 16:34:27,471 INFO  [IPC Server handler 2 on 41747] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:41009 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00f7c500-e06a-45d6-a639-a05f8d6f9993:NORMAL:127.0.0.1:41009|RBW]]} size 0
2017-07-11 16:34:27,472 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:35253, dest: /127.0.0.1:41009, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1635820394_1, offset: 0, srvID: 4b22dd25-0809-40ac-b4ec-5cf9714d5a87, blockid: BP-365712166-172.17.0.3-1499783666015:blk_1073741829_1005, duration: 2740601
2017-07-11 16:34:27,473 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:27,479 INFO  [IPC Server handler 0 on 41747] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_-1635820394_1
2017-07-11 16:34:27,483 INFO  [IPC Server handler 3 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:27,490 INFO  [IPC Server handler 4 on 41747] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-365712166-172.17.0.3-1499783666015 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00f7c500-e06a-45d6-a639-a05f8d6f9993:NORMAL:127.0.0.1:41009|RBW]]}
2017-07-11 16:34:27,494 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_-1635820394_1 at /127.0.0.1:35254 [Receiving block BP-365712166-172.17.0.3-1499783666015:blk_1073741830_1006]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-365712166-172.17.0.3-1499783666015:blk_1073741830_1006 src: /127.0.0.1:35254 dest: /127.0.0.1:41009
2017-07-11 16:34:27,513 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:35254, dest: /127.0.0.1:41009, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1635820394_1, offset: 0, srvID: 4b22dd25-0809-40ac-b4ec-5cf9714d5a87, blockid: BP-365712166-172.17.0.3-1499783666015:blk_1073741830_1006, duration: 10128371
2017-07-11 16:34:27,514 INFO  [PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-365712166-172.17.0.3-1499783666015:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:27,515 INFO  [IPC Server handler 5 on 41747] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:41009 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-c2460802-0d46-446e-8715-14418c4392e3:NORMAL:127.0.0.1:41009|FINALIZED]]} size 0
2017-07-11 16:34:27,520 INFO  [IPC Server handler 6 on 41747] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_-1635820394_1
2017-07-11 16:34:27,523 INFO  [IPC Server handler 7 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-11 16:34:27,537 INFO  [IPC Server handler 8 on 41747] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-11 16:34:27,545 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-11 16:34:27,545 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-11 16:34:27,546 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-11 16:34:27,546 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@7ce9e05a] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-11 16:34:27,555 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:27,656 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-11 16:34:27,657 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 39858
2017-07-11 16:34:27,660 INFO  [IPC Server listener on 39858] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 39858
2017-07-11 16:34:27,660 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid 4b22dd25-0809-40ac-b4ec-5cf9714d5a87) service to localhost/127.0.0.1:41747 interrupted
2017-07-11 16:34:27,660 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid 4b22dd25-0809-40ac-b4ec-5cf9714d5a87) service to localhost/127.0.0.1:41747
2017-07-11 16:34:27,661 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-365712166-172.17.0.3-1499783666015 (Datanode Uuid 4b22dd25-0809-40ac-b4ec-5cf9714d5a87)
2017-07-11 16:34:27,662 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:27,662 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-365712166-172.17.0.3-1499783666015 from blockPoolScannerMap
2017-07-11 16:34:27,663 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41747] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-365712166-172.17.0.3-1499783666015
2017-07-11 16:34:27,666 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@6b563f50] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-11 16:34:27,666 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-11 16:34:27,667 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-11 16:34:27,668 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-11 16:34:27,668 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-11 16:34:27,669 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-11 16:34:27,669 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:27,671 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-11 16:34:27,671 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@7a231dfd] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-11 16:34:27,671 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@30814f43] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-11 16:34:27,672 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 33 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 23 SyncTimes(ms): 2 4 
2017-07-11 16:34:27,673 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000033
2017-07-11 16:34:27,675 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000033
2017-07-11 16:34:27,675 INFO  [CacheReplicationMonitor(706952232)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-11 16:34:27,677 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 41747
2017-07-11 16:34:27,679 INFO  [IPC Server listener on 41747] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 41747
2017-07-11 16:34:27,679 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:27,680 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@33b082c5] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-11 16:34:27,680 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@bc4d5e1] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-11 16:34:27,700 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:27,701 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-11 16:34:27,711 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:27,812 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-11 16:34:27,815 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-11 16:34:27,816 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-11 16:34:27,860 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-11 16:34:27,869 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:27,870 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:27,872 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:27,872 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:27,873 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:27,873 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:27
2017-07-11 16:34:27,874 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:27,874 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:27,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:27,875 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:28,050 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:28,050 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:28,051 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:28,051 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:28,051 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:28,051 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:28,052 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:28,052 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:28,052 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:28,053 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:28,053 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:28,053 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:28,054 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:28,054 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:28,055 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:28,055 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,056 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:28,056 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:28,064 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:28,064 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:28,064 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,065 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:28,065 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:28,068 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:28,068 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:28,068 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:28,068 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:28,069 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:28,069 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:28,069 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,070 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:28,070 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:28,071 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:28,072 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:28,072 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:28,074 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:28,096 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-11 16:34:28,120 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-11 16:34:28,204 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-11 16:34:28,207 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-11 16:34:28,209 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-11 16:34:28,210 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-11 16:34:28,211 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-11 16:34:28,212 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-11 16:34:28,217 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-11 16:34:28,217 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-11 16:34:28,218 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:28,219 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-11 16:34:28,220 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:28,220 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-11 16:34:28,221 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:28,223 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 39060
2017-07-11 16:34:28,223 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:28,230 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_39060_hdfs____.lwsjv6/webapp
2017-07-11 16:34:28,416 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39060
2017-07-11 16:34:28,419 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:28,420 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:28,422 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:28,422 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:28,423 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:28,424 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:28
2017-07-11 16:34:28,425 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:28,425 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,425 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:28,426 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:28,441 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:28,442 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:28,442 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:28,442 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:28,443 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:28,443 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:28,443 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:28,443 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:28,444 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:28,444 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:28,444 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:28,445 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:28,445 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:28,446 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:28,446 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:28,447 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,447 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:28,447 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:28,457 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:28,457 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:28,458 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,458 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:28,459 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:28,462 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:28,462 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:28,463 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:28,463 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:28,463 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:28,464 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:28,464 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:28,465 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:28,466 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:28,468 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:28,468 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:28,468 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:28,482 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:28,499 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:28,503 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current
2017-07-11 16:34:28,503 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current
2017-07-11 16:34:28,504 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-11 16:34:28,506 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-11 16:34:28,507 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-11 16:34:28,507 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-11 16:34:28,508 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-11 16:34:28,509 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-11 16:34:28,539 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-11 16:34:28,540 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 71 msecs
2017-07-11 16:34:28,546 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-11 16:34:28,547 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:28,549 INFO  [Socket Reader #1 for port 40806] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 40806
2017-07-11 16:34:28,555 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:40806 to access this namenode/service.
2017-07-11 16:34:28,556 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-11 16:34:28,580 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:28,580 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:28,581 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-11 16:34:28,581 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-11 16:34:28,582 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-11 16:34:28,582 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-11 16:34:28,664 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-11 16:34:28,664 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-11 16:34:28,664 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-11 16:34:28,664 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-11 16:34:28,664 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-11 16:34:28,665 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 84 msec
2017-07-11 16:34:28,674 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:28,674 INFO  [IPC Server listener on 40806] ipc.Server (Server.java:run(674)) - IPC Server listener on 40806: starting
2017-07-11 16:34:28,687 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:40806
2017-07-11 16:34:28,688 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-11 16:34:28,707 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2
2017-07-11 16:34:28,708 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-11 16:34:28,709 INFO  [CacheReplicationMonitor(177944332)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-11 16:34:28,713 INFO  [CacheReplicationMonitor(177944332)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 451218911 milliseconds
2017-07-11 16:34:28,717 INFO  [CacheReplicationMonitor(177944332)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2017-07-11 16:34:28,744 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-11 16:34:28,745 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-11 16:34:28,746 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-11 16:34:28,747 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:60846
2017-07-11 16:34:28,748 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-11 16:34:28,748 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-11 16:34:28,749 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-11 16:34:28,751 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:28,751 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-11 16:34:28,752 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:28,753 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:28,754 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 55315
2017-07-11 16:34:28,754 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:28,761 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_55315_datanode____zh7l26/webapp
2017-07-11 16:34:28,945 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55315
2017-07-11 16:34:28,955 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-11 16:34:28,955 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-11 16:34:28,956 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:28,957 INFO  [Socket Reader #1 for port 33272] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 33272
2017-07-11 16:34:28,967 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:33272
2017-07-11 16:34:28,970 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-11 16:34:28,971 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-11 16:34:28,973 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40806 starting to offer service
2017-07-11 16:34:28,974 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:28,974 INFO  [IPC Server listener on 33272] ipc.Server (Server.java:run(674)) - IPC Server listener on 33272: starting
2017-07-11 16:34:29,048 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:29,049 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:29,058 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-11 16:34:29,071 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:29,071 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,071 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:29,110 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:29,111 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,111 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:29,154 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:29,156 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:29,203 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,204 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:29,204 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1109933422-172.17.0.3-1499783668074 is not formatted.
2017-07-11 16:34:29,204 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:29,204 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1109933422-172.17.0.3-1499783668074 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-1109933422-172.17.0.3-1499783668074/current
2017-07-11 16:34:29,224 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:29,225 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1109933422-172.17.0.3-1499783668074 is not formatted.
2017-07-11 16:34:29,225 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:29,225 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-1109933422-172.17.0.3-1499783668074 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-1109933422-172.17.0.3-1499783668074/current
2017-07-11 16:34:29,239 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:29,240 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:29,258 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:29,259 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:29,271 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1601684733;bpid=BP-1109933422-172.17.0.3-1499783668074;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1601684733;c=0;bpid=BP-1109933422-172.17.0.3-1499783668074;dnuuid=null
2017-07-11 16:34:29,303 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 70b34de5-a089-4be4-bcff-7e283853258c
2017-07-11 16:34:29,304 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-11 16:34:29,304 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-11 16:34:29,304 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-11 16:34:29,304 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-11 16:34:29,306 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-11 16:34:29,314 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499790505314 with interval 21600000
2017-07-11 16:34:29,317 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,318 INFO  [Thread-310] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1109933422-172.17.0.3-1499783668074 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:29,320 INFO  [Thread-311] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-1109933422-172.17.0.3-1499783668074 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:29,337 INFO  [Thread-311] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1109933422-172.17.0.3-1499783668074 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 17ms
2017-07-11 16:34:29,338 INFO  [Thread-310] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-1109933422-172.17.0.3-1499783668074 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 19ms
2017-07-11 16:34:29,338 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-1109933422-172.17.0.3-1499783668074: 20ms
2017-07-11 16:34:29,339 INFO  [Thread-314] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1109933422-172.17.0.3-1499783668074 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:29,339 INFO  [Thread-315] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-1109933422-172.17.0.3-1499783668074 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:29,339 INFO  [Thread-314] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1109933422-172.17.0.3-1499783668074 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-11 16:34:29,340 INFO  [Thread-315] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-1109933422-172.17.0.3-1499783668074 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-11 16:34:29,341 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-11 16:34:29,341 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid null) service to localhost/127.0.0.1:40806 beginning handshake with NN
2017-07-11 16:34:29,349 INFO  [IPC Server handler 5 on 40806] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=70b34de5-a089-4be4-bcff-7e283853258c, infoPort=55315, ipcPort=33272, storageInfo=lv=-56;cid=testClusterID;nsid=1601684733;c=0) storage 70b34de5-a089-4be4-bcff-7e283853258c
2017-07-11 16:34:29,350 INFO  [IPC Server handler 5 on 40806] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:29,350 INFO  [IPC Server handler 5 on 40806] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:60846
2017-07-11 16:34:29,352 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid null) service to localhost/127.0.0.1:40806 successfully registered with NN
2017-07-11 16:34:29,352 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:40806 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-11 16:34:29,355 INFO  [IPC Server handler 6 on 40806] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:29,355 INFO  [IPC Server handler 6 on 40806] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda for DN 127.0.0.1:60846
2017-07-11 16:34:29,355 INFO  [IPC Server handler 6 on 40806] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-d976a613-f23a-4b64-afa4-c7c901f23431 for DN 127.0.0.1:60846
2017-07-11 16:34:29,357 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid 70b34de5-a089-4be4-bcff-7e283853258c) service to localhost/127.0.0.1:40806 trying to claim ACTIVE state with txid=1
2017-07-11 16:34:29,357 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid 70b34de5-a089-4be4-bcff-7e283853258c) service to localhost/127.0.0.1:40806
2017-07-11 16:34:29,360 INFO  [IPC Server handler 7 on 40806] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:29,361 INFO  [IPC Server handler 7 on 40806] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda node DatanodeRegistration(127.0.0.1, datanodeUuid=70b34de5-a089-4be4-bcff-7e283853258c, infoPort=55315, ipcPort=33272, storageInfo=lv=-56;cid=testClusterID;nsid=1601684733;c=0), blocks: 0, hasStaleStorages: true, processing time: 2 msecs
2017-07-11 16:34:29,362 INFO  [IPC Server handler 7 on 40806] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-d976a613-f23a-4b64-afa4-c7c901f23431,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:29,363 INFO  [IPC Server handler 7 on 40806] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-d976a613-f23a-4b64-afa4-c7c901f23431 node DatanodeRegistration(127.0.0.1, datanodeUuid=70b34de5-a089-4be4-bcff-7e283853258c, infoPort=55315, ipcPort=33272, storageInfo=lv=-56;cid=testClusterID;nsid=1601684733;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2017-07-11 16:34:29,363 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:29,364 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 6 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@b74d052
2017-07-11 16:34:29,365 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,366 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-11 16:34:29,367 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:29,376 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-11 16:34:29,376 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-11 16:34:29,388 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,388 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:29,397 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-1109933422-172.17.0.3-1499783668074 to blockPoolScannerMap, new size=1
2017-07-11 16:34:29,405 INFO  [IPC Server handler 9 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-11 16:34:29,410 INFO  [IPC Server handler 2 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:29,423 INFO  [IPC Server handler 0 on 40806] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1. BP-1109933422-172.17.0.3-1499783668074 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d976a613-f23a-4b64-afa4-c7c901f23431:NORMAL:127.0.0.1:60846|RBW]]}
2017-07-11 16:34:29,431 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_330500618_1 at /127.0.0.1:45377 [Receiving block BP-1109933422-172.17.0.3-1499783668074:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1109933422-172.17.0.3-1499783668074:blk_1073741825_1001 src: /127.0.0.1:45377 dest: /127.0.0.1:60846
2017-07-11 16:34:29,649 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45377, dest: /127.0.0.1:60846, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330500618_1, offset: 0, srvID: 70b34de5-a089-4be4-bcff-7e283853258c, blockid: BP-1109933422-172.17.0.3-1499783668074:blk_1073741825_1001, duration: 212412004
2017-07-11 16:34:29,650 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:29,654 INFO  [IPC Server handler 4 on 40806] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60846 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda:NORMAL:127.0.0.1:60846|FINALIZED]]} size 0
2017-07-11 16:34:29,657 INFO  [IPC Server handler 5 on 40806] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1 is closed by DFSClient_NONMAPREDUCE_330500618_1
2017-07-11 16:34:29,661 INFO  [IPC Server handler 6 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:29,669 INFO  [IPC Server handler 8 on 40806] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2. BP-1109933422-172.17.0.3-1499783668074 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda:NORMAL:127.0.0.1:60846|RBW]]}
2017-07-11 16:34:29,673 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_330500618_1 at /127.0.0.1:45379 [Receiving block BP-1109933422-172.17.0.3-1499783668074:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1109933422-172.17.0.3-1499783668074:blk_1073741826_1002 src: /127.0.0.1:45379 dest: /127.0.0.1:60846
2017-07-11 16:34:29,688 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45379, dest: /127.0.0.1:60846, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330500618_1, offset: 0, srvID: 70b34de5-a089-4be4-bcff-7e283853258c, blockid: BP-1109933422-172.17.0.3-1499783668074:blk_1073741826_1002, duration: 9010351
2017-07-11 16:34:29,688 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:29,696 INFO  [IPC Server handler 7 on 40806] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60846 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d976a613-f23a-4b64-afa4-c7c901f23431:NORMAL:127.0.0.1:60846|FINALIZED]]} size 0
2017-07-11 16:34:29,699 INFO  [IPC Server handler 1 on 40806] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2 is closed by DFSClient_NONMAPREDUCE_330500618_1
2017-07-11 16:34:29,702 INFO  [IPC Server handler 9 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:29,705 INFO  [IPC Server handler 2 on 40806] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3. BP-1109933422-172.17.0.3-1499783668074 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d976a613-f23a-4b64-afa4-c7c901f23431:NORMAL:127.0.0.1:60846|RBW]]}
2017-07-11 16:34:29,707 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_330500618_1 at /127.0.0.1:45382 [Receiving block BP-1109933422-172.17.0.3-1499783668074:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1109933422-172.17.0.3-1499783668074:blk_1073741827_1003 src: /127.0.0.1:45382 dest: /127.0.0.1:60846
2017-07-11 16:34:29,716 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45382, dest: /127.0.0.1:60846, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330500618_1, offset: 0, srvID: 70b34de5-a089-4be4-bcff-7e283853258c, blockid: BP-1109933422-172.17.0.3-1499783668074:blk_1073741827_1003, duration: 5877280
2017-07-11 16:34:29,717 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:29,717 INFO  [IPC Server handler 0 on 40806] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60846 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda:NORMAL:127.0.0.1:60846|FINALIZED]]} size 0
2017-07-11 16:34:29,719 INFO  [IPC Server handler 0 on 40806] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3 is closed by DFSClient_NONMAPREDUCE_330500618_1
2017-07-11 16:34:29,722 INFO  [IPC Server handler 4 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:29,724 INFO  [IPC Server handler 5 on 40806] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4. BP-1109933422-172.17.0.3-1499783668074 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-626adb8d-f3e3-4ab4-9d2f-9275f2474eda:NORMAL:127.0.0.1:60846|RBW]]}
2017-07-11 16:34:29,726 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_330500618_1 at /127.0.0.1:45383 [Receiving block BP-1109933422-172.17.0.3-1499783668074:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-1109933422-172.17.0.3-1499783668074:blk_1073741828_1004 src: /127.0.0.1:45383 dest: /127.0.0.1:60846
2017-07-11 16:34:29,733 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:45383, dest: /127.0.0.1:60846, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330500618_1, offset: 0, srvID: 70b34de5-a089-4be4-bcff-7e283853258c, blockid: BP-1109933422-172.17.0.3-1499783668074:blk_1073741828_1004, duration: 4123787
2017-07-11 16:34:29,734 INFO  [PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-1109933422-172.17.0.3-1499783668074:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:29,734 INFO  [IPC Server handler 6 on 40806] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:60846 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d976a613-f23a-4b64-afa4-c7c901f23431:NORMAL:127.0.0.1:60846|FINALIZED]]} size 0
2017-07-11 16:34:29,736 INFO  [IPC Server handler 8 on 40806] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4 is closed by DFSClient_NONMAPREDUCE_330500618_1
2017-07-11 16:34:29,738 INFO  [IPC Server handler 7 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-11 16:34:29,740 INFO  [IPC Server handler 1 on 40806] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-11 16:34:29,742 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-11 16:34:29,742 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-11 16:34:29,742 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-11 16:34:29,742 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6240651f] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-11 16:34:29,747 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:29,849 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-11 16:34:29,850 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 33272
2017-07-11 16:34:29,852 INFO  [IPC Server listener on 33272] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 33272
2017-07-11 16:34:29,852 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:29,852 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid 70b34de5-a089-4be4-bcff-7e283853258c) service to localhost/127.0.0.1:40806 interrupted
2017-07-11 16:34:29,853 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid 70b34de5-a089-4be4-bcff-7e283853258c) service to localhost/127.0.0.1:40806
2017-07-11 16:34:29,853 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1109933422-172.17.0.3-1499783668074 (Datanode Uuid 70b34de5-a089-4be4-bcff-7e283853258c)
2017-07-11 16:34:29,853 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-1109933422-172.17.0.3-1499783668074 from blockPoolScannerMap
2017-07-11 16:34:29,854 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:40806] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-1109933422-172.17.0.3-1499783668074
2017-07-11 16:34:29,856 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@4af1f924] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-11 16:34:29,857 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-11 16:34:29,857 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-11 16:34:29,858 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-11 16:34:29,858 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-11 16:34:29,858 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-11 16:34:29,858 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:29,859 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@188b6035] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-11 16:34:29,859 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@4e93dcb9] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-11 16:34:29,859 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-11 16:34:29,860 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 2 2 
2017-07-11 16:34:29,861 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-11 16:34:29,862 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-11 16:34:29,863 INFO  [CacheReplicationMonitor(177944332)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-11 16:34:29,864 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 40806
2017-07-11 16:34:29,866 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:29,866 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@4726927c] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-11 16:34:29,866 INFO  [IPC Server listener on 40806] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 40806
2017-07-11 16:34:29,866 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@571a9686] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-11 16:34:29,884 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:29,884 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-11 16:34:29,892 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:29,993 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-11 16:34:29,995 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-11 16:34:29,996 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
2017-07-11 16:34:30,038 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(442)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2017-07-11 16:34:30,047 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:30,049 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:30,051 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:30,051 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:30,052 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:30,053 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:30
2017-07-11 16:34:30,054 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:30,054 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,055 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:30,056 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:30,072 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:30,072 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:30,073 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:30,073 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:30,074 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:30,074 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:30,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:30,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:30,075 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:30,076 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:30,076 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:30,077 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:30,078 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:30,078 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:30,079 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:30,079 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,080 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:30,081 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:30,089 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:30,090 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:30,110 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,110 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:30,111 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:30,113 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:30,113 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:30,113 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:30,114 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:30,115 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:30,115 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:30,115 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,116 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:30,116 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:30,118 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:30,118 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:30,118 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:30,120 INFO  [main] namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:30,149 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1 has been successfully formatted.
2017-07-11 16:34:30,171 INFO  [main] common.Storage (NNStorage.java:format(552)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2 has been successfully formatted.
2017-07-11 16:34:30,290 INFO  [main] namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2017-07-11 16:34:30,292 INFO  [main] namenode.NameNode (NameNode.java:createNameNode(1367)) - createNameNode []
2017-07-11 16:34:30,294 WARN  [main] impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2017-07-11 16:34:30,297 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(376)) - Scheduled snapshot period at 10 second(s).
2017-07-11 16:34:30,297 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2017-07-11 16:34:30,302 INFO  [main] namenode.NameNode (NameNode.java:setClientNamenodeAddress(349)) - fs.defaultFS is hdfs://127.0.0.1:0
2017-07-11 16:34:30,307 INFO  [main] hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1694)) - Starting Web-server for hdfs at: http://localhost:0
2017-07-11 16:34:30,307 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
2017-07-11 16:34:30,308 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:30,309 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-07-11 16:34:30,310 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:30,311 INFO  [main] http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-07-11 16:34:30,311 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:30,312 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 59267
2017-07-11 16:34:30,313 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:30,363 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/hdfs to /tmp/Jetty_localhost_59267_hdfs____.g67kzr/webapp
2017-07-11 16:34:30,503 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59267
2017-07-11 16:34:30,504 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - No KeyProvider found.
2017-07-11 16:34:30,504 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(786)) - fsLock is fair:true
2017-07-11 16:34:30,505 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(232)) - dfs.block.invalidate.limit=1000
2017-07-11 16:34:30,505 INFO  [main] blockmanagement.DatanodeManager (DatanodeManager.java:<init>(238)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2017-07-11 16:34:30,505 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(71)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-07-11 16:34:30,507 INFO  [main] blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(76)) - The block deletion will start around 2017 Jul 11 16:34:30
2017-07-11 16:34:30,508 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
2017-07-11 16:34:30,508 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,508 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 7.0 GB = 143.3 MB
2017-07-11 16:34:30,508 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^24 = 16777216 entries
2017-07-11 16:34:30,519 INFO  [main] blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(364)) - dfs.block.access.token.enable=false
2017-07-11 16:34:30,519 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(349)) - defaultReplication         = 1
2017-07-11 16:34:30,519 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(350)) - maxReplication             = 512
2017-07-11 16:34:30,519 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(351)) - minReplication             = 1
2017-07-11 16:34:30,520 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(352)) - maxReplicationStreams      = 2
2017-07-11 16:34:30,520 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(353)) - shouldCheckForEnoughRacks  = false
2017-07-11 16:34:30,520 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(354)) - replicationRecheckInterval = 3000
2017-07-11 16:34:30,520 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(355)) - encryptDataTransfer        = false
2017-07-11 16:34:30,520 INFO  [main] blockmanagement.BlockManager (BlockManager.java:<init>(356)) - maxNumBlocksToLog          = 1000
2017-07-11 16:34:30,521 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(809)) - fsOwner             = root (auth:SIMPLE)
2017-07-11 16:34:30,521 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(810)) - supergroup          = supergroup
2017-07-11 16:34:30,521 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(811)) - isPermissionEnabled = true
2017-07-11 16:34:30,521 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(822)) - HA Enabled: false
2017-07-11 16:34:30,521 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(859)) - Append Enabled: true
2017-07-11 16:34:30,522 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
2017-07-11 16:34:30,522 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,522 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 7.0 GB = 71.7 MB
2017-07-11 16:34:30,523 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^23 = 8388608 entries
2017-07-11 16:34:30,528 INFO  [main] namenode.NameNode (FSDirectory.java:<init>(234)) - Caching file names occuring more than 10 times
2017-07-11 16:34:30,529 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
2017-07-11 16:34:30,529 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,529 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 7.0 GB = 17.9 MB
2017-07-11 16:34:30,529 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^21 = 2097152 entries
2017-07-11 16:34:30,533 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5663)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-07-11 16:34:30,533 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5664)) - dfs.namenode.safemode.min.datanodes = 0
2017-07-11 16:34:30,533 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:<init>(5665)) - dfs.namenode.safemode.extension     = 0
2017-07-11 16:34:30,533 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(957)) - Retry cache on namenode is enabled
2017-07-11 16:34:30,533 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initRetryCache(965)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-07-11 16:34:30,534 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
2017-07-11 16:34:30,534 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:30,534 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 7.0 GB = 2.1 MB
2017-07-11 16:34:30,534 INFO  [main] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
2017-07-11 16:34:30,536 INFO  [main] namenode.NNConf (NNConf.java:<init>(62)) - ACLs enabled? false
2017-07-11 16:34:30,536 INFO  [main] namenode.NNConf (NNConf.java:<init>(66)) - XAttrs enabled? true
2017-07-11 16:34:30,536 INFO  [main] namenode.NNConf (NNConf.java:<init>(74)) - Maximum size of an xattr: 16384
2017-07-11 16:34:30,546 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:30,563 INFO  [main] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:30,565 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current
2017-07-11 16:34:30,565 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current
2017-07-11 16:34:30,574 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(656)) - No edit log streams selected.
2017-07-11 16:34:30,575 INFO  [main] namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(214)) - Loading 1 INodes.
2017-07-11 16:34:30,576 INFO  [main] namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(182)) - Loaded FSImage in 0 seconds.
2017-07-11 16:34:30,576 INFO  [main] namenode.FSImage (FSImage.java:loadFSImage(937)) - Loaded image for txid 0 from /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/fsimage_0000000000000000000
2017-07-11 16:34:30,576 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1027)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-07-11 16:34:30,589 INFO  [main] namenode.FSEditLog (FSEditLog.java:startLogSegment(1173)) - Starting log segment at 1
2017-07-11 16:34:30,629 INFO  [main] namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2017-07-11 16:34:30,630 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(746)) - Finished loading FSImage in 94 msecs
2017-07-11 16:34:30,630 INFO  [main] namenode.NameNode (NameNodeRpcServer.java:<init>(329)) - RPC server is binding to localhost:0
2017-07-11 16:34:30,633 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:30,635 INFO  [Socket Reader #1 for port 33273] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 33273
2017-07-11 16:34:30,659 INFO  [main] namenode.NameNode (NameNode.java:initialize(603)) - Clients are to use localhost:33273 to access this namenode/service.
2017-07-11 16:34:30,662 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:registerMBean(6642)) - Registered FSNamesystemState MBean
2017-07-11 16:34:30,685 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:30,686 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(6253)) - Number of blocks under construction: 0
2017-07-11 16:34:30,686 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1219)) - initializing replication queues
2017-07-11 16:34:30,686 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5737)) - STATE* Leaving safe mode after 0 secs
2017-07-11 16:34:30,687 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5748)) - STATE* Network topology has 0 racks and 0 datanodes
2017-07-11 16:34:30,687 INFO  [main] hdfs.StateChange (FSNamesystem.java:leave(5751)) - STATE* UnderReplicatedBlocks has 0 blocks
2017-07-11 16:34:30,709 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:30,709 INFO  [IPC Server listener on 33273] ipc.Server (Server.java:run(674)) - IPC Server listener on 33273: starting
2017-07-11 16:34:30,735 INFO  [main] namenode.NameNode (NameNode.java:startCommonServices(646)) - NameNode RPC up at: localhost/127.0.0.1:33273
2017-07-11 16:34:30,735 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2612)) - Total number of blocks            = 0
2017-07-11 16:34:30,736 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2613)) - Number of invalid blocks          = 0
2017-07-11 16:34:30,736 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2614)) - Number of under-replicated blocks = 0
2017-07-11 16:34:30,746 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2615)) - Number of  over-replicated blocks = 0
2017-07-11 16:34:30,746 INFO  [Replication Queue Initializer] blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2617)) - Number of blocks being written    = 0
2017-07-11 16:34:30,746 INFO  [Replication Queue Initializer] hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2618)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 59 msec
2017-07-11 16:34:30,747 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1140)) - Starting services required for active state
2017-07-11 16:34:30,761 INFO  [CacheReplicationMonitor(1045407371)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-07-11 16:34:30,761 INFO  [CacheReplicationMonitor(1045407371)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 451220959 milliseconds
2017-07-11 16:34:30,763 INFO  [CacheReplicationMonitor(1045407371)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2017-07-11 16:34:30,771 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1407)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1,[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2
2017-07-11 16:34:30,772 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1412)) - Starting DataNode 0 with hostname set to: localhost
2017-07-11 16:34:30,784 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2017-07-11 16:34:30,785 INFO  [main] datanode.DataNode (DataNode.java:<init>(414)) - Configured hostname is localhost
2017-07-11 16:34:30,785 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1049)) - Starting DataNode with maxLockedMemory = 0
2017-07-11 16:34:30,786 INFO  [main] datanode.DataNode (DataNode.java:initDataXceiver(848)) - Opened streaming server at /127.0.0.1:58721
2017-07-11 16:34:30,786 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(76)) - Balancing bandwith is 1048576 bytes/s
2017-07-11 16:34:30,786 INFO  [main] datanode.DataNode (DataXceiverServer.java:<init>(77)) - Number threads for balancing is 5
2017-07-11 16:34:30,787 INFO  [main] http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
2017-07-11 16:34:30,788 INFO  [main] http.HttpServer2 (HttpServer2.java:addGlobalFilter(699)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-07-11 16:34:30,788 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(677)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-07-11 16:34:30,788 INFO  [main] http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-07-11 16:34:30,789 INFO  [main] http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(603)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-07-11 16:34:30,791 INFO  [main] http.HttpServer2 (HttpServer2.java:openListeners(887)) - Jetty bound to port 46789
2017-07-11 16:34:30,791 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
2017-07-11 16:34:30,795 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Extract jar:file:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar!/webapps/datanode to /tmp/Jetty_localhost_46789_datanode____qg1i1p/webapp
2017-07-11 16:34:30,914 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46789
2017-07-11 16:34:30,915 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1066)) - dnUserName = root
2017-07-11 16:34:30,916 INFO  [main] datanode.DataNode (DataNode.java:startDataNode(1067)) - supergroup = supergroup
2017-07-11 16:34:30,917 INFO  [main] ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-07-11 16:34:30,918 INFO  [Socket Reader #1 for port 42796] ipc.Server (Server.java:run(605)) - Starting Socket Reader #1 for port 42796
2017-07-11 16:34:30,920 INFO  [main] datanode.DataNode (DataNode.java:initIpcServer(723)) - Opened IPC server at /127.0.0.1:42796
2017-07-11 16:34:30,922 INFO  [main] datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
2017-07-11 16:34:30,922 INFO  [main] datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
2017-07-11 16:34:30,932 INFO  [IPC Server Responder] ipc.Server (Server.java:run(827)) - IPC Server Responder: starting
2017-07-11 16:34:30,933 INFO  [IPC Server listener on 42796] ipc.Server (Server.java:run(674)) - IPC Server listener on 42796: starting
2017-07-11 16:34:30,936 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:run(821)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:33273 starting to offer service
2017-07-11 16:34:30,958 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (DataStorage.java:recoverTransitionRead(399)) - DataNode version: -56 and NameNode layout version: -60
2017-07-11 16:34:30,959 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:30,959 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:31,062 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:31,062 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:31,083 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:31,084 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1 is not formatted for BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:31,084 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:31,164 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:31,165 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:31,178 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (Storage.java:tryLock(715)) - Lock on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 262@spirals-librepair
2017-07-11 16:34:31,179 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (DataStorage.java:addStorageLocations(281)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2 is not formatted for BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:31,179 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (DataStorage.java:addStorageLocations(283)) - Formatting ...
2017-07-11 16:34:31,257 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(141)) - Analyzing storage directories for bpid BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:31,257 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:31,258 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-157719376-172.17.0.3-1499783670120 is not formatted.
2017-07-11 16:34:31,258 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:31,258 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-157719376-172.17.0.3-1499783670120 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current/BP-157719376-172.17.0.3-1499783670120/current
2017-07-11 16:34:31,266 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2230)) - dnInfo.length != numDataNodes
2017-07-11 16:34:31,267 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:31,268 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (Storage.java:lock(675)) - Locking is disabled
2017-07-11 16:34:31,268 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(172)) - Storage directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-157719376-172.17.0.3-1499783670120 is not formatted.
2017-07-11 16:34:31,268 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(173)) - Formatting ...
2017-07-11 16:34:31,268 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:format(225)) - Formatting block pool BP-157719376-172.17.0.3-1499783670120 directory /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current/BP-157719376-172.17.0.3-1499783670120/current
2017-07-11 16:34:31,283 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:31,284 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] common.Storage (BlockPoolSliceStorage.java:doTransition(313)) - Restored 0 block files from trash.
2017-07-11 16:34:31,315 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (DataNode.java:initStorage(1314)) - Setting up storage: nsid=1899198786;bpid=BP-157719376-172.17.0.3-1499783670120;lv=-56;nsInfo=lv=-60;cid=testClusterID;nsid=1899198786;c=0;bpid=BP-157719376-172.17.0.3-1499783670120;dnuuid=null
2017-07-11 16:34:31,347 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (DataNode.java:checkDatanodeUuid(1142)) - Generated and persisted new Datanode UUID 210114cf-85da-4e14-a419-1b254842f456
2017-07-11 16:34:31,349 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current
2017-07-11 16:34:31,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current, StorageType: DISK
2017-07-11 16:34:31,350 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsVolumeList.java:addVolume(222)) - Added new volume: /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current
2017-07-11 16:34:31,351 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(317)) - Added volume - /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current, StorageType: DISK
2017-07-11 16:34:31,352 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1804)) - Registered FSDatasetState MBean
2017-07-11 16:34:31,353 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1499797796352 with interval 21600000
2017-07-11 16:34:31,353 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2207)) - Adding block pool BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:31,354 INFO  [Thread-396] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-157719376-172.17.0.3-1499783670120 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:31,358 INFO  [Thread-397] impl.FsDatasetImpl (FsVolumeList.java:run(254)) - Scanning block pool BP-157719376-172.17.0.3-1499783670120 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:31,362 INFO  [Thread-396] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-157719376-172.17.0.3-1499783670120 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 4ms
2017-07-11 16:34:31,362 INFO  [Thread-397] impl.FsDatasetImpl (FsVolumeList.java:run(259)) - Time taken to scan block pool BP-157719376-172.17.0.3-1499783670120 on /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 4ms
2017-07-11 16:34:31,362 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(283)) - Total time to scan all replicas for block pool BP-157719376-172.17.0.3-1499783670120: 9ms
2017-07-11 16:34:31,364 INFO  [Thread-400] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-157719376-172.17.0.3-1499783670120 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current...
2017-07-11 16:34:31,365 INFO  [Thread-400] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-157719376-172.17.0.3-1499783670120 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/current: 0ms
2017-07-11 16:34:31,365 INFO  [Thread-401] impl.FsDatasetImpl (FsVolumeList.java:run(134)) - Adding replicas to map for block pool BP-157719376-172.17.0.3-1499783670120 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current...
2017-07-11 16:34:31,366 INFO  [Thread-401] impl.FsDatasetImpl (FsVolumeList.java:run(139)) - Time to add replicas to map for block pool BP-157719376-172.17.0.3-1499783670120 on volume /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/current: 0ms
2017-07-11 16:34:31,366 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(162)) - Total time to add all replicas to map: 2ms
2017-07-11 16:34:31,366 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:register(781)) - Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid null) service to localhost/127.0.0.1:33273 beginning handshake with NN
2017-07-11 16:34:31,368 INFO  [IPC Server handler 5 on 33273] hdfs.StateChange (DatanodeManager.java:registerDatanode(903)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=210114cf-85da-4e14-a419-1b254842f456, infoPort=46789, ipcPort=42796, storageInfo=lv=-56;cid=testClusterID;nsid=1899198786;c=0) storage 210114cf-85da-4e14-a419-1b254842f456
2017-07-11 16:34:31,368 INFO  [IPC Server handler 5 on 33273] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:31,368 INFO  [IPC Server handler 5 on 33273] net.NetworkTopology (NetworkTopology.java:add(419)) - Adding a new node: /default-rack/127.0.0.1:58721
2017-07-11 16:34:31,371 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:register(794)) - Block pool Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid null) service to localhost/127.0.0.1:33273 successfully registered with NN
2017-07-11 16:34:31,371 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:offerService(653)) - For namenode localhost/127.0.0.1:33273 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-07-11 16:34:31,371 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2246)) - dn.getCapacity() == 0
2017-07-11 16:34:31,371 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2182)) - Waiting for cluster to become active
2017-07-11 16:34:31,372 INFO  [IPC Server handler 7 on 33273] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(388)) - Number of failed storage changes from 0 to 0
2017-07-11 16:34:31,373 INFO  [IPC Server handler 7 on 33273] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-214cf832-ea50-43e3-8130-e453001f0372 for DN 127.0.0.1:58721
2017-07-11 16:34:31,373 INFO  [IPC Server handler 7 on 33273] blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(743)) - Adding new storage ID DS-eb42f053-83ca-445b-8f41-ec36a34c52c1 for DN 127.0.0.1:58721
2017-07-11 16:34:31,374 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(503)) - Namenode Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid 210114cf-85da-4e14-a419-1b254842f456) service to localhost/127.0.0.1:33273 trying to claim ACTIVE state with txid=1
2017-07-11 16:34:31,374 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(515)) - Acknowledging ACTIVE Namenode Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid 210114cf-85da-4e14-a419-1b254842f456) service to localhost/127.0.0.1:33273
2017-07-11 16:34:31,375 INFO  [IPC Server handler 8 on 33273] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-214cf832-ea50-43e3-8130-e453001f0372,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:31,376 INFO  [IPC Server handler 8 on 33273] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-214cf832-ea50-43e3-8130-e453001f0372 node DatanodeRegistration(127.0.0.1, datanodeUuid=210114cf-85da-4e14-a419-1b254842f456, infoPort=46789, ipcPort=42796, storageInfo=lv=-56;cid=testClusterID;nsid=1899198786;c=0), blocks: 0, hasStaleStorages: true, processing time: 1 msecs
2017-07-11 16:34:31,376 INFO  [IPC Server handler 8 on 33273] blockmanagement.BlockManager (BlockManager.java:processReport(1815)) - BLOCK* processReport: Received first block report from DatanodeStorage[DS-eb42f053-83ca-445b-8f41-ec36a34c52c1,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2017-07-11 16:34:31,376 INFO  [IPC Server handler 8 on 33273] BlockStateChange (BlockManager.java:processReport(1831)) - BLOCK* processReport: from storage DS-eb42f053-83ca-445b-8f41-ec36a34c52c1 node DatanodeRegistration(127.0.0.1, datanodeUuid=210114cf-85da-4e14-a419-1b254842f456, infoPort=46789, ipcPort=42796, storageInfo=lv=-56;cid=testClusterID;nsid=1899198786;c=0), blocks: 0, hasStaleStorages: false, processing time: 0 msecs
2017-07-11 16:34:31,377 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:blockReport(514)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@214a0001
2017-07-11 16:34:31,377 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPOfferService.java:processCommandFromActive(689)) - Got finalize command for block pool BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:31,378 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
2017-07-11 16:34:31,378 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
2017-07-11 16:34:31,378 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 7.0 GB = 35.8 MB
2017-07-11 16:34:31,378 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^22 = 4194304 entries
2017-07-11 16:34:31,774 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(190)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:31,776 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(264)) - Added bpid=BP-157719376-172.17.0.3-1499783670120 to blockPoolScannerMap, new size=1
2017-07-11 16:34:31,778 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:31,794 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2213)) - Cluster is active
2017-07-11 16:34:31,800 INFO  [IPC Server handler 2 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/minidfsTest	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2017-07-11 16:34:31,804 INFO  [IPC Server handler 4 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/1.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:31,809 INFO  [IPC Server handler 3 on 33273] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/1.txt. BP-157719376-172.17.0.3-1499783670120 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-eb42f053-83ca-445b-8f41-ec36a34c52c1:NORMAL:127.0.0.1:58721|RBW]]}
2017-07-11 16:34:31,813 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_477306253_1 at /127.0.0.1:54241 [Receiving block BP-157719376-172.17.0.3-1499783670120:blk_1073741825_1001]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-157719376-172.17.0.3-1499783670120:blk_1073741825_1001 src: /127.0.0.1:54241 dest: /127.0.0.1:58721
2017-07-11 16:34:31,829 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:54241, dest: /127.0.0.1:58721, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_477306253_1, offset: 0, srvID: 210114cf-85da-4e14-a419-1b254842f456, blockid: BP-157719376-172.17.0.3-1499783670120:blk_1073741825_1001, duration: 1431740
2017-07-11 16:34:31,830 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:31,832 INFO  [IPC Server handler 6 on 33273] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58721 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-214cf832-ea50-43e3-8130-e453001f0372:NORMAL:127.0.0.1:58721|FINALIZED]]} size 0
2017-07-11 16:34:31,834 INFO  [IPC Server handler 7 on 33273] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/1.txt is closed by DFSClient_NONMAPREDUCE_477306253_1
2017-07-11 16:34:31,847 INFO  [IPC Server handler 8 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/2.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:31,851 INFO  [IPC Server handler 9 on 33273] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/2.txt. BP-157719376-172.17.0.3-1499783670120 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-214cf832-ea50-43e3-8130-e453001f0372:NORMAL:127.0.0.1:58721|RBW]]}
2017-07-11 16:34:31,854 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_477306253_1 at /127.0.0.1:54243 [Receiving block BP-157719376-172.17.0.3-1499783670120:blk_1073741826_1002]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-157719376-172.17.0.3-1499783670120:blk_1073741826_1002 src: /127.0.0.1:54243 dest: /127.0.0.1:58721
2017-07-11 16:34:31,859 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:54243, dest: /127.0.0.1:58721, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_477306253_1, offset: 0, srvID: 210114cf-85da-4e14-a419-1b254842f456, blockid: BP-157719376-172.17.0.3-1499783670120:blk_1073741826_1002, duration: 1409489
2017-07-11 16:34:31,859 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:31,861 INFO  [IPC Server handler 0 on 33273] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58721 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-eb42f053-83ca-445b-8f41-ec36a34c52c1:NORMAL:127.0.0.1:58721|FINALIZED]]} size 0
2017-07-11 16:34:31,862 INFO  [IPC Server handler 0 on 33273] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/2.txt is closed by DFSClient_NONMAPREDUCE_477306253_1
2017-07-11 16:34:31,870 INFO  [IPC Server handler 1 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/3.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:31,874 INFO  [IPC Server handler 4 on 33273] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/3.txt. BP-157719376-172.17.0.3-1499783670120 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-214cf832-ea50-43e3-8130-e453001f0372:NORMAL:127.0.0.1:58721|RBW]]}
2017-07-11 16:34:31,876 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_477306253_1 at /127.0.0.1:54245 [Receiving block BP-157719376-172.17.0.3-1499783670120:blk_1073741827_1003]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-157719376-172.17.0.3-1499783670120:blk_1073741827_1003 src: /127.0.0.1:54245 dest: /127.0.0.1:58721
2017-07-11 16:34:31,881 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:54245, dest: /127.0.0.1:58721, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_477306253_1, offset: 0, srvID: 210114cf-85da-4e14-a419-1b254842f456, blockid: BP-157719376-172.17.0.3-1499783670120:blk_1073741827_1003, duration: 2340291
2017-07-11 16:34:31,881 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:31,882 INFO  [IPC Server handler 3 on 33273] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58721 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-214cf832-ea50-43e3-8130-e453001f0372:NORMAL:127.0.0.1:58721|RBW]]} size 0
2017-07-11 16:34:31,886 INFO  [IPC Server handler 5 on 33273] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/3.txt is closed by DFSClient_NONMAPREDUCE_477306253_1
2017-07-11 16:34:31,889 INFO  [IPC Server handler 6 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/minidfsTest/4.txt	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2017-07-11 16:34:31,895 INFO  [IPC Server handler 7 on 33273] hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3660)) - BLOCK* allocateBlock: /minidfsTest/4.txt. BP-157719376-172.17.0.3-1499783670120 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-214cf832-ea50-43e3-8130-e453001f0372:NORMAL:127.0.0.1:58721|RBW]]}
2017-07-11 16:34:31,899 INFO  [DataXceiver for client DFSClient_NONMAPREDUCE_477306253_1 at /127.0.0.1:54246 [Receiving block BP-157719376-172.17.0.3-1499783670120:blk_1073741828_1004]] datanode.DataNode (DataXceiver.java:writeBlock(593)) - Receiving BP-157719376-172.17.0.3-1499783670120:blk_1073741828_1004 src: /127.0.0.1:54246 dest: /127.0.0.1:58721
2017-07-11 16:34:31,906 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1292)) - src: /127.0.0.1:54246, dest: /127.0.0.1:58721, bytes: 1, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_477306253_1, offset: 0, srvID: 210114cf-85da-4e14-a419-1b254842f456, blockid: BP-157719376-172.17.0.3-1499783670120:blk_1073741828_1004, duration: 2692814
2017-07-11 16:34:31,907 INFO  [PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[]] datanode.DataNode (BlockReceiver.java:run(1273)) - PacketResponder: BP-157719376-172.17.0.3-1499783670120:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2017-07-11 16:34:31,908 INFO  [IPC Server handler 8 on 33273] BlockStateChange (BlockManager.java:logAddStoredBlock(2473)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:58721 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-eb42f053-83ca-445b-8f41-ec36a34c52c1:NORMAL:127.0.0.1:58721|FINALIZED]]} size 0
2017-07-11 16:34:31,909 INFO  [IPC Server handler 9 on 33273] hdfs.StateChange (FSNamesystem.java:completeFile(3581)) - DIR* completeFile: /minidfsTest/4.txt is closed by DFSClient_NONMAPREDUCE_477306253_1
2017-07-11 16:34:31,910 INFO  [IPC Server handler 0 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-11 16:34:31,912 INFO  [IPC Server handler 2 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/minidfsTest	dst=null	perm=null	proto=rpc
2017-07-11 16:34:31,926 INFO  [IPC Server handler 1 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/1.txt	dst=null	perm=null	proto=rpc
2017-07-11 16:34:31,967 INFO  [IPC Server handler 4 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/2.txt	dst=null	perm=null	proto=rpc
2017-07-11 16:34:31,975 INFO  [IPC Server handler 3 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/3.txt	dst=null	perm=null	proto=rpc
2017-07-11 16:34:31,979 INFO  [IPC Server handler 5 on 33273] FSNamesystem.audit (FSNamesystem.java:logAuditMessage(9332)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/minidfsTest/4.txt	dst=null	perm=null	proto=rpc
2017-07-11 16:34:31,982 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1706)) - Shutting down the Mini HDFS Cluster
2017-07-11 16:34:31,982 INFO  [main] hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1738)) - Shutting down DataNode 0
2017-07-11 16:34:31,982 WARN  [main] datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
2017-07-11 16:34:31,982 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4f6b687e] datanode.DataNode (DataXceiverServer.java:closeAllPeers(263)) - Closing all peers.
2017-07-11 16:34:31,991 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:32,091 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 1
2017-07-11 16:34:32,094 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1655)) - Waiting for threadgroup to exit, active threads is 0
2017-07-11 16:34:32,094 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 42796
2017-07-11 16:34:32,094 INFO  [IPC Server listener on 42796] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 42796
2017-07-11 16:34:32,095 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:32,095 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:offerService(738)) - BPOfferService for Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid 210114cf-85da-4e14-a419-1b254842f456) service to localhost/127.0.0.1:33273 interrupted
2017-07-11 16:34:32,097 WARN  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BPServiceActor.java:run(861)) - Ending block pool service for: Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid 210114cf-85da-4e14-a419-1b254842f456) service to localhost/127.0.0.1:33273
2017-07-11 16:34:32,097 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-157719376-172.17.0.3-1499783670120 (Datanode Uuid 210114cf-85da-4e14-a419-1b254842f456)
2017-07-11 16:34:32,097 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(273)) - Removed bpid=BP-157719376-172.17.0.3-1499783670120 from blockPoolScannerMap
2017-07-11 16:34:32,097 INFO  [DataNode: [[[DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data1/, [DISK]file:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33273] impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2217)) - Removing block pool BP-157719376-172.17.0.3-1499783670120
2017-07-11 16:34:32,099 INFO  [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter@2d814357] impl.FsDatasetImpl (FsDatasetImpl.java:run(2662)) - LazyWriter was interrupted, exiting
2017-07-11 16:34:32,099 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(172)) - Shutting down all async disk service threads
2017-07-11 16:34:32,100 INFO  [main] impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(180)) - All async disk service threads have been shut down
2017-07-11 16:34:32,100 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(161)) - Shutting down all async lazy persist service threads
2017-07-11 16:34:32,100 INFO  [main] impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(168)) - All async lazy persist service threads have been shut down
2017-07-11 16:34:32,101 INFO  [main] datanode.DataNode (DataNode.java:shutdown(1720)) - Shutdown complete.
2017-07-11 16:34:32,101 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:32,101 INFO  [main] namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1214)) - Ending log segment 1
2017-07-11 16:34:32,101 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@7d2998d8] namenode.FSNamesystem (FSNamesystem.java:run(5274)) - LazyPersistFileScrubber was interrupted, exiting
2017-07-11 16:34:32,101 INFO  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@5a237731] namenode.FSNamesystem (FSNamesystem.java:run(5207)) - NameNodeEditLogRoller was interrupted, exiting
2017-07-11 16:34:32,102 INFO  [main] namenode.FSEditLog (FSEditLog.java:printStatistics(691)) - Number of transactions: 23 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 5 1 
2017-07-11 16:34:32,103 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000023
2017-07-11 16:34:32,103 INFO  [main] namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(133)) - Finalizing edits file /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /root/workspace/apache/incubator-samoa/252350217/samoa-api/target/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000023
2017-07-11 16:34:32,104 INFO  [CacheReplicationMonitor(1045407371)] blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2017-07-11 16:34:32,105 INFO  [main] ipc.Server (Server.java:stop(2437)) - Stopping server on 33273
2017-07-11 16:34:32,106 INFO  [IPC Server listener on 33273] ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 33273
2017-07-11 16:34:32,106 INFO  [IPC Server Responder] ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2017-07-11 16:34:32,106 INFO  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@70cccd8f] blockmanagement.BlockManager (BlockManager.java:run(3533)) - Stopping ReplicationMonitor.
2017-07-11 16:34:32,107 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@34d4860f] blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
2017-07-11 16:34:32,119 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1246)) - Stopping services started for active state
2017-07-11 16:34:32,119 INFO  [main] namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1334)) - Stopping services started for standby state
2017-07-11 16:34:32,123 INFO  [main] mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0
2017-07-11 16:34:32,224 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2017-07-11 16:34:32,225 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2017-07-11 16:34:32,225 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(605)) - DataNode metrics system shutdown complete.
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.339 sec - in org.apache.samoa.streams.fs.HDFSFileStreamSourceTest
Running org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec - in org.apache.samoa.streams.fs.LocalFileStreamSourceTest
Running org.apache.samoa.streams.kafka.KafkaUtilsTest
2017-07-11 16:34:32,367 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-11 16:34:32,367 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:host.name=spirals-librepair
2017-07-11 16:34:32,367 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.version=1.8.0_121
2017-07-11 16:34:32,367 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.vendor=Oracle Corporation
2017-07-11 16:34:32,367 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-11 16:34:32,367 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.class.path=/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/252350217/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-11 16:34:32,368 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-11 16:34:32,368 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.io.tmpdir=/tmp
2017-07-11 16:34:32,368 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:java.compiler=<NA>
2017-07-11 16:34:32,368 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.name=Linux
2017-07-11 16:34:32,368 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.arch=amd64
2017-07-11 16:34:32,369 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:os.version=3.16.0-4-amd64
2017-07-11 16:34:32,369 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.name=root
2017-07-11 16:34:32,369 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.home=/root
2017-07-11 16:34:32,369 INFO  [main] server.ZooKeeperServer (Environment.java:logEnv(100)) - Server environment:user.dir=/root/workspace/apache/incubator-samoa/252350217/samoa-api
2017-07-11 16:34:32,390 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-8997698465085918152/version-2 snapdir /tmp/kafka-8934867105634684745/version-2
2017-07-11 16:34:32,397 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-11 16:34:32,432 INFO  [ZkClient-EventThread-482-127.0.0.1:38435] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-11 16:34:32,438 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2017-07-11 16:34:32,438 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:host.name=spirals-librepair
2017-07-11 16:34:32,438 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.version=1.8.0_121
2017-07-11 16:34:32,438 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.vendor=Oracle Corporation
2017-07-11 16:34:32,438 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2017-07-11 16:34:32,438 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.class.path=/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/test-classes:/root/workspace/apache/incubator-samoa/252350217/samoa-api/target/classes:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/slf4j/slf4j-api/1.7.2/slf4j-api-1.7.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/github/javacliparser/javacliparser/0.5.0/javacliparser-0.5.0.jar:/root/workspace/apache/incubator-samoa/252350217/samoa-instances/target/classes:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/guava/guava/17.0/guava-17.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/ow2/asm/asm/4.0/asm-4.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/dreizak/miniball/1.0.3/miniball-1.0.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-annotations/2.6.0/hadoop-annotations-2.6.0.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/activation/activation/1.1/activation-1.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/asm/asm/3.1/asm-3.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-auth/2.6.0/hadoop-auth-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-minicluster/2.6.0/hadoop-minicluster-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-common/2.6.0/hadoop-common-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0/hadoop-hdfs-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.6.0/hadoop-yarn-server-tests-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0/hadoop-yarn-server-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.6.0/hadoop-yarn-server-nodemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0/hadoop-yarn-server-resourcemanager-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0/hadoop-yarn-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0/hadoop-mapreduce-client-common-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0/hadoop-yarn-client-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0/hadoop-mapreduce-client-shuffle-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0/hadoop-mapreduce-client-app-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0/hadoop-yarn-server-web-proxy-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0/hadoop-yarn-api-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/hadoop-mapreduce-client-core-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0/hadoop-mapreduce-client-jobclient-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.6.0/hadoop-mapreduce-client-hs-2.6.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka-clients/0.10.2.0/kafka-clients-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/net/sf/jopt-simple/jopt-simple/5.0.3/jopt-simple-5.0.3.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/com/101tec/zkclient/0.10/zkclient-0.10.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/apache/kafka/kafka_2.11/0.10.2.0/kafka_2.11-0.10.2.0-test.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/jmockit/jmockit/1.13/jmockit-1.13.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/junit/junit/4.10/junit-4.10.jar:/root/./workspace/apache/incubator-samoa/252350217/.m2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar:
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.io.tmpdir=/tmp
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.compiler=<NA>
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.name=Linux
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.arch=amd64
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.version=3.16.0-4-amd64
2017-07-11 16:34:32,439 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.name=root
2017-07-11 16:34:32,440 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.home=/root
2017-07-11 16:34:32,440 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.dir=/root/workspace/apache/incubator-samoa/252350217/samoa-api
2017-07-11 16:34:32,441 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:38435 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@6f1a16fe
2017-07-11 16:34:32,456 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-11 16:34:32,460 INFO  [main-SendThread(127.0.0.1:38435)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:38435. Will not attempt to authenticate using SASL (unknown error)
2017-07-11 16:34:32,460 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:51636
2017-07-11 16:34:32,463 INFO  [main-SendThread(127.0.0.1:38435)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:38435, initiating session
2017-07-11 16:34:32,469 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:51636
2017-07-11 16:34:32,472 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-11 16:34:32,496 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d3212b2540000 with negotiated timeout 10000 for client /127.0.0.1:51636
2017-07-11 16:34:32,496 INFO  [main-SendThread(127.0.0.1:38435)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:38435, sessionid = 0x15d3212b2540000, negotiated timeout = 10000
2017-07-11 16:34:32,499 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-11 16:34:32,822 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafkaUtils-817924831428150360
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:38435
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-11 16:34:32,910 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-11 16:34:32,913 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:38435
2017-07-11 16:34:32,915 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:38435 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@705a8dbc
2017-07-11 16:34:32,916 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-11 16:34:32,916 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-11 16:34:32,918 INFO  [main-SendThread(127.0.0.1:38435)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:38435. Will not attempt to authenticate using SASL (unknown error)
2017-07-11 16:34:32,918 INFO  [main-SendThread(127.0.0.1:38435)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:38435, initiating session
2017-07-11 16:34:32,919 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:51638
2017-07-11 16:34:32,919 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:51638
2017-07-11 16:34:32,921 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d3212b2540001 with negotiated timeout 6000 for client /127.0.0.1:51638
2017-07-11 16:34:32,921 INFO  [main-SendThread(127.0.0.1:38435)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:38435, sessionid = 0x15d3212b2540001, negotiated timeout = 6000
2017-07-11 16:34:32,921 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-11 16:34:32,948 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-11 16:34:32,956 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-11 16:34:32,964 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-11 16:34:33,030 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x1b zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-11 16:34:33,037 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = PxnQzOPgRIO7S-yM4nyzFA
2017-07-11 16:34:33,042 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-817924831428150360/meta.properties
2017-07-11 16:34:33,085 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-11 16:34:33,090 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-11 16:34:33,146 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-11 16:34:33,158 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-11 16:34:33,211 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-11 16:34:33,215 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-11 16:34:33,218 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-11 16:34:33,221 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-11 16:34:33,277 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-11 16:34:33,281 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-11 16:34:33,320 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:33,322 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:33,362 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-11 16:34:33,371 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-11 16:34:33,378 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-11 16:34:33,380 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-11 16:34:33,381 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-11 16:34:33,386 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:setData cxid:0x25 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-11 16:34:33,390 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-11 16:34:33,413 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-11 16:34:33,414 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-11 16:34:33,414 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-11 16:34:33,420 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-11 16:34:33,421 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-11 16:34:33,426 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-11 16:34:33,431 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-11 16:34:33,432 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-11 16:34:33,437 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-11 16:34:33,437 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-11 16:34:33,438 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-11 16:34:33,476 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-11 16:34:33,480 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-11 16:34:33,481 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-11 16:34:33,483 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-11 16:34:33,485 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-11 16:34:33,488 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:delete cxid:0x36 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-11 16:34:33,491 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-11 16:34:33,492 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-11 16:34:33,497 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:33,501 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:33,503 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:33,520 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-11 16:34:33,522 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-11 16:34:33,526 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 3 milliseconds.
2017-07-11 16:34:33,555 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-11 16:34:33,589 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-11 16:34:33,592 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-11 16:34:33,592 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-11 16:34:33,595 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-11 16:34:33,596 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-11 16:34:33,597 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafkaUtils-817924831428150360/meta.properties
2017-07-11 16:34:33,627 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:33,627 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:33,633 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-11 16:34:33,633 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-11 16:34:33,638 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-11 16:34:33,696 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-r Error:KeeperErrorCode = NoNode for /config/topics/test-r
2017-07-11 16:34:33,696 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-11 16:34:33,698 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:33,702 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:33,724 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-11 16:34:33,726 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-11 16:34:33,743 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540000 type:setData cxid:0xb zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-11 16:34:33,746 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540000 type:create cxid:0xc zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:33,752 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:33,755 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-11 16:34:33,778 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-r)], deleted topics: [Set()], new partition replica assignment [Map([test-r,0] -> List(0))]
2017-07-11 16:34:33,783 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-r,0]
2017-07-11 16:34:33,789 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:33,790 INFO  [Thread-426] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:33,803 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-r,0]
2017-07-11 16:34:33,804 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-r,0]
2017-07-11 16:34:33,812 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-r,Partition=0,Replica=0]
2017-07-11 16:34:33,819 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-r,0]
2017-07-11 16:34:33,825 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x4c zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions/0
2017-07-11 16:34:33,826 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x4d zxid:0x25 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-r/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-r/partitions
2017-07-11 16:34:33,849 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-r,Partition=0,Replica=0]
2017-07-11 16:34:33,861 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-11 16:34:33,862 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:33,863 INFO  [Thread-426] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:33,871 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-11 16:34:33,874 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-11 16:34:33,875 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-11 16:34:33,876 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-11 16:34:33,877 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-11 16:34:33,878 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x56 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-11 16:34:33,881 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x57 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-11 16:34:34,009 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-11 16:34:34,133 INFO  [kafka-request-handler-3] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-r-0
2017-07-11 16:34:34,150 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:34,172 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:setData cxid:0x63 zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-11 16:34:34,199 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x64 zxid:0x2f txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:34,206 INFO  [kafka-request-handler-2] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-11 16:34:34,214 INFO  [kafka-request-handler-2] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-11 16:34:34,227 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log test-r-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,232 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-r,0] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,234 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [test-r,0] on broker 0: No checkpointed highwatermark is found for partition test-r-0
2017-07-11 16:34:34,282 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-11 16:34:34,286 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:34,300 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:34,297 INFO  [kafka-request-handler-6] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-11 16:34:34,309 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:34,310 INFO  [kafka-request-handler-6] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,311 WARN  [Thread-426] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 4 : {test-r=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:34,315 INFO  [kafka-request-handler-6] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,318 INFO  [kafka-request-handler-6] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-11 16:34:34,326 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-11 16:34:34,387 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:34,389 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xa4 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-11 16:34:34,392 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xa5 zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-11 16:34:34,406 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xa9 zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-11 16:34:34,425 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xac zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-11 16:34:34,438 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xb1 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-11 16:34:34,445 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xb5 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-11 16:34:34,450 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xb8 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-11 16:34:34,455 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xbb zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-11 16:34:34,461 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xbe zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-11 16:34:34,480 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xc1 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-11 16:34:34,485 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xc4 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-11 16:34:34,490 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xc7 zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-11 16:34:34,496 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xca zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-11 16:34:34,502 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xcd zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-11 16:34:34,510 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xd0 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-11 16:34:34,515 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xd3 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-11 16:34:34,529 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xd6 zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-11 16:34:34,540 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xdb zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-11 16:34:34,566 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xdf zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-11 16:34:34,591 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xe2 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-11 16:34:34,606 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xe5 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-11 16:34:34,612 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xe8 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-11 16:34:34,617 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xeb zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-11 16:34:34,624 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xee zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-11 16:34:34,629 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xf1 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-11 16:34:34,636 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xf4 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-11 16:34:34,642 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xf9 zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-11 16:34:34,647 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0xfc zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-11 16:34:34,653 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x100 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-11 16:34:34,658 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x103 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-11 16:34:34,677 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x106 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-11 16:34:34,683 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x109 zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-11 16:34:34,691 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x10c zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-11 16:34:34,696 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x10f zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-11 16:34:34,703 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x112 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-11 16:34:34,709 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x115 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-11 16:34:34,714 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x118 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-11 16:34:34,719 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x11b zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-11 16:34:34,724 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x11e zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-11 16:34:34,730 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x121 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-11 16:34:34,745 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x125 zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-11 16:34:34,768 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x12a zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-11 16:34:34,783 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x12d zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-11 16:34:34,790 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x130 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-11 16:34:34,795 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x133 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-11 16:34:34,800 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x136 zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-11 16:34:34,805 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x139 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-11 16:34:34,811 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x13c zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-11 16:34:34,816 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x13f zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-11 16:34:34,821 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x142 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-11 16:34:34,833 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212b2540001 type:create cxid:0x145 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-11 16:34:34,853 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-11 16:34:34,933 INFO  [kafka-request-handler-1] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-11 16:34:34,940 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,943 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,943 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-11 16:34:34,949 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,951 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,952 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-11 16:34:34,959 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,960 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,961 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-11 16:34:34,968 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,971 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,972 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-11 16:34:34,978 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,981 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,981 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-11 16:34:34,989 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:34,991 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:34,992 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-11 16:34:34,998 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,000 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,001 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-11 16:34:35,008 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,010 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,011 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-11 16:34:35,018 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,020 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,021 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-11 16:34:35,028 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,031 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,032 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-11 16:34:35,038 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,041 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,041 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-11 16:34:35,047 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,050 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,050 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-11 16:34:35,056 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,058 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,059 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-11 16:34:35,066 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,068 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,069 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-11 16:34:35,075 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,077 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,078 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-11 16:34:35,084 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,086 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,086 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-11 16:34:35,092 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,093 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,094 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-11 16:34:35,099 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,101 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,102 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-11 16:34:35,107 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,109 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,115 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-11 16:34:35,123 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,125 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,125 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-11 16:34:35,131 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,133 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,134 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-11 16:34:35,139 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,141 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,142 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-11 16:34:35,148 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,150 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,150 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-11 16:34:35,159 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,161 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,162 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-11 16:34:35,168 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,174 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,175 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-11 16:34:35,184 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,185 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,186 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-11 16:34:35,191 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,193 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,194 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-11 16:34:35,199 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,201 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,202 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-11 16:34:35,208 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,210 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,211 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-11 16:34:35,217 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,219 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,219 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-11 16:34:35,226 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,228 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,228 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-11 16:34:35,234 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,236 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,236 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-11 16:34:35,243 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,253 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,253 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-11 16:34:35,263 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,265 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,266 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-11 16:34:35,271 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,272 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,273 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-11 16:34:35,278 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,280 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,280 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-11 16:34:35,285 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,287 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,287 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-11 16:34:35,292 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,294 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,294 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-11 16:34:35,300 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,302 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,302 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-11 16:34:35,308 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,310 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,310 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-11 16:34:35,315 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,317 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,318 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-11 16:34:35,323 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,325 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,325 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-11 16:34:35,331 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,332 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,333 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-11 16:34:35,338 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,340 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,340 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-11 16:34:35,345 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,347 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,347 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-11 16:34:35,353 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,355 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,357 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-11 16:34:35,362 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,364 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,365 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-11 16:34:35,371 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,372 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,373 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-11 16:34:35,378 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,380 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,380 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-11 16:34:35,385 INFO  [kafka-request-handler-1] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:35,387 INFO  [kafka-request-handler-1] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafkaUtils-817924831428150360 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:35,388 INFO  [kafka-request-handler-1] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-11 16:34:35,396 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-11 16:34:35,420 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 21 milliseconds.
2017-07-11 16:34:35,421 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-11 16:34:35,423 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-11 16:34:35,424 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-11 16:34:35,426 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 2 milliseconds.
2017-07-11 16:34:35,426 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-11 16:34:35,427 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-11 16:34:35,428 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-11 16:34:35,429 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-11 16:34:35,429 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-11 16:34:35,431 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 2 milliseconds.
2017-07-11 16:34:35,431 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-11 16:34:35,432 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-11 16:34:35,432 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-11 16:34:35,434 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 1 milliseconds.
2017-07-11 16:34:35,434 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-11 16:34:35,435 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-11 16:34:35,436 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-11 16:34:35,437 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 1 milliseconds.
2017-07-11 16:34:35,437 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-11 16:34:35,439 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-11 16:34:35,439 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-11 16:34:35,441 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 2 milliseconds.
2017-07-11 16:34:35,442 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-11 16:34:35,444 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 1 milliseconds.
2017-07-11 16:34:35,444 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-11 16:34:35,446 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 2 milliseconds.
2017-07-11 16:34:35,447 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-11 16:34:35,448 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 1 milliseconds.
2017-07-11 16:34:35,448 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-11 16:34:35,450 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 2 milliseconds.
2017-07-11 16:34:35,451 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-11 16:34:35,452 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-11 16:34:35,453 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-11 16:34:35,456 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 2 milliseconds.
2017-07-11 16:34:35,459 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-11 16:34:35,460 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-11 16:34:35,460 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-11 16:34:35,462 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-11 16:34:35,462 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-11 16:34:35,463 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 1 milliseconds.
2017-07-11 16:34:35,463 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-11 16:34:35,465 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-11 16:34:35,465 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-11 16:34:35,465 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:35,467 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 2 milliseconds.
2017-07-11 16:34:35,467 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-11 16:34:35,468 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-11 16:34:35,468 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-11 16:34:35,470 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-11 16:34:35,470 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-11 16:34:35,471 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-11 16:34:35,472 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-11 16:34:35,473 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-11 16:34:35,473 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-11 16:34:35,479 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-11 16:34:35,479 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-11 16:34:35,479 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-11 16:34:35,479 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:35,480 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-11 16:34:35,481 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-11 16:34:35,482 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-11 16:34:35,482 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-11 16:34:35,484 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-11 16:34:35,484 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-11 16:34:35,485 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-11 16:34:35,486 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-11 16:34:35,487 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-11 16:34:35,487 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-11 16:34:35,489 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-11 16:34:35,489 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-11 16:34:35,490 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-11 16:34:35,490 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-11 16:34:35,492 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-11 16:34:35,492 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-11 16:34:35,493 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-11 16:34:35,494 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-11 16:34:35,495 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-11 16:34:35,495 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-11 16:34:35,496 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-11 16:34:35,497 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-11 16:34:35,498 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-11 16:34:35,498 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-11 16:34:35,500 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 2 milliseconds.
2017-07-11 16:34:35,500 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-11 16:34:35,501 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-11 16:34:35,502 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-11 16:34:35,503 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-11 16:34:35,503 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-11 16:34:35,504 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 1 milliseconds.
2017-07-11 16:34:35,505 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-11 16:34:35,506 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 1 milliseconds.
2017-07-11 16:34:35,506 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-11 16:34:35,507 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-11 16:34:35,508 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-11 16:34:35,509 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-11 16:34:35,509 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-11 16:34:35,512 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-11 16:34:35,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 53 milliseconds.
2017-07-11 16:34:35,563 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-11 16:34:35,564 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 1 milliseconds.
2017-07-11 16:34:35,564 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-11 16:34:35,566 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-11 16:34:35,617 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:35,617 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:35,630 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-11 16:34:35,641 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-11 16:34:35,660 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-11 16:34:35,729 INFO  [Thread-426] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-11 16:34:35,734 INFO  [Thread-426] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-11 16:34:35,766 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-11 16:34:35,767 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 2 is now empty
2017-07-11 16:34:36,266 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:36,267 INFO  [Thread-427] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:36,273 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:36,273 INFO  [Thread-427] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:36,284 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:36,286 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-11 16:34:36,291 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:36,296 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 2
2017-07-11 16:34:36,298 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 3
2017-07-11 16:34:36,300 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 3
2017-07-11 16:34:36,304 INFO  [Thread-427] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 3
2017-07-11 16:34:36,305 INFO  [Thread-427] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-11 16:34:37,272 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:37,274 INFO  [Thread-428] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:37,284 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = sendM-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-11 16:34:37,287 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:37,287 INFO  [Thread-428] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:37,302 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:37,306 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-11 16:34:37,306 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:37,311 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-11 16:34:37,311 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:37,312 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:37,315 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 3
2017-07-11 16:34:37,507 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 4
2017-07-11 16:34:37,513 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 4
2017-07-11 16:34:37,518 INFO  [Thread-428] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 4
2017-07-11 16:34:37,525 INFO  [Thread-428] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-r-0] for group test
2017-07-11 16:34:37,657 INFO  [main] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-11 16:34:39,233 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = rcv-test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-11 16:34:39,240 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-11 16:34:39,240 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:39,241 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:39,243 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:39,244 INFO  [main] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-4
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:39,248 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:39,248 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:40,672 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 4
2017-07-11 16:34:40,673 INFO  [kafka-request-handler-4] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 5 is now empty
2017-07-11 16:34:42,626 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:42,629 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-11 16:34:42,634 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:42,639 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 5
2017-07-11 16:34:42,639 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 6
2017-07-11 16:34:42,642 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 6
2017-07-11 16:34:42,644 INFO  [main] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 6
2017-07-11 16:34:42,645 INFO  [main] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-s-0] for group test
2017-07-11 16:34:42,666 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 6
2017-07-11 16:34:42,666 INFO  [kafka-request-handler-7] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 7 is now empty
2017-07-11 16:34:42,714 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-11 16:34:42,716 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-11 16:34:42,734 INFO  [kafka-request-handler-2] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-11 16:34:42,761 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-11 16:34:42,765 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-11 16:34:42,782 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-11 16:34:42,785 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-11 16:34:42,793 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-11 16:34:42,802 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-11 16:34:43,087 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-11 16:34:43,088 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-11 16:34:43,088 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-11 16:34:43,092 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-11 16:34:43,092 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-11 16:34:43,093 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-11 16:34:43,096 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-11 16:34:43,096 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-11 16:34:43,099 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-11 16:34:43,099 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:43,162 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:43,162 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:43,163 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:43,218 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:43,218 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:43,323 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-11 16:34:43,324 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:43,417 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:43,417 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:43,420 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-11 16:34:43,420 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:43,603 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:43,603 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:43,604 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:43,620 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:43,620 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:43,622 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-11 16:34:43,629 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-11 16:34:43,630 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-11 16:34:43,631 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-11 16:34:43,631 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-11 16:34:43,631 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-11 16:34:44,038 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-11 16:34:44,049 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-11 16:34:44,051 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-11 16:34:44,065 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-11 16:34:44,065 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-11 16:34:44,066 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-11 16:34:44,068 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-11 16:34:44,069 INFO  [ZkClient-EventThread-486-127.0.0.1:38435] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-11 16:34:44,071 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d3212b2540001
2017-07-11 16:34:44,073 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d3212b2540001 closed
2017-07-11 16:34:44,074 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-11 16:34:44,078 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:51638 which had sessionid 0x15d3212b2540001
2017-07-11 16:34:44,080 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-11 16:34:44,081 INFO  [ZkClient-EventThread-482-127.0.0.1:38435] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-11 16:34:44,082 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d3212b2540000
2017-07-11 16:34:44,084 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:51636 which had sessionid 0x15d3212b2540000
2017-07-11 16:34:44,084 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-11 16:34:44,085 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d3212b2540000 closed
2017-07-11 16:34:44,086 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-11 16:34:44,087 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-11 16:34:44,087 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-11 16:34:44,087 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-11 16:34:44,088 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-11 16:34:44,088 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-11 16:34:44,088 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-11 16:34:44,098 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-11 16:34:44,099 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-11 16:34:44,099 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-11 16:34:44,099 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-11 16:34:44,100 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-11 16:34:44,100 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863 sec - in org.apache.samoa.streams.kafka.KafkaUtilsTest
Running org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
2017-07-11 16:34:44,113 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-2258993541128522560/version-2 snapdir /tmp/kafka-1611993770856236926/version-2
2017-07-11 16:34:44,113 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-11 16:34:44,122 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:33882 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@4ea56bdb
2017-07-11 16:34:44,123 INFO  [ZkClient-EventThread-542-127.0.0.1:33882] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-11 16:34:44,124 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-11 16:34:44,126 INFO  [main-SendThread(127.0.0.1:33882)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:33882. Will not attempt to authenticate using SASL (unknown error)
2017-07-11 16:34:44,127 INFO  [main-SendThread(127.0.0.1:33882)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:33882, initiating session
2017-07-11 16:34:44,127 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:35787
2017-07-11 16:34:44,127 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:35787
2017-07-11 16:34:44,128 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-11 16:34:44,175 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d3212e0160000 with negotiated timeout 10000 for client /127.0.0.1:35787
2017-07-11 16:34:44,176 INFO  [main-SendThread(127.0.0.1:33882)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:33882, sessionid = 0x15d3212e0160000, negotiated timeout = 10000
2017-07-11 16:34:44,177 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-11 16:34:44,180 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-3149999740655070354
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:33882
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-11 16:34:44,189 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-11 16:34:44,190 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:33882
2017-07-11 16:34:44,190 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:33882 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@32227215
2017-07-11 16:34:44,191 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-11 16:34:44,192 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-11 16:34:44,193 INFO  [main-SendThread(127.0.0.1:33882)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:33882. Will not attempt to authenticate using SASL (unknown error)
2017-07-11 16:34:44,194 INFO  [main-SendThread(127.0.0.1:33882)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:33882, initiating session
2017-07-11 16:34:44,194 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:35789
2017-07-11 16:34:44,194 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:35789
2017-07-11 16:34:44,196 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d3212e0160001 with negotiated timeout 6000 for client /127.0.0.1:35789
2017-07-11 16:34:44,196 INFO  [main-SendThread(127.0.0.1:33882)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:33882, sessionid = 0x15d3212e0160001, negotiated timeout = 6000
2017-07-11 16:34:44,199 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-11 16:34:44,205 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-11 16:34:44,216 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-11 16:34:44,228 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-11 16:34:44,241 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-11 16:34:44,246 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = tWs03V-XRb6FKoVUxuoP8A
2017-07-11 16:34:44,247 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-3149999740655070354/meta.properties
2017-07-11 16:34:44,250 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-11 16:34:44,251 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-11 16:34:44,253 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-11 16:34:44,254 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-11 16:34:44,285 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-11 16:34:44,286 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-11 16:34:44,287 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-11 16:34:44,288 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-11 16:34:44,295 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-11 16:34:44,297 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-11 16:34:44,299 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:44,302 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:44,303 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-11 16:34:44,306 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-11 16:34:44,311 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-11 16:34:44,312 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-11 16:34:44,312 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-11 16:34:44,315 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-11 16:34:44,319 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-11 16:34:44,332 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-11 16:34:44,333 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-11 16:34:44,334 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-11 16:34:44,336 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-11 16:34:44,337 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-11 16:34:44,338 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-11 16:34:44,340 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-11 16:34:44,341 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-11 16:34:44,342 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-11 16:34:44,342 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-11 16:34:44,343 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-11 16:34:44,344 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-11 16:34:44,345 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-11 16:34:44,345 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-11 16:34:44,346 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-11 16:34:44,346 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-11 16:34:44,348 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-11 16:34:44,349 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-11 16:34:44,350 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-11 16:34:44,353 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:44,356 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-11 16:34:44,357 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:44,359 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:44,360 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-11 16:34:44,361 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-11 16:34:44,361 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-11 16:34:44,372 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-11 16:34:44,374 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 55 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-11 16:34:44,387 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-11 16:34:44,388 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x41 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-11 16:34:44,389 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x42 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-11 16:34:44,392 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-11 16:34:44,392 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-11 16:34:44,393 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-3149999740655070354/meta.properties
2017-07-11 16:34:44,393 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-11 16:34:44,400 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-11 16:34:44,403 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-11 16:34:44,404 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-11 16:34:44,405 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-11 16:34:44,443 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:44,444 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:44,444 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-11 16:34:44,454 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160000 type:setData cxid:0x5 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-11 16:34:44,456 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160000 type:create cxid:0x6 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:44,458 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:44,466 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos)], deleted topics: [Set()], new partition replica assignment [Map([samoa_test-oos,0] -> List(0))]
2017-07-11 16:34:44,467 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [samoa_test-oos,0]
2017-07-11 16:34:44,469 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [samoa_test-oos,0]
2017-07-11 16:34:44,469 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [samoa_test-oos,0]
2017-07-11 16:34:44,470 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-11 16:34:44,471 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [samoa_test-oos,0]
2017-07-11 16:34:44,472 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x4c zxid:0x20 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-11 16:34:44,474 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x4d zxid:0x21 txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-11 16:34:44,478 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:44,479 INFO  [Thread-433] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-5
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:44,484 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:44,484 INFO  [Thread-433] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:44,486 INFO  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-11 16:34:44,488 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:setData cxid:0x54 zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-11 16:34:44,490 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-11 16:34:44,491 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x55 zxid:0x26 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:44,492 INFO  [kafka-request-handler-4] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0
2017-07-11 16:34:44,492 WARN  [Thread-434] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-11 16:34:44,492 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:44,492 INFO  [Thread-434] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:44,495 INFO  [kafka-request-handler-1] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:44,499 INFO  [kafka-request-handler-1] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-11 16:34:44,500 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-11 16:34:44,500 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 56 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:44,504 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:44,506 INFO  [kafka-request-handler-4] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:44,506 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0))]
2017-07-11 16:34:44,506 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0]
2017-07-11 16:34:44,508 INFO  [kafka-request-handler-4] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:44,508 INFO  [kafka-request-handler-4] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-11 16:34:44,510 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0]
2017-07-11 16:34:44,511 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0]
2017-07-11 16:34:44,512 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-11 16:34:44,513 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0]
2017-07-11 16:34:44,514 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x64 zxid:0x29 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-11 16:34:44,527 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x66 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-11 16:34:44,528 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:setData cxid:0x67 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-11 16:34:44,529 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x69 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:44,547 INFO  [kafka-request-handler-6] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-11 16:34:44,550 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0]
2017-07-11 16:34:44,551 INFO  [kafka-request-handler-6] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-11 16:34:44,551 INFO  [kafka-request-handler-5] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-s-0
2017-07-11 16:34:44,557 INFO  [kafka-request-handler-5] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:44,559 INFO  [kafka-request-handler-5] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:44,560 INFO  [kafka-request-handler-5] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-11 16:34:44,586 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-11 16:34:44,589 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:44,593 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:44,595 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:44,600 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-11 16:34:44,643 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:44,645 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xa8 zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-11 16:34:44,648 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xa9 zxid:0x33 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-11 16:34:44,655 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xad zxid:0x37 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-11 16:34:44,661 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xb0 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-11 16:34:44,666 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xb3 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-11 16:34:44,674 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xb6 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-11 16:34:44,678 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xb9 zxid:0x43 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-11 16:34:44,684 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xbc zxid:0x46 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-11 16:34:44,689 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xbf zxid:0x49 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-11 16:34:44,709 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xc2 zxid:0x4c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-11 16:34:44,715 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xc7 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-11 16:34:44,720 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xca zxid:0x52 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-11 16:34:44,725 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xce zxid:0x55 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-11 16:34:44,730 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xd1 zxid:0x58 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-11 16:34:44,735 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xd4 zxid:0x5b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-11 16:34:44,739 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xd7 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-11 16:34:44,744 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xda zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-11 16:34:44,749 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xdd zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-11 16:34:44,754 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xe0 zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-11 16:34:44,768 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xe3 zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-11 16:34:44,778 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xe6 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-11 16:34:44,783 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xe9 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-11 16:34:44,787 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xec zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-11 16:34:44,792 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xef zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-11 16:34:44,797 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xf2 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-11 16:34:44,801 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xf5 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-11 16:34:44,806 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xf8 zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-11 16:34:44,811 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0xfb zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-11 16:34:44,816 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x100 zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-11 16:34:44,832 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x104 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-11 16:34:44,842 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x107 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-11 16:34:44,846 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x10a zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-11 16:34:44,851 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x10d zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-11 16:34:44,856 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x110 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-11 16:34:44,862 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x113 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-11 16:34:44,866 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x116 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-11 16:34:44,877 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x119 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-11 16:34:45,022 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x11e zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-11 16:34:45,029 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x121 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-11 16:34:45,037 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x127 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-11 16:34:45,055 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x12b zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-11 16:34:45,061 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x12e zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-11 16:34:45,065 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x131 zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-11 16:34:45,077 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x134 zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-11 16:34:45,081 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x137 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-11 16:34:45,085 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x13a zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-11 16:34:45,090 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x13d zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-11 16:34:45,094 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x140 zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-11 16:34:45,099 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x143 zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-11 16:34:45,103 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x146 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-11 16:34:45,151 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d3212e0160001 type:create cxid:0x14b zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-11 16:34:45,162 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-11 16:34:45,211 INFO  [kafka-request-handler-3] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-11 16:34:45,216 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,217 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,218 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-11 16:34:45,223 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,224 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,224 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-11 16:34:45,230 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,231 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,232 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-11 16:34:45,237 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,239 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,240 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-11 16:34:45,249 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,250 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,251 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-11 16:34:45,256 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,257 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,258 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-11 16:34:45,263 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,264 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,265 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-11 16:34:45,270 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,271 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,272 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-11 16:34:45,277 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,278 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,278 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-11 16:34:45,284 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,285 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,285 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-11 16:34:45,290 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,291 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,292 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-11 16:34:45,296 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,297 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,298 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-11 16:34:45,302 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,303 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,304 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-11 16:34:45,309 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,310 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,310 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-11 16:34:45,315 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,316 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,316 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-11 16:34:45,321 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,322 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,323 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-11 16:34:45,327 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,328 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,329 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-11 16:34:45,334 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,335 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,335 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-11 16:34:45,340 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,341 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,342 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-11 16:34:45,350 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,351 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,351 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-11 16:34:45,356 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,357 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,358 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-11 16:34:45,362 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,363 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,364 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-11 16:34:45,368 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,369 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,370 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-11 16:34:45,376 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,377 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,378 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-11 16:34:45,382 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,383 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,384 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-11 16:34:45,388 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,389 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,390 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-11 16:34:45,394 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,395 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,396 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-11 16:34:45,401 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,402 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,402 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-11 16:34:45,407 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,408 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,409 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-11 16:34:45,414 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,415 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,415 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-11 16:34:45,420 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,421 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,422 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-11 16:34:45,427 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,428 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,428 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-11 16:34:45,432 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,433 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,433 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-11 16:34:45,437 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,438 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,438 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-11 16:34:45,442 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,443 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,444 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-11 16:34:45,451 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,452 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,452 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-11 16:34:45,457 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,458 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,458 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-11 16:34:45,462 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,463 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,464 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-11 16:34:45,468 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,469 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,470 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-11 16:34:45,477 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,478 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,478 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-11 16:34:45,486 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,487 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,487 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-11 16:34:45,492 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,493 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,493 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-11 16:34:45,498 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,499 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,500 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-11 16:34:45,505 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,506 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,507 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-11 16:34:45,511 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,513 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,514 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-11 16:34:45,518 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,519 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,520 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-11 16:34:45,525 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,526 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,526 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-11 16:34:45,531 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,532 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,533 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-11 16:34:45,538 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,539 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,539 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-11 16:34:45,545 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:45,546 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-3149999740655070354 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:45,546 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-11 16:34:45,550 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-11 16:34:45,554 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 4 milliseconds.
2017-07-11 16:34:45,554 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-11 16:34:45,556 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-11 16:34:45,556 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-11 16:34:45,557 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-11 16:34:45,557 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-11 16:34:45,559 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 2 milliseconds.
2017-07-11 16:34:45,559 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-11 16:34:45,561 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 2 milliseconds.
2017-07-11 16:34:45,561 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-11 16:34:45,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-11 16:34:45,562 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-11 16:34:45,564 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-11 16:34:45,564 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-11 16:34:45,568 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 4 milliseconds.
2017-07-11 16:34:45,569 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-11 16:34:45,572 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 3 milliseconds.
2017-07-11 16:34:45,573 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-11 16:34:45,576 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 3 milliseconds.
2017-07-11 16:34:45,576 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-11 16:34:45,579 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 3 milliseconds.
2017-07-11 16:34:45,579 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-11 16:34:45,583 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 4 milliseconds.
2017-07-11 16:34:45,583 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-11 16:34:45,587 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 4 milliseconds.
2017-07-11 16:34:45,588 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-11 16:34:45,592 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 4 milliseconds.
2017-07-11 16:34:45,592 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-11 16:34:45,596 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 4 milliseconds.
2017-07-11 16:34:45,596 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-11 16:34:45,599 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 3 milliseconds.
2017-07-11 16:34:45,599 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-11 16:34:45,603 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 3 milliseconds.
2017-07-11 16:34:45,603 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-11 16:34:45,607 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 4 milliseconds.
2017-07-11 16:34:45,607 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-11 16:34:45,742 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 135 milliseconds.
2017-07-11 16:34:45,742 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-11 16:34:45,743 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-11 16:34:45,743 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-11 16:34:45,745 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 0 milliseconds.
2017-07-11 16:34:45,745 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-11 16:34:45,745 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:45,746 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-11 16:34:45,746 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-11 16:34:45,748 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-11 16:34:45,748 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-11 16:34:45,749 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-11 16:34:45,749 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-11 16:34:45,749 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-11 16:34:45,750 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:45,751 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-11 16:34:45,751 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-11 16:34:45,752 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-11 16:34:45,752 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-11 16:34:45,754 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 2 milliseconds.
2017-07-11 16:34:45,754 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-11 16:34:45,756 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 2 milliseconds.
2017-07-11 16:34:45,756 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-11 16:34:45,756 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-11 16:34:45,757 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-11 16:34:45,758 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-11 16:34:45,759 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-11 16:34:45,759 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-11 16:34:45,760 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 1 milliseconds.
2017-07-11 16:34:45,761 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-11 16:34:45,762 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-11 16:34:45,762 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-11 16:34:45,763 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-11 16:34:45,763 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-11 16:34:45,765 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-11 16:34:45,765 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-11 16:34:45,766 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-11 16:34:45,766 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-11 16:34:45,768 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-11 16:34:45,768 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-11 16:34:45,770 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-11 16:34:45,770 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-11 16:34:45,771 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-11 16:34:45,771 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-11 16:34:45,772 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-11 16:34:45,772 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-11 16:34:45,774 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 0 milliseconds.
2017-07-11 16:34:45,774 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-11 16:34:45,775 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 1 milliseconds.
2017-07-11 16:34:45,775 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-11 16:34:45,776 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-11 16:34:45,776 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-11 16:34:45,777 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-11 16:34:45,778 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-11 16:34:45,779 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 1 milliseconds.
2017-07-11 16:34:45,779 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-11 16:34:45,780 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 1 milliseconds.
2017-07-11 16:34:45,780 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-11 16:34:45,781 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-11 16:34:45,782 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-11 16:34:45,783 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-11 16:34:45,783 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-11 16:34:45,785 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 0 milliseconds.
2017-07-11 16:34:45,785 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-11 16:34:45,786 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 1 milliseconds.
2017-07-11 16:34:45,786 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-11 16:34:45,788 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 2 milliseconds.
2017-07-11 16:34:45,863 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:45,864 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:45,865 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-11 16:34:45,865 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-11 16:34:45,876 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-11 16:34:45,878 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-11 16:34:45,880 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-11 16:34:54,046 INFO  [Thread-434] producer.KafkaProducer (KafkaProducer.java:close(689)) - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2017-07-11 16:34:54,046 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-11 16:34:54,055 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-11 16:34:54,069 INFO  [kafka-request-handler-1] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-11 16:34:54,071 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-11 16:34:54,071 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-11 16:34:54,072 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-11 16:34:54,074 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-11 16:34:54,074 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-11 16:34:54,074 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-11 16:34:54,076 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-11 16:34:54,252 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-11 16:34:54,253 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-11 16:34:54,253 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-11 16:34:54,254 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-11 16:34:54,254 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-11 16:34:54,255 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-11 16:34:54,255 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-11 16:34:54,255 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-11 16:34:54,255 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-11 16:34:54,256 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:54,403 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:54,403 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:54,403 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:54,414 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:54,415 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:54,429 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-11 16:34:54,430 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:54,615 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:54,615 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:54,615 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-11 16:34:54,616 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:54,622 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:54,622 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:54,622 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:34:54,820 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:34:54,821 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:34:54,821 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-11 16:34:54,821 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-11 16:34:54,821 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-11 16:34:54,821 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-11 16:34:54,822 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-11 16:34:54,822 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-11 16:34:55,077 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-11 16:34:55,078 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-11 16:34:55,078 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-11 16:34:55,080 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-11 16:34:55,080 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-11 16:34:55,080 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-11 16:34:55,080 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-11 16:34:55,081 INFO  [ZkClient-EventThread-545-127.0.0.1:33882] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-11 16:34:55,081 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d3212e0160001
2017-07-11 16:34:55,083 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d3212e0160001 closed
2017-07-11 16:34:55,083 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-11 16:34:55,083 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:35789 which had sessionid 0x15d3212e0160001
2017-07-11 16:34:55,085 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-11 16:34:55,085 INFO  [ZkClient-EventThread-542-127.0.0.1:33882] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-11 16:34:55,090 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d3212e0160000
2017-07-11 16:34:55,090 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d3212e0160000 closed
2017-07-11 16:34:55,091 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-11 16:34:55,091 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-11 16:34:55,091 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-11 16:34:55,091 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-11 16:34:55,091 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:35787 which had sessionid 0x15d3212e0160000
2017-07-11 16:34:55,091 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-11 16:34:55,091 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-11 16:34:55,092 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-11 16:34:55,092 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-11 16:34:55,093 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-11 16:34:55,093 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-11 16:34:55,094 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-11 16:34:55,094 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-11 16:34:55,094 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-11 16:34:55,094 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.979 sec - in org.apache.samoa.streams.kafka.KafkaEntranceProcessorTest
Running org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
2017-07-11 16:34:55,097 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:<init>(162)) - Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir /tmp/kafka-4351514691518417623/version-2 snapdir /tmp/kafka-6271737578728217164/version-2
2017-07-11 16:34:55,097 INFO  [main] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:configure(94)) - binding to port /127.0.0.1:0
2017-07-11 16:34:55,099 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:37801 sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@1a2e0d57
2017-07-11 16:34:55,099 INFO  [ZkClient-EventThread-593-127.0.0.1:37801] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-11 16:34:55,099 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-11 16:34:55,101 INFO  [main-SendThread(127.0.0.1:37801)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:37801. Will not attempt to authenticate using SASL (unknown error)
2017-07-11 16:34:55,101 INFO  [main-SendThread(127.0.0.1:37801)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:37801, initiating session
2017-07-11 16:34:55,101 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:43630
2017-07-11 16:34:55,101 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:43630
2017-07-11 16:34:55,102 INFO  [SyncThread:0] persistence.FileTxnLog (FileTxnLog.java:append(199)) - Creating new log file: log.1
2017-07-11 16:34:55,113 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d32130afa0000 with negotiated timeout 10000 for client /127.0.0.1:43630
2017-07-11 16:34:55,113 INFO  [main-SendThread(127.0.0.1:37801)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:37801, sessionid = 0x15d32130afa0000, negotiated timeout = 10000
2017-07-11 16:34:55,114 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-11 16:34:55,115 INFO  [main] server.KafkaConfig (AbstractConfig.java:logAll(180)) - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.10.2-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = PLAINTEXT://127.0.0.1:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-8736217898375340280
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.10.2-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	unclean.leader.election.enable = true
	zookeeper.connect = 127.0.0.1:37801
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2017-07-11 16:34:55,116 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - starting
2017-07-11 16:34:55,116 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Connecting to zookeeper on 127.0.0.1:37801
2017-07-11 16:34:55,117 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=127.0.0.1:37801 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5f025000
2017-07-11 16:34:55,117 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] zkclient.ZkEventThread (ZkEventThread.java:run(65)) - Starting ZkClient event thread.
2017-07-11 16:34:55,117 INFO  [main] zkclient.ZkClient (ZkClient.java:waitForKeeperState(936)) - Waiting for keeper state SyncConnected
2017-07-11 16:34:55,118 INFO  [main-SendThread(127.0.0.1:37801)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 127.0.0.1/127.0.0.1:37801. Will not attempt to authenticate using SASL (unknown error)
2017-07-11 16:34:55,118 INFO  [main-SendThread(127.0.0.1:37801)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 127.0.0.1/127.0.0.1:37801, initiating session
2017-07-11 16:34:55,118 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:43631
2017-07-11 16:34:55,118 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:43631
2017-07-11 16:34:55,121 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x15d32130afa0001 with negotiated timeout 6000 for client /127.0.0.1:43631
2017-07-11 16:34:55,121 INFO  [main-SendThread(127.0.0.1:37801)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 127.0.0.1/127.0.0.1:37801, sessionid = 0x15d32130afa0001, negotiated timeout = 6000
2017-07-11 16:34:55,121 INFO  [main-EventThread] zkclient.ZkClient (ZkClient.java:processStateChanged(713)) - zookeeper state changed (SyncConnected)
2017-07-11 16:34:55,125 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x4 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2017-07-11 16:34:55,130 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xa zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2017-07-11 16:34:55,135 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x12 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2017-07-11 16:34:55,141 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x1a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2017-07-11 16:34:55,146 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - Cluster ID = JKl32iAcT5GsGE8MkNQdQQ
2017-07-11 16:34:55,146 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-8736217898375340280/meta.properties
2017-07-11 16:34:55,147 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Starting 
2017-07-11 16:34:55,148 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Starting 
2017-07-11 16:34:55,149 INFO  [main] log.LogManager (Logging.scala:info(70)) - Loading logs.
2017-07-11 16:34:55,150 INFO  [main] log.LogManager (Logging.scala:info(70)) - Logs loading complete in 0 ms.
2017-07-11 16:34:55,171 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log cleanup with a period of 300000 ms.
2017-07-11 16:34:55,171 INFO  [main] log.LogManager (Logging.scala:info(70)) - Starting log flusher with a default period of 9223372036854775807 ms.
2017-07-11 16:34:55,172 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Starting the log cleaner
2017-07-11 16:34:55,173 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Starting 
2017-07-11 16:34:55,176 INFO  [main] network.Acceptor (Logging.scala:info(70)) - Awaiting socket connections on 127.0.0.1:9092.
2017-07-11 16:34:55,177 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Started 1 acceptor threads
2017-07-11 16:34:55,179 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:55,179 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:55,180 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller starting up
2017-07-11 16:34:55,181 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /controller (is it secure? false)
2017-07-11 16:34:55,183 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-11 16:34:55,184 INFO  [main] server.ZookeeperLeaderElector (Logging.scala:info(70)) - 0 successfully elected as leader
2017-07-11 16:34:55,184 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 starting become controller state transition
2017-07-11 16:34:55,185 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:setData cxid:0x24 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2017-07-11 16:34:55,187 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller 0 incremented epoch to 1
2017-07-11 16:34:55,192 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions undergoing preferred replica election: 
2017-07-11 16:34:55,192 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions that completed preferred replica election: 
2017-07-11 16:34:55,192 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming preferred replica election for partitions: 
2017-07-11 16:34:55,193 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions being reassigned: Map()
2017-07-11 16:34:55,193 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Partitions already reassigned: Set()
2017-07-11 16:34:55,193 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Resuming reassignment of partitions: Map()
2017-07-11 16:34:55,194 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics to be deleted: 
2017-07-11 16:34:55,194 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: List of topics ineligible for deletion: 
2017-07-11 16:34:55,194 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently active brokers in the cluster: Set()
2017-07-11 16:34:55,194 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Currently shutting brokers in the cluster: Set()
2017-07-11 16:34:55,195 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Current list of topics in the cluster: Set()
2017-07-11 16:34:55,195 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Started replica state machine with initial state -> Map()
2017-07-11 16:34:55,195 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map()
2017-07-11 16:34:55,195 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 1
2017-07-11 16:34:55,196 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Starting preferred replica leader election for partitions 
2017-07-11 16:34:55,196 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions 
2017-07-11 16:34:55,196 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:delete cxid:0x35 zxid:0x18 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2017-07-11 16:34:55,197 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: starting the partition rebalance scheduler
2017-07-11 16:34:55,197 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Controller startup complete
2017-07-11 16:34:55,199 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:55,199 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:55,200 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Starting 
2017-07-11 16:34:55,200 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] server.ZookeeperLeaderElector$LeaderChangeListener (Logging.scala:info(70)) - New leader is 0
2017-07-11 16:34:55,201 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Starting up.
2017-07-11 16:34:55,201 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Startup complete.
2017-07-11 16:34:55,201 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds.
2017-07-11 16:34:55,204 INFO  [main] utils.Mx4jLoader$ (Logging.scala:info(70)) - Will not load MX4J, mx4j-tools.jar is not in the classpath
2017-07-11 16:34:55,204 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9262 : {samoa_test-oos=INVALID_REPLICATION_FACTOR}
2017-07-11 16:34:55,204 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 58 : {test-s=INVALID_REPLICATION_FACTOR}
2017-07-11 16:34:55,208 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Creating /brokers/ids/0 (is it secure? false)
2017-07-11 16:34:55,209 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x42 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
2017-07-11 16:34:55,209 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x43 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
2017-07-11 16:34:55,211 INFO  [main] utils.ZKCheckedEphemeral (Logging.scala:info(70)) - Result of znode creation is: OK
2017-07-11 16:34:55,212 INFO  [main] utils.ZkUtils (Logging.scala:info(70)) - Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(127.0.0.1,9092,ListenerName(PLAINTEXT),PLAINTEXT)
2017-07-11 16:34:55,212 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children 0
2017-07-11 16:34:55,212 WARN  [main] server.BrokerMetadataCheckpoint (Logging.scala:warn(85)) - No meta.properties file under dir /tmp/kafka-8736217898375340280/meta.properties
2017-07-11 16:34:55,215 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine$BrokerChangeListener (Logging.scala:info(70)) - [BrokerChangeListener on Controller 0]: Newly added brokers: 0, deleted brokers: , all live brokers: 0
2017-07-11 16:34:55,217 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New broker startup callback for 0
2017-07-11 16:34:55,218 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Starting 
2017-07-11 16:34:55,218 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Controller 0 connected to 127.0.0.1:9092 (id: 0 rack: null) for sending state change requests
2017-07-11 16:34:55,220 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:55,220 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:55,220 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], started
2017-07-11 16:34:55,229 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0000 type:setData cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/topics/test-kdp Error:KeeperErrorCode = NoNode for /config/topics/test-kdp
2017-07-11 16:34:55,231 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0000 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:55,233 INFO  [main] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:55,236 INFO  [main] producer.ProducerConfig (AbstractConfig.java:logAll(180)) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = test
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2017-07-11 16:34:55,240 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(test-kdp)], deleted topics: [Set()], new partition replica assignment [Map([test-kdp,0] -> List(0))]
2017-07-11 16:34:55,240 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-kdp,0]
2017-07-11 16:34:55,241 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-kdp,0]
2017-07-11 16:34:55,241 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-kdp,0]
2017-07-11 16:34:55,241 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-11 16:34:55,242 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'group.id' was supplied but isn't a known config.
2017-07-11 16:34:55,242 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-kdp,0]
2017-07-11 16:34:55,242 WARN  [main] producer.ProducerConfig (AbstractConfig.java:logUnused(188)) - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2017-07-11 16:34:55,243 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:55,243 INFO  [main] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:55,243 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x4d zxid:0x20 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions/0
2017-07-11 16:34:55,247 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:55,247 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x4e zxid:0x21 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-kdp/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-kdp/partitions
2017-07-11 16:34:55,248 INFO  [Thread-439] consumer.ConsumerConfig (AbstractConfig.java:logAll(180)) - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = consumer-6
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2017-07-11 16:34:55,254 WARN  [kafka-producer-network-thread | test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 1 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,255 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-kdp,Partition=0,Replica=0]
2017-07-11 16:34:55,256 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(83)) - Kafka version : 0.10.2.0
2017-07-11 16:34:55,257 INFO  [Thread-439] utils.AppInfoParser (AppInfoParser.java:<init>(84)) - Kafka commitId : 576d93a8dc0cf421
2017-07-11 16:34:55,258 INFO  [kafka-request-handler-2] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions test-kdp-0
2017-07-11 16:34:55,261 INFO  [kafka-request-handler-2] log.Log (Logging.scala:info(70)) - Completed load of log test-kdp-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,263 INFO  [kafka-request-handler-2] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-kdp,0] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,263 INFO  [kafka-request-handler-2] cluster.Partition (Logging.scala:info(70)) - Partition [test-kdp,0] on broker 0: No checkpointed highwatermark is found for partition test-kdp-0
2017-07-11 16:34:55,267 WARN  [Thread-439] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 2 : {test-kdp=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,272 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:setData cxid:0x5d zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2017-07-11 16:34:55,282 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x5e zxid:0x26 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:55,285 INFO  [kafka-request-handler-3] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}}
2017-07-11 16:34:55,296 INFO  [kafka-request-handler-3] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful
2017-07-11 16:34:55,314 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map([__consumer_offsets,19] -> List(0), [__consumer_offsets,30] -> List(0), [__consumer_offsets,47] -> List(0), [__consumer_offsets,29] -> List(0), [__consumer_offsets,41] -> List(0), [__consumer_offsets,39] -> List(0), [__consumer_offsets,10] -> List(0), [__consumer_offsets,17] -> List(0), [__consumer_offsets,14] -> List(0), [__consumer_offsets,40] -> List(0), [__consumer_offsets,18] -> List(0), [__consumer_offsets,26] -> List(0), [__consumer_offsets,0] -> List(0), [__consumer_offsets,24] -> List(0), [__consumer_offsets,33] -> List(0), [__consumer_offsets,20] -> List(0), [__consumer_offsets,21] -> List(0), [__consumer_offsets,3] -> List(0), [__consumer_offsets,5] -> List(0), [__consumer_offsets,22] -> List(0), [__consumer_offsets,12] -> List(0), [__consumer_offsets,8] -> List(0), [__consumer_offsets,23] -> List(0), [__consumer_offsets,15] -> List(0), [__consumer_offsets,48] -> List(0), [__consumer_offsets,11] -> List(0), [__consumer_offsets,13] -> List(0), [__consumer_offsets,49] -> List(0), [__consumer_offsets,6] -> List(0), [__consumer_offsets,28] -> List(0), [__consumer_offsets,4] -> List(0), [__consumer_offsets,37] -> List(0), [__consumer_offsets,31] -> List(0), [__consumer_offsets,44] -> List(0), [__consumer_offsets,42] -> List(0), [__consumer_offsets,34] -> List(0), [__consumer_offsets,46] -> List(0), [__consumer_offsets,25] -> List(0), [__consumer_offsets,45] -> List(0), [__consumer_offsets,27] -> List(0), [__consumer_offsets,32] -> List(0), [__consumer_offsets,43] -> List(0), [__consumer_offsets,36] -> List(0), [__consumer_offsets,35] -> List(0), [__consumer_offsets,7] -> List(0), [__consumer_offsets,9] -> List(0), [__consumer_offsets,38] -> List(0), [__consumer_offsets,1] -> List(0), [__consumer_offsets,16] -> List(0), [__consumer_offsets,2] -> List(0))]
2017-07-11 16:34:55,315 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:55,316 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:55,317 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:setData cxid:0x6a zxid:0x29 txntype:-1 reqpath:n/a Error Path:/config/topics/test-s Error:KeeperErrorCode = NoNode for /config/topics/test-s
2017-07-11 16:34:55,317 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:55,318 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x6b zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:55,319 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-11 16:34:55,320 INFO  [kafka-request-handler-4] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:55,321 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:setData cxid:0x72 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/samoa_test-oos Error:KeeperErrorCode = NoNode for /config/topics/samoa_test-oos
2017-07-11 16:34:55,322 INFO  [kafka-request-handler-4] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic test-s with 1 partitions and replication factor 1 is successful
2017-07-11 16:34:55,322 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x74 zxid:0x2e txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
2017-07-11 16:34:55,322 WARN  [kafka-producer-network-thread | rcv-test] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 59 : {test-s=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,324 INFO  [kafka-request-handler-0] admin.AdminUtils$ (Logging.scala:info(70)) - Topic creation {"version":1,"partitions":{"0":[0]}}
2017-07-11 16:34:55,326 INFO  [kafka-request-handler-0] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Auto creation of topic samoa_test-oos with 1 partitions and replication factor 1 is successful
2017-07-11 16:34:55,326 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9263 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,352 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [__consumer_offsets,19],[__consumer_offsets,30],[__consumer_offsets,47],[__consumer_offsets,29],[__consumer_offsets,41],[__consumer_offsets,39],[__consumer_offsets,10],[__consumer_offsets,17],[__consumer_offsets,14],[__consumer_offsets,40],[__consumer_offsets,18],[__consumer_offsets,26],[__consumer_offsets,0],[__consumer_offsets,24],[__consumer_offsets,33],[__consumer_offsets,20],[__consumer_offsets,21],[__consumer_offsets,3],[__consumer_offsets,5],[__consumer_offsets,22],[__consumer_offsets,12],[__consumer_offsets,8],[__consumer_offsets,23],[__consumer_offsets,15],[__consumer_offsets,48],[__consumer_offsets,11],[__consumer_offsets,13],[__consumer_offsets,49],[__consumer_offsets,6],[__consumer_offsets,28],[__consumer_offsets,4],[__consumer_offsets,37],[__consumer_offsets,31],[__consumer_offsets,44],[__consumer_offsets,42],[__consumer_offsets,34],[__consumer_offsets,46],[__consumer_offsets,25],[__consumer_offsets,45],[__consumer_offsets,27],[__consumer_offsets,32],[__consumer_offsets,43],[__consumer_offsets,36],[__consumer_offsets,35],[__consumer_offsets,7],[__consumer_offsets,9],[__consumer_offsets,38],[__consumer_offsets,1],[__consumer_offsets,16],[__consumer_offsets,2]
2017-07-11 16:34:55,353 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xa9 zxid:0x31 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19
2017-07-11 16:34:55,354 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xaa zxid:0x32 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions
2017-07-11 16:34:55,359 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xae zxid:0x36 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30
2017-07-11 16:34:55,362 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xb1 zxid:0x39 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47
2017-07-11 16:34:55,366 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xb4 zxid:0x3c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29
2017-07-11 16:34:55,370 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xb8 zxid:0x3f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41
2017-07-11 16:34:55,376 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xbd zxid:0x42 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39
2017-07-11 16:34:55,379 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xc0 zxid:0x45 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10
2017-07-11 16:34:55,382 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xc3 zxid:0x48 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17
2017-07-11 16:34:55,391 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xc6 zxid:0x4b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14
2017-07-11 16:34:55,401 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xc9 zxid:0x4e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40
2017-07-11 16:34:55,405 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xcc zxid:0x51 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18
2017-07-11 16:34:55,408 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xcf zxid:0x54 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26
2017-07-11 16:34:55,411 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xd2 zxid:0x57 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0
2017-07-11 16:34:55,414 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xd5 zxid:0x5a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24
2017-07-11 16:34:55,417 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xd8 zxid:0x5d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33
2017-07-11 16:34:55,420 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xdb zxid:0x60 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20
2017-07-11 16:34:55,423 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xde zxid:0x63 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21
2017-07-11 16:34:55,427 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xe1 zxid:0x66 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3
2017-07-11 16:34:55,446 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xe7 zxid:0x69 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5
2017-07-11 16:34:55,451 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xea zxid:0x6c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22
2017-07-11 16:34:55,454 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9265 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,457 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xee zxid:0x6f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12
2017-07-11 16:34:55,464 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xf2 zxid:0x72 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8
2017-07-11 16:34:55,469 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xf6 zxid:0x75 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23
2017-07-11 16:34:55,474 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xfb zxid:0x78 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15
2017-07-11 16:34:55,479 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0xfe zxid:0x7b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48
2017-07-11 16:34:55,484 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x102 zxid:0x7e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11
2017-07-11 16:34:55,490 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x105 zxid:0x81 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13
2017-07-11 16:34:55,496 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x108 zxid:0x84 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49
2017-07-11 16:34:55,500 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!
2017-07-11 16:34:55,501 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x10b zxid:0x87 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6
2017-07-11 16:34:55,523 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x10e zxid:0x8a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28
2017-07-11 16:34:55,528 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x111 zxid:0x8d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4
2017-07-11 16:34:55,533 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x114 zxid:0x90 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37
2017-07-11 16:34:55,546 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x117 zxid:0x93 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31
2017-07-11 16:34:55,551 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x11a zxid:0x96 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44
2017-07-11 16:34:55,556 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x11f zxid:0x99 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42
2017-07-11 16:34:55,561 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x122 zxid:0x9c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34
2017-07-11 16:34:55,564 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9267 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,567 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x127 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46
2017-07-11 16:34:55,582 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x12e zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25
2017-07-11 16:34:55,589 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x132 zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45
2017-07-11 16:34:55,605 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x135 zxid:0xa8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27
2017-07-11 16:34:55,614 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x138 zxid:0xab txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32
2017-07-11 16:34:55,618 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x13b zxid:0xae txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43
2017-07-11 16:34:55,622 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x13e zxid:0xb1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36
2017-07-11 16:34:55,627 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x141 zxid:0xb4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35
2017-07-11 16:34:55,632 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x144 zxid:0xb7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7
2017-07-11 16:34:55,636 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x147 zxid:0xba txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9
2017-07-11 16:34:55,641 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x14a zxid:0xbd txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38
2017-07-11 16:34:55,646 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x14d zxid:0xc0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1
2017-07-11 16:34:55,651 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x150 zxid:0xc3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16
2017-07-11 16:34:55,657 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x153 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2
2017-07-11 16:34:55,678 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=__consumer_offsets,Partition=25,Replica=0],[Topic=__consumer_offsets,Partition=12,Replica=0],[Topic=__consumer_offsets,Partition=31,Replica=0],[Topic=__consumer_offsets,Partition=40,Replica=0],[Topic=__consumer_offsets,Partition=35,Replica=0],[Topic=__consumer_offsets,Partition=9,Replica=0],[Topic=__consumer_offsets,Partition=43,Replica=0],[Topic=__consumer_offsets,Partition=2,Replica=0],[Topic=__consumer_offsets,Partition=11,Replica=0],[Topic=__consumer_offsets,Partition=29,Replica=0],[Topic=__consumer_offsets,Partition=30,Replica=0],[Topic=__consumer_offsets,Partition=4,Replica=0],[Topic=__consumer_offsets,Partition=42,Replica=0],[Topic=__consumer_offsets,Partition=26,Replica=0],[Topic=__consumer_offsets,Partition=34,Replica=0],[Topic=__consumer_offsets,Partition=17,Replica=0],[Topic=__consumer_offsets,Partition=37,Replica=0],[Topic=__consumer_offsets,Partition=27,Replica=0],[Topic=__consumer_offsets,Partition=10,Replica=0],[Topic=__consumer_offsets,Partition=41,Replica=0],[Topic=__consumer_offsets,Partition=20,Replica=0],[Topic=__consumer_offsets,Partition=28,Replica=0],[Topic=__consumer_offsets,Partition=46,Replica=0],[Topic=__consumer_offsets,Partition=39,Replica=0],[Topic=__consumer_offsets,Partition=47,Replica=0],[Topic=__consumer_offsets,Partition=49,Replica=0],[Topic=__consumer_offsets,Partition=22,Replica=0],[Topic=__consumer_offsets,Partition=1,Replica=0],[Topic=__consumer_offsets,Partition=24,Replica=0],[Topic=__consumer_offsets,Partition=6,Replica=0],[Topic=__consumer_offsets,Partition=36,Replica=0],[Topic=__consumer_offsets,Partition=8,Replica=0],[Topic=__consumer_offsets,Partition=38,Replica=0],[Topic=__consumer_offsets,Partition=16,Replica=0],[Topic=__consumer_offsets,Partition=21,Replica=0],[Topic=__consumer_offsets,Partition=18,Replica=0],[Topic=__consumer_offsets,Partition=0,Replica=0],[Topic=__consumer_offsets,Partition=48,Replica=0],[Topic=__consumer_offsets,Partition=5,Replica=0],[Topic=__consumer_offsets,Partition=13,Replica=0],[Topic=__consumer_offsets,Partition=3,Replica=0],[Topic=__consumer_offsets,Partition=44,Replica=0],[Topic=__consumer_offsets,Partition=15,Replica=0],[Topic=__consumer_offsets,Partition=7,Replica=0],[Topic=__consumer_offsets,Partition=19,Replica=0],[Topic=__consumer_offsets,Partition=33,Replica=0],[Topic=__consumer_offsets,Partition=45,Replica=0],[Topic=__consumer_offsets,Partition=23,Replica=0],[Topic=__consumer_offsets,Partition=32,Replica=0],[Topic=__consumer_offsets,Partition=14,Replica=0]
2017-07-11 16:34:55,686 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9269 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,690 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine$TopicChangeListener (Logging.scala:info(70)) - [TopicChangeListener on Controller 0]: New topics: [Set(samoa_test-oos, test-s)], deleted topics: [Set()], new partition replica assignment [Map([test-s,0] -> List(0), [samoa_test-oos,0] -> List(0))]
2017-07-11 16:34:55,690 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New topic creation callback for [test-s,0],[samoa_test-oos,0]
2017-07-11 16:34:55,693 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: New partition creation callback for [test-s,0],[samoa_test-oos,0]
2017-07-11 16:34:55,694 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test-s,0],[samoa_test-oos,0]
2017-07-11 16:34:55,695 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-11 16:34:55,696 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test-s,0],[samoa_test-oos,0]
2017-07-11 16:34:55,703 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x167 zxid:0xc9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions/0
2017-07-11 16:34:55,707 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x168 zxid:0xca txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-s/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-s/partitions
2017-07-11 16:34:55,714 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x16c zxid:0xce txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions/0
2017-07-11 16:34:55,716 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15d32130afa0001 type:create cxid:0x16d zxid:0xcf txntype:-1 reqpath:n/a Error Path:/brokers/topics/samoa_test-oos/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/samoa_test-oos/partitions
2017-07-11 16:34:55,723 INFO  [kafka-request-handler-0] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40
2017-07-11 16:34:55,723 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test-s,Partition=0,Replica=0],[Topic=samoa_test-oos,Partition=0,Replica=0]
2017-07-11 16:34:55,731 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,732 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,0] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,733 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0
2017-07-11 16:34:55,739 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-29 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,740 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,29] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,741 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29
2017-07-11 16:34:55,746 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-48 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,747 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,48] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,748 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48
2017-07-11 16:34:55,770 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-10 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,778 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,10] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,778 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10
2017-07-11 16:34:55,788 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-45 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,789 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,45] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,790 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45
2017-07-11 16:34:55,804 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9271 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:55,807 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-26 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,810 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,26] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,811 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26
2017-07-11 16:34:55,822 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-7 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,823 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,7] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,824 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7
2017-07-11 16:34:55,831 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-42 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,832 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,42] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,833 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42
2017-07-11 16:34:55,843 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-4 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,844 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,4] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,846 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4
2017-07-11 16:34:55,853 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-23 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,854 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,23] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,854 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23
2017-07-11 16:34:55,861 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-1 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,862 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,1] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,863 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1
2017-07-11 16:34:55,869 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-20 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,870 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,20] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,871 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20
2017-07-11 16:34:55,877 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-39 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,879 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,39] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,879 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39
2017-07-11 16:34:55,887 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-17 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:55,900 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,17] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:55,901 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17
2017-07-11 16:34:56,406 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-36 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,407 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,36] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,408 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36
2017-07-11 16:34:56,415 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9273 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:56,417 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-14 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,418 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,14] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,419 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14
2017-07-11 16:34:56,427 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-33 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,428 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,33] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,432 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33
2017-07-11 16:34:56,437 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-49 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,438 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,49] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,438 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49
2017-07-11 16:34:56,442 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-11 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,443 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,11] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,444 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11
2017-07-11 16:34:56,447 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-30 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,448 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,30] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,449 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30
2017-07-11 16:34:56,452 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-46 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,453 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,46] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,454 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46
2017-07-11 16:34:56,457 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-27 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,458 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,27] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,459 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27
2017-07-11 16:34:56,463 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-8 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,464 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,8] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,464 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8
2017-07-11 16:34:56,468 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-24 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,469 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,24] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,470 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24
2017-07-11 16:34:56,473 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-43 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,474 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,43] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,475 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43
2017-07-11 16:34:56,478 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-5 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,479 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,5] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,480 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5
2017-07-11 16:34:56,484 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-21 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,485 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,21] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,488 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21
2017-07-11 16:34:56,493 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-2 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,494 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,2] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,494 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2
2017-07-11 16:34:56,500 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-40 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,501 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,40] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,501 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40
2017-07-11 16:34:56,505 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-37 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,506 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,37] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,507 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37
2017-07-11 16:34:56,511 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-18 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,512 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,18] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,512 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18
2017-07-11 16:34:56,516 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-34 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,517 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,34] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,517 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34
2017-07-11 16:34:56,520 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9275 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:56,522 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-15 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,523 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,15] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,524 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15
2017-07-11 16:34:56,532 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-12 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,533 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,12] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,533 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12
2017-07-11 16:34:56,537 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-31 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,540 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,31] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,540 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31
2017-07-11 16:34:56,544 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-9 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,545 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,9] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,546 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9
2017-07-11 16:34:56,549 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-47 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,550 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,47] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,551 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47
2017-07-11 16:34:56,554 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-19 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,555 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,19] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,556 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19
2017-07-11 16:34:56,560 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-28 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,561 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,28] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,562 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28
2017-07-11 16:34:56,565 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-38 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,566 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,38] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,567 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38
2017-07-11 16:34:56,570 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-35 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,571 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,35] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,572 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35
2017-07-11 16:34:56,575 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-44 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,576 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,44] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,577 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44
2017-07-11 16:34:56,580 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-6 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,581 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,6] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,581 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6
2017-07-11 16:34:56,585 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-25 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,586 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,25] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,586 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25
2017-07-11 16:34:56,589 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-16 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,590 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,16] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,593 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16
2017-07-11 16:34:56,596 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-22 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,597 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,22] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,598 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22
2017-07-11 16:34:56,604 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-41 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,605 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,41] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,605 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41
2017-07-11 16:34:56,608 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-32 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,609 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,32] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,609 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32
2017-07-11 16:34:56,612 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-3 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,613 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,3] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,613 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3
2017-07-11 16:34:56,616 INFO  [kafka-request-handler-0] log.Log (Logging.scala:info(70)) - Completed load of log __consumer_offsets-13 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,617 INFO  [kafka-request-handler-0] log.LogManager (Logging.scala:info(70)) - Created log for partition [__consumer_offsets,13] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,617 INFO  [kafka-request-handler-0] cluster.Partition (Logging.scala:info(70)) - Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13
2017-07-11 16:34:56,618 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-22
2017-07-11 16:34:56,619 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-22 in 1 milliseconds.
2017-07-11 16:34:56,620 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-25
2017-07-11 16:34:56,621 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-25 in 1 milliseconds.
2017-07-11 16:34:56,621 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-28
2017-07-11 16:34:56,622 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-28 in 1 milliseconds.
2017-07-11 16:34:56,622 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-31
2017-07-11 16:34:56,623 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-31 in 1 milliseconds.
2017-07-11 16:34:56,623 INFO  [kafka-request-handler-3] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions samoa_test-oos-0,test-s-0
2017-07-11 16:34:56,623 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-34
2017-07-11 16:34:56,625 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-34 in 1 milliseconds.
2017-07-11 16:34:56,625 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-37
2017-07-11 16:34:56,626 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-37 in 1 milliseconds.
2017-07-11 16:34:56,626 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-40
2017-07-11 16:34:56,627 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log samoa_test-oos-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,627 WARN  [Thread-433] clients.NetworkClient (NetworkClient.java:handleCompletedMetadataResponse(707)) - Error while fetching metadata with correlation id 9277 : {samoa_test-oos=LEADER_NOT_AVAILABLE}
2017-07-11 16:34:56,627 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-40 in 1 milliseconds.
2017-07-11 16:34:56,627 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-43
2017-07-11 16:34:56,628 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:56,628 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [samoa_test-oos,0] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,629 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-43 in 2 milliseconds.
2017-07-11 16:34:56,629 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-46
2017-07-11 16:34:56,629 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [samoa_test-oos,0] on broker 0: No checkpointed highwatermark is found for partition samoa_test-oos-0
2017-07-11 16:34:56,630 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-46 in 1 milliseconds.
2017-07-11 16:34:56,630 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-49
2017-07-11 16:34:56,631 WARN  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:maybeAutoCommitOffsetsSync(648)) - Auto-commit of offsets {samoa_test-oos-0=OffsetAndMetadata{offset=11111, metadata=''}} failed for group test: Partition samoa_test-oos-0 may not exist or user may not have Describe access to topic
2017-07-11 16:34:56,631 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [samoa_test-oos-0] for group test
2017-07-11 16:34:56,631 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:56,632 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-49 in 2 milliseconds.
2017-07-11 16:34:56,632 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:coordinatorDead(618)) - Marking the coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) dead for group test
2017-07-11 16:34:56,632 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-41
2017-07-11 16:34:56,633 INFO  [kafka-request-handler-3] log.Log (Logging.scala:info(70)) - Completed load of log test-s-0 with 1 log segments and log end offset 0 in 0 ms
2017-07-11 16:34:56,633 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-41 in 1 milliseconds.
2017-07-11 16:34:56,634 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-44
2017-07-11 16:34:56,634 INFO  [kafka-request-handler-3] log.LogManager (Logging.scala:info(70)) - Created log for partition [test-s,0] in /tmp/kafka-8736217898375340280 with properties {compression.type -> producer, message.format.version -> 0.10.2-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> true, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2017-07-11 16:34:56,635 INFO  [kafka-request-handler-3] cluster.Partition (Logging.scala:info(70)) - Partition [test-s,0] on broker 0: No checkpointed highwatermark is found for partition test-s-0
2017-07-11 16:34:56,635 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-44 in 1 milliseconds.
2017-07-11 16:34:56,635 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-47
2017-07-11 16:34:56,637 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-47 in 1 milliseconds.
2017-07-11 16:34:56,637 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-1
2017-07-11 16:34:56,638 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-1 in 1 milliseconds.
2017-07-11 16:34:56,638 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-4
2017-07-11 16:34:56,639 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-4 in 1 milliseconds.
2017-07-11 16:34:56,639 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-7
2017-07-11 16:34:56,641 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-7 in 1 milliseconds.
2017-07-11 16:34:56,641 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-10
2017-07-11 16:34:56,642 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-10 in 1 milliseconds.
2017-07-11 16:34:56,642 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-13
2017-07-11 16:34:56,643 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-13 in 1 milliseconds.
2017-07-11 16:34:56,643 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-16
2017-07-11 16:34:56,645 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-16 in 1 milliseconds.
2017-07-11 16:34:56,645 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-19
2017-07-11 16:34:56,646 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-19 in 1 milliseconds.
2017-07-11 16:34:56,646 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-2
2017-07-11 16:34:56,648 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-2 in 2 milliseconds.
2017-07-11 16:34:56,648 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-5
2017-07-11 16:34:56,649 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-5 in 1 milliseconds.
2017-07-11 16:34:56,649 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-8
2017-07-11 16:34:56,650 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-8 in 1 milliseconds.
2017-07-11 16:34:56,650 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-11
2017-07-11 16:34:56,651 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-11 in 1 milliseconds.
2017-07-11 16:34:56,651 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-14
2017-07-11 16:34:56,652 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-14 in 1 milliseconds.
2017-07-11 16:34:56,653 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-17
2017-07-11 16:34:56,654 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-17 in 1 milliseconds.
2017-07-11 16:34:56,654 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-20
2017-07-11 16:34:56,655 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-20 in 1 milliseconds.
2017-07-11 16:34:56,656 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-23
2017-07-11 16:34:56,657 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-23 in 1 milliseconds.
2017-07-11 16:34:56,657 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-26
2017-07-11 16:34:56,658 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-26 in 1 milliseconds.
2017-07-11 16:34:56,658 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-29
2017-07-11 16:34:56,659 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-29 in 1 milliseconds.
2017-07-11 16:34:56,659 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-32
2017-07-11 16:34:56,660 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-32 in 0 milliseconds.
2017-07-11 16:34:56,661 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-35
2017-07-11 16:34:56,662 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-35 in 1 milliseconds.
2017-07-11 16:34:56,662 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-38
2017-07-11 16:34:56,663 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-38 in 1 milliseconds.
2017-07-11 16:34:56,663 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-0
2017-07-11 16:34:56,664 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-0 in 1 milliseconds.
2017-07-11 16:34:56,664 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-3
2017-07-11 16:34:56,666 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-3 in 1 milliseconds.
2017-07-11 16:34:56,666 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-6
2017-07-11 16:34:56,667 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-6 in 1 milliseconds.
2017-07-11 16:34:56,667 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-9
2017-07-11 16:34:56,668 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-9 in 1 milliseconds.
2017-07-11 16:34:56,669 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-12
2017-07-11 16:34:56,670 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-12 in 1 milliseconds.
2017-07-11 16:34:56,670 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-15
2017-07-11 16:34:56,671 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-15 in 1 milliseconds.
2017-07-11 16:34:56,671 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-18
2017-07-11 16:34:56,672 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-18 in 1 milliseconds.
2017-07-11 16:34:56,672 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-21
2017-07-11 16:34:56,673 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-21 in 0 milliseconds.
2017-07-11 16:34:56,674 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-24
2017-07-11 16:34:56,675 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-24 in 1 milliseconds.
2017-07-11 16:34:56,675 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-27
2017-07-11 16:34:56,676 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-27 in 1 milliseconds.
2017-07-11 16:34:56,676 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-30
2017-07-11 16:34:56,677 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-30 in 1 milliseconds.
2017-07-11 16:34:56,678 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-33
2017-07-11 16:34:56,679 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-33 in 1 milliseconds.
2017-07-11 16:34:56,679 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-36
2017-07-11 16:34:56,680 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-36 in 1 milliseconds.
2017-07-11 16:34:56,680 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-39
2017-07-11 16:34:56,681 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-39 in 1 milliseconds.
2017-07-11 16:34:56,682 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-42
2017-07-11 16:34:56,683 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-42 in 1 milliseconds.
2017-07-11 16:34:56,683 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-45
2017-07-11 16:34:56,684 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-45 in 1 milliseconds.
2017-07-11 16:34:56,684 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Loading offsets and group metadata from __consumer_offsets-48
2017-07-11 16:34:56,685 INFO  [group-metadata-manager-0] coordinator.GroupMetadataManager (Logging.scala:info(70)) - [Group Metadata Manager on Broker 0]: Finished loading offsets from __consumer_offsets-48 in 1 milliseconds.
2017-07-11 16:34:56,700 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:56,701 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [] for group test
2017-07-11 16:34:56,706 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:56,708 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 0
2017-07-11 16:34:56,708 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 1
2017-07-11 16:34:56,710 INFO  [kafka-request-handler-3] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 1
2017-07-11 16:34:56,711 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 1
2017-07-11 16:34:56,711 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-11 16:34:56,734 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(573)) - Discovered coordinator 127.0.0.1:9092 (id: 2147483647 rack: null) for group test.
2017-07-11 16:34:56,735 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:56,735 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:56,736 INFO  [kafka-request-handler-0] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 1
2017-07-11 16:34:59,771 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [test-kdp-0] for group test
2017-07-11 16:34:59,772 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:34:59,774 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 2
2017-07-11 16:34:59,779 INFO  [kafka-request-handler-6] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 2
2017-07-11 16:34:59,780 INFO  [Thread-433] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 2
2017-07-11 16:34:59,780 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 2
2017-07-11 16:34:59,780 INFO  [Thread-433] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [samoa_test-oos-0] for group test
2017-07-11 16:34:59,780 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-11 16:34:59,786 INFO  [kafka-request-handler-5] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 2
2017-07-11 16:35:02,805 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinPrepare(393)) - Revoking previously assigned partitions [test-kdp-0] for group test
2017-07-11 16:35:02,805 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:sendJoinGroupRequest(407)) - (Re-)joining group test
2017-07-11 16:35:02,810 INFO  [kafka-request-handler-1] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Stabilized group test generation 3
2017-07-11 16:35:02,812 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Assignment received from leader for group test for generation 3
2017-07-11 16:35:02,814 INFO  [Thread-439] internals.AbstractCoordinator (AbstractCoordinator.java:onSuccess(375)) - Successfully joined group test with generation 3
2017-07-11 16:35:02,814 INFO  [Thread-439] internals.ConsumerCoordinator (ConsumerCoordinator.java:onJoinComplete(252)) - Setting newly assigned partitions [test-kdp-0] for group test
2017-07-11 16:35:03,764 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Preparing to restabilize group test with old generation 3
2017-07-11 16:35:03,764 INFO  [kafka-request-handler-2] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Group test with generation 4 is now empty
2017-07-11 16:35:05,761 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shutting down
2017-07-11 16:35:05,762 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Starting controlled shutdown
2017-07-11 16:35:05,770 INFO  [kafka-request-handler-1] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Shutting down broker 0
2017-07-11 16:35:05,771 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], Controlled shutdown succeeded
2017-07-11 16:35:05,772 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutting down
2017-07-11 16:35:05,775 INFO  [main] network.SocketServer (Logging.scala:info(70)) - [Socket Server on Broker 0], Shutdown completed
2017-07-11 16:35:05,775 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shutting down
2017-07-11 16:35:05,775 INFO  [main] server.KafkaRequestHandlerPool (Logging.scala:info(70)) - [Kafka Request Handler on Broker 0], shut down completely
2017-07-11 16:35:05,777 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutting down
2017-07-11 16:35:06,395 INFO  [ThrottledRequestReaper-Fetch] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Stopped 
2017-07-11 16:35:06,396 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Fetch], Shutdown completed
2017-07-11 16:35:06,396 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutting down
2017-07-11 16:35:07,396 INFO  [ThrottledRequestReaper-Produce] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Stopped 
2017-07-11 16:35:07,396 INFO  [main] server.ClientQuotaManager$ThrottledRequestReaper (Logging.scala:info(70)) - [ThrottledRequestReaper-Produce], Shutdown completed
2017-07-11 16:35:07,396 INFO  [main] server.KafkaApis (Logging.scala:info(70)) - [KafkaApi-0] Shutdown complete.
2017-07-11 16:35:07,397 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shutting down
2017-07-11 16:35:07,397 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutting down
2017-07-11 16:35:07,397 INFO  [main] server.ReplicaFetcherManager (Logging.scala:info(70)) - [ReplicaFetcherManager on broker 0] shutdown completed
2017-07-11 16:35:07,397 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:35:07,463 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:35:07,463 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:35:07,464 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:35:07,606 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:35:07,606 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:35:07,636 INFO  [main] server.ReplicaManager (Logging.scala:info(70)) - [Replica Manager on Broker 0]: Shut down completely
2017-07-11 16:35:07,636 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:35:07,806 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:35:07,806 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:35:07,807 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutting down.
2017-07-11 16:35:07,808 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:35:08,003 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:35:08,004 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:35:08,004 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutting down
2017-07-11 16:35:08,006 INFO  [ExpirationReaper-0] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Stopped 
2017-07-11 16:35:08,006 INFO  [main] server.DelayedOperationPurgatory$ExpiredOperationReaper (Logging.scala:info(70)) - [ExpirationReaper-0], Shutdown completed
2017-07-11 16:35:08,006 INFO  [main] coordinator.GroupCoordinator (Logging.scala:info(70)) - [GroupCoordinator 0]: Shutdown complete.
2017-07-11 16:35:08,007 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutting down.
2017-07-11 16:35:08,007 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - Shutting down the log cleaner.
2017-07-11 16:35:08,007 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutting down
2017-07-11 16:35:08,007 INFO  [kafka-log-cleaner-thread-0] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Stopped 
2017-07-11 16:35:08,007 INFO  [main] log.LogCleaner (Logging.scala:info(70)) - [kafka-log-cleaner-thread-0], Shutdown completed
2017-07-11 16:35:08,392 INFO  [main] log.LogManager (Logging.scala:info(70)) - Shutdown complete.
2017-07-11 16:35:08,393 INFO  [main] controller.PartitionStateMachine (Logging.scala:info(70)) - [Partition state machine on Controller 0]: Stopped partition state machine
2017-07-11 16:35:08,393 INFO  [main] controller.ReplicaStateMachine (Logging.scala:info(70)) - [Replica state machine on controller 0]: Stopped replica state machine
2017-07-11 16:35:08,395 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutting down
2017-07-11 16:35:08,395 INFO  [main] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Shutdown completed
2017-07-11 16:35:08,395 INFO  [Controller-0-to-broker-0-send-thread] controller.RequestSendThread (Logging.scala:info(70)) - [Controller-0-to-broker-0-send-thread], Stopped 
2017-07-11 16:35:08,395 INFO  [main] controller.KafkaController (Logging.scala:info(70)) - [Controller 0]: Broker 0 resigned as the controller
2017-07-11 16:35:08,396 INFO  [ZkClient-EventThread-596-127.0.0.1:37801] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-11 16:35:08,396 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d32130afa0001
2017-07-11 16:35:08,400 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d32130afa0001 closed
2017-07-11 16:35:08,400 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-11 16:35:08,401 INFO  [main] server.KafkaServer (Logging.scala:info(70)) - [Kafka Server 0], shut down completed
2017-07-11 16:35:08,401 INFO  [ZkClient-EventThread-593-127.0.0.1:37801] zkclient.ZkEventThread (ZkEventThread.java:run(83)) - Terminate ZkClient event thread.
2017-07-11 16:35:08,401 WARN  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:doIO(357)) - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x15d32130afa0001, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
	at java.lang.Thread.run(Thread.java:745)
2017-07-11 16:35:08,406 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:43631 which had sessionid 0x15d32130afa0001
2017-07-11 16:35:08,406 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x15d32130afa0000
2017-07-11 16:35:08,410 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x15d32130afa0000 closed
2017-07-11 16:35:08,410 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:43630 which had sessionid 0x15d32130afa0000
2017-07-11 16:35:08,410 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-11 16:35:08,410 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-11 16:35:08,410 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-11 16:35:08,410 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2017-07-11 16:35:08,410 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-11 16:35:08,411 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:run(143)) - PrepRequestProcessor exited loop!
2017-07-11 16:35:08,411 INFO  [SyncThread:0] server.SyncRequestProcessor (SyncRequestProcessor.java:run(187)) - SyncRequestProcessor exited!
2017-07-11 16:35:08,411 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
2017-07-11 16:35:08,412 INFO  [NIOServerCxn.Factory:/127.0.0.1:0] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(224)) - NIOServerCnxn factory exited run method
2017-07-11 16:35:08,413 INFO  [main] server.ZooKeeperServer (ZooKeeperServer.java:shutdown(441)) - shutting down
2017-07-11 16:35:08,413 INFO  [main] server.SessionTrackerImpl (SessionTrackerImpl.java:shutdown(225)) - Shutting down
2017-07-11 16:35:08,413 INFO  [main] server.PrepRequestProcessor (PrepRequestProcessor.java:shutdown(761)) - Shutting down
2017-07-11 16:35:08,413 INFO  [main] server.SyncRequestProcessor (SyncRequestProcessor.java:shutdown(209)) - Shutting down
2017-07-11 16:35:08,413 INFO  [main] server.FinalRequestProcessor (FinalRequestProcessor.java:shutdown(415)) - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.319 sec - in org.apache.samoa.streams.kafka.KafkaDestinationProcessorTest
Running org.apache.samoa.core.DoubleVectorTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec - in org.apache.samoa.core.DoubleVectorTest
2017-07-11 16:35:08,501 INFO  [SessionTracker] server.SessionTrackerImpl (SessionTrackerImpl.java:run(162)) - SessionTrackerImpl exited loop!

Results :

Tests run: 22, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-test 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-test ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-test ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/252350217/samoa-test/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/252350217/samoa-test/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ samoa-test ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/252350217/samoa-test/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building samoa-local 0.5.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- license-maven-plugin:1.8:update-file-header (first) @ samoa-local ---
[WARNING] No file to scan.
[INFO] 
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ samoa-local ---
[INFO] 
[INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:compile (default-compile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ samoa-local ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/workspace/apache/incubator-samoa/252350217/samoa-local/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.5:testCompile (default-testCompile) @ samoa-local ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.18:test (default-test) @ samoa-local ---
[INFO] Surefire report directory: /root/workspace/apache/incubator-samoa/252350217/samoa-local/target/surefire-reports
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.pom (3 KB at 86.9 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-providers/2.18/surefire-providers-2.18.pom (3 KB at 64.5 KB/sec)
[INFO] Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar
[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit4/2.18/surefire-junit4-2.18.jar (67 KB at 1950.7 KB/sec)

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.samoa.topology.impl.SimpleEngineTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.123 sec - in org.apache.samoa.topology.impl.SimpleEngineTest
Running org.apache.samoa.topology.impl.SimpleStreamTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.146 sec - in org.apache.samoa.topology.impl.SimpleStreamTest
Running org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.047 sec - in org.apache.samoa.topology.impl.SimpleComponentFactoryTest
Running org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.369 sec <<< FAILURE! - in org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest
testStartSendingEvents(org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest)  Time elapsed: 0.32 sec  <<< ERROR!
java.lang.IllegalStateException: Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest$4.<init>(SimpleEntranceProcessingItemTest.java:159)
	at org.apache.samoa.topology.impl.SimpleEntranceProcessingItemTest.testStartSendingEvents(SimpleEntranceProcessingItemTest.java:155)

Running org.apache.samoa.topology.impl.SimpleTopologyTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.065 sec - in org.apache.samoa.topology.impl.SimpleTopologyTest
Running org.apache.samoa.topology.impl.SimpleProcessingItemTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.126 sec - in org.apache.samoa.topology.impl.SimpleProcessingItemTest
Running org.apache.samoa.AlgosTest
2017-07-11 16:35:14,149 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test6106126118840719400test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test6106126118840719400test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-11 16:35:14,217 [pool-1-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-11 16:35:15,571 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-11 16:35:15,581 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 65.645
Kappa Statistic (percent) = 27.592
Kappa Temporal Statistic (percent) = 30.173
2017-07-11 16:35:16,176 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:16,176 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 70.69
Kappa Statistic (percent) = 39.422
Kappa Temporal Statistic (percent) = 40.272
2017-07-11 16:35:16,575 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:16,575 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 73.663
Kappa Statistic (percent) = 45.786
Kappa Temporal Statistic (percent) = 46.436
2017-07-11 16:35:16,855 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:16,856 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 75.545
Kappa Statistic (percent) = 49.77
Kappa Temporal Statistic (percent) = 50.387
2017-07-11 16:35:17,305 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:17,307 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 77.083
Kappa Statistic (percent) = 53.033
Kappa Temporal Statistic (percent) = 53.669
2017-07-11 16:35:17,646 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:17,647 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 78.397
Kappa Statistic (percent) = 55.776
Kappa Temporal Statistic (percent) = 56.372
2017-07-11 16:35:17,910 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:17,911 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 79.531
Kappa Statistic (percent) = 58.117
Kappa Temporal Statistic (percent) = 58.646
2017-07-11 16:35:18,197 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:18,198 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 80.527
Kappa Statistic (percent) = 60.173
Kappa Temporal Statistic (percent) = 60.566
2017-07-11 16:35:18,549 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:18,549 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 81.387
Kappa Statistic (percent) = 61.939
Kappa Temporal Statistic (percent) = 62.332
2017-07-11 16:35:18,819 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:18,820 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 82.144
Kappa Statistic (percent) = 63.505
Kappa Temporal Statistic (percent) = 63.842
2017-07-11 16:35:18,820 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-11 16:35:18,820 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-11 16:35:18,821 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,65.645,27.592487187843105,30.172764227642272
40000.0,40000.0,70.69,39.42219140754782,40.27204646186763
60000.0,60000.0,73.66333333333334,45.78556998992169,46.43571404359175
80000.0,80000.0,75.545,49.770097068022935,50.386731925037395
100000.0,100000.0,77.083,53.033355520313066,53.66933527413876
120000.0,120000.0,78.3975,55.77588004396512,56.37180652327577
140000.0,140000.0,79.53071428571428,58.11662806596387,58.64611743654127
160000.0,160000.0,80.526875,60.17259494934151,60.56625026894989
180000.0,180000.0,81.38666666666666,61.93941888230634,62.3317780650964
200000.0,200000.0,82.1435,63.50464751569946,63.842259795484466

2017-07-11 16:35:18,821 [pool-1-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 4 seconds for 200000 instances
2017-07-11 16:35:24,185 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test6106126118840719400test
2017-07-11 16:35:24,229 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test3379110851459630337test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=60.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test3379110851459630337test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.ensemble.Bagging) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 0 -u 10)
2017-07-11 16:35:24,247 [pool-4-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-11 16:35:25,832 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-11 16:35:25,832 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 85.385
Kappa Statistic (percent) = 70.033
Kappa Temporal Statistic (percent) = 69.9
2017-07-11 16:35:26,747 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:26,748 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 88.873
Kappa Statistic (percent) = 77.202
Kappa Temporal Statistic (percent) = 77.151
2017-07-11 16:35:27,750 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-11 16:35:27,751 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 90.738
Kappa Statistic (percent) = 81.086
Kappa Temporal Statistic (percent) = 81.064
2017-07-11 16:35:28,719 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:28,720 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 91.95
Kappa Statistic (percent) = 83.573
Kappa Temporal Statistic (percent) = 83.652
2017-07-11 16:35:29,504 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:29,504 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 92.84
Kappa Statistic (percent) = 85.393
Kappa Temporal Statistic (percent) = 85.431
2017-07-11 16:35:30,206 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:30,207 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 93.499
Kappa Statistic (percent) = 86.73
Kappa Temporal Statistic (percent) = 86.753
2017-07-11 16:35:31,000 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:31,001 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 94.027
Kappa Statistic (percent) = 87.81
Kappa Temporal Statistic (percent) = 87.821
2017-07-11 16:35:31,838 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:31,839 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 94.451
Kappa Statistic (percent) = 88.673
Kappa Temporal Statistic (percent) = 88.664
2017-07-11 16:35:32,900 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 1 seconds for 20000 instances
2017-07-11 16:35:32,901 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 94.775
Kappa Statistic (percent) = 89.334
Kappa Temporal Statistic (percent) = 89.324
2017-07-11 16:35:33,559 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:33,560 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 95.037
Kappa Statistic (percent) = 89.871
Kappa Temporal Statistic (percent) = 89.858
2017-07-11 16:35:33,560 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-11 16:35:33,560 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-11 16:35:33,561 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,85.385,70.03298033011168,69.90011327360726
40000.0,40000.0,88.8725,77.2016001393207,77.15092402464066
60000.0,60000.0,90.73833333333333,81.08637660862715,81.06385878825051
80000.0,80000.0,91.95,83.57287390708915,83.65233284256486
100000.0,100000.0,92.84,85.39349911745563,85.43086784006512
120000.0,120000.0,93.49916666666667,86.73019934004779,86.75281891047412
140000.0,140000.0,94.02714285714285,87.81036697821708,87.8209703025095
160000.0,160000.0,94.450625,88.67304442791819,88.66390041493776
180000.0,180000.0,94.77499999999999,89.33411336345391,89.32354042978284
200000.0,200000.0,95.037,89.87078671832509,89.85849297573436

2017-07-11 16:35:33,562 [pool-4-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 9 seconds for 200000 instances
2017-07-11 16:35:34,236 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test3379110851459630337test
2017-07-11 16:35:34,244 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test6479573464246564734test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=65.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialEvaluation -d %s -i %d -f %d -w %d -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialEvaluation -d /tmp/test6479573464246564734test -i 200000 -f 20000 -w 0 -l (classifiers.SingleClassifier -l org.apache.samoa.learners.classifiers.NaiveBayes) -s (org.apache.samoa.streams.generators.HyperplaneGenerator -c 2)
2017-07-11 16:35:34,265 [pool-16-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialEvaluation
2017-07-11 16:35:34,635 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:34,636 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 20,000
classified instances = 20,000
classifications correct (percent) = 84.275
Kappa Statistic (percent) = 68.502
Kappa Temporal Statistic (percent) = 68.778
2017-07-11 16:35:34,934 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:34,935 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 40,000
classified instances = 40,000
classifications correct (percent) = 83.803
Kappa Statistic (percent) = 67.594
Kappa Temporal Statistic (percent) = 67.819
2017-07-11 16:35:35,251 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:35,252 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 60,000
classified instances = 60,000
classifications correct (percent) = 83.59
Kappa Statistic (percent) = 67.214
Kappa Temporal Statistic (percent) = 67.392
2017-07-11 16:35:35,496 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:35,497 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 80,000
classified instances = 80,000
classifications correct (percent) = 83.546
Kappa Statistic (percent) = 67.127
Kappa Temporal Statistic (percent) = 67.269
2017-07-11 16:35:35,675 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:35,676 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 100,000
classified instances = 100,000
classifications correct (percent) = 83.559
Kappa Statistic (percent) = 67.135
Kappa Temporal Statistic (percent) = 67.209
2017-07-11 16:35:35,848 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:35,849 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 120,000
classified instances = 120,000
classifications correct (percent) = 83.531
Kappa Statistic (percent) = 67.072
Kappa Temporal Statistic (percent) = 67.151
2017-07-11 16:35:36,035 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:36,036 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 140,000
classified instances = 140,000
classifications correct (percent) = 83.504
Kappa Statistic (percent) = 67.03
Kappa Temporal Statistic (percent) = 67.054
2017-07-11 16:35:36,241 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:36,242 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 160,000
classified instances = 160,000
classifications correct (percent) = 83.518
Kappa Statistic (percent) = 67.05
Kappa Temporal Statistic (percent) = 67.065
2017-07-11 16:35:36,416 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:36,416 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 180,000
classified instances = 180,000
classifications correct (percent) = 83.54
Kappa Statistic (percent) = 67.089
Kappa Temporal Statistic (percent) = 67.126
2017-07-11 16:35:36,590 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:83) - 0 seconds for 20000 instances
2017-07-11 16:35:36,590 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:169) - evaluation instances = 200,000
classified instances = 200,000
classifications correct (percent) = 83.523
Kappa Statistic (percent) = 67.057
Kappa Temporal Statistic (percent) = 67.143
2017-07-11 16:35:36,591 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:183) - last event is received!
2017-07-11 16:35:36,591 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:184) - total count: 200000
2017-07-11 16:35:36,592 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:187) - org.apache.samoa.evaluation.EvaluatorProcessorid = 0
evaluation instances,classified instances,classifications correct (percent),Kappa Statistic (percent),Kappa Temporal Statistic (percent)
20000.0,20000.0,84.275,68.50233079747552,68.77792117541944
40000.0,40000.0,83.80250000000001,67.59439866955276,67.81900362588786
60000.0,60000.0,83.59,67.21382484118784,67.39195230998509
80000.0,80000.0,83.54625,67.12663795918682,67.26924607121543
100000.0,100000.0,83.55900000000001,67.13483590334214,67.20915853926087
120000.0,120000.0,83.53083333333333,67.07192304919516,67.15145269596435
140000.0,140000.0,83.50357142857143,67.02990305706534,67.05373828442632
160000.0,160000.0,83.518125,67.04964827201962,67.06465673356729
180000.0,180000.0,83.54,67.08882037897833,67.1260235670062
200000.0,200000.0,83.5235,67.05662351796806,67.142614990378

2017-07-11 16:35:36,592 [pool-16-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorProcessor (EvaluatorProcessor.java:191) - total evaluation time: 2 seconds for 200000 instances
2017-07-11 16:35:44,247 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test6479573464246564734test
2017-07-11 16:35:44,250 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:53) - Starting test, output file is /tmp/test7274683705217656927test, test config is 
TestParams{
inputInstances=200000
samplingSize=20000
evaluationInstances=200000
classifiedInstances=200000
classificationsCorrect=75.0
kappaStat=0.0
kappaTempStat=0.0
cliStringTemplate='PrequentialCVEvaluation -d %s -i %d -f %d -w %d -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)'
pollTimeoutSeconds=10
prePollWait=10
taskClassName='org.apache.samoa.LocalDoTask'
inputDelayMicroSec=0
}
Command line string =  PrequentialCVEvaluation -d /tmp/test7274683705217656927test -i 200000 -f 20000 -w 0 -l (org.apache.samoa.learners.classifiers.trees.VerticalHoeffdingTree -p 4) -s (org.apache.samoa.streams.generators.RandomTreeGenerator -c 2 -o 10 -u 10)
2017-07-11 16:35:44,259 [pool-18-thread-1] INFO  org.apache.samoa.LocalDoTask (LocalDoTask.java:80) - Successfully instantiating org.apache.samoa.tasks.PrequentialCVEvaluation
2017-07-11 16:35:46,608 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-11 16:35:46,609 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 20,000
[avg] classified instances = 20,000
[err] classified instances = 0
[avg] classifications correct (percent) = 64.627
[err] classifications correct (percent) = 0.189
[avg] Kappa Statistic (percent) = 25.294
[err] Kappa Statistic (percent) = 0.352
[avg] Kappa Temporal Statistic (percent) = 28.104
[err] Kappa Temporal Statistic (percent) = 0.384
2017-07-11 16:35:49,346 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-11 16:35:49,348 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 40,000
[avg] classified instances = 40,000
[err] classified instances = 0
[avg] classifications correct (percent) = 70.032
[err] classifications correct (percent) = 0.203
[avg] Kappa Statistic (percent) = 38.074
[err] Kappa Statistic (percent) = 0.445
[avg] Kappa Temporal Statistic (percent) = 38.931
[err] Kappa Temporal Statistic (percent) = 0.414
2017-07-11 16:35:52,206 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-11 16:35:52,207 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 60,000
[avg] classified instances = 60,000
[err] classified instances = 0
[avg] classifications correct (percent) = 73.498
[err] classifications correct (percent) = 0.308
[avg] Kappa Statistic (percent) = 45.523
[err] Kappa Statistic (percent) = 0.671
[avg] Kappa Temporal Statistic (percent) = 46.1
[err] Kappa Temporal Statistic (percent) = 0.627
2017-07-11 16:35:54,923 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-11 16:35:54,924 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 80,000
[avg] classified instances = 80,000
[err] classified instances = 0
[avg] classifications correct (percent) = 75.64
[err] classifications correct (percent) = 0.397
[avg] Kappa Statistic (percent) = 50.074
[err] Kappa Statistic (percent) = 0.86
[avg] Kappa Temporal Statistic (percent) = 50.579
[err] Kappa Temporal Statistic (percent) = 0.806
2017-07-11 16:35:58,000 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-11 16:35:58,001 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 100,000
[avg] classified instances = 100,000
[err] classified instances = 0
[avg] classifications correct (percent) = 77.273
[err] classifications correct (percent) = 0.442
[avg] Kappa Statistic (percent) = 53.513
[err] Kappa Statistic (percent) = 0.947
[avg] Kappa Temporal Statistic (percent) = 54.053
[err] Kappa Temporal Statistic (percent) = 0.894
2017-07-11 16:36:01,603 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-11 16:36:01,604 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 120,000
[avg] classified instances = 120,000
[err] classified instances = 0
[avg] classifications correct (percent) = 78.553
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 56.166
[err] Kappa Statistic (percent) = 0.983
[avg] Kappa Temporal Statistic (percent) = 56.687
[err] Kappa Temporal Statistic (percent) = 0.938
2017-07-11 16:36:04,890 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-11 16:36:04,891 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 140,000
[avg] classified instances = 140,000
[err] classified instances = 0
[avg] classifications correct (percent) = 79.767
[err] classifications correct (percent) = 0.471
[avg] Kappa Statistic (percent) = 58.667
[err] Kappa Statistic (percent) = 0.991
[avg] Kappa Temporal Statistic (percent) = 59.123
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-11 16:36:07,224 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-11 16:36:07,225 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 160,000
[avg] classified instances = 160,000
[err] classified instances = 0
[avg] classifications correct (percent) = 80.8
[err] classifications correct (percent) = 0.464
[avg] Kappa Statistic (percent) = 60.79
[err] Kappa Statistic (percent) = 0.973
[avg] Kappa Temporal Statistic (percent) = 61.12
[err] Kappa Temporal Statistic (percent) = 0.939
2017-07-11 16:36:10,098 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 2 seconds for 20000 instances
2017-07-11 16:36:10,099 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 180,000
[avg] classified instances = 180,000
[err] classified instances = 0
[avg] classifications correct (percent) = 81.614
[err] classifications correct (percent) = 0.462
[avg] Kappa Statistic (percent) = 62.451
[err] Kappa Statistic (percent) = 0.969
[avg] Kappa Temporal Statistic (percent) = 62.792
[err] Kappa Temporal Statistic (percent) = 0.936
2017-07-11 16:36:14,048 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:108) - 3 seconds for 20000 instances
2017-07-11 16:36:14,054 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:201) - evaluation instances = 200,000
[avg] classified instances = 200,000.9
[err] classified instances = 0.9
[avg] classifications correct (percent) = 82.333
[err] classifications correct (percent) = 0.47
[avg] Kappa Statistic (percent) = 63.93
[err] Kappa Statistic (percent) = 0.984
[avg] Kappa Temporal Statistic (percent) = 64.226
[err] Kappa Temporal Statistic (percent) = 0.952
2017-07-11 16:36:14,055 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:215) - last event is received!
2017-07-11 16:36:14,056 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:216) - total count: 200001
2017-07-11 16:36:14,056 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:219) - org.apache.samoa.evaluation.EvaluatorCVProcessorid = 0
evaluation instances,[avg] classified instances,[err] classified instances,[avg] classifications correct (percent),[err] classifications correct (percent),[avg] Kappa Statistic (percent),[err] Kappa Statistic (percent),[avg] Kappa Temporal Statistic (percent),[err] Kappa Temporal Statistic (percent)
20000.0,20000.0,0.0,64.62700000000001,0.18916071943661472,25.293690418855896,0.3519627618187893,28.103658536585368,0.38447300698498993
40000.0,40000.0,0.0,70.032,0.20324410610560478,38.07419026287571,0.44506467062749777,38.931173264048084,0.4141710858537992
60000.0,60000.0,0.0,73.49833333333333,0.3080789741624503,45.52336429068201,0.6710378342453998,46.10013219890851,0.6265800633791055
80000.0,80000.0,0.0,75.63975,0.3973700346584335,50.07382624902415,0.8597101949765279,50.57895671138387,0.8061674935377634
100000.0,100000.0,0.0,77.27289999999999,0.4422633566854323,53.51347314843381,0.9469128869060368,54.05325084910237,0.8941115896115002
120000.0,120000.0,0.0,78.5535,0.4643362247656641,56.1661763049406,0.9830956362259596,56.68686256689892,0.9377688069588297
140000.0,140000.0,0.0,79.76700000000001,0.47139196346560175,58.66745091659613,0.9910982373644812,59.12348297906114,0.9523482240960528
160000.0,160000.0,0.0,80.80025000000002,0.4637947455202826,60.789541927185965,0.9729687801934683,61.11984407234435,0.9392003554346215
180000.0,180000.0,0.0,81.61388888888888,0.46241870972111077,62.451145738939225,0.9693840059052787,62.791612794423514,0.9358071589161842
200000.0,200000.9,0.8999999999996271,82.33328358598864,0.4701473780403362,63.930181311539584,0.9842393397954078,64.2263845297155,0.9520407427484003

2017-07-11 16:36:14,057 [pool-18-thread-1] INFO  org.apache.samoa.evaluation.EvaluatorCVProcessor (EvaluatorCVProcessor.java:223) - total evaluation time: 29 seconds for 200001 instances
2017-07-11 16:36:14,256 [main] INFO  org.apache.samoa.TestUtils (TestUtils.java:96) - Checking results file /tmp/test7274683705217656927test
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 60.699 sec - in org.apache.samoa.AlgosTest

Results :


Tests in error: 
  Missing invocation to mocked type at this point; please make sure there is an associated mock field or mock parameter in scope


Tests run: 28, Failures: 0, Errors: 1, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache SAMOA ....................................... SUCCESS [  4.053 s]
[INFO] samoa-instances .................................... SUCCESS [  3.614 s]
[INFO] samoa-api .......................................... SUCCESS [ 56.640 s]
[INFO] samoa-test ......................................... SUCCESS [  1.941 s]
[INFO] samoa-local ........................................ FAILURE [01:03 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:10 min
[INFO] Finished at: 2017-07-11T16:36:14+02:00
[INFO] Final Memory: 41M/1099M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18:test (default-test) on project samoa-local: There are test failures.
[ERROR] 
[ERROR] Please refer to /root/workspace/apache/incubator-samoa/252350217/samoa-local/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :samoa-local
