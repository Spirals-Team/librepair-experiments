nopolinfo #0
location: FailureLocation{className='org.apache.storm.hdfs.spout.TestHdfsSpout', failingMethods=[org.apache.storm.hdfs.spout.TestHdfsSpout#testSimpleSequenceFile, org.apache.storm.hdfs.spout.TestHdfsSpout#testResumeAbandoned_Seq_NoAck], erroringMethods=[org.apache.storm.hdfs.spout.TestHdfsSpout#testReadFailures, org.apache.storm.hdfs.spout.TestHdfsSpout#testLocking, org.apache.storm.hdfs.spout.TestHdfsSpout#testResumeAbandoned_Text_NoAck, org.apache.storm.hdfs.spout.TestHdfsSpout#testMultipleFileConsumption_Ack, org.apache.storm.hdfs.spout.TestHdfsSpout#testSimpleText_noACK], failures=[FailureType{failureName='java.lang.AssertionError', failureDetail='expected:<10> but was:<0>', isError=false}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit3571870345478481437/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit5653002873441364721/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='java.lang.AssertionError', failureDetail='expected:<2> but was:<0>', isError=false}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit3774061222145494423/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit2168872029454153848/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit9179995449552715618/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}], nbFailures=2, nbErrors=5}
status: NOPATCH
dateEnd: Tue May 16 17:50:31 CEST 2017
allocatedtime: 120minutes 
passingTime: 0minutes 
nb patches: 0
nopol context: Config{synthesisDepth=3, collectStaticMethods=true, collectStaticFields=false, collectLiterals=false, onlyOneSynthesisResult=false, sortExpressions=true, maxLineInvocationPerTest=250, timeoutMethodInvocation=2000, dataCollectionTimeoutInSecondForSynthesis=900, addWeight=0.19478, subWeight=0.04554, mulWeight=0.0102, divWeight=0.00613, andWeight=0.10597, orWeight=0.05708, eqWeight=0.22798, nEqWeight=0.0, lessEqWeight=0.0255, lessWeight=0.0947, methodCallWeight=0.1, fieldAccessWeight=0.08099, constantWeight=0.14232, variableWeight=0.05195, mode=REPAIR, type=COND_THEN_PRE, synthesis=DYNAMOTH, oracle=ANGELIC, solver=Z3, solverPath='./z3_for_linux', projectSources=[/root/workspace/apache/storm/232796019/external/storm-hdfs/src/main/java], projectClasspath='[Ljava.net.URL;@6b474074', projectTests=[org.apache.storm.hdfs.spout.TestHdfsSpout], complianceLevel=8, outputFolder=null, json=false}
exception: null
nbStatements: 0
nbAngelicValues: 0
ignoreStatus: IGNORE_ERRORING
----------



nopolinfo #1
location: FailureLocation{className='org.apache.storm.hdfs.spout.TestHdfsSpout', failingMethods=[org.apache.storm.hdfs.spout.TestHdfsSpout#testSimpleSequenceFile, org.apache.storm.hdfs.spout.TestHdfsSpout#testResumeAbandoned_Seq_NoAck], erroringMethods=[org.apache.storm.hdfs.spout.TestHdfsSpout#testReadFailures, org.apache.storm.hdfs.spout.TestHdfsSpout#testLocking, org.apache.storm.hdfs.spout.TestHdfsSpout#testResumeAbandoned_Text_NoAck, org.apache.storm.hdfs.spout.TestHdfsSpout#testMultipleFileConsumption_Ack, org.apache.storm.hdfs.spout.TestHdfsSpout#testSimpleText_noACK], failures=[FailureType{failureName='java.lang.AssertionError', failureDetail='expected:<10> but was:<0>', isError=false}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit3571870345478481437/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit5653002873441364721/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='java.lang.AssertionError', failureDetail='expected:<2> but was:<0>', isError=false}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit3774061222145494423/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit2168872029454153848/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/junit9179995449552715618/hdfsspout/source/file1.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}], nbFailures=2, nbErrors=5}
status: NOPATCH
dateEnd: Tue May 16 17:50:35 CEST 2017
allocatedtime: 120minutes 
passingTime: 0minutes 
nb patches: 0
nopol context: Config{synthesisDepth=3, collectStaticMethods=true, collectStaticFields=false, collectLiterals=false, onlyOneSynthesisResult=false, sortExpressions=true, maxLineInvocationPerTest=250, timeoutMethodInvocation=2000, dataCollectionTimeoutInSecondForSynthesis=900, addWeight=0.19478, subWeight=0.04554, mulWeight=0.0102, divWeight=0.00613, andWeight=0.10597, orWeight=0.05708, eqWeight=0.22798, nEqWeight=0.0, lessEqWeight=0.0255, lessWeight=0.0947, methodCallWeight=0.1, fieldAccessWeight=0.08099, constantWeight=0.14232, variableWeight=0.05195, mode=REPAIR, type=COND_THEN_PRE, synthesis=DYNAMOTH, oracle=ANGELIC, solver=Z3, solverPath='./z3_for_linux', projectSources=[/root/workspace/apache/storm/232796019/external/storm-hdfs/src/main/java], projectClasspath='[Ljava.net.URL;@30ec7d21', projectTests=[org.apache.storm.hdfs.spout.TestHdfsSpout], complianceLevel=8, outputFolder=null, json=false}
exception: null
nbStatements: 0
nbAngelicValues: 0
ignoreStatus: IGNORE_FAILING
----------



nopolinfo #2
location: FailureLocation{className='org.apache.storm.hdfs.blobstore.BlobStoreTest', failingMethods=[], erroringMethods=[org.apache.storm.hdfs.blobstore.BlobStoreTest#testHdfsReplication, org.apache.storm.hdfs.blobstore.BlobStoreTest#testHdfsWithAuth, org.apache.storm.hdfs.blobstore.BlobStoreTest#testMultipleHdfs, org.apache.storm.hdfs.blobstore.BlobStoreTest#testBasicHdfs], failures=[FailureType{failureName='java.lang.NoClassDefFoundError', failureDetail='org/apache/storm/thrift/TBase', isError=true}, FailureType{failureName='java.lang.NoClassDefFoundError', failureDetail='org/apache/storm/thrift/TBase', isError=true}, FailureType{failureName='java.lang.NoClassDefFoundError', failureDetail='org/apache/storm/thrift/TBase', isError=true}, FailureType{failureName='java.lang.NoClassDefFoundError', failureDetail='org/apache/storm/thrift/TBase', isError=true}], nbFailures=0, nbErrors=4}
status: NOPATCH
dateEnd: Tue May 16 17:50:39 CEST 2017
allocatedtime: 120minutes 
passingTime: 0minutes 
nb patches: 0
nopol context: Config{synthesisDepth=3, collectStaticMethods=true, collectStaticFields=false, collectLiterals=false, onlyOneSynthesisResult=false, sortExpressions=true, maxLineInvocationPerTest=250, timeoutMethodInvocation=2000, dataCollectionTimeoutInSecondForSynthesis=900, addWeight=0.19478, subWeight=0.04554, mulWeight=0.0102, divWeight=0.00613, andWeight=0.10597, orWeight=0.05708, eqWeight=0.22798, nEqWeight=0.0, lessEqWeight=0.0255, lessWeight=0.0947, methodCallWeight=0.1, fieldAccessWeight=0.08099, constantWeight=0.14232, variableWeight=0.05195, mode=REPAIR, type=COND_THEN_PRE, synthesis=DYNAMOTH, oracle=ANGELIC, solver=Z3, solverPath='./z3_for_linux', projectSources=[/root/workspace/apache/storm/232796019/external/storm-hdfs/src/main/java], projectClasspath='[Ljava.net.URL;@153d6d74', projectTests=[org.apache.storm.hdfs.blobstore.BlobStoreTest], complianceLevel=8, outputFolder=null, json=false}
exception: null
nbStatements: 0
nbAngelicValues: 0
ignoreStatus: NOTHING_TO_IGNORE
----------



nopolinfo #3
location: FailureLocation{className='org.apache.storm.hdfs.spout.TestFileLock', failingMethods=[], erroringMethods=[org.apache.storm.hdfs.spout.TestFileLock#testStaleLockDetection_MultipleLocks, org.apache.storm.hdfs.spout.TestFileLock#testLockRecovery, org.apache.storm.hdfs.spout.TestFileLock#testHeartbeat], failures=[FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/locskdir/file3 could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/locskdir/file1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}, FailureType{failureName='org.apache.hadoop.ipc.RemoteException', failureDetail='File /tmp/locskdir/file1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1562)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3245)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:663)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)
', isError=true}], nbFailures=0, nbErrors=3}
status: NOPATCH
dateEnd: Tue May 16 17:50:43 CEST 2017
allocatedtime: 120minutes 
passingTime: 0minutes 
nb patches: 0
nopol context: Config{synthesisDepth=3, collectStaticMethods=true, collectStaticFields=false, collectLiterals=false, onlyOneSynthesisResult=false, sortExpressions=true, maxLineInvocationPerTest=250, timeoutMethodInvocation=2000, dataCollectionTimeoutInSecondForSynthesis=900, addWeight=0.19478, subWeight=0.04554, mulWeight=0.0102, divWeight=0.00613, andWeight=0.10597, orWeight=0.05708, eqWeight=0.22798, nEqWeight=0.0, lessEqWeight=0.0255, lessWeight=0.0947, methodCallWeight=0.1, fieldAccessWeight=0.08099, constantWeight=0.14232, variableWeight=0.05195, mode=REPAIR, type=COND_THEN_PRE, synthesis=DYNAMOTH, oracle=ANGELIC, solver=Z3, solverPath='./z3_for_linux', projectSources=[/root/workspace/apache/storm/232796019/external/storm-hdfs/src/main/java], projectClasspath='[Ljava.net.URL;@4a2e1e52', projectTests=[org.apache.storm.hdfs.spout.TestFileLock], complianceLevel=8, outputFolder=null, json=false}
exception: null
nbStatements: 0
nbAngelicValues: 0
ignoreStatus: NOTHING_TO_IGNORE
----------



